diff --git a/CHANGELOG.md b/CHANGELOG.md
index 2a9e45e91..e95e360ff 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,6 +1,98 @@
 # Changelog
 
-## v21.04: ZNS NVMe bdev, PMR, ADQ initiator, RPM
+## v21.07: (Upcoming Release)
+
+### accel_fw
+
+Added API `spdk_accel_submit_copy_crc32c` to perform a CRC32C while copying data.
+
+Added API `spdk_accel_batch_prep_copy_crc32c` to batch CRC32C + copy commands.
+
+### bdev
+
+Change `spdk_bdev_read_blocks_with_md` arg offset definiton from int64_t to uint64_t.
+
+Red-black tree has been used to organize the bdev names and aliases uniformly
+to provide faster lookup.
+
+### dpdk
+
+Updated DPDK submodule to DPDK 21.05.
+
+### idxd
+
+Remove the probe_cb parameter in spdk_idxd_probe function. And remove the definition
+of spdk_idxd_probe_cb function pointer. It should be implemented in idxd_user.c.
+
+Added API `spdk_idxd_submit_copy_crc32c` to perform a CRC32C while copying data.
+
+Added API `spdk_idxd_batch_prep_copy_crc32c` to prepare a batch operation to perform
+a CRC32C while copying data.
+
+### iscsi
+
+New parameters, `pdu_pool_size`, `immediate_data_pool_size`, and `data_out_pool_size`,
+were added to the RPC `iscsi_set_options` to run iSCSI target with varying amount of
+available memory.
+
+### util
+
+`spdk_crc32c_iov_update` function was added to support calculating the crc32c of the
+iovs.
+
+### nvmf
+
+Added `min_cntlid` and `max_cntlid` to `nvmf_create_subsystem` to limit the controller ID range.
+
+`spdk_nvmf_request_get_buffers_multi` API is removed.
+
+Added the `nvmf_set_crdt` RPC for setting command retry delay times.
+
+### nvme
+
+Added a new function `spdk_nvme_ns_cmd_copy` to submit a Simple Copy Command to a Namespace.
+
+Update the spdk_nvme_generic_command_status_code structure with new status code
+according to the definition in nvme 1.4 spec.
+
+spdk_nvme_ctrlr_get_default_ctrlr_opts now sets use_cmb_sqs to false. This means
+that if a controller has a CMB and supports SQs in the CMB, SPDK will not use
+the CMB for SQs by default - the user must set use_cmb_sqs to true in
+the spdk_nvme_ctrlr_opts structure prior to controller attach.
+
+### rpc
+
+New RPC `bdev_rbd_register_cluster` and `bdev_rbd_unregister_cluster` was added, it allows to create
+and delete the rados object cluster, then users can choose the cluster to create related rbd
+device.
+
+Revised `bdev_rbd_create` parameter, it allows to use an optional parameter --cluster-name
+to create a rbd bdev with  an already registered Rados Cluster Object.
+
+New RPC `bdev_rbd_get_clusters_info` was added, it allows to get the info of the registered
+Rados Cluster names.
+
+Revised a parameter `--stripe-size_kb` to `--stripe-size-kb` of `bdev_raid_create` method
+provided in `scripts/rpc.py` for consistency.
+
+### bdev
+
+Removed ZCOPY emulation: The bdev module can be checked to see if it supports ZCOPY
+and if not supported then use existing READ/WRITE commands.
+
+Added iov to spdk_bdev_zcopy_start
+
+### thread
+
+Red-black tree has been used for timed pollers to provide faster insertion and deletion
+and for io_devices to provide faster lookup.
+
+### util
+
+Red-black tree macros has been added by using the macros provided by the FreeBSD operating system
+under the same BSD license.
+
+## v21.04:
 
 ### accel
 
@@ -166,6 +258,11 @@ use `enable-zerocopy-send-server` or `enable-zerocopy-send-client` instead.
 Parameter `disable-zerocopy-send` of RPC `sock_impl_set_options` is deprecated and will be removed in SPDK 21.07,
 use `disable-zerocopy-send-server` or `disable-zerocopy-send-client` instead.
 
+Added cmd_parser.py used to parse the args from argparse. There are
+two benefit to use command parser:
+- Simplify the definition of rpc method. It will reduce the rpc method code.
+- Make the rpc call more versatile. User can add private args into rpc method.
+
 ### rpm
 
 Added support for new RPM spec, rpmbuild/spdk.spec, which can be used for packaging the
diff --git a/CONFIG b/CONFIG
index 8c49c7d2b..bfa27597c 100644
--- a/CONFIG
+++ b/CONFIG
@@ -76,6 +76,9 @@ CONFIG_UNIT_TESTS=y
 # Build examples
 CONFIG_EXAMPLES=y
 
+# Build apps
+CONFIG_APPS=y
+
 # Build with Control-flow Enforcement Technology (CET)
 CONFIG_CET=n
 
@@ -86,6 +89,10 @@ CONFIG_ENV=
 # This directory should contain 'include' and 'lib' directories for your DPDK
 # installation.
 CONFIG_DPDK_DIR=
+# Automatically set via pkg-config when bare --with-dpdk is set
+CONFIG_DPDK_LIB_DIR=
+CONFIG_DPDK_INC_DIR=
+CONFIG_DPDK_PKG_CONFIG=n
 
 # This directory should contain 'include' and 'lib' directories for WPDK.
 CONFIG_WPDK_DIR=
@@ -172,3 +179,6 @@ CONFIG_RAID5=n
 
 # Build with IDXD support
 CONFIG_IDXD=n
+
+# Build with USDT support
+CONFIG_USDT=n
diff --git a/Makefile b/Makefile
index 2bff27ae3..cab44a781 100644
--- a/Makefile
+++ b/Makefile
@@ -40,8 +40,9 @@ include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 DIRS-y += lib
 DIRS-y += module
 DIRS-$(CONFIG_SHARED) += shared_lib
-DIRS-y += app include
+DIRS-y += include
 DIRS-$(CONFIG_EXAMPLES) += examples
+DIRS-$(CONFIG_APPS) += app
 DIRS-y += test
 DIRS-$(CONFIG_IPSEC_MB) += ipsecbuild
 DIRS-$(CONFIG_ISAL) += isalbuild
@@ -57,11 +58,13 @@ export MAKE_PID := $(shell echo $$PPID)
 ifeq ($(SPDK_ROOT_DIR)/lib/env_dpdk,$(CONFIG_ENV))
 ifeq ($(CURDIR)/dpdk/build,$(CONFIG_DPDK_DIR))
 ifneq ($(SKIP_DPDK_BUILD),1)
+ifneq ($(CONFIG_DPDK_PKG_CONFIG),y)
 DPDKBUILD = dpdkbuild
 DIRS-y += dpdkbuild
 endif
 endif
 endif
+endif
 
 ifeq ($(OS),Windows)
 ifeq ($(CURDIR)/wpdk/build,$(CONFIG_WPDK_DIR))
diff --git a/app/iscsi_tgt/Makefile b/app/iscsi_tgt/Makefile
index 31a273610..a5b38e90f 100644
--- a/app/iscsi_tgt/Makefile
+++ b/app/iscsi_tgt/Makefile
@@ -43,7 +43,7 @@ CFLAGS += -I$(SPDK_ROOT_DIR)/lib
 
 C_SRCS := iscsi_tgt.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_iscsi event_net
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_iscsi event_net
 
 ifeq ($(SPDK_ROOT_DIR)/lib/env_dpdk,$(CONFIG_ENV))
 SPDK_LIB_LIST += env_dpdk_rpc
diff --git a/app/nvmf_tgt/Makefile b/app/nvmf_tgt/Makefile
index a27d57ac5..282f0fe7e 100644
--- a/app/nvmf_tgt/Makefile
+++ b/app/nvmf_tgt/Makefile
@@ -39,7 +39,7 @@ APP = nvmf_tgt
 
 C_SRCS := nvmf_main.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_nvmf
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_nvmf
 
 ifeq ($(SPDK_ROOT_DIR)/lib/env_dpdk,$(CONFIG_ENV))
 SPDK_LIB_LIST += env_dpdk_rpc
diff --git a/app/spdk_dd/Makefile b/app/spdk_dd/Makefile
index f2b614dd5..e302c12f5 100644
--- a/app/spdk_dd/Makefile
+++ b/app/spdk_dd/Makefile
@@ -39,6 +39,6 @@ APP = spdk_dd
 
 C_SRCS := spdk_dd.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/app/spdk_tgt/Makefile b/app/spdk_tgt/Makefile
index 91d590aa7..627cfe269 100644
--- a/app/spdk_tgt/Makefile
+++ b/app/spdk_tgt/Makefile
@@ -41,7 +41,7 @@ C_SRCS := spdk_tgt.c
 
 SPDK_LIB_LIST = $(ALL_MODULES_LIST)
 
-SPDK_LIB_LIST += event_iscsi event_nvmf
+SPDK_LIB_LIST += event event_iscsi event_nvmf
 
 ifeq ($(SPDK_ROOT_DIR)/lib/env_dpdk,$(CONFIG_ENV))
 SPDK_LIB_LIST += env_dpdk_rpc
diff --git a/app/spdk_top/spdk_top.c b/app/spdk_top/spdk_top.c
index 84a2e20a3..2f10f949e 100644
--- a/app/spdk_top/spdk_top.c
+++ b/app/spdk_top/spdk_top.c
@@ -137,12 +137,12 @@ struct core_info {
 	uint64_t last_idle;
 	uint64_t busy;
 	uint64_t last_busy;
+	uint32_t core_freq;
 };
 
 uint8_t g_sleep_time = 1;
 uint16_t g_selected_row;
 uint16_t g_max_selected_row;
-struct rpc_thread_info *g_thread_info[MAX_THREADS];
 const char *poller_type_str[SPDK_POLLER_TYPES_COUNT] = {"Active", "Timed", "Paused"};
 const char *g_tab_title[NUMBER_OF_TABS] = {"[1] THREADS", "[2] POLLERS", "[3] CORES"};
 struct spdk_jsonrpc_client *g_rpc_client;
@@ -156,6 +156,8 @@ uint16_t g_data_win_size, g_max_data_rows;
 uint32_t g_last_threads_count, g_last_pollers_count, g_last_cores_count;
 uint8_t g_current_sort_col[NUMBER_OF_TABS] = {0, 0, 0};
 bool g_interval_data = true;
+bool g_quit_app = false;
+pthread_mutex_t g_thread_lock;
 static struct col_desc g_col_desc[NUMBER_OF_TABS][TABS_COL_COUNT] = {
 	{	{.name = "Thread name", .max_data_string = MAX_THREAD_NAME_LEN},
 		{.name = "Core", .max_data_string = MAX_CORE_STR_LEN},
@@ -187,7 +189,7 @@ static struct col_desc g_col_desc[NUMBER_OF_TABS][TABS_COL_COUNT] = {
 struct rpc_thread_info {
 	char *name;
 	uint64_t id;
-	uint32_t core_num;
+	int core_num;
 	char *cpumask;
 	uint64_t busy;
 	uint64_t last_busy;
@@ -275,8 +277,6 @@ struct rpc_cores_stats {
 struct rpc_threads_stats g_threads_stats;
 struct rpc_pollers_stats g_pollers_stats;
 struct rpc_cores_stats g_cores_stats;
-struct rpc_poller_info g_pollers_history[RPC_MAX_POLLERS];
-struct rpc_thread_info g_thread_history[RPC_MAX_THREADS];
 
 static void
 init_str_len(void)
@@ -546,55 +546,227 @@ rpc_send_req(char *rpc_name, struct spdk_jsonrpc_client_response **resp)
 	return 0;
 }
 
+
 static int
-get_data(void)
+sort_threads(const void *p1, const void *p2)
+{
+	const struct rpc_thread_info thread_info1 = *(struct rpc_thread_info *)p1;
+	const struct rpc_thread_info thread_info2 = *(struct rpc_thread_info *)p2;
+	uint64_t count1, count2;
+
+	switch (g_current_sort_col[THREADS_TAB]) {
+	case 0: /* Sort by name */
+		return strcmp(thread_info1.name, thread_info2.name);
+	case 1: /* Sort by core */
+		count2 = thread_info1.core_num;
+		count1 = thread_info2.core_num;
+		break;
+	case 2: /* Sort by active pollers number */
+		count1 = thread_info1.active_pollers_count;
+		count2 = thread_info2.active_pollers_count;
+		break;
+	case 3: /* Sort by timed pollers number */
+		count1 = thread_info1.timed_pollers_count;
+		count2 = thread_info2.timed_pollers_count;
+		break;
+	case 4: /* Sort by paused pollers number */
+		count1 = thread_info1.paused_pollers_count;
+		count2 = thread_info2.paused_pollers_count;
+		break;
+	case 5: /* Sort by idle time */
+		count1 = thread_info1.idle - thread_info1.last_idle;
+		count2 = thread_info2.idle - thread_info2.last_idle;
+		break;
+	case 6: /* Sort by busy time */
+		count1 = thread_info1.busy - thread_info1.last_busy;
+		count2 = thread_info2.busy - thread_info2.last_busy;
+		break;
+	default:
+		return 0;
+	}
+
+	if (count2 > count1) {
+		return 1;
+	} else if (count2 < count1) {
+		return -1;
+	} else {
+		return 0;
+	}
+}
+
+static void
+store_last_run_counter(const char *poller_name, uint64_t thread_id, uint64_t last_run_counter)
+{
+	struct run_counter_history *history;
+
+	TAILQ_FOREACH(history, &g_run_counter_history, link) {
+		if (!strcmp(history->poller_name, poller_name) && history->thread_id == thread_id) {
+			history->last_run_counter = last_run_counter;
+			return;
+		}
+	}
+
+	history = calloc(1, sizeof(*history));
+	if (history == NULL) {
+		fprintf(stderr, "Unable to allocate a history object in store_last_run_counter.\n");
+		return;
+	}
+	history->poller_name = strdup(poller_name);
+	if (!history->poller_name) {
+		fprintf(stderr, "Unable to allocate poller_name of a history object in store_last_run_counter.\n");
+		free(history);
+		return;
+	}
+	history->thread_id = thread_id;
+	history->last_run_counter = last_run_counter;
+
+	TAILQ_INSERT_TAIL(&g_run_counter_history, history, link);
+}
+
+static int
+get_thread_data(void)
 {
 	struct spdk_jsonrpc_client_response *json_resp = NULL;
+	struct rpc_threads_stats threads_stats;
 	struct rpc_thread_info *thread_info;
 	struct rpc_core_info *core_info;
-	uint64_t i, j;
+	uint64_t i, j, k;
 	int rc = 0;
 
 	rc = rpc_send_req("thread_get_stats", &json_resp);
 	if (rc) {
-		goto end;
+		return rc;
 	}
 
 	/* Decode json */
-	memset(&g_threads_stats, 0, sizeof(g_threads_stats));
+	memset(&threads_stats, 0, sizeof(threads_stats));
 	if (spdk_json_decode_object(json_resp->result, rpc_threads_stats_decoders,
-				    SPDK_COUNTOF(rpc_threads_stats_decoders), &g_threads_stats)) {
+				    SPDK_COUNTOF(rpc_threads_stats_decoders), &threads_stats)) {
 		rc = -EINVAL;
 		goto end;
 	}
 
-	spdk_jsonrpc_client_free_response(json_resp);
+	pthread_mutex_lock(&g_thread_lock);
+
+	/* This is to free allocated char arrays with old thread names */
+	free_rpc_threads_stats(&g_threads_stats);
+
+	for (i = 0; i < threads_stats.threads.threads_count; i++) {
+		for (j = 0; j < g_threads_stats.threads.threads_count; j++) {
+			if (g_threads_stats.threads.thread_info[j].id == threads_stats.threads.thread_info[i].id) {
+				threads_stats.threads.thread_info[i].last_busy = g_threads_stats.threads.thread_info[j].busy;
+				threads_stats.threads.thread_info[i].last_idle = g_threads_stats.threads.thread_info[j].idle;
+			}
+		}
+	}
+	memcpy(&g_threads_stats, &threads_stats, sizeof(struct rpc_threads_stats));
+	qsort(&g_threads_stats.threads.thread_info, threads_stats.threads.threads_count,
+	      sizeof(g_threads_stats.threads.thread_info[0]), sort_threads);
 
 	for (i = 0; i < g_threads_stats.threads.threads_count; i++) {
-		thread_info = &g_threads_stats.threads.thread_info[i];
-		g_thread_info[thread_info->id] = thread_info;
+		g_threads_stats.threads.thread_info[i].core_num = -1;
+	}
+
+	for (i = 0; i < g_cores_stats.cores.cores_count; i++) {
+		core_info = &g_cores_stats.cores.core[i];
+
+		for (j = 0; j < core_info->threads.threads_count; j++) {
+			for (k = 0; k < g_threads_stats.threads.threads_count; k++) {
+				/* For each thread on current core: check if it's ID also exists
+				 * in g_thread_info data structure. If it does then assign current
+				 * core's number to that thread, otherwise application state is inconsistent
+				 * (e.g. scheduler is moving threads between cores). */
+				thread_info = &g_threads_stats.threads.thread_info[k];
+				if (thread_info->id == core_info->threads.thread[j].id) {
+					thread_info->core_num = core_info->lcore;
+					break;
+				}
+			}
+		}
 	}
 
+	pthread_mutex_unlock(&g_thread_lock);
+
+end:
+	spdk_jsonrpc_client_free_response(json_resp);
+	return rc;
+}
+
+static int
+get_pollers_data(void)
+{
+	struct spdk_jsonrpc_client_response *json_resp = NULL;
+	int rc = 0;
+	struct rpc_pollers *pollers;
+	struct rpc_poller_info *poller;
+	uint64_t pollers_count = 0;
+	uint64_t i, j;
+
 	rc = rpc_send_req("thread_get_pollers", &json_resp);
 	if (rc) {
-		goto end;
+		return rc;
+	}
+
+	pthread_mutex_lock(&g_thread_lock);
+
+	/* Save last run counter of each poller before updating g_pollers_stats */
+	for (i = 0; i < g_pollers_stats.pollers_threads.threads_count; i++) {
+		pollers = &g_pollers_stats.pollers_threads.threads[i].active_pollers;
+		for (j = 0; j < pollers->pollers_count; j++) {
+			poller = &pollers->pollers[j];
+			store_last_run_counter(poller->name, poller->thread_id, poller->run_count);
+		}
+		pollers_count += pollers->pollers_count;
+
+		pollers = &g_pollers_stats.pollers_threads.threads[i].timed_pollers;
+		for (j = 0; j < pollers->pollers_count; j++) {
+			poller = &pollers->pollers[j];
+			store_last_run_counter(poller->name, poller->thread_id, poller->run_count);
+		}
+		pollers_count += pollers->pollers_count;
+
+		pollers = &g_pollers_stats.pollers_threads.threads[i].paused_pollers;
+		for (j = 0; j < pollers->pollers_count; j++) {
+			poller = &pollers->pollers[j];
+			store_last_run_counter(poller->name, poller->thread_id, poller->run_count);
+		}
+		pollers_count += pollers->pollers_count;
 	}
+	g_last_pollers_count = pollers_count;
+
+	/* Free old pollers values before allocating memory for new ones */
+	free_rpc_pollers_stats(&g_pollers_stats);
 
 	/* Decode json */
 	memset(&g_pollers_stats, 0, sizeof(g_pollers_stats));
 	if (spdk_json_decode_object(json_resp->result, rpc_pollers_stats_decoders,
 				    SPDK_COUNTOF(rpc_pollers_stats_decoders), &g_pollers_stats)) {
 		rc = -EINVAL;
-		goto end;
 	}
 
+	pthread_mutex_unlock(&g_thread_lock);
 	spdk_jsonrpc_client_free_response(json_resp);
+	return rc;
+}
+
+static int
+get_cores_data(void)
+{
+	struct spdk_jsonrpc_client_response *json_resp = NULL;
+	struct rpc_core_info *core_info;
+	uint64_t i, j, k;
+	int rc = 0;
 
 	rc = rpc_send_req("framework_get_reactors", &json_resp);
 	if (rc) {
-		goto end;
+		return rc;
 	}
 
+	pthread_mutex_lock(&g_thread_lock);
+
+	/* Free old cores values before allocating memory for new ones */
+	free_rpc_cores_stats(&g_cores_stats);
+
 	/* Decode json */
 	memset(&g_cores_stats, 0, sizeof(g_cores_stats));
 	if (spdk_json_decode_object(json_resp->result, rpc_cores_stats_decoders,
@@ -607,23 +779,20 @@ get_data(void)
 		core_info = &g_cores_stats.cores.core[i];
 
 		for (j = 0; j < core_info->threads.threads_count; j++) {
-			g_thread_info[core_info->threads.thread[j].id]->core_num = core_info->lcore;
+			for (k = 0; k < g_threads_stats.threads.threads_count; k++) {
+				if (core_info->threads.thread[j].id == g_threads_stats.threads.thread_info[k].id) {
+					g_threads_stats.threads.thread_info[k].core_num = core_info->lcore;
+				}
+			}
 		}
 	}
 
 end:
+	pthread_mutex_unlock(&g_thread_lock);
 	spdk_jsonrpc_client_free_response(json_resp);
 	return rc;
 }
 
-static void
-free_data(void)
-{
-	free_rpc_threads_stats(&g_threads_stats);
-	free_rpc_pollers_stats(&g_pollers_stats);
-	free_rpc_cores_stats(&g_cores_stats);
-}
-
 enum str_alignment {
 	ALIGN_LEFT,
 	ALIGN_RIGHT,
@@ -797,63 +966,6 @@ get_time_str(uint64_t ticks, char *time_str)
 	snprintf(time_str, MAX_TIME_STR_LEN, "%" PRIu64, time);
 }
 
-static int
-sort_threads(const void *p1, const void *p2)
-{
-	const struct rpc_thread_info *thread_info1 = *(struct rpc_thread_info **)p1;
-	const struct rpc_thread_info *thread_info2 = *(struct rpc_thread_info **)p2;
-	uint64_t count1, count2;
-
-	/* thread IDs may not be allocated contiguously, so we need
-	 * to account for NULL thread_info pointers */
-	if (thread_info1 == NULL && thread_info2 == NULL) {
-		return 0;
-	} else if (thread_info1 == NULL) {
-		return 1;
-	} else if (thread_info2 == NULL) {
-		return -1;
-	}
-
-	switch (g_current_sort_col[THREADS_TAB]) {
-	case 0: /* Sort by name */
-		return strcmp(thread_info1->name, thread_info2->name);
-	case 1: /* Sort by core */
-		count2 = thread_info1->core_num;
-		count1 = thread_info2->core_num;
-		break;
-	case 2: /* Sort by active pollers number */
-		count1 = thread_info1->active_pollers_count;
-		count2 = thread_info2->active_pollers_count;
-		break;
-	case 3: /* Sort by timed pollers number */
-		count1 = thread_info1->timed_pollers_count;
-		count2 = thread_info2->timed_pollers_count;
-		break;
-	case 4: /* Sort by paused pollers number */
-		count1 = thread_info1->paused_pollers_count;
-		count2 = thread_info2->paused_pollers_count;
-		break;
-	case 5: /* Sort by idle time */
-		count1 = thread_info1->idle - thread_info1->last_idle;
-		count2 = thread_info2->idle - thread_info2->last_idle;
-		break;
-	case 6: /* Sort by busy time */
-		count1 = thread_info1->busy - thread_info1->last_busy;
-		count2 = thread_info2->busy - thread_info2->last_busy;
-		break;
-	default:
-		return 0;
-	}
-
-	if (count2 > count1) {
-		return 1;
-	} else if (count2 < count1) {
-		return -1;
-	} else {
-		return 0;
-	}
-}
-
 static void
 draw_row_background(uint8_t item_index, uint8_t tab)
 {
@@ -872,13 +984,13 @@ refresh_threads_tab(uint8_t current_page)
 {
 	struct col_desc *col_desc = g_col_desc[THREADS_TAB];
 	uint64_t i, threads_count;
-	uint16_t j, k;
+	uint16_t j;
 	uint16_t col;
 	uint8_t max_pages, item_index;
 	static uint8_t last_page = 0;
 	char pollers_number[MAX_POLLER_COUNT_STR_LEN], idle_time[MAX_TIME_STR_LEN],
 	     busy_time[MAX_TIME_STR_LEN], core_str[MAX_CORE_MASK_STR_LEN];
-	struct rpc_thread_info *thread_info[g_threads_stats.threads.threads_count];
+	struct rpc_thread_info *thread_info[RPC_MAX_THREADS];
 
 	threads_count = g_threads_stats.threads.threads_count;
 
@@ -893,35 +1005,16 @@ refresh_threads_tab(uint8_t current_page)
 		g_last_threads_count = threads_count;
 	}
 
-	/* From g_thread_info copy to thread_info without null elements.
-	 * The index of g_thread_info equals to Thread IDs, so it starts from '1'. */
-	for (i = 0, j = 1; i <  g_threads_stats.threads.threads_count; i++) {
-		while (g_thread_info[j] == NULL) {
-			j++;
-		}
-		memcpy(&thread_info[i], &g_thread_info[j], sizeof(struct rpc_thread_info *));
-		j++;
+	for (i = 0; i < threads_count; i++) {
+		thread_info[i] = &g_threads_stats.threads.thread_info[i];
 	}
 
 	if (last_page != current_page) {
-		for (i = 0; i < threads_count; i++) {
-			/* Thread IDs start from 1, so we have to do i + 1 */
-			g_threads_stats.threads.thread_info[i].last_idle = g_thread_info[i + 1]->idle;
-			g_threads_stats.threads.thread_info[i].last_busy = g_thread_info[i + 1]->busy;
-		}
-
 		last_page = current_page;
 	}
 
 	max_pages = (threads_count + g_max_data_rows - 1) / g_max_data_rows;
 
-	qsort(thread_info, threads_count, sizeof(thread_info[0]), sort_threads);
-
-	for (k = 0; k < threads_count; k++) {
-		g_thread_history[thread_info[k]->id].busy = thread_info[k]->busy - thread_info[k]->last_busy;
-		g_thread_history[thread_info[k]->id].idle = thread_info[k]->idle - thread_info[k]->last_idle;
-	}
-
 	for (i = current_page * g_max_data_rows;
 	     i < spdk_min(threads_count, (uint64_t)((current_page + 1) * g_max_data_rows));
 	     i++) {
@@ -965,7 +1058,6 @@ refresh_threads_tab(uint8_t current_page)
 			col += col_desc[4].max_data_string + 2;
 		}
 
-		g_thread_history[thread_info[i]->id].idle = thread_info[i]->idle - thread_info[i]->last_idle;
 		if (!col_desc[5].disabled) {
 			if (g_interval_data == true) {
 				get_time_str(thread_info[i]->idle - thread_info[i]->last_idle, idle_time);
@@ -977,7 +1069,6 @@ refresh_threads_tab(uint8_t current_page)
 			col += col_desc[5].max_data_string;
 		}
 
-		g_thread_history[thread_info[i]->id].busy = thread_info[i]->busy - thread_info[i]->last_busy;
 		if (!col_desc[6].disabled) {
 			if (g_interval_data == true) {
 				get_time_str(thread_info[i]->busy - thread_info[i]->last_busy, busy_time);
@@ -993,11 +1084,6 @@ refresh_threads_tab(uint8_t current_page)
 		}
 	}
 
-	for (k = 0; k < threads_count; k++) {
-		thread_info[k]->last_idle = thread_info[k]->idle;
-		thread_info[k]->last_busy = thread_info[k]->busy;
-	}
-
 	g_max_selected_row = i - current_page * g_max_data_rows - 1;
 
 	return max_pages;
@@ -1017,30 +1103,6 @@ get_last_run_counter(const char *poller_name, uint64_t thread_id)
 	return NULL;
 }
 
-static void
-store_last_run_counter(const char *poller_name, uint64_t thread_id, uint64_t last_run_counter)
-{
-	struct run_counter_history *history;
-
-	TAILQ_FOREACH(history, &g_run_counter_history, link) {
-		if (!strcmp(history->poller_name, poller_name) && history->thread_id == thread_id) {
-			history->last_run_counter = last_run_counter;
-			return;
-		}
-	}
-
-	history = calloc(1, sizeof(*history));
-	if (history == NULL) {
-		fprintf(stderr, "Unable to allocate a history object in store_last_run_counter.\n");
-		return;
-	}
-	history->poller_name = strdup(poller_name);
-	history->thread_id = thread_id;
-	history->last_run_counter = last_run_counter;
-
-	TAILQ_INSERT_TAIL(&g_run_counter_history, history, link);
-}
-
 enum sort_type {
 	BY_NAME,
 	USE_GLOBAL,
@@ -1103,33 +1165,37 @@ copy_pollers(struct rpc_pollers *pollers, uint64_t pollers_count, enum spdk_poll
 	     struct rpc_poller_info **pollers_info)
 {
 	uint64_t *last_run_counter;
-	uint64_t i;
+	uint64_t i, j;
+	struct rpc_thread_info *thread_info;
 
 	for (i = 0; i < pollers_count; i++) {
-		if (reset_last_counter) {
-			last_run_counter = get_last_run_counter(pollers->pollers[i].name, thread->id);
-			if (last_run_counter == NULL) {
-				store_last_run_counter(pollers->pollers[i].name, thread->id, pollers->pollers[i].run_count);
-				last_run_counter = get_last_run_counter(pollers->pollers[i].name, thread->id);
+		for (j = 0; j < g_threads_stats.threads.threads_count; j++) {
+			thread_info = &g_threads_stats.threads.thread_info[j];
+			/* Check if poller's thread exists in g_threads_stats
+			 * (if poller is not "hanging" without a thread). */
+			if (thread_info->id != thread->id) {
+				continue;
 			}
 
-			assert(last_run_counter != NULL);
-			*last_run_counter = pollers->pollers[i].run_count;
+			if (reset_last_counter) {
+				last_run_counter = get_last_run_counter(pollers->pollers[i].name, thread->id);
+				if (last_run_counter == NULL) {
+					store_last_run_counter(pollers->pollers[i].name, thread->id, pollers->pollers[i].run_count);
+					last_run_counter = get_last_run_counter(pollers->pollers[i].name, thread->id);
+				}
+
+				assert(last_run_counter != NULL);
+				*last_run_counter = pollers->pollers[i].run_count;
+			}
+			pollers_info[*current_count] = &pollers->pollers[i];
+			snprintf(pollers_info[*current_count]->thread_name, MAX_POLLER_NAME - 1, "%s", thread->name);
+			pollers_info[*current_count]->thread_id = thread->id;
+			pollers_info[(*current_count)++]->type = type;
+			break;
 		}
-		pollers_info[*current_count] = &pollers->pollers[i];
-		snprintf(pollers_info[*current_count]->thread_name, MAX_POLLER_NAME - 1, "%s", thread->name);
-		pollers_info[*current_count]->thread_id = thread->id;
-		pollers_info[(*current_count)++]->type = type;
 	}
 }
 
-static void
-store_pollers_last_stats(uint64_t poller, uint64_t run_counter, uint64_t period_ticks_counter)
-{
-	g_pollers_history[poller].run_count = run_counter;
-	g_pollers_history[poller].period_ticks = period_ticks_counter;
-}
-
 static uint8_t
 prepare_poller_data(uint8_t current_page, struct rpc_poller_info **pollers,
 		    uint64_t *count, uint8_t last_page)
@@ -1193,8 +1259,6 @@ refresh_pollers_tab(uint8_t current_page)
 			}
 		}
 
-		g_last_pollers_count = count;
-
 		/* We need to run store_last_run_counter() again, so the easiest way is to call this function
 		 * again with changed g_last_page value */
 		g_last_page = 0xF;
@@ -1233,7 +1297,6 @@ refresh_pollers_tab(uint8_t current_page)
 			col += col_desc[2].max_data_string + 1;
 		}
 
-		store_pollers_last_stats(i, pollers[i]->run_count - *last_run_counter, pollers[i]->period_ticks);
 		if (!col_desc[3].disabled) {
 			if (g_interval_data == true) {
 				snprintf(run_count, MAX_TIME_STR_LEN, "%" PRIu64, pollers[i]->run_count - *last_run_counter);
@@ -1254,8 +1317,6 @@ refresh_pollers_tab(uint8_t current_page)
 			col += col_desc[3].max_data_string + 4;
 		}
 
-		store_last_run_counter(pollers[i]->name, pollers[i]->thread_id, pollers[i]->run_count);
-
 		if (!col_desc[5].disabled) {
 			if (pollers[i]->busy_count > 0) {
 				if (item_index != g_selected_row) {
@@ -1363,7 +1424,7 @@ refresh_cores_tab(uint8_t current_page)
 {
 	struct col_desc *col_desc = g_col_desc[CORES_TAB];
 	uint64_t i;
-	uint32_t core_num;
+	int core_num;
 	uint16_t offset, count = 0;
 	uint8_t max_pages, item_index;
 	static uint8_t last_page = 0;
@@ -1376,6 +1437,10 @@ refresh_cores_tab(uint8_t current_page)
 
 	for (i = 0; i < g_threads_stats.threads.threads_count; i++) {
 		core_num = g_threads_stats.threads.thread_info[i].core_num;
+		/* If the thread is hanging, do not count it. */
+		if (core_num == -1) {
+			continue;
+		}
 		cores[core_num].threads_count++;
 		cores[core_num].pollers_count += g_threads_stats.threads.thread_info[i].active_pollers_count +
 						 g_threads_stats.threads.thread_info[i].timed_pollers_count +
@@ -1389,6 +1454,7 @@ refresh_cores_tab(uint8_t current_page)
 		cores[core_num].core = core_num;
 		cores[core_num].busy = g_cores_stats.cores.core[i].busy;
 		cores[core_num].idle = g_cores_stats.cores.core[i].idle;
+		cores[core_num].core_freq = g_cores_stats.cores.core[i].core_freq;
 		if (last_page != current_page) {
 			store_core_last_stats(cores[core_num].core, cores[core_num].idle, cores[core_num].busy);
 		}
@@ -1459,11 +1525,11 @@ refresh_cores_tab(uint8_t current_page)
 		}
 
 		if (!col_desc[5].disabled) {
-			if (!g_cores_stats.cores.core[core_num].core_freq) {
+			if (!cores[core_num].core_freq) {
 				snprintf(core_freq,  MAX_CORE_FREQ_STR_LEN, "%s", "N/A");
 			} else {
 				snprintf(core_freq, MAX_CORE_FREQ_STR_LEN, "%" PRIu32,
-					 g_cores_stats.cores.core[core_num].core_freq);
+					 cores[core_num].core_freq);
 			}
 			print_max_len(g_tabs[CORES_TAB], TABS_DATA_START_ROW + item_index, offset,
 				      col_desc[5].max_data_string, ALIGN_RIGHT, core_freq);
@@ -1933,7 +1999,7 @@ change_refresh_rate(void)
 }
 
 static void
-free_resources(void)
+free_poller_history(void)
 {
 	struct run_counter_history *history, *tmp;
 
@@ -1968,6 +2034,7 @@ display_thread(struct rpc_thread_info *thread_info)
 	bool stop_loop = false;
 	char idle_time[MAX_TIME_STR_LEN], busy_time[MAX_TIME_STR_LEN], run_count[MAX_POLLER_COUNT_STR_LEN];
 
+	pthread_mutex_lock(&g_thread_lock);
 	pollers_count = thread_info->active_pollers_count +
 			thread_info->timed_pollers_count +
 			thread_info->paused_pollers_count;
@@ -1995,9 +2062,9 @@ display_thread(struct rpc_thread_info *thread_info)
 		  thread_info->core_num);
 
 	if (g_interval_data) {
-		get_time_str(g_thread_history[thread_info->id].idle, idle_time);
+		get_time_str(thread_info->idle - thread_info->last_idle, idle_time);
 		mvwprintw(thread_win, 3, THREAD_WIN_FIRST_COL + 32, idle_time);
-		get_time_str(g_thread_history[thread_info->id].busy, busy_time);
+		get_time_str(thread_info->busy - thread_info->last_busy, busy_time);
 		mvwprintw(thread_win, 3, THREAD_WIN_FIRST_COL + 54, busy_time);
 	} else {
 		get_time_str(thread_info->idle, idle_time);
@@ -2060,6 +2127,7 @@ display_thread(struct rpc_thread_info *thread_info)
 	refresh();
 	wrefresh(thread_win);
 
+	pthread_mutex_unlock(&g_thread_lock);
 	while (!stop_loop) {
 		c = wgetch(thread_win);
 
@@ -2079,35 +2147,33 @@ display_thread(struct rpc_thread_info *thread_info)
 static void
 show_thread(uint8_t current_page)
 {
-	struct rpc_thread_info *thread_info[g_threads_stats.threads.threads_count];
+	struct rpc_thread_info thread_info;
 	uint64_t thread_number = current_page * g_max_data_rows + g_selected_row;
-	uint64_t i;
-
-	get_data();
 
+	pthread_mutex_lock(&g_thread_lock);
 	assert(thread_number < g_threads_stats.threads.threads_count);
-	for (i = 0; i < g_threads_stats.threads.threads_count; i++) {
-		thread_info[i] = &g_threads_stats.threads.thread_info[i];
-	}
-
-	qsort(thread_info, g_threads_stats.threads.threads_count, sizeof(thread_info[0]), sort_threads);
-
-	display_thread(thread_info[thread_number]);
+	thread_info = g_threads_stats.threads.thread_info[thread_number];
+	pthread_mutex_unlock(&g_thread_lock);
 
-	free_data();
+	display_thread(&thread_info);
 }
 
 static void
 show_single_thread(uint64_t thread_id)
 {
 	uint64_t i;
+	struct rpc_thread_info thread_info;
 
+	pthread_mutex_lock(&g_thread_lock);
 	for (i = 0; i < g_threads_stats.threads.threads_count; i++) {
 		if (g_threads_stats.threads.thread_info[i].id == thread_id) {
-			display_thread(&g_threads_stats.threads.thread_info[i]);
-			break;
+			thread_info = g_threads_stats.threads.thread_info[i];
+			pthread_mutex_unlock(&g_thread_lock);
+			display_thread(&thread_info);
+			return;
 		}
 	}
+	pthread_mutex_unlock(&g_thread_lock);
 }
 
 static void
@@ -2118,20 +2184,21 @@ show_core(uint8_t current_page)
 	uint64_t core_number = current_page * g_max_data_rows + g_selected_row;
 	struct rpc_core_info *core_info[g_cores_stats.cores.cores_count];
 	uint64_t threads_count, i, j;
+	uint64_t thread_id;
 	uint16_t current_threads_row;
 	int c;
 	char core_win_title[25];
 	bool stop_loop = false;
 	char idle_time[MAX_TIME_STR_LEN], busy_time[MAX_TIME_STR_LEN];
 
-	get_data();
-
+	pthread_mutex_lock(&g_thread_lock);
 	assert(core_number < g_cores_stats.cores.cores_count);
 	for (i = 0; i < g_cores_stats.cores.cores_count; i++) {
 		core_info[i] = &g_cores_stats.cores.core[i];
 	}
 
-	threads_count = g_cores_stats.cores.core->threads.threads_count;
+	threads_count = g_cores_stats.cores.core[core_number].threads.threads_count;
+
 	core_win = newwin(threads_count + CORE_WIN_HEIGHT, CORE_WIN_WIDTH,
 			  get_position_for_window(CORE_WIN_HEIGHT + threads_count, g_max_row),
 			  get_position_for_window(CORE_WIN_WIDTH, g_max_col));
@@ -2144,7 +2211,8 @@ show_core(uint8_t current_page)
 	doupdate();
 
 	box(core_win, 0, 0);
-	snprintf(core_win_title, sizeof(core_win_title), "Core %" PRIu64 " details", core_number);
+	snprintf(core_win_title, sizeof(core_win_title), "Core %" PRIu32 " details",
+		 core_info[core_number]->lcore);
 	print_in_middle(core_win, 1, 0, CORE_WIN_WIDTH, core_win_title, COLOR_PAIR(3));
 
 	mvwaddch(core_win, -1, 0, ACS_LTEE);
@@ -2164,30 +2232,31 @@ show_core(uint8_t current_page)
 	print_left(core_win, 5, 1, CORE_WIN_WIDTH, "Thread count:          Idle time:", COLOR_PAIR(5));
 
 	mvwprintw(core_win, 5, CORE_WIN_FIRST_COL, "%" PRIu64,
-		  g_cores_history[core_number].threads_count);
+		  g_cores_history[core_info[core_number]->lcore].threads_count);
 
 	if (g_interval_data == true) {
-		get_time_str(g_cores_history[core_number].idle, idle_time);
-		get_time_str(g_cores_history[core_number].busy, busy_time);
+		get_time_str(g_cores_history[core_info[core_number]->lcore].idle, idle_time);
+		get_time_str(g_cores_history[core_info[core_number]->lcore].busy, busy_time);
 	} else {
 		get_time_str(core_info[core_number]->idle, idle_time);
 		get_time_str(core_info[core_number]->busy, busy_time);
 	}
 	mvwprintw(core_win, 5, CORE_WIN_FIRST_COL + 20, idle_time);
+	mvwhline(core_win, 6, 1, ACS_HLINE, CORE_WIN_WIDTH - 2);
 
 	print_left(core_win, 7, 1, CORE_WIN_WIDTH, "Poller count:          Busy time:", COLOR_PAIR(5));
 	mvwprintw(core_win, 7, CORE_WIN_FIRST_COL, "%" PRIu64,
-		  g_cores_history[core_number].pollers_count);
+		  g_cores_history[core_info[core_number]->lcore].pollers_count);
 
 	mvwprintw(core_win, 7, CORE_WIN_FIRST_COL + 20, busy_time);
 
-	mvwhline(core_win, 6, 1, ACS_HLINE, CORE_WIN_WIDTH - 2);
 	mvwhline(core_win, 8, 1, ACS_HLINE, CORE_WIN_WIDTH - 2);
 	print_left(core_win, 9, 1, CORE_WIN_WIDTH, "Threads on this core", COLOR_PAIR(5));
 
 	for (j = 0; j < core_info[core_number]->threads.threads_count; j++) {
 		mvwprintw(core_win, j + 10, 1, core_info[core_number]->threads.thread[j].name);
 	}
+	pthread_mutex_unlock(&g_thread_lock);
 
 	refresh();
 	wrefresh(core_win);
@@ -2195,6 +2264,7 @@ show_core(uint8_t current_page)
 	current_threads_row = 0;
 
 	while (!stop_loop) {
+		pthread_mutex_lock(&g_thread_lock);
 		for (j = 0; j < core_info[core_number]->threads.threads_count; j++) {
 			if (j != current_threads_row) {
 				mvwprintw(core_win, j + 10, 1, core_info[core_number]->threads.thread[j].name);
@@ -2203,13 +2273,17 @@ show_core(uint8_t current_page)
 					   core_info[core_number]->threads.thread[j].name, COLOR_PAIR(2));
 			}
 		}
+		pthread_mutex_unlock(&g_thread_lock);
 
 		wrefresh(core_win);
 
 		c = wgetch(core_win);
 		switch (c) {
 		case 10: /* ENTER */
-			show_single_thread(core_info[core_number]->threads.thread[current_threads_row].id);
+			pthread_mutex_lock(&g_thread_lock);
+			thread_id = core_info[core_number]->threads.thread[current_threads_row].id;
+			pthread_mutex_unlock(&g_thread_lock);
+			show_single_thread(thread_id);
 			break;
 		case 27: /* ESC */
 			stop_loop = true;
@@ -2220,9 +2294,11 @@ show_core(uint8_t current_page)
 			}
 			break;
 		case KEY_DOWN:
+			pthread_mutex_lock(&g_thread_lock);
 			if (current_threads_row != core_info[core_number]->threads.threads_count - 1) {
 				current_threads_row++;
 			}
+			pthread_mutex_unlock(&g_thread_lock);
 			break;
 		default:
 			break;
@@ -2231,8 +2307,6 @@ show_core(uint8_t current_page)
 
 	del_panel(core_panel);
 	delwin(core_win);
-
-	free_data();
 }
 
 static void
@@ -2241,17 +2315,21 @@ show_poller(uint8_t current_page)
 	PANEL *poller_panel;
 	WINDOW *poller_win;
 	uint64_t count = 0;
+	uint64_t *last_run_counter;
 	uint64_t poller_number = current_page * g_max_data_rows + g_selected_row;
 	struct rpc_poller_info *pollers[RPC_MAX_POLLERS];
 	bool stop_loop = false;
 	char poller_period[MAX_TIME_STR_LEN];
 	int c;
 
-	get_data();
-
+	pthread_mutex_lock(&g_thread_lock);
 	prepare_poller_data(current_page, pollers, &count, current_page);
 	assert(poller_number < count);
 
+	last_run_counter = get_last_run_counter(pollers[poller_number]->name,
+						pollers[poller_number]->thread_id);
+	assert(last_run_counter != NULL);
+
 	poller_win = newwin(POLLER_WIN_HEIGHT, POLLER_WIN_WIDTH,
 			    get_position_for_window(POLLER_WIN_HEIGHT, g_max_row),
 			    get_position_for_window(POLLER_WIN_WIDTH, g_max_col));
@@ -2278,7 +2356,7 @@ show_poller(uint8_t current_page)
 
 	if (g_interval_data) {
 		mvwprintw(poller_win, 4, POLLER_WIN_FIRST_COL, "%" PRIu64,
-			  g_pollers_history[poller_number].run_count);
+			  pollers[poller_number]->run_count - *last_run_counter);
 	} else {
 		mvwprintw(poller_win, 4, POLLER_WIN_FIRST_COL, "%" PRIu64,
 			  pollers[poller_number]->run_count);
@@ -2286,7 +2364,7 @@ show_poller(uint8_t current_page)
 
 	if (pollers[poller_number]->period_ticks != 0) {
 		print_left(poller_win, 4, 28, POLLER_WIN_WIDTH, "Period:", COLOR_PAIR(5));
-		get_time_str(g_pollers_history[poller_number].period_ticks, poller_period);
+		get_time_str(pollers[poller_number]->period_ticks, poller_period);
 		mvwprintw(poller_win, 4, POLLER_WIN_FIRST_COL + 23, poller_period);
 	}
 	mvwhline(poller_win, 5, 1, ACS_HLINE, POLLER_WIN_WIDTH - 2);
@@ -2301,6 +2379,7 @@ show_poller(uint8_t current_page)
 	refresh();
 	wrefresh(poller_win);
 
+	pthread_mutex_unlock(&g_thread_lock);
 	while (!stop_loop) {
 		c = wgetch(poller_win);
 		switch (c) {
@@ -2314,18 +2393,56 @@ show_poller(uint8_t current_page)
 
 	del_panel(poller_panel);
 	delwin(poller_win);
+}
 
-	free_data();
+static void
+print_bottom_error_message(char *msg)
+{
+	mvprintw(g_max_row - 1, g_max_col - strlen(msg) - 2, msg);
+}
+
+static void *
+data_thread_routine(void *arg)
+{
+	int rc;
+
+	while (1) {
+		pthread_mutex_lock(&g_thread_lock);
+		if (g_quit_app) {
+			pthread_mutex_unlock(&g_thread_lock);
+			break;
+		}
+		pthread_mutex_unlock(&g_thread_lock);
+
+		/* Get data from RPC for each object type.
+		 * Start with cores since their number should not change. */
+		rc = get_cores_data();
+		if (rc) {
+			print_bottom_error_message("ERROR occurred while getting cores data");
+		}
+		rc = get_thread_data();
+		if (rc) {
+			print_bottom_error_message("ERROR occurred while getting threads data");
+		}
+
+		rc = get_pollers_data();
+		if (rc) {
+			print_bottom_error_message("ERROR occurred while getting pollers data");
+		}
+
+		usleep(g_sleep_time * SPDK_SEC_TO_USEC);
+	}
+
+	return NULL;
 }
 
 static void
-show_stats(void)
+show_stats(pthread_t *data_thread)
 {
 	const int CURRENT_PAGE_STR_LEN = 50;
-	const char *refresh_error = "ERROR occurred while getting data";
 	long int time_last, time_dif;
 	struct timespec time_now;
-	int c, rc;
+	int c;
 	int max_row, max_col;
 	uint8_t active_tab = THREADS_TAB;
 	uint8_t current_page = 0;
@@ -2351,9 +2468,29 @@ show_stats(void)
 			resize_interface(active_tab);
 		}
 
+		clock_gettime(CLOCK_REALTIME, &time_now);
+		time_dif = time_now.tv_sec - time_last;
+		if (time_dif < 0) {
+			time_dif = g_sleep_time;
+		}
+
+		if (time_dif >= g_sleep_time || force_refresh) {
+			time_last = time_now.tv_sec;
+			pthread_mutex_lock(&g_thread_lock);
+			max_pages = refresh_tab(active_tab, current_page);
+			pthread_mutex_unlock(&g_thread_lock);
+
+			snprintf(current_page_str, CURRENT_PAGE_STR_LEN - 1, "Page: %d/%d", current_page + 1, max_pages);
+			mvprintw(g_max_row - 1, 1, current_page_str);
+
+			refresh();
+		}
+
 		c = getch();
 		if (c == 'q') {
-			free_resources();
+			pthread_mutex_lock(&g_thread_lock);
+			g_quit_app = true;
+			pthread_mutex_unlock(&g_thread_lock);
 			break;
 		}
 
@@ -2429,30 +2566,16 @@ show_stats(void)
 			force_refresh = false;
 			break;
 		}
+	}
 
-		clock_gettime(CLOCK_REALTIME, &time_now);
-		time_dif = time_now.tv_sec - time_last;
-		if (time_dif < 0) {
-			time_dif = g_sleep_time;
-		}
-
-		if (time_dif >= g_sleep_time || force_refresh) {
-			time_last = time_now.tv_sec;
-			rc = get_data();
-			if (rc) {
-				mvprintw(g_max_row - 1, g_max_col - strlen(refresh_error) - 2, refresh_error);
-			}
-
-			max_pages = refresh_tab(active_tab, current_page);
-
-			snprintf(current_page_str, CURRENT_PAGE_STR_LEN - 1, "Page: %d/%d", current_page + 1, max_pages);
-			mvprintw(g_max_row - 1, 1, current_page_str);
+	pthread_join(*data_thread, NULL);
 
-			free_data();
+	free_poller_history();
 
-			refresh();
-		}
-	}
+	/* Free memory holding current data states before quitting application */
+	free_rpc_threads_stats(&g_threads_stats);
+	free_rpc_pollers_stats(&g_pollers_stats);
+	free_rpc_cores_stats(&g_cores_stats);
 }
 
 static void
@@ -2488,7 +2611,8 @@ draw_interface(void)
 	doupdate();
 }
 
-static void finish(int sig)
+static void
+finish(int sig)
 {
 	/* End ncurses mode */
 	endwin();
@@ -2538,7 +2662,7 @@ usage(const char *program_name)
 }
 
 static int
-wait_init(void)
+wait_init(pthread_t *data_thread)
 {
 	struct spdk_jsonrpc_client_response *json_resp = NULL;
 	char *uninit_log = "Waiting for SPDK target application to initialize...",
@@ -2561,6 +2685,39 @@ wait_init(void)
 	}
 
 	spdk_jsonrpc_client_free_response(json_resp);
+
+	rc = pthread_mutex_init(&g_thread_lock, NULL);
+	if (rc) {
+		fprintf(stderr, "mutex lock failed to initialize: %d\n", errno);
+		return -1;
+	}
+
+	memset(&g_threads_stats, 0, sizeof(g_threads_stats));
+
+	/* This is to get first batch of data for display functions.
+	 * Since data thread makes RPC calls that take more time than
+	 * startup of display functions on main thread, without these
+	 * calls both threads would be subject to a race condition. */
+	rc = get_thread_data();
+	if (rc) {
+		return -1;
+	}
+
+	rc = get_pollers_data();
+	if (rc) {
+		return -1;
+	}
+
+	rc = get_cores_data();
+	if (rc) {
+		return -1;
+	}
+
+	rc = pthread_create(data_thread, NULL, &data_thread_routine, NULL);
+	if (rc) {
+		fprintf(stderr, "data thread creation failed: %d\n", errno);
+		return -1;
+	}
 	return 0;
 }
 
@@ -2568,6 +2725,7 @@ int main(int argc, char **argv)
 {
 	int op, rc;
 	char *socket = SPDK_DEFAULT_RPC_ADDR;
+	pthread_t data_thread;
 
 	while ((op = getopt(argc, argv, "r:h")) != -1) {
 		switch (op) {
@@ -2591,9 +2749,9 @@ int main(int argc, char **argv)
 	setup_ncurses();
 	draw_interface();
 
-	rc = wait_init();
+	rc = wait_init(&data_thread);
 	if (!rc) {
-		show_stats();
+		show_stats(&data_thread);
 	}
 
 	finish(0);
diff --git a/app/trace/Makefile b/app/trace/Makefile
index 92fb60cf6..d222496a9 100644
--- a/app/trace/Makefile
+++ b/app/trace/Makefile
@@ -38,6 +38,8 @@ include $(SPDK_ROOT_DIR)/mk/spdk.modules.mk
 APP = spdk_trace
 SPDK_NO_LINK_ENV = 1
 
+SPDK_LIB_LIST += json
+
 CXX_SRCS := trace.cpp
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app_cxx.mk
diff --git a/app/trace/trace.cpp b/app/trace/trace.cpp
index 71350e85c..109bee7c0 100644
--- a/app/trace/trace.cpp
+++ b/app/trace/trace.cpp
@@ -32,6 +32,10 @@
  */
 
 #include "spdk/stdinc.h"
+#include "spdk/env.h"
+#include "spdk/json.h"
+#include "spdk/string.h"
+#include "spdk/util.h"
 
 #include <map>
 
@@ -41,8 +45,29 @@ extern "C" {
 }
 
 static struct spdk_trace_histories *g_histories;
+static struct spdk_json_write_ctx *g_json;
 static bool g_print_tsc = false;
 
+/* This is a bit ugly, but we don't want to include env_dpdk in the app, while spdk_util, which we
+ * do need, uses some of the functions implemented there.  We're not actually using the functions
+ * that depend on those, so just define them as no-ops to allow the app to link.
+ */
+extern "C" {
+	void *
+	spdk_realloc(void *buf, size_t size, size_t align)
+	{
+		assert(false);
+
+		return NULL;
+	}
+
+	void
+	spdk_free(void *buf)
+	{
+		assert(false);
+	}
+} /* extern "C" */
+
 static void usage(void);
 
 struct entry_key {
@@ -82,7 +107,6 @@ struct object_stats {
 struct object_stats g_stats[SPDK_TRACE_MAX_OBJECT];
 
 static char *g_exe_name;
-static int g_verbose = 1;
 
 static uint64_t g_tsc_rate;
 static uint64_t g_first_tsc = 0x0;
@@ -93,10 +117,19 @@ get_us_from_tsc(uint64_t tsc, uint64_t tsc_rate)
 	return ((float)tsc) * 1000 * 1000 / tsc_rate;
 }
 
+static const char *
+format_argname(const char *name)
+{
+	static char namebuf[16];
+
+	snprintf(namebuf, sizeof(namebuf), "%s: ", name);
+	return namebuf;
+}
+
 static void
 print_ptr(const char *arg_string, uint64_t arg)
 {
-	printf("%-7.7s0x%-14jx ", arg_string, arg);
+	printf("%-7.7s0x%-14jx ", format_argname(arg_string), arg);
 }
 
 static void
@@ -107,14 +140,13 @@ print_uint64(const char *arg_string, uint64_t arg)
 	 *  for FLUSH WRITEBUF when writev() returns -1 due to full
 	 *  socket buffer.
 	 */
-	printf("%-7.7s%-16jd ", arg_string, arg);
+	printf("%-7.7s%-16jd ", format_argname(arg_string), arg);
 }
 
 static void
-print_string(const char *arg_string, uint64_t arg)
+print_string(const char *arg_string, const char *arg)
 {
-	char *str = (char *)&arg;
-	printf("%-7.7s%.8s ", arg_string, str);
+	printf("%-7.7s%-16.16s ", format_argname(arg_string), arg);
 }
 
 static void
@@ -136,26 +168,42 @@ print_object_id(uint8_t type, uint64_t id)
 static void
 print_float(const char *arg_string, float arg)
 {
-	printf("%-7s%-16.3f ", arg_string, arg);
+	printf("%-7s%-16.3f ", format_argname(arg_string), arg);
 }
 
 static void
-print_arg(uint8_t arg_type, const char *arg_string, uint64_t arg)
+print_arg(uint8_t arg_type, const char *arg_string, const void *arg)
 {
-	if (arg_string[0] == 0) {
-		printf("%24s", "");
-		return;
-	}
+	uint64_t value;
 
 	switch (arg_type) {
 	case SPDK_TRACE_ARG_TYPE_PTR:
-		print_ptr(arg_string, arg);
+		memcpy(&value, arg, sizeof(value));
+		print_ptr(arg_string, value);
+		break;
+	case SPDK_TRACE_ARG_TYPE_INT:
+		memcpy(&value, arg, sizeof(value));
+		print_uint64(arg_string, value);
+		break;
+	case SPDK_TRACE_ARG_TYPE_STR:
+		print_string(arg_string, (const char *)arg);
 		break;
+	}
+}
+
+static void
+print_arg_json(uint8_t arg_type, const void *arg)
+{
+	uint64_t value;
+
+	switch (arg_type) {
+	case SPDK_TRACE_ARG_TYPE_PTR:
 	case SPDK_TRACE_ARG_TYPE_INT:
-		print_uint64(arg_string, arg);
+		memcpy(&value, arg, sizeof(value));
+		spdk_json_write_uint64(g_json, value);
 		break;
 	case SPDK_TRACE_ARG_TYPE_STR:
-		print_string(arg_string, arg);
+		spdk_json_write_string(g_json, (const char *)arg);
 		break;
 	}
 }
@@ -167,17 +215,10 @@ print_event(struct spdk_trace_entry *e, uint64_t tsc_rate,
 	struct spdk_trace_tpoint	*d;
 	struct object_stats		*stats;
 	float				us;
+	size_t				i, offset;
 
 	d = &g_histories->flags.tpoint[e->tpoint_id];
 	stats = &g_stats[d->object_type];
-
-	if (d->new_object) {
-		stats->index[e->object_id] = stats->counter++;
-		stats->tpoint_id[e->object_id] = e->tpoint_id;
-		stats->start[e->object_id] = e->tsc;
-		stats->size[e->object_id] = e->size;
-	}
-
 	us = get_us_from_tsc(e->tsc - tsc_offset, tsc_rate);
 
 	printf("%2d: %10.3f ", lcore, us);
@@ -193,7 +234,6 @@ print_event(struct spdk_trace_entry *e, uint64_t tsc_rate,
 	printf("%-*s ", (int)sizeof(d->name), d->name);
 	print_size(e->size);
 
-	print_arg(d->arg1_type, d->arg1_name, e->arg1);
 	if (d->new_object) {
 		print_object_id(d->object_type, stats->index[e->object_id]);
 	} else if (d->object_type != OBJECT_NONE) {
@@ -201,22 +241,101 @@ print_event(struct spdk_trace_entry *e, uint64_t tsc_rate,
 			us = get_us_from_tsc(e->tsc - stats->start[e->object_id],
 					     tsc_rate);
 			print_object_id(d->object_type, stats->index[e->object_id]);
-			print_float("time:", us);
+			print_float("time", us);
 		} else {
 			printf("id:    N/A");
 		}
 	} else if (e->object_id != 0) {
-		print_arg(SPDK_TRACE_ARG_TYPE_PTR, "object: ", e->object_id);
+		print_arg(SPDK_TRACE_ARG_TYPE_PTR, "object", &e->object_id);
+	}
+
+	for (i = 0, offset = 0; i < d->num_args; ++i) {
+		assert(offset < sizeof(e->args));
+		print_arg(d->args[i].type, d->args[i].name, &e->args[offset]);
+		offset += d->args[i].size;
 	}
 	printf("\n");
 }
 
+static void
+print_event_json(struct spdk_trace_entry *e, uint64_t tsc_rate,
+		 uint64_t tsc_offset, uint16_t lcore)
+{
+	struct spdk_trace_tpoint *d;
+	struct object_stats *stats;
+	size_t i, offset;
+
+	d = &g_histories->flags.tpoint[e->tpoint_id];
+	stats = &g_stats[d->object_type];
+
+	spdk_json_write_object_begin(g_json);
+	spdk_json_write_named_uint64(g_json, "lcore", lcore);
+	spdk_json_write_named_uint64(g_json, "tpoint", e->tpoint_id);
+	spdk_json_write_named_uint64(g_json, "tsc", e->tsc);
+
+	if (g_histories->flags.owner[d->owner_type].id_prefix) {
+		spdk_json_write_named_string_fmt(g_json, "poller", "%c%02d",
+						 g_histories->flags.owner[d->owner_type].id_prefix,
+						 e->poller_id);
+	}
+	if (e->size != 0) {
+		spdk_json_write_named_uint32(g_json, "size", e->size);
+	}
+	if (d->new_object || d->object_type != OBJECT_NONE || e->object_id != 0) {
+		char object_type;
+
+		spdk_json_write_named_object_begin(g_json, "object");
+		if (d->new_object) {
+			object_type =  g_histories->flags.object[d->object_type].id_prefix;
+			spdk_json_write_named_string_fmt(g_json, "id", "%c%lu", object_type,
+							 stats->index[e->object_id]);
+		} else if (d->object_type != OBJECT_NONE) {
+			object_type =  g_histories->flags.object[d->object_type].id_prefix;
+			if (stats->start.find(e->object_id) != stats->start.end()) {
+				spdk_json_write_named_string_fmt(g_json, "id", "%c%lu",
+								 object_type,
+								 stats->index[e->object_id]);
+				spdk_json_write_named_uint64(g_json, "time",
+							     e->tsc - stats->start[e->object_id]);
+			}
+		}
+		spdk_json_write_named_uint64(g_json, "value", e->object_id);
+		spdk_json_write_object_end(g_json);
+	}
+	if (d->num_args > 0) {
+		spdk_json_write_named_array_begin(g_json, "args");
+		for (i = 0, offset = 0; i < d->num_args; ++i) {
+			assert(offset < sizeof(e->args));
+			print_arg_json(d->args[i].type, &e->args[offset]);
+			offset += d->args[i].size;
+		}
+		spdk_json_write_array_end(g_json);
+	}
+
+	spdk_json_write_object_end(g_json);
+}
+
 static void
 process_event(struct spdk_trace_entry *e, uint64_t tsc_rate,
 	      uint64_t tsc_offset, uint16_t lcore)
 {
-	if (g_verbose) {
+	struct spdk_trace_tpoint	*d;
+	struct object_stats		*stats;
+
+	d = &g_histories->flags.tpoint[e->tpoint_id];
+	stats = &g_stats[d->object_type];
+
+	if (d->new_object) {
+		stats->index[e->object_id] = stats->counter++;
+		stats->tpoint_id[e->object_id] = e->tpoint_id;
+		stats->start[e->object_id] = e->tsc;
+		stats->size[e->object_id] = e->size;
+	}
+
+	if (g_json == NULL) {
 		print_event(e, tsc_rate, tsc_offset, lcore);
+	} else {
+		print_event_json(e, tsc_rate, tsc_offset, lcore);
 	}
 }
 
@@ -276,11 +395,68 @@ populate_events(struct spdk_trace_history *history, int num_entries)
 	return (0);
 }
 
+static void
+print_tpoint_definitions(void)
+{
+	struct spdk_trace_tpoint *tpoint;
+	size_t i, j;
+
+	/* We only care about these when printing JSON */
+	if (!g_json) {
+		return;
+	}
+
+	spdk_json_write_named_uint64(g_json, "tsc_rate", g_tsc_rate);
+	spdk_json_write_named_array_begin(g_json, "tpoints");
+
+	for (i = 0; i < SPDK_COUNTOF(g_histories->flags.tpoint); ++i) {
+		tpoint = &g_histories->flags.tpoint[i];
+		if (tpoint->tpoint_id == 0) {
+			continue;
+		}
+
+		spdk_json_write_object_begin(g_json);
+		spdk_json_write_named_string(g_json, "name", tpoint->name);
+		spdk_json_write_named_uint32(g_json, "id", tpoint->tpoint_id);
+		spdk_json_write_named_bool(g_json, "new_object", tpoint->new_object);
+
+		spdk_json_write_named_array_begin(g_json, "args");
+		for (j = 0; j < tpoint->num_args; ++j) {
+			spdk_json_write_object_begin(g_json);
+			spdk_json_write_named_string(g_json, "name", tpoint->args[j].name);
+			spdk_json_write_named_uint32(g_json, "type", tpoint->args[j].type);
+			spdk_json_write_named_uint32(g_json, "size", tpoint->args[j].size);
+			spdk_json_write_object_end(g_json);
+		}
+		spdk_json_write_array_end(g_json);
+		spdk_json_write_object_end(g_json);
+	}
+
+	spdk_json_write_array_end(g_json);
+}
+
+static int
+print_json(void *cb_ctx, const void *data, size_t size)
+{
+	ssize_t rc;
+
+	while (size > 0) {
+		rc = write(STDOUT_FILENO, data, size);
+		if (rc < 0) {
+			fprintf(stderr, "%s: %s\n", g_exe_name, spdk_strerror(errno));
+			abort();
+		}
+
+		size -= rc;
+	}
+
+	return 0;
+}
+
 static void usage(void)
 {
 	fprintf(stderr, "usage:\n");
 	fprintf(stderr, "   %s <option> <lcore#>\n", g_exe_name);
-	fprintf(stderr, "        option = '-q' to disable verbose mode\n");
 	fprintf(stderr, "                 '-c' to display single lcore history\n");
 	fprintf(stderr, "                 '-t' to display TSC offset for each event\n");
 	fprintf(stderr, "                 '-s' to specify spdk_trace shm name for a\n");
@@ -291,6 +467,7 @@ static void usage(void)
 	fprintf(stderr, "                       -i or -p must be specified)\n");
 	fprintf(stderr, "                 '-f' to specify a tracepoint file name\n");
 	fprintf(stderr, "                      (-s and -f are mutually exclusive)\n");
+	fprintf(stderr, "                 '-j' to use JSON to format the output\n");
 }
 
 int main(int argc, char **argv)
@@ -307,9 +484,10 @@ int main(int argc, char **argv)
 	int			shm_id = -1, shm_pid = -1;
 	uint64_t		trace_histories_size;
 	struct stat		_stat;
+	bool			json = false;
 
 	g_exe_name = argv[0];
-	while ((op = getopt(argc, argv, "c:f:i:p:qs:t")) != -1) {
+	while ((op = getopt(argc, argv, "c:f:i:jp:s:t")) != -1) {
 		switch (op) {
 		case 'c':
 			lcore = atoi(optarg);
@@ -326,9 +504,6 @@ int main(int argc, char **argv)
 		case 'p':
 			shm_pid = atoi(optarg);
 			break;
-		case 'q':
-			g_verbose = 0;
-			break;
 		case 's':
 			app_name = optarg;
 			break;
@@ -338,6 +513,9 @@ int main(int argc, char **argv)
 		case 't':
 			g_print_tsc = true;
 			break;
+		case 'j':
+			json = true;
+			break;
 		default:
 			usage();
 			exit(1);
@@ -356,6 +534,14 @@ int main(int argc, char **argv)
 		exit(1);
 	}
 
+	if (json) {
+		g_json = spdk_json_write_begin(print_json, NULL, 0);
+		if (g_json == NULL) {
+			fprintf(stderr, "Failed to allocate JSON write context\n");
+			exit(1);
+		}
+	}
+
 	if (file_name) {
 		fd = open(file_name, O_RDONLY);
 	} else {
@@ -402,7 +588,7 @@ int main(int argc, char **argv)
 		exit(-1);
 	}
 
-	if (g_verbose) {
+	if (!g_json) {
 		printf("TSC Rate: %ju\n", g_tsc_rate);
 	}
 
@@ -430,7 +616,7 @@ int main(int argc, char **argv)
 				continue;
 			}
 
-			if (g_verbose && history->num_entries) {
+			if (!g_json && history->num_entries) {
 				printf("Trace Size of lcore (%d): %ju\n", i, history->num_entries);
 			}
 
@@ -439,7 +625,7 @@ int main(int argc, char **argv)
 	} else {
 		history = spdk_get_per_lcore_history(g_histories, lcore);
 		if (history->num_entries > 0 && history->entries[0].tsc != 0) {
-			if (g_verbose && history->num_entries) {
+			if (!g_json && history->num_entries) {
 				printf("Trace Size of lcore (%d): %ju\n", lcore, history->num_entries);
 			}
 
@@ -447,6 +633,12 @@ int main(int argc, char **argv)
 		}
 	}
 
+	if (g_json != NULL) {
+		spdk_json_write_object_begin(g_json);
+		print_tpoint_definitions();
+		spdk_json_write_named_array_begin(g_json, "entries");
+	}
+
 	tsc_offset = g_first_tsc;
 	for (entry_map::iterator it = g_entry_map.begin(); it != g_entry_map.end(); it++) {
 		if (it->first.tsc < g_first_tsc) {
@@ -455,6 +647,12 @@ int main(int argc, char **argv)
 		process_event(it->second, g_tsc_rate, tsc_offset, it->first.lcore);
 	}
 
+	if (g_json != NULL) {
+		spdk_json_write_array_end(g_json);
+		spdk_json_write_object_end(g_json);
+		spdk_json_write_end(g_json);
+	}
+
 	munmap(history_ptr, trace_histories_size);
 	close(fd);
 
diff --git a/app/vhost/Makefile b/app/vhost/Makefile
index 7df6b0a9b..e6b6c2def 100644
--- a/app/vhost/Makefile
+++ b/app/vhost/Makefile
@@ -39,7 +39,7 @@ APP = vhost
 
 C_SRCS := vhost.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_vhost event_nbd
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_vhost event_nbd
 
 ifeq ($(SPDK_ROOT_DIR)/lib/env_dpdk,$(CONFIG_ENV))
 SPDK_LIB_LIST += env_dpdk_rpc
diff --git a/autobuild.sh b/autobuild.sh
index f66c063d2..db76f9e7c 100755
--- a/autobuild.sh
+++ b/autobuild.sh
@@ -14,6 +14,10 @@ source "$1"
 source "$rootdir/test/common/autotest_common.sh"
 source "$rootdir/scripts/common.sh"
 
+if [[ -n $EXTERNAL_MAKE_HUGEMEM ]]; then
+	export EXTERNAL_MAKE_HUGEMEM
+fi
+
 out=$output_dir
 if [ -n "$SPDK_TEST_NATIVE_DPDK" ]; then
 	scanbuild_exclude=" --exclude $(dirname $SPDK_RUN_EXTERNAL_DPDK)"
@@ -51,7 +55,10 @@ function ocf_precompile() {
 function build_native_dpdk() {
 	local external_dpdk_dir
 	local external_dpdk_base_dir
+	local gcc_version
 
+	gcc_version=$(gcc -dumpversion)
+	gcc_version=${gcc_version//./}
 	external_dpdk_dir="$SPDK_RUN_EXTERNAL_DPDK"
 	external_dpdk_base_dir="$(dirname $external_dpdk_dir)"
 
@@ -65,9 +72,17 @@ function build_native_dpdk() {
 	git clone --branch $SPDK_TEST_NATIVE_DPDK --depth 1 http://dpdk.org/git/dpdk "$external_dpdk_base_dir"
 	git -C "$external_dpdk_base_dir" log --oneline -n 5
 
-	dpdk_cflags="-fPIC -g -Werror -fcommon"
+	dpdk_cflags="-fPIC -g -fcommon"
 	dpdk_ldflags=""
 
+	if [[ $gcc_version -ge 5 ]]; then
+		dpdk_cflags+=" -Werror"
+	fi
+
+	if [[ $gcc_version -ge 10 ]]; then
+		dpdk_cflags+=" -Wno-stringop-overflow"
+	fi
+
 	# the drivers we use
 	# net/i40e driver is not really needed by us, but it's built as a workaround
 	# for DPDK issue: https://bugs.dpdk.org/show_bug.cgi?id=576
@@ -113,7 +128,6 @@ function build_native_dpdk() {
 
 	cd $external_dpdk_base_dir
 	if [ "$(uname -s)" = "Linux" ]; then
-		dpdk_cflags+=" -Wno-stringop-overflow"
 		# Fix for freeing device if not kernel driver configured.
 		# TODO: Remove once this is merged in upstream DPDK
 		if grep "20.08.0" $external_dpdk_base_dir/VERSION; then
@@ -123,8 +137,8 @@ function build_native_dpdk() {
 			wget https://github.com/karlatec/dpdk/commit/3219c0cfc38803aec10c809dde16e013b370bda9.patch -O dpdk-pci.patch
 			wget https://github.com/karlatec/dpdk/commit/adf8f7638de29bc4bf9ba3faf12bbdae73acda0c.patch -O dpdk-qat.patch
 		else
-			wget https://github.com/karlatec/dpdk/commit/eac05db0580091ef8e4d338aa5d2210695521894.patch -O dpdk-pci.patch
-			wget https://github.com/karlatec/dpdk/commit/d649d5efb7bb404ce59dea81768adeb994b284f7.patch -O dpdk-qat.patch
+			wget https://github.com/karlatec/dpdk/commit/f95e331be3a1f856b816948990dd2afc67ea4020.patch -O dpdk-pci.patch
+			wget https://github.com/karlatec/dpdk/commit/6fd2fa906ffdcee04e6ce5da40e61cb841be9827.patch -O dpdk-qat.patch
 		fi
 		git config --local user.name "spdk"
 		git config --local user.email "nomail@all.com"
@@ -158,6 +172,7 @@ function make_fail_cleanup() {
 
 function scanbuild_make() {
 	pass=true
+	"$rootdir/configure" $config_params --without-shared
 	$scanbuild $MAKE $MAKEFLAGS > $out/build_output.txt && rm -rf $out/scan-build-tmp || make_fail_cleanup
 	xtrace_disable
 
@@ -255,12 +270,9 @@ function build_doc() {
 function autobuild_test_suite() {
 	run_test "autobuild_check_format" ./scripts/check_format.sh
 	run_test "autobuild_external_code" sudo -E --preserve-env=PATH LD_LIBRARY_PATH=$LD_LIBRARY_PATH $rootdir/test/external_code/test_make.sh $rootdir
-	if [ "$SPDK_TEST_OCF" -eq 1 ]; then
-		run_test "autobuild_ocf_precompile" ocf_precompile
-	fi
 	run_test "autobuild_check_so_deps" $rootdir/test/make/check_so_deps.sh $1
 	./configure $config_params --without-shared
-	run_test "scanbuild_make" scanbuild_make
+	$MAKE $MAKEFLAGS
 	run_test "autobuild_generated_files_check" porcelain_check
 	run_test "autobuild_header_dependency_check" header_dependency_check
 	run_test "autobuild_make_install" $MAKE $MAKEFLAGS install DESTDIR="$SPDK_WORKSPACE" prefix=/usr
@@ -268,6 +280,11 @@ function autobuild_test_suite() {
 	run_test "autobuild_build_doc" build_doc
 }
 
+function unittest_build() {
+	"$rootdir/configure" $config_params --without-shared
+	$MAKE $MAKEFLAGS
+}
+
 if [ $SPDK_RUN_VALGRIND -eq 1 ]; then
 	run_test "valgrind" echo "using valgrind"
 fi
@@ -291,12 +308,17 @@ $MAKE cc_version
 $MAKE cxx_version
 echo "** END ** Info for Hostname: $HOSTNAME"
 
-if [ "$SPDK_TEST_AUTOBUILD" -eq 1 ]; then
+if [[ $SPDK_TEST_OCF -eq 1 ]]; then
+	run_test "autobuild_ocf_precompile" ocf_precompile
+fi
+
+if [[ $SPDK_TEST_AUTOBUILD -eq 1 ]]; then
 	run_test "autobuild" autobuild_test_suite $1
+elif [[ $SPDK_TEST_UNITTEST -eq 1 ]]; then
+	run_test "unittest_build" unittest_build
+elif [[ $SPDK_TEST_SCANBUILD -eq 1 ]]; then
+	run_test "scanbuild_make" scanbuild_make
 else
-	if [ "$SPDK_TEST_OCF" -eq 1 ]; then
-		run_test "autobuild_ocf_precompile" ocf_precompile
-	fi
 	# if we aren't testing the unittests, build with shared objects.
 	./configure $config_params --with-shared
 	run_test "make" $MAKE $MAKEFLAGS
diff --git a/autopackage.sh b/autopackage.sh
index 7f5c81e16..d73801901 100755
--- a/autopackage.sh
+++ b/autopackage.sh
@@ -11,6 +11,7 @@ fi
 source "$1"
 
 rootdir=$(readlink -f $(dirname $0))
+testdir=$rootdir # to get the storage space for tests
 source "$rootdir/test/common/autotest_common.sh"
 
 function build_rpms() (
@@ -20,7 +21,7 @@ function build_rpms() (
 	unset -v LD_LIBRARY_PATH
 
 	install_uninstall_rpms() {
-		rpms=("$HOME/rpmbuild/RPMS/x86_64/"spdk{,-devel,{,-dpdk}-libs}-$version-1.x86_64.rpm)
+		rpms=("$builddir/rpm/x86_64/"spdk{,-devel,{,-dpdk}-libs}-$version-1.x86_64.rpm)
 
 		sudo rpm -i "${rpms[@]}"
 		rpms=("${rpms[@]##*/}") rpms=("${rpms[@]%.rpm}")
@@ -31,11 +32,12 @@ function build_rpms() (
 	}
 
 	build_rpm() {
-		MAKEFLAGS="$MAKEFLAGS" SPDK_VERSION="$version" DEPS=no "$rootdir/rpmbuild/rpm.sh" "$@"
+		BUILDDIR=$builddir MAKEFLAGS="$MAKEFLAGS" SPDK_VERSION="$version" DEPS=no "$rootdir/rpmbuild/rpm.sh" "$@"
 		install_uninstall_rpms
 	}
 
 	version="test_shared"
+	builddir=$SPDK_TEST_STORAGE/test-rpm
 	run_test "build_shared_rpm" build_rpm --with-shared
 
 	if [[ -n $SPDK_TEST_NATIVE_DPDK ]]; then
diff --git a/autorun.sh b/autorun.sh
index c85fdb217..9b5207e1a 100755
--- a/autorun.sh
+++ b/autorun.sh
@@ -12,11 +12,14 @@ if [[ ! -f $conf ]]; then
 	echo "ERROR: $conf doesn't exist"
 	exit 1
 fi
+source "$conf"
 
 echo "Test configuration:"
 cat "$conf"
 
 # Runs agent scripts
 $rootdir/autobuild.sh "$conf"
-sudo -E $rootdir/autotest.sh "$conf"
+if ((SPDK_TEST_UNITTEST == 1 || SPDK_RUN_FUNCTIONAL_TEST == 1)); then
+	sudo -E $rootdir/autotest.sh "$conf"
+fi
 $rootdir/autopackage.sh "$conf"
diff --git a/autotest.sh b/autotest.sh
index 1b2f2f6cb..554b8e3fb 100755
--- a/autotest.sh
+++ b/autotest.sh
@@ -50,7 +50,7 @@ if [ $(uname -s) = Linux ]; then
 	fi
 fi
 
-trap "process_core; autotest_cleanup; exit 1" SIGINT SIGTERM EXIT
+trap "process_core || :; autotest_cleanup; exit 1" SIGINT SIGTERM EXIT
 
 timing_enter autotest
 
@@ -152,11 +152,6 @@ timing_enter afterboot
 timing_exit afterboot
 
 if [[ $SPDK_TEST_CRYPTO -eq 1 || $SPDK_TEST_REDUCE -eq 1 ]]; then
-	# Make sure that memory is distributed across all NUMA nodes - by default, all goes to
-	# node0, but if QAT devices are attached to a different node, all of their VFs will end
-	# up under that node too and memory needs to be available there for the tests.
-	CLEAR_HUGE=yes HUGE_EVEN_ALLOC=yes ./scripts/setup.sh
-	./scripts/setup.sh status
 	if [[ $SPDK_TEST_USE_IGB_UIO -eq 1 ]]; then
 		./scripts/qat_setup.sh igb_uio
 	else
@@ -194,11 +189,16 @@ if [ $SPDK_RUN_FUNCTIONAL_TEST -eq 1 ]; then
 		run_test "bdev_raid" test/bdev/bdev_raid.sh
 		run_test "bdevperf_config" test/bdev/bdevperf/test_config.sh
 		if [[ $(uname -s) == Linux ]]; then
-			run_test "spdk_dd" test/dd/dd.sh
 			run_test "reactor_set_interrupt" test/interrupt/reactor_set_interrupt.sh
 		fi
 	fi
 
+	if [[ $(uname -s) == Linux ]]; then
+		if [[ $SPDK_TEST_BLOCKDEV -eq 1 || $SPDK_TEST_URING -eq 1 ]]; then
+			run_test "spdk_dd" test/dd/dd.sh
+		fi
+	fi
+
 	if [ $SPDK_TEST_JSON -eq 1 ]; then
 		run_test "test_converter" test/config_converter/test_converter.sh
 	fi
@@ -210,6 +210,9 @@ if [ $SPDK_RUN_FUNCTIONAL_TEST -eq 1 ]; then
 		if [[ $SPDK_TEST_NVME_PMR -eq 1 ]]; then
 			run_test "nvme_pmr" test/nvme/nvme_pmr.sh
 		fi
+		if [[ $SPDK_TEST_NVME_SCC -eq 1 ]]; then
+			run_test "nvme_scc" test/nvme/nvme_scc.sh
+		fi
 		if [[ $SPDK_TEST_NVME_CUSE -eq 1 ]]; then
 			run_test "nvme_cuse" test/nvme/cuse/nvme_cuse.sh
 		fi
diff --git a/configure b/configure
index 3a785ce94..6bd6bf17b 100755
--- a/configure
+++ b/configure
@@ -35,6 +35,7 @@ function usage() {
 	echo " --disable-tests           Disable building of functional tests"
 	echo " --disable-unit-tests      Disable building of unit tests"
 	echo " --disable-examples        Disable building of examples"
+	echo " --disable-apps            Disable building of apps"
 	echo ""
 	echo "Specifying Dependencies:"
 	echo "--with-DEPENDENCY[=path]   Use the given dependency. Optionally, provide the"
@@ -98,6 +99,8 @@ function usage() {
 	echo "                           No path required."
 	echo " wpdk                      Build using WPDK to provide support for Windows (experimental)."
 	echo "                           The argument must be a directory containing lib and include."
+	echo " usdt                      Build with userspace DTrace probes enabled."
+	echo "                           No path required."
 	echo ""
 	echo "Environment variables:"
 	echo ""
@@ -163,7 +166,7 @@ fi
 
 #check nasm only on x86
 if [[ $arch == x86_64* ]]; then
-	ver=$(nasm -v 2> /dev/null | awk '{print $3}')
+	ver=$(nasm -v 2> /dev/null | awk '{print $3}' | awk -Fr '{print $1}')
 	if lt "$ver" 2.14; then
 		# ISA-L, compression & crypto require NASM version 2.14 or newer.
 		CONFIG[ISAL]=n
@@ -263,6 +266,12 @@ for i in "$@"; do
 		--disable-examples)
 			CONFIG[EXAMPLES]=n
 			;;
+		--enable-apps)
+			CONFIG[APPS]=y
+			;;
+		--disable-apps)
+			CONFIG[APPS]=N
+			;;
 		--enable-werror)
 			CONFIG[WERROR]=y
 			;;
@@ -275,6 +284,17 @@ for i in "$@"; do
 		--disable-cet)
 			CONFIG[CET]=n
 			;;
+		--with-dpdk)
+			if pkg-config --exists libdpdk; then
+				CONFIG[DPDK_LIB_DIR]=$(pkg-config --variable=libdir libdpdk)
+				CONFIG[DPDK_INC_DIR]=$(pkg-config --variable=includedir libdpdk)
+				CONFIG[DPDK_PKG_CONFIG]=y
+				CFLAGS="${CFLAGS:+$CFLAGS }$(pkg-config --cflags libdpdk)"
+			else
+				echo "libdpdk.pc not found, aborting"
+				exit 1
+			fi
+			;;
 		--with-dpdk=*)
 			check_dir "$i"
 			CONFIG[DPDK_DIR]=$(readlink -f ${i#*=})
@@ -452,6 +472,12 @@ for i in "$@"; do
 		--without-idxd)
 			CONFIG[IDXD]=n
 			;;
+		--with-usdt)
+			CONFIG[USDT]=y
+			;;
+		--without-usdt)
+			CONFIG[USDT]=n
+			;;
 		--)
 			break
 			;;
@@ -472,7 +498,7 @@ BUILD_CMD+=(-I/usr/local/include -L/usr/local/lib)
 
 if [[ "${CONFIG[VFIO_USER]}" = "y" ]]; then
 
-	if ! hash cmake; then
+	if ! bash -c "command -v cmake3 cmake" > /dev/null; then
 		echo "ERROR: --with-vfio-user requires cmake"
 		echo "Please install then re-run this script"
 		exit 1
@@ -520,7 +546,7 @@ fi
 if [ -z "${CONFIG[ENV]}" ]; then
 	CONFIG[ENV]=$rootdir/lib/env_dpdk
 	echo "Using default SPDK env in ${CONFIG[ENV]}"
-	if [ -z "${CONFIG[DPDK_DIR]}" ]; then
+	if [[ -z "${CONFIG[DPDK_DIR]}" && "${CONFIG[DPDK_PKG_CONFIG]}" == n ]]; then
 		if [ ! -f "$rootdir"/dpdk/config/meson.build ]; then
 			echo "DPDK not found; please specify --with-dpdk=<path> or run:"
 			echo
@@ -532,7 +558,7 @@ if [ -z "${CONFIG[ENV]}" ]; then
 		fi
 	fi
 else
-	if [ -n "${CONFIG[DPDK_DIR]}" ]; then
+	if [[ -n "${CONFIG[DPDK_DIR]}" || "${CONFIG[DPDK_PKG_CONFIG]}" == y ]]; then
 		echo "--with-env and --with-dpdk are mutually exclusive."
 		exit 1
 	fi
@@ -549,6 +575,14 @@ else
 	CONFIG[VIRTIO]="n"
 fi
 
+if [[ "${CONFIG[DPDK_PKG_CONFIG]}" == y ]]; then
+	if [[ "${CONFIG[SHARED]}" == n ]]; then
+		# dpdk-devel doesn't provide static libs
+		echo "Build against packaged DPDK requested, enabling shared libraries"
+		CONFIG[SHARED]=y
+	fi
+fi
+
 if [[ $sys_name == "Windows" ]]; then
 	if [ -z "${CONFIG[WPDK_DIR]}" ]; then
 		if [ ! -f "$rootdir"/wpdk/Makefile ]; then
@@ -830,6 +864,20 @@ if [[ "${CONFIG[ISAL]}" = "y" ]]; then
 	cd $rootdir
 fi
 
+# For ARM Neoverse-N1 platform, debug build needs gcc version newer than 8.4
+if [[ "${CONFIG[DEBUG]}" = "y" && $arch = aarch64* && "$CC_TYPE" = "gcc" ]]; then
+	GCC_VERSION=$($CC -dumpfullversion)
+	PART_NUM=$(grep -i -m 1 "CPU part" /proc/cpuinfo | awk '{print $4}')
+
+	if [[ "$(printf '%s\n' "8.4.0" "$GCC_VERSION" | sort -V | head -n1)" != "8.4.0" ]]; then
+		if [[ $PART_NUM = 0xd0c ]]; then
+			echo "WARNING: For ARM Neoverse-N1 platform, debug build needs GCC version newer than 8.4."
+			echo "         Will work around this by using armv8.2-a+crypto as target architecture for now."
+			CONFIG[ARCH]=armv8.2-a+crypto
+		fi
+	fi
+fi
+
 # We are now ready to generate final configuration. But first do sanity
 # check to see if all keys in CONFIG array have its reflection in CONFIG file.
 if (($(grep -cE "^\s*CONFIG_[[:alnum:]_]+=" "$rootdir/CONFIG") != ${#CONFIG[@]})); then
@@ -837,7 +885,7 @@ if (($(grep -cE "^\s*CONFIG_[[:alnum:]_]+=" "$rootdir/CONFIG") != ${#CONFIG[@]})
 	echo "BUG: Some configuration options are not present in CONFIG file. Please update this file."
 	echo "Missing options in CONFIG (+) file and in current config (-): "
 	diff -u --label "CONFIG file" --label "CONFIG[@]" \
-		<(sed -r -e '/^\s*$/d; /^\s*#.*/d; s/(CONFIG_[[:alnum:]_]+)=.*/\1/g' CONFIG | sort) \
+		<(sed -r -e '/^[[:space:]]*$/d; /^[[:space:]]*#.*/d; s/(CONFIG_[[:alnum:]_]+)=.*/\1/g' CONFIG | sort) \
 		<(printf "CONFIG_%s\n" "${!CONFIG[@]}" | sort)
 	exit 1
 fi
@@ -845,7 +893,7 @@ fi
 echo -n "Creating mk/config.mk..."
 cp -f $rootdir/CONFIG $rootdir/mk/config.mk
 for key in "${!CONFIG[@]}"; do
-	sed -i.bak -r "s#^\s*CONFIG_${key}=.*#CONFIG_${key}\?=${CONFIG[$key]}#g" $rootdir/mk/config.mk
+	sed -i.bak -r "s#[[:space:]]*CONFIG_${key}=.*#CONFIG_${key}\?=${CONFIG[$key]}#g" $rootdir/mk/config.mk
 done
 # On FreeBSD sed -i 'SUFFIX' - SUFFIX is mandatory. So no way but to delete the backed file.
 rm -f $rootdir/mk/config.mk.bak
diff --git a/doc/Doxyfile b/doc/Doxyfile
index 7ecc216aa..c32c98622 100644
--- a/doc/Doxyfile
+++ b/doc/Doxyfile
@@ -843,6 +843,7 @@ INPUT                  += \
                          spdk_top.md \
                          ssd_internals.md \
                          system_configuration.md \
+                         usdt.md \
                          userspace.md \
                          vagrant.md \
                          vhost.md \
diff --git a/doc/event.md b/doc/event.md
index 657ca93e1..c0eb975e3 100644
--- a/doc/event.md
+++ b/doc/event.md
@@ -73,3 +73,11 @@ spdk_app_start() is called, it will block the current thread until the applicati
 terminates by calling spdk_app_stop() or an error condition occurs during the
 initialization code within spdk_app_start(), itself, before invoking the caller's
 supplied function.
+
+## Custom shutdown callback {#event_component_shutdown}
+
+When creating SPDK based application user may add custom shutdown callback which
+will be called before the application framework starts the shutdown process.
+To do that set shutdown_cb function callback in spdk_app_opts structure passed
+to spdk_app_start(). Custom shutdown callback should call spdk_app_stop() before
+returning to continue application shutdown process.
diff --git a/doc/jsonrpc.md b/doc/jsonrpc.md
index e5e4d748a..021a45658 100644
--- a/doc/jsonrpc.md
+++ b/doc/jsonrpc.md
@@ -2107,6 +2107,7 @@ Name                    | Optional | Type        | Description
 ----------------------- | -------- | ----------- | -----------
 name                    | Required | string      | Bdev name to use
 mode                    | Required | string      | OCF cache mode: wb, wt, pt, wa, wi, wo
+cache_line_size         | Optional | int         | OCF cache line size in KiB: 4, 8, 16, 32, 64
 cache_bdev_name         | Required | string      | Name of underlying cache bdev
 core_bdev_name          | Required | string      | Name of underlying core bdev
 
@@ -2123,7 +2124,8 @@ Example request:
   "params": {
     "name": "ocf0",
     "mode": "wt",
-    "cache_bdev_name": "Nvme0n1"
+    "cache_line_size": 64,
+    "cache_bdev_name": "Nvme0n1",
     "core_bdev_name": "aio0"
   },
   "jsonrpc": "2.0",
@@ -3301,6 +3303,147 @@ Example response:
 }
 ~~~
 
+## bdev_rbd_register_cluster {#rpc_bdev_rbd_register_cluster}
+
+This method is available only if SPDK was build with Ceph RBD support.
+
+### Parameters
+
+Name                    | Optional | Type        | Description
+----------------------- | -------- | ----------- | -----------
+name                    | Required | string      | Registerd Rados cluster object name
+user_id                 | Optional | string      | Ceph ID (i.e. admin, not client.admin)
+config_param            | Optional | string map  | Explicit librados configuration
+config_file             | Optional | string      | File path of libraodos configuration file
+
+This RPC registers a Rados Cluster object handle which is only known
+to rbd module, it uses user_id + config_param or user_id + config_file to
+identify a Rados cluster object.
+
+If no config_param is specified, Ceph configuration files must exist with
+all relevant settings for accessing the Ceph cluster. If a config map is
+passed, the configuration files are ignored and instead all key/value
+pairs are passed to rados_conf_set to configure cluster access. In
+practice, "mon_host" (= list of monitor address+port) and "key" (= the
+secret key stored in Ceph keyrings) are enough.
+
+When accessing the Ceph cluster as some user other than "admin" (the
+default), the "user_id" has to be set.
+
+### Result
+
+Name of newly created Rados cluster object.
+
+### Example
+
+Example request with `key` from `/etc/ceph/ceph.client.admin.keyring`:
+
+~~
+{
+  "params": {
+    "name": "rbd_cluster",
+    "config_param": {
+      "mon_host": "192.168.7.1:6789,192.168.7.2:6789",
+      "key": "AQDwf8db7zR1GRAA5k7NKXjS5S5V4mntwUDnGQ==",
+    }
+  },
+  "jsonrpc": "2.0",
+  "method": "bdev_rbd_register_cluster",
+  "id": 1
+}
+~~
+
+Example response:
+
+~~
+response:
+{
+  "jsonrpc": "2.0",
+  "id": 1,
+  "result": "rbd_cluster"
+}
+~~
+
+## bdev_rbd_unregister_cluster {#rpc_bdev_rbd_unregister_cluster}
+
+This method is available only if SPDK was build with Ceph RBD support.
+If there is still rbd bdev using this cluster, the unregisteration operation
+will fail.
+
+### Result
+
+`true` if Rados cluster object with provided name was deleted or `false` otherwise.
+
+### Parameters
+
+Name                    | Optional | Type        | Description
+----------------------- | -------- | ----------- | -------------------------
+name                    | Required | string      | Rados cluster object name
+
+### Example
+
+Example request:
+
+~~
+{
+  "params": {
+    "name": "rbd_cluster"
+  },
+  "jsonrpc": "2.0",
+  "method": "bdev_rbd_unregister_cluster",
+  "id": 1
+}
+~~
+
+Example response:
+
+~~
+{
+  "jsonrpc": "2.0",
+  "id": 1,
+  "result": true
+}
+~~
+
+## bdev_rbd_get_clusters_info {#rpc_bdev_rbd_get_clusters_info}
+
+This method is available only if SPDK was build with Ceph RBD support.
+
+### Result
+
+Returns the cluster info of the Rados Cluster name if provided. Otherwise, it
+returns the cluster info of every registered Raods Cluster name.
+
+### Parameters
+
+Name                    | Optional | Type        | Description
+----------------------- | -------- | ----------- | -------------------------
+name                    | Optional | string      | Rados cluster object name
+
+### Example
+
+Example request:
+
+~~
+{
+  "params": {
+    "name": "rbd_cluster"
+  },
+  "jsonrpc": "2.0",
+  "method": "bdev_rbd_get_clusters_info",
+  "id": 1
+}
+~~
+
+Example response:
+
+~~
+{
+  "jsonrpc": "2.0",
+  "cluster_name": "rbd_cluster"
+}
+~~
+
 ## bdev_rbd_create {#rpc_bdev_rbd_create}
 
 Create @ref bdev_config_rbd bdev
@@ -3317,6 +3460,7 @@ pool_name               | Required | string      | Pool name
 rbd_name                | Required | string      | Image name
 block_size              | Required | number      | Block size
 config                  | Optional | string map  | Explicit librados configuration
+cluster_name            | Optional | string      | Rados cluster object name created in this module.
 
 If no config is specified, Ceph configuration files must exist with
 all relevant settings for accessing the pool. If a config map is
@@ -3328,6 +3472,10 @@ secret key stored in Ceph keyrings) are enough.
 When accessing the image as some user other than "admin" (the
 default), the "user_id" has to be set.
 
+If provided with cluster_name option, it will use the Rados cluster object
+referenced by the name (created by bdev_rbd_register_cluster RPC) and ignores
+"user_id + config" combination to create its own Rados cluster.
+
 ### Result
 
 Name of newly created bdev.
@@ -3364,6 +3512,33 @@ response:
 }
 ~~~
 
+Example request with `cluster_name`:
+
+~~
+{
+  "params": {
+    "pool_name": "rbd",
+    "rbd_name": "foo",
+    "block_size": 4096,
+    "cluster_name": "rbd_cluster"
+  },
+  "jsonrpc": "2.0",
+  "method": "bdev_rbd_create",
+  "id": 1
+}
+~~
+
+Example response:
+
+~~
+response:
+{
+  "jsonrpc": "2.0",
+  "id": 1,
+  "result": "Ceph0"
+}
+~~
+
 ## bdev_rbd_delete {#rpc_bdev_rbd_delete}
 
 Delete @ref bdev_config_rbd bdev
@@ -4443,6 +4618,9 @@ error_recovery_level            | Optional | number  | Session specific paramete
 allow_duplicated_isid           | Optional | boolean | Allow duplicated initiator session ID (default: `false`)
 max_large_datain_per_connection | Optional | number  | Max number of outstanding split read I/Os per connection (default: 64)
 max_r2t_per_connection          | Optional | number  | Max number of outstanding R2Ts per connection (default: 4)
+pdu_pool_size                   | Optional | number  | Number of PDUs in the pool (default: approximately 2 * max_sessions * (max_queue_depth + max_connections_per_session))
+immediate_data_pool_size        | Optional | number  | Number of immediate data buffers in the pool (default: 128 * max_sessions)
+data_out_pool_size              | Optional | number  | Number of data out buffers in the pool (default: 16 * max_sessions)
 
 To load CHAP shared secret file, its path is required to specify explicitly in the parameter `auth_file`.
 
@@ -5909,6 +6087,8 @@ model_number            | Optional | string      | Model number of virtual contr
 max_namespaces          | Optional | number      | Maximum number of namespaces that can be attached to the subsystem. Default: 0 (Unlimited)
 allow_any_host          | Optional | boolean     | Allow any host (`true`) or enforce allowed host list (`false`). Default: `false`.
 ana_reporting           | Optional | boolean     | Enable ANA reporting feature (default: `false`).
+min_cntlid              | Optional | number      | Minimum controller ID. Default: 1
+max_cntlid              | Optional | number      | Maximum controller ID. Default: 0xffef
 
 ### Example
 
@@ -6593,6 +6773,7 @@ tgt_name                    | Optional | string      | Parent NVMe-oF target nam
 ### Response
 
 The response is an object containing NVMf subsystem statistics.
+In the response, `admin_qpairs` and `io_qpairs` are reflecting cumulative queue pair counts while `current_admin_qpairs` and `current_io_qpairs` are showing the current number.
 
 ### Example
 
@@ -6617,6 +6798,8 @@ Example response:
         "name": "app_thread",
         "admin_qpairs": 1,
         "io_qpairs": 4,
+        "current_admin_qpairs": 1,
+        "current_io_qpairs": 2,
         "pending_bdev_io": 1721,
         "transports": [
           {
@@ -6660,6 +6843,22 @@ Example response:
 }
 ~~~
 
+
+## nvmf_set_crdt {#rpc_nvmf_set_crdt}
+
+Set the 3 CRDT (Command Retry Delay Time) values. For details about
+CRDT, please refer to the NVMe specification. Currently all the
+SPDK nvmf subsystems share the same CRDT values. The default values
+are 0. This rpc can only be invoked in STARTUP stage. All values are
+in units of 100 milliseconds (same as the NVMe specification).
+
+### Parameters
+Name                    | Optional | Type        | Description
+----------------------- | -------- | ----------- | -----------
+crdt1                   | Optional | number      | Command Retry Delay Time 1
+crdt2                   | Optional | number      | Command Retry Delay Time 2
+crdt3                   | Optional | number      | Command Retry Delay Time 3
+
 # Vhost Target {#jsonrpc_components_vhost_tgt}
 
 The following common preconditions need to be met in all target types.
@@ -7667,7 +7866,7 @@ Example request:
       "Malloc2",
       "Malloc3"
     ],
-    "strip_size": 4096
+    "strip_size_kb": 4
   }
 }
 ~~~
diff --git a/doc/misc.md b/doc/misc.md
index a290afe94..23796c61b 100644
--- a/doc/misc.md
+++ b/doc/misc.md
@@ -2,3 +2,4 @@
 
 - @subpage peer_2_peer
 - @subpage containers
+- @subpage rpms
diff --git a/doc/nvmf_tracing.md b/doc/nvmf_tracing.md
index f23659fe8..71d540724 100644
--- a/doc/nvmf_tracing.md
+++ b/doc/nvmf_tracing.md
@@ -174,11 +174,13 @@ SPDK_TRACE_REGISTER_FN(nvmf_trace)
 	spdk_trace_register_object(OBJECT_NVMF_RDMA_IO, 'r');
 	spdk_trace_register_description("RDMA_REQ_NEW", "",
 					TRACE_RDMA_REQUEST_STATE_NEW,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 1, 1, "cmid:	");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 1,
+					SPDK_TRACE_ARG_TYPE_PTR, "cmid:	");
 	...
 	spdk_trace_register_description("NEW_RDMA_REQ_NAME", "",
 					NEW_TRACE_POINT_NAME,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:	");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "cmid:	");
 }
 ~~~
 
diff --git a/doc/performance_reports.md b/doc/performance_reports.md
index efd68d8eb..3b4617568 100644
--- a/doc/performance_reports.md
+++ b/doc/performance_reports.md
@@ -1,5 +1,11 @@
 # Performance Reports {#performance_reports}
 
+## Release 21.04
+
+- [SPDK 21.04 NVMe-oF TCP Performance Report](https://ci.spdk.io/download/performance-reports/SPDK_tcp_perf_report_2104.pdf)
+- [SPDK 21.04 NVMe-oF RDMA Performance Report](https://ci.spdk.io/download/performance-reports/SPDK_rdma_perf_report_2104.pdf)
+- [SPDK 21.04 Vhost Performance Report](https://ci.spdk.io/download/performance-reports/SPDK_vhost_perf_report_2104.pdf)
+
 ## Release 21.01
 
 - [SPDK 21.01 NVMe Bdev Performance Report](https://ci.spdk.io/download/performance-reports/SPDK_nvme_bdev_perf_report_2101.pdf)
diff --git a/doc/usdt.md b/doc/usdt.md
new file mode 100644
index 000000000..08c258076
--- /dev/null
+++ b/doc/usdt.md
@@ -0,0 +1,204 @@
+# Userspace DTrace (USDT) {#usdt}
+
+## Package Dependencies
+
+These dependencies are needed for building bpftrace and
+the sys/sdt.h header file that SPDK libraries will include
+for DTRACE_PROBE macro definitions.
+
+Fedora:
+libbpf
+gtest-devel
+gmock-devel
+bcc-devel
+systemtap-sdt-devel
+llvm-devel
+bison
+flex
+
+Ubuntu:
+systemtap-sdt-dev
+libbpfcc-dev
+libclang-7-dev
+bison
+flex
+
+## Building bpftrace
+
+We have found issues with the packaged bpftrace on both Ubuntu 20.04
+and Fedora 33.  So bpftrace should be built and installed from source.
+
+```
+git clone https://github.com/iovisor/bpftrace.git
+mkdir bpftrace/build
+cd bpftrace/build
+cmake -DCMAKE_BUILD_TYPE=Release ..
+make
+sudo make install
+```
+
+## bpftrace.sh
+
+bpftrace.sh is a helper script that facilitates running bpftrace scripts
+against a running SPDK application.  Here is a typical usage:
+
+```
+scripts/bpftrace.sh `pidof spdk_tgt` scripts/bpf/nvmf.bt
+```
+
+Attaching to USDT probes requires the full path of the binary in the
+probe description. SPDK bpftrace scripts can be written with an __EXE__
+marker instead of a full path name, and bpftrace.sh will dynamically
+replace that string with the full path name using information from procfs.
+
+It is also useful to filter certain kernel events (such as system calls)
+based on the PID of the SPDK application.  SPDK bpftrace scripts can be
+written with a __PID__ marker, and bpftrace.sh will dynamically replace
+that string with the PID provided to the script.
+
+## Configuring SPDK Build
+
+```
+./configure --with-usdt
+```
+
+## Start SPDK application and bpftrace script
+
+From first terminal:
+
+```
+build/bin/spdk_tgt -m 0xC
+```
+
+From second terminal:
+
+```
+scripts/bpftrace.sh `pidof spdk_tgt` scripts/bpf/nvmf.bt
+```
+
+nvmf.bt will print information about nvmf subsystem and poll
+group info state transitions.
+
+From third terminal:
+
+```
+scripts/rpc.py <<EOF
+nvmf_create_transport -t tcp
+nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -m 10
+nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 127.0.0.1 -s 4420
+bdev_null_create null0 1000 512
+nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 null0
+EOF
+```
+
+This creates the nvmf tcp transport, a new nvmf subsystem that listens on a tcp
+port, and a null bdev which is added as a namespace to the new nvmf subsystem.
+
+You will see output from the second terminal that looks like this:
+
+```
+2110.935735: nvmf_tgt reached state NONE
+2110.954316: nvmf_tgt reached state CREATE_TARGET
+2110.967905: nvmf_tgt reached state CREATE_POLL_GROUPS
+2111.235982: nvmf_tgt reached state START_SUBSYSTEMS
+2111.253560: nqn.2014-08.org.nvmexpress.discovery change state from INACTIVE to ACTIVE start
+2111.260278: nqn.2014-08.org.nvmexpress.discovery on thread 2 state to ACTIVE start
+2111.264281: nqn.2014-08.org.nvmexpress.discovery on thread 2 state to ACTIVE done
+2111.284083: nqn.2014-08.org.nvmexpress.discovery change state from INACTIVE to ACTIVE done
+2111.289197: nvmf_tgt reached state RUNNING
+2111.271573: nqn.2014-08.org.nvmexpress.discovery on thread 3 state to ACTIVE start
+2111.279787: nqn.2014-08.org.nvmexpress.discovery on thread 3 state to ACTIVE done
+2189.921492: nqn.2016-06.io.spdk:cnode1 change state from INACTIVE to ACTIVE start
+2189.952508: nqn.2016-06.io.spdk:cnode1 on thread 2 state to ACTIVE start
+2189.959125: nqn.2016-06.io.spdk:cnode1 on thread 2 state to ACTIVE done
+2190.005832: nqn.2016-06.io.spdk:cnode1 change state from INACTIVE to ACTIVE done
+2189.969058: nqn.2016-06.io.spdk:cnode1 on thread 3 state to ACTIVE start
+2189.999889: nqn.2016-06.io.spdk:cnode1 on thread 3 state to ACTIVE done
+2197.859104: nqn.2016-06.io.spdk:cnode1 change state from ACTIVE to PAUSED start
+2197.879199: nqn.2016-06.io.spdk:cnode1 on thread 2 state to PAUSED start
+2197.883416: nqn.2016-06.io.spdk:cnode1 on thread 2 state to PAUSED done
+2197.902291: nqn.2016-06.io.spdk:cnode1 change state from ACTIVE to PAUSED done
+2197.908939: nqn.2016-06.io.spdk:cnode1 change state from PAUSED to ACTIVE start
+2197.912644: nqn.2016-06.io.spdk:cnode1 on thread 2 state to ACTIVE start
+2197.927409: nqn.2016-06.io.spdk:cnode1 on thread 2 state to ACTIVE done
+2197.949742: nqn.2016-06.io.spdk:cnode1 change state from PAUSED to ACTIVE done
+2197.890812: nqn.2016-06.io.spdk:cnode1 on thread 3 state to PAUSED start
+2197.897233: nqn.2016-06.io.spdk:cnode1 on thread 3 state to PAUSED done
+2197.931278: nqn.2016-06.io.spdk:cnode1 on thread 3 state to ACTIVE start
+2197.946124: nqn.2016-06.io.spdk:cnode1 on thread 3 state to ACTIVE done
+2205.859904: nqn.2016-06.io.spdk:cnode1 change state from ACTIVE to PAUSED start
+2205.891392: nqn.2016-06.io.spdk:cnode1 on thread 2 state to PAUSED start
+2205.896588: nqn.2016-06.io.spdk:cnode1 on thread 2 state to PAUSED done
+2205.920133: nqn.2016-06.io.spdk:cnode1 change state from ACTIVE to PAUSED done
+2205.905900: nqn.2016-06.io.spdk:cnode1 on thread 3 state to PAUSED start
+2205.914856: nqn.2016-06.io.spdk:cnode1 on thread 3 state to PAUSED done
+2206.091084: nqn.2016-06.io.spdk:cnode1 change state from PAUSED to ACTIVE start
+2206.099222: nqn.2016-06.io.spdk:cnode1 on thread 2 state to ACTIVE start
+2206.105445: nqn.2016-06.io.spdk:cnode1 on thread 2 state to ACTIVE done
+2206.119271: nqn.2016-06.io.spdk:cnode1 change state from PAUSED to ACTIVE done
+2206.109144: nqn.2016-06.io.spdk:cnode1 on thread 3 state to ACTIVE start
+2206.115636: nqn.2016-06.io.spdk:cnode1 on thread 3 state to ACTIVE done
+```
+
+Now stop the bpftrace.sh running in the second terminal, and start
+it again with the send_msg.bt script.  This script keeps a count of
+functions executed as part of an spdk_for_each_channel or
+spdk_thread_send_msg function call.
+
+```
+scripts/bpftrace.sh `pidof spdk_tgt` scripts/bpf/send_msg.bt
+```
+
+From the third terminal, create another null bdev and add it as a
+namespace to the cnode1 subsystem.
+
+```
+scripts/rpc.py <<EOF
+bdev_null_create null1 1000 512
+nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 null1
+EOF
+```
+
+Now Ctrl-C the bpftrace.sh in the second terminal, and it will
+print the final results of the maps.
+
+```
+@for_each_channel[subsystem_state_change_on_pg]: 2
+
+@send_msg[_finish_unregister]: 1
+@send_msg[_call_completion]: 2
+@send_msg[put_io_channel]: 4
+@send_msg[_call_channel]: 4
+```
+
+## TODOs and known limitations
+
+- add definitions for common nvmf data structures - then we can pass the subsystem pointer
+  itself as a probe parameter and let the script decide which fields it wants to access
+  (Note: these would need to be kept up-to-date with the C definitions of the struct - it is
+  not possible to include the header files in a bpftrace script)
+
+- investigate using pahole to generate data structure definitions that can be included in
+  bpftrace scripts; this would allow us to pass the subsystem pointer itself as a probe
+  argument, and let the script decide which fields it wants to access; for example,
+  `pahole -E -C spdk_nvmf_subsystem build/bin/spdk_tgt` gets us close to what we need,
+  but there are some limiters:
+
+	- our structures have char arrays (not char pointers) for things like subnqn; large
+	  arrays like these cannot currently be passed to bpftrace printf without generating
+	  a stack space error (probe points are limited to 512 bytes of stack); we could
+	  modify SPDK to have the char array for storage, and a char pointer that points to
+	  that storage, the latter could easily then be used in bpftrace scripts
+	- our structures include fields with their enum types instead of int; bpftrace will
+	  complain it does not know about the enum (pahole doesn't print out enum
+	  descriptions); information on enums can be found in the applications .debug_info
+	  section, but we would need something that can convert that into a file we can
+	  include in a bpftrace script
+
+- Note that bpftrace prints are not always printed in exact chronological order; this can
+  be seen especially with spdk_for_each_channel iterations, where we execute trace points
+  on multiple threads in a very short period of time, and those may not get printed to the
+  console in exact order; this is why the nvmf.bt script prints out a msec.nsec timestamp
+  so that the user can understand the ordering and even pipe through sort if desired
+
+- flesh out more DTrace probes in the nvmf code
diff --git a/doc/user_guides.md b/doc/user_guides.md
index adf4465e6..741f7ba10 100644
--- a/doc/user_guides.md
+++ b/doc/user_guides.md
@@ -11,3 +11,4 @@
 - @subpage blobfs
 - @subpage jsonrpc
 - @subpage jsonrpc_proxy
+- @subpage usdt
diff --git a/doc/vhost.md b/doc/vhost.md
index a44454fd3..6282bbc2d 100644
--- a/doc/vhost.md
+++ b/doc/vhost.md
@@ -280,9 +280,9 @@ host:~# taskset -c 2,3 qemu-system-x86_64 \
   -drive file=guest_os_image.qcow2,if=none,id=disk \
   -device ide-hd,drive=disk,bootindex=0 \
   -chardev socket,id=spdk_vhost_scsi0,path=/var/tmp/vhost.0 \
-  -device vhost-user-scsi-pci,id=scsi0,chardev=spdk_vhost_scsi0,num_queues=4 \
+  -device vhost-user-scsi-pci,id=scsi0,chardev=spdk_vhost_scsi0,num_queues=2 \
   -chardev socket,id=spdk_vhost_blk0,path=/var/tmp/vhost.1 \
-  -device vhost-user-blk-pci,chardev=spdk_vhost_blk0,num-queues=4
+  -device vhost-user-blk-pci,chardev=spdk_vhost_blk0,num-queues=2
 ~~~
 
 Please note the following two commands are run on the guest VM.
@@ -331,6 +331,10 @@ to set `num_queues=4` to saturate physical device. Adding too many queues might
 vhost performance degradation if many vhost devices are used because each device will require
 additional `num_queues` to be polled.
 
+Some Linux distributions report a kernel panic when starting the VM if the number of I/O queues
+specified via the `num-queues` parameter is greater than number of vCPUs. If you need to use
+more I/O queues than vCPUs, check that your OS image supports that configuration.
+
 ## Hot-attach/hot-detach {#vhost_hotattach}
 
 Hotplug/hotremove within a vhost controller is called hot-attach/detach. This is to
diff --git a/dpdkbuild/Makefile b/dpdkbuild/Makefile
index b540d2fa0..d4af3a0a4 100644
--- a/dpdkbuild/Makefile
+++ b/dpdkbuild/Makefile
@@ -137,7 +137,7 @@ ifeq ($(MAKE_PID),)
 MAKE_PID := $(shell echo $$PPID)
 endif
 
-MAKE_NUMJOBS := $(shell ps T | sed -nE 's/\s*$(MAKE_PID)\s.* (-j|--jobs=)( *[0-9]+).*/\1\2/p')
+MAKE_NUMJOBS := $(shell ps T | sed -nE 's/[[:space:]]*$(MAKE_PID)[[:space:]].* (-j|--jobs=)( *[0-9]+).*/\1\2/p')
 
 all: $(SPDK_ROOT_DIR)/dpdk/build-tmp
 	$(Q)# DPDK doesn't handle nested make calls, so unset MAKEFLAGS
diff --git a/examples/Makefile b/examples/Makefile
index fa7c0d9b5..098814b1b 100644
--- a/examples/Makefile
+++ b/examples/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-DIRS-y += accel bdev blob ioat nvme sock vmd nvmf
+DIRS-y += accel bdev blob ioat nvme sock vmd nvmf util
 
 ifeq ($(OS),Linux)
 DIRS-$(CONFIG_VHOST) += interrupt_tgt
diff --git a/examples/accel/perf/Makefile b/examples/accel/perf/Makefile
index 53b9ae6cb..f1983d98f 100644
--- a/examples/accel/perf/Makefile
+++ b/examples/accel/perf/Makefile
@@ -39,6 +39,6 @@ APP = accel_perf
 
 C_SRCS := accel_perf.c
 
-SPDK_LIB_LIST = $(ACCEL_MODULES_LIST) event_accel
+SPDK_LIB_LIST = $(ACCEL_MODULES_LIST) event event_accel
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/examples/accel/perf/accel_perf.c b/examples/accel/perf/accel_perf.c
index 0dce1af4e..8fc139b94 100644
--- a/examples/accel/perf/accel_perf.c
+++ b/examples/accel/perf/accel_perf.c
@@ -44,11 +44,16 @@
 #define DATA_PATTERN 0x5a
 #define ALIGN_4K 0x1000
 
+static bool g_using_sw_engine = false;
 static uint64_t	g_tsc_rate;
 static uint64_t g_tsc_end;
 static int g_rc;
 static int g_xfer_size_bytes = 4096;
 static int g_queue_depth = 32;
+/* g_allocate_depth indicates how many tasks we allocate per worker. It will
+ * be at least as much as the queue depth.
+ */
+static int g_allocate_depth = 0;
 static int g_ops_per_batch = 0;
 static int g_threads_per_core = 1;
 static int g_time_in_sec = 5;
@@ -62,7 +67,6 @@ static enum accel_capability g_workload_selection;
 static struct worker_thread *g_workers = NULL;
 static int g_num_workers = 0;
 static pthread_mutex_t g_workers_lock = PTHREAD_MUTEX_INITIALIZER;
-uint64_t g_capabilites;
 
 struct worker_thread;
 static void accel_done(void *ref, int status);
@@ -130,6 +134,7 @@ dump_user_config(struct spdk_app_opts *opts)
 	}
 	printf("Transfer size:  %u bytes\n", g_xfer_size_bytes);
 	printf("Queue depth:    %u\n", g_queue_depth);
+	printf("Allocate depth: %u\n", g_allocate_depth);
 	printf("# threads/core: %u\n", g_threads_per_core);
 	printf("Run time:       %u seconds\n", g_time_in_sec);
 	if (g_ops_per_batch > 0) {
@@ -157,38 +162,67 @@ usage(void)
 	printf("\t[-f for fill workload, use this BYTE value (default 255)\n");
 	printf("\t[-y verify result if this switch is on]\n");
 	printf("\t[-b batch this number of operations at a time (default 0 = disabled)]\n");
+	printf("\t[-a tasks to allocate per core (default: same value as -q)]\n");
+	printf("\t\tCan be used to spread operations across a wider range of memory.\n");
 }
 
 static int
 parse_args(int argc, char *argv)
 {
+	int argval;
+
 	switch (argc) {
+	case 'a':
 	case 'b':
-		g_ops_per_batch = spdk_strtol(optarg, 10);
+	case 'C':
+	case 'f':
+	case 'T':
+	case 'o':
+	case 'P':
+	case 'q':
+	case 's':
+	case 't':
+		argval = spdk_strtol(optarg, 10);
+		if (argval < 0) {
+			fprintf(stderr, "-%c option must be non-negative.\n", argc);
+			usage();
+			return 1;
+		}
+		break;
+	default:
+		break;
+	};
+
+	switch (argc) {
+	case 'a':
+		g_allocate_depth = argval;
+		break;
+	case 'b':
+		g_ops_per_batch = argval;
 		break;
 	case 'C':
-		g_crc32c_chained_count = spdk_strtol(optarg, 10);
+		g_crc32c_chained_count = argval;
 		break;
 	case 'f':
-		g_fill_pattern = (uint8_t)spdk_strtol(optarg, 10);
+		g_fill_pattern = (uint8_t)argval;
 		break;
 	case 'T':
-		g_threads_per_core = spdk_strtol(optarg, 10);
+		g_threads_per_core = argval;
 		break;
 	case 'o':
-		g_xfer_size_bytes = spdk_strtol(optarg, 10);
+		g_xfer_size_bytes = argval;
 		break;
 	case 'P':
-		g_fail_percent_goal = spdk_strtol(optarg, 10);
+		g_fail_percent_goal = argval;
 		break;
 	case 'q':
-		g_queue_depth = spdk_strtol(optarg, 10);
+		g_queue_depth = argval;
 		break;
 	case 's':
-		g_crc32c_seed = spdk_strtol(optarg, 10);
+		g_crc32c_seed = argval;
 		break;
 	case 't':
-		g_time_in_sec = spdk_strtol(optarg, 10);
+		g_time_in_sec = argval;
 		break;
 	case 'y':
 		g_verify = true;
@@ -318,8 +352,6 @@ _get_task(struct worker_thread *worker)
 		return NULL;
 	}
 
-	task->worker = worker;
-	task->worker->current_queue_depth++;
 	return task;
 }
 
@@ -559,20 +591,6 @@ batch_done(void *cb_arg, int status)
 	spdk_thread_send_msg(worker_batch->worker->thread, _batch_done, worker_batch);
 }
 
-static uint32_t
-_update_crc32c_iov(struct iovec *iov, int iovcnt, uint32_t crc32c)
-{
-	int i;
-
-	for (i = 0; i < iovcnt; i++) {
-		assert(iov[i].iov_base != NULL);
-		assert(iov[i].iov_len != 0);
-		crc32c = spdk_crc32c_update(iov[i].iov_base, iov[i].iov_len, crc32c);
-
-	}
-	return crc32c;
-}
-
 static void
 _accel_done(void *arg1)
 {
@@ -586,7 +604,7 @@ _accel_done(void *arg1)
 	if (g_verify && task->status == 0) {
 		switch (g_workload_selection) {
 		case ACCEL_CRC32C:
-			sw_crc32c = _update_crc32c_iov(task->iovs, task->iov_cnt, ~g_crc32c_seed);
+			sw_crc32c = spdk_crc32c_iov_update(task->iovs, task->iov_cnt, ~g_crc32c_seed);
 			if (*(uint32_t *)task->dst != sw_crc32c) {
 				SPDK_NOTICELOG("CRC-32C miscompare\n");
 				worker->xfer_failed++;
@@ -634,6 +652,8 @@ _accel_done(void *arg1)
 	worker->current_queue_depth--;
 
 	if (!worker->is_draining) {
+		TAILQ_INSERT_TAIL(&worker->tasks_pool, task, link);
+		task = _get_task(worker);
 		if (g_ops_per_batch == 0) {
 			_submit_single(worker, task);
 			worker->current_queue_depth++;
@@ -740,10 +760,11 @@ _init_thread(void *arg1)
 	int i, rc, num_batches;
 	int max_per_batch;
 	int remaining = g_queue_depth;
-	int num_tasks = g_queue_depth;
+	int num_tasks = g_allocate_depth;
 	struct accel_batch *tmp;
 	struct accel_batch *worker_batch = NULL;
 	struct display_info *display = arg1;
+	uint64_t capabilities;
 
 	worker = calloc(1, sizeof(*worker));
 	if (worker == NULL) {
@@ -764,6 +785,15 @@ _init_thread(void *arg1)
 	pthread_mutex_unlock(&g_workers_lock);
 	worker->ch = spdk_accel_engine_get_io_channel();
 
+	if (g_num_workers == 1) {
+		capabilities = spdk_accel_get_capabilities(worker->ch);
+		if ((capabilities & g_workload_selection) != g_workload_selection) {
+			g_using_sw_engine = true;
+			SPDK_WARNLOG("The selected workload is not natively supported by the current engine\n");
+			SPDK_WARNLOG("The software engine will be used instead.\n\n");
+		}
+	}
+
 	TAILQ_INIT(&worker->tasks_pool);
 
 	if (g_ops_per_batch > 0) {
@@ -818,6 +848,7 @@ _init_thread(void *arg1)
 	task = worker->task_base;
 	for (i = 0; i < num_tasks; i++) {
 		TAILQ_INSERT_TAIL(&worker->tasks_pool, task, link);
+		task->worker = worker;
 		if (_get_task_data_bufs(task)) {
 			fprintf(stderr, "Unable to get data bufs\n");
 			goto error;
@@ -848,6 +879,7 @@ _init_thread(void *arg1)
 
 			for (i = 0; i < g_ops_per_batch; i++) {
 				task = _get_task(worker);
+				worker->current_queue_depth++;
 				if (task == NULL) {
 					goto error;
 				}
@@ -877,6 +909,7 @@ _init_thread(void *arg1)
 	/* Submit as singles when no batching is enabled or we ran out of batches. */
 	for (i = 0; i < remaining; i++) {
 		task = _get_task(worker);
+		worker->current_queue_depth++;
 		if (task == NULL) {
 			goto error;
 		}
@@ -908,13 +941,16 @@ accel_done(void *cb_arg, int status)
 	assert(worker);
 
 	task->status = status;
-	spdk_thread_send_msg(worker->thread, _accel_done, task);
+	if (g_using_sw_engine == false) {
+		_accel_done(task);
+	} else {
+		spdk_thread_send_msg(worker->thread, _accel_done, task);
+	}
 }
 
 static void
 accel_perf_start(void *arg1)
 {
-	struct spdk_io_channel *accel_ch;
 	struct spdk_cpuset tmp_cpumask = {};
 	char thread_name[32];
 	uint32_t i;
@@ -922,15 +958,6 @@ accel_perf_start(void *arg1)
 	struct spdk_thread *thread;
 	struct display_info *display;
 
-	accel_ch = spdk_accel_engine_get_io_channel();
-	g_capabilites = spdk_accel_get_capabilities(accel_ch);
-	spdk_put_io_channel(accel_ch);
-
-	if ((g_capabilites & g_workload_selection) != g_workload_selection) {
-		SPDK_WARNLOG("The selected workload is not natively supported by the current engine\n");
-		SPDK_WARNLOG("The software engine will be used instead.\n\n");
-	}
-
 	g_tsc_rate = spdk_get_ticks_hz();
 	g_tsc_end = spdk_get_ticks() + g_time_in_sec * g_tsc_rate;
 
@@ -966,7 +993,7 @@ main(int argc, char **argv)
 	pthread_mutex_init(&g_workers_lock, NULL);
 	spdk_app_opts_init(&opts, sizeof(opts));
 	opts.reactor_mask = "0x1";
-	if (spdk_app_parse_args(argc, argv, &opts, "C:o:q:t:yw:P:f:b:T:", NULL, parse_args,
+	if (spdk_app_parse_args(argc, argv, &opts, "a:C:o:q:t:yw:P:f:b:T:", NULL, parse_args,
 				usage) != SPDK_APP_PARSE_ARGS_SUCCESS) {
 		g_rc = -1;
 		goto cleanup;
@@ -989,6 +1016,17 @@ main(int argc, char **argv)
 		goto cleanup;
 	}
 
+	if (g_allocate_depth > 0 && g_queue_depth > g_allocate_depth) {
+		fprintf(stdout, "allocate depth must be at least as big as queue depth\n");
+		usage();
+		g_rc = -1;
+		goto cleanup;
+	}
+
+	if (g_allocate_depth == 0) {
+		g_allocate_depth = g_queue_depth;
+	}
+
 	if (g_workload_selection == ACCEL_CRC32C &&
 	    g_crc32c_chained_count == 0) {
 		usage();
diff --git a/examples/bdev/fio_plugin/Makefile b/examples/bdev/fio_plugin/Makefile
index 1d48d1e99..b93456651 100644
--- a/examples/bdev/fio_plugin/Makefile
+++ b/examples/bdev/fio_plugin/Makefile
@@ -40,6 +40,6 @@ FIO_PLUGIN := spdk_bdev
 
 C_SRCS = fio_plugin.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev
 
 include $(SPDK_ROOT_DIR)/mk/spdk.fio.mk
diff --git a/examples/bdev/fio_plugin/README.md b/examples/bdev/fio_plugin/README.md
index 1c968a513..aaa65e64e 100644
--- a/examples/bdev/fio_plugin/README.md
+++ b/examples/bdev/fio_plugin/README.md
@@ -71,3 +71,52 @@ engine's full path via the ioengine parameter - LD_PRELOAD is recommended to avo
 When testing random workloads, it is recommended to set norandommap=1.  fio's random map
 processing consumes extra CPU cycles which will degrade performance over time with
 the fio_plugin since all I/O are submitted and completed on a single CPU core.
+
+# Zoned Block Devices
+
+SPDK has a zoned block device API (bdev_zone.h) which currently supports Open-channel SSDs,
+NVMe Zoned Namespaces (ZNS), and the virtual zoned block device SPDK module.
+
+If you wish to run fio against a SPDK zoned block device, you can use the fio option:
+
+    zonemode=zbd
+
+It is recommended to use a fio version newer than version 3.26, if using --numjobs > 1.
+If using --numjobs=1, fio version >= 3.23 should suffice.
+
+See zbd_example.fio in this directory for a zoned block device example config.
+
+## Maximum Open Zones
+
+Most zoned block devices have a resource constraint on the amount of zones which can be in an opened
+state at any point in time. It is very important to not exceed this limit.
+
+You can control how many zones fio will keep in an open state by using the
+``--max_open_zones`` option.
+
+If you use a fio version newer than 3.26, fio will automatically detect and set the proper value.
+If you use an old version of fio, make sure to provide the proper --max_open_zones value yourself.
+
+## Maximum Active Zones
+
+Zoned block devices may also have a resource constraint on the number of zones that can be active at
+any point in time. Unlike ``max_open_zones``, fio currently does not manage this constraint, and
+there is thus no option to limit it either.
+
+Since the max active zones limit (by definition) has to be greater than or equal to the max open
+zones limit, the easiest way to work around that fio does not manage this constraint, is to start
+with a clean state each run (except for read-only workloads), by resetting all zones before fio
+starts running its jobs by using the engine option:
+
+    --initial_zone_reset=1
+
+## Zone Append
+
+When running fio against a zoned block device you need to specify --iodepth=1 to avoid
+"Zone Invalid Write: The write to a zone was not at the write pointer." I/O errors.
+However, if your zoned block device supports Zone Append, you can use the engine option:
+
+    --zone_append=1
+
+To send zone append commands instead of write commands to the zoned block device.
+When using zone append, you will be able to specify a --iodepth greater than 1.
diff --git a/examples/bdev/fio_plugin/bdev_zoned.json b/examples/bdev/fio_plugin/bdev_zoned.json
new file mode 100644
index 000000000..d09ce18a1
--- /dev/null
+++ b/examples/bdev/fio_plugin/bdev_zoned.json
@@ -0,0 +1,26 @@
+{
+  "subsystems": [
+    {
+      "subsystem": "bdev",
+      "config": [
+        {
+          "method": "bdev_malloc_create",
+          "params": {
+            "name": "Malloc0",
+            "num_blocks": 2097152,
+            "block_size": 512
+          }
+        },
+        {
+          "method": "bdev_zone_block_create",
+          "params": {
+            "base_bdev": "Malloc0",
+            "name": "Zone0",
+            "zone_capacity": 262144,
+            "optimal_open_zones": 8
+          }
+        }
+      ]
+    }
+  ]
+}
diff --git a/examples/bdev/fio_plugin/fio_plugin.c b/examples/bdev/fio_plugin/fio_plugin.c
index 8d82e0363..9d047dd1b 100644
--- a/examples/bdev/fio_plugin/fio_plugin.c
+++ b/examples/bdev/fio_plugin/fio_plugin.c
@@ -34,8 +34,10 @@
 #include "spdk/stdinc.h"
 
 #include "spdk/bdev.h"
+#include "spdk/bdev_zone.h"
 #include "spdk/accel_engine.h"
 #include "spdk/env.h"
+#include "spdk/init.h"
 #include "spdk/thread.h"
 #include "spdk/log.h"
 #include "spdk/string.h"
@@ -48,6 +50,12 @@
 #include "fio.h"
 #include "optgroup.h"
 
+#ifdef for_each_rw_ddir
+#define FIO_HAS_ZBD (FIO_IOOPS_VERSION >= 26)
+#else
+#define FIO_HAS_ZBD (0)
+#endif
+
 /* FreeBSD is missing CLOCK_MONOTONIC_RAW,
  * so alternative is provided. */
 #ifndef CLOCK_MONOTONIC_RAW /* Defined in glibc bits/time.h */
@@ -60,6 +68,8 @@ struct spdk_fio_options {
 	char *json_conf;
 	unsigned mem_mb;
 	int mem_single_seg;
+	int initial_zone_reset;
+	int zone_append;
 };
 
 struct spdk_fio_request {
@@ -71,6 +81,7 @@ struct spdk_fio_target {
 	struct spdk_bdev	*bdev;
 	struct spdk_bdev_desc	*desc;
 	struct spdk_io_channel	*ch;
+	bool zone_append_enabled;
 
 	TAILQ_ENTRY(spdk_fio_target) link;
 };
@@ -89,12 +100,24 @@ struct spdk_fio_thread {
 	TAILQ_ENTRY(spdk_fio_thread)	link;
 };
 
+struct spdk_fio_zone_cb_arg {
+	struct spdk_fio_target *target;
+	struct spdk_bdev_zone_info *spdk_zones;
+	int completed;
+	uint64_t offset_blocks;
+	struct zbd_zone *fio_zones;
+	unsigned int nr_zones;
+};
+
 static bool g_spdk_env_initialized = false;
 static const char *g_json_config_file = NULL;
 
 static int spdk_fio_init(struct thread_data *td);
 static void spdk_fio_cleanup(struct thread_data *td);
 static size_t spdk_fio_poll_thread(struct spdk_fio_thread *fio_thread);
+static int spdk_fio_handle_options(struct thread_data *td, struct fio_file *f,
+				   struct spdk_bdev *bdev);
+static int spdk_fio_handle_options_per_target(struct thread_data *td, struct fio_file *f);
 
 static pthread_t g_init_thread_id = 0;
 static pthread_mutex_t g_init_mtx = PTHREAD_MUTEX_INITIALIZER;
@@ -196,8 +219,8 @@ spdk_fio_bdev_init_start(void *arg)
 {
 	bool *done = arg;
 
-	spdk_app_json_config_load(g_json_config_file, SPDK_DEFAULT_RPC_ADDR,
-				  spdk_fio_bdev_init_done, done, true);
+	spdk_subsystem_init_from_json_config(g_json_config_file, SPDK_DEFAULT_RPC_ADDR,
+					     spdk_fio_bdev_init_done, done, true);
 }
 
 static void
@@ -455,6 +478,7 @@ spdk_fio_setup(struct thread_data *td)
 
 	for_each_file(td, f, i) {
 		struct spdk_bdev *bdev;
+		int rc;
 
 		if (strcmp(f->file_name, "*") == 0) {
 			continue;
@@ -468,7 +492,13 @@ spdk_fio_setup(struct thread_data *td)
 
 		f->real_file_size = spdk_bdev_get_num_blocks(bdev) *
 				    spdk_bdev_get_block_size(bdev);
+		f->filetype = FIO_TYPE_BLOCK;
+		fio_file_set_size_known(f);
 
+		rc = spdk_fio_handle_options(td, f, bdev);
+		if (rc) {
+			return rc;
+		}
 	}
 
 	return 0;
@@ -528,21 +558,48 @@ spdk_fio_bdev_open(void *arg)
 
 		f->engine_data = target;
 
+		rc = spdk_fio_handle_options_per_target(td, f);
+		if (rc) {
+			SPDK_ERRLOG("Failed to handle options for: %s\n", f->file_name);
+			f->engine_data = NULL;
+			spdk_put_io_channel(target->ch);
+			spdk_bdev_close(target->desc);
+			free(target);
+			fio_thread->failed = true;
+			return;
+		}
+
 		TAILQ_INSERT_TAIL(&fio_thread->targets, target, link);
 	}
 }
 
 /* Called for each thread, on that thread, shortly after the thread
  * starts.
+ *
+ * Also called by spdk_fio_report_zones(), since we need an I/O channel
+ * in order to get the zone report. (fio calls the .report_zones callback
+ * before it calls the .init callback.)
+ * Therefore, if fio was run with --zonemode=zbd, the thread will already
+ * be initialized by the time that fio calls the .init callback.
  */
 static int
 spdk_fio_init(struct thread_data *td)
 {
 	struct spdk_fio_thread *fio_thread;
+	int rc;
 
-	spdk_fio_init_thread(td);
+	/* If thread has already been initialized, do nothing. */
+	if (td->io_ops_data) {
+		return 0;
+	}
+
+	rc = spdk_fio_init_thread(td);
+	if (rc) {
+		return rc;
+	}
 
 	fio_thread = td->io_ops_data;
+	assert(fio_thread);
 	fio_thread->failed = false;
 
 	spdk_thread_send_msg(fio_thread->thread, spdk_fio_bdev_open, td);
@@ -644,6 +701,17 @@ typedef enum fio_q_status fio_q_status_t;
 typedef int fio_q_status_t;
 #endif
 
+static uint64_t
+spdk_fio_zone_bytes_to_blocks(struct spdk_bdev *bdev, uint64_t offset_bytes, uint64_t *zone_start,
+			      uint64_t num_bytes, uint64_t *num_blocks)
+{
+	uint32_t block_size = spdk_bdev_get_block_size(bdev);
+	*zone_start = (offset_bytes / (spdk_bdev_get_zone_size(bdev) * block_size)) *
+		      spdk_bdev_get_zone_size(bdev);
+	*num_blocks = num_bytes / block_size;
+	return (offset_bytes % block_size) | (num_bytes % block_size);
+}
+
 static fio_q_status_t
 spdk_fio_queue(struct thread_data *td, struct io_u *io_u)
 {
@@ -666,9 +734,21 @@ spdk_fio_queue(struct thread_data *td, struct io_u *io_u)
 				    spdk_fio_completion_cb, fio_req);
 		break;
 	case DDIR_WRITE:
-		rc = spdk_bdev_write(target->desc, target->ch,
-				     io_u->buf, io_u->offset, io_u->xfer_buflen,
-				     spdk_fio_completion_cb, fio_req);
+		if (!target->zone_append_enabled) {
+			rc = spdk_bdev_write(target->desc, target->ch,
+					     io_u->buf, io_u->offset, io_u->xfer_buflen,
+					     spdk_fio_completion_cb, fio_req);
+		} else {
+			uint64_t zone_start, num_blocks;
+			if (spdk_fio_zone_bytes_to_blocks(target->bdev, io_u->offset, &zone_start,
+							  io_u->xfer_buflen, &num_blocks) != 0) {
+				rc = -EINVAL;
+				break;
+			}
+			rc = spdk_bdev_zone_append(target->desc, target->ch, io_u->buf,
+						   zone_start, num_blocks, spdk_fio_completion_cb,
+						   fio_req);
+		}
 		break;
 	case DDIR_TRIM:
 		rc = spdk_bdev_unmap(target->desc, target->ch,
@@ -755,6 +835,308 @@ spdk_fio_invalidate(struct thread_data *td, struct fio_file *f)
 	return 0;
 }
 
+#if FIO_HAS_ZBD
+static int
+spdk_fio_get_zoned_model(struct thread_data *td, struct fio_file *f, enum zbd_zoned_model *model)
+{
+	struct spdk_bdev *bdev;
+
+	if (f->filetype != FIO_TYPE_BLOCK) {
+		SPDK_ERRLOG("Unsupported filetype: %d\n", f->filetype);
+		return -EINVAL;
+	}
+
+	bdev = spdk_bdev_get_by_name(f->file_name);
+	if (!bdev) {
+		SPDK_ERRLOG("Cannot get zoned model, no bdev with name: %s\n", f->file_name);
+		return -ENODEV;
+	}
+
+	if (spdk_bdev_is_zoned(bdev)) {
+		*model = ZBD_HOST_MANAGED;
+	} else {
+		*model = ZBD_NONE;
+	}
+
+	return 0;
+}
+
+static void
+spdk_fio_bdev_get_zone_info_done(struct spdk_bdev_io *bdev_io, bool success, void *arg)
+{
+	struct spdk_fio_zone_cb_arg *cb_arg = arg;
+	unsigned int i;
+	int handled_zones = 0;
+
+	if (!success) {
+		spdk_bdev_free_io(bdev_io);
+		cb_arg->completed = -EIO;
+		return;
+	}
+
+	for (i = 0; i < cb_arg->nr_zones; i++) {
+		struct spdk_bdev_zone_info *zone_src = &cb_arg->spdk_zones[handled_zones];
+		struct zbd_zone *zone_dest = &cb_arg->fio_zones[handled_zones];
+		uint32_t block_size = spdk_bdev_get_block_size(cb_arg->target->bdev);
+
+		zone_dest->type = ZBD_ZONE_TYPE_SWR;
+		zone_dest->len = spdk_bdev_get_zone_size(cb_arg->target->bdev) * block_size;
+		zone_dest->capacity = zone_src->capacity * block_size;
+		zone_dest->start = zone_src->zone_id * block_size;
+		zone_dest->wp = zone_src->write_pointer * block_size;
+
+		switch (zone_src->state) {
+		case SPDK_BDEV_ZONE_STATE_EMPTY:
+			zone_dest->cond = ZBD_ZONE_COND_EMPTY;
+			break;
+		case SPDK_BDEV_ZONE_STATE_IMP_OPEN:
+			zone_dest->cond = ZBD_ZONE_COND_IMP_OPEN;
+			break;
+		case SPDK_BDEV_ZONE_STATE_EXP_OPEN:
+			zone_dest->cond = ZBD_ZONE_COND_EXP_OPEN;
+			break;
+		case SPDK_BDEV_ZONE_STATE_FULL:
+			zone_dest->cond = ZBD_ZONE_COND_FULL;
+			break;
+		case SPDK_BDEV_ZONE_STATE_CLOSED:
+			zone_dest->cond = ZBD_ZONE_COND_CLOSED;
+			break;
+		case SPDK_BDEV_ZONE_STATE_READ_ONLY:
+			zone_dest->cond = ZBD_ZONE_COND_READONLY;
+			break;
+		case SPDK_BDEV_ZONE_STATE_OFFLINE:
+			zone_dest->cond = ZBD_ZONE_COND_OFFLINE;
+			break;
+		default:
+			spdk_bdev_free_io(bdev_io);
+			cb_arg->completed = -EIO;
+			return;
+		}
+		handled_zones++;
+	}
+
+	spdk_bdev_free_io(bdev_io);
+	cb_arg->completed = handled_zones;
+}
+
+static void
+spdk_fio_bdev_get_zone_info(void *arg)
+{
+	struct spdk_fio_zone_cb_arg *cb_arg = arg;
+	struct spdk_fio_target *target = cb_arg->target;
+	int rc;
+
+	rc = spdk_bdev_get_zone_info(target->desc, target->ch, cb_arg->offset_blocks,
+				     cb_arg->nr_zones, cb_arg->spdk_zones,
+				     spdk_fio_bdev_get_zone_info_done, cb_arg);
+	if (rc < 0) {
+		cb_arg->completed = rc;
+	}
+}
+
+static int
+spdk_fio_report_zones(struct thread_data *td, struct fio_file *f, uint64_t offset,
+		      struct zbd_zone *zones, unsigned int nr_zones)
+{
+	struct spdk_fio_target *target;
+	struct spdk_fio_thread *fio_thread;
+	struct spdk_fio_zone_cb_arg cb_arg;
+	uint32_t block_size;
+	int rc;
+
+	if (nr_zones == 0) {
+		return 0;
+	}
+
+	/* spdk_fio_report_zones() is only called before the bdev I/O channels have been created.
+	 * Since we need an I/O channel for report_zones(), call spdk_fio_init() to initialize
+	 * the thread early.
+	 * spdk_fio_report_zones() might be called several times by fio, if e.g. the zone report
+	 * for all zones does not fit in the buffer that fio has allocated for the zone report.
+	 * It is safe to call spdk_fio_init(), even if the thread has already been initialized.
+	 */
+	rc = spdk_fio_init(td);
+	if (rc) {
+		return rc;
+	}
+	fio_thread = td->io_ops_data;
+	target = f->engine_data;
+
+	assert(fio_thread);
+	assert(target);
+
+	block_size = spdk_bdev_get_block_size(target->bdev);
+
+	cb_arg.target = target;
+	cb_arg.completed = 0;
+	cb_arg.offset_blocks = offset / block_size;
+	cb_arg.fio_zones = zones;
+	cb_arg.nr_zones = spdk_min(nr_zones, spdk_bdev_get_num_zones(target->bdev));
+
+	cb_arg.spdk_zones = calloc(1, sizeof(*cb_arg.spdk_zones) * cb_arg.nr_zones);
+	if (!cb_arg.spdk_zones) {
+		SPDK_ERRLOG("Could not allocate memory for zone report!\n");
+		rc = -ENOMEM;
+		goto cleanup_thread;
+	}
+
+	spdk_thread_send_msg(fio_thread->thread, spdk_fio_bdev_get_zone_info, &cb_arg);
+	do {
+		spdk_fio_poll_thread(fio_thread);
+	} while (!cb_arg.completed);
+
+	/* Free cb_arg.spdk_zones. The report in fio format is stored in cb_arg.fio_zones/zones. */
+	free(cb_arg.spdk_zones);
+
+	rc = cb_arg.completed;
+	if (rc < 0) {
+		SPDK_ERRLOG("Failed to get zone info: %d\n", rc);
+		goto cleanup_thread;
+	}
+
+	/* Return the amount of zones successfully copied. */
+	return rc;
+
+cleanup_thread:
+	spdk_fio_cleanup(td);
+
+	return rc;
+}
+
+static void
+spdk_fio_bdev_zone_reset_done(struct spdk_bdev_io *bdev_io, bool success, void *arg)
+{
+	struct spdk_fio_zone_cb_arg *cb_arg = arg;
+
+	spdk_bdev_free_io(bdev_io);
+
+	if (!success) {
+		cb_arg->completed = -EIO;
+	} else {
+		cb_arg->completed = 1;
+	}
+}
+
+static void
+spdk_fio_bdev_zone_reset(void *arg)
+{
+	struct spdk_fio_zone_cb_arg *cb_arg = arg;
+	struct spdk_fio_target *target = cb_arg->target;
+	int rc;
+
+	rc = spdk_bdev_zone_management(target->desc, target->ch, cb_arg->offset_blocks,
+				       SPDK_BDEV_ZONE_RESET,
+				       spdk_fio_bdev_zone_reset_done, cb_arg);
+	if (rc < 0) {
+		cb_arg->completed = rc;
+	}
+}
+
+static int
+spdk_fio_reset_zones(struct spdk_fio_thread *fio_thread, struct spdk_fio_target *target,
+		     uint64_t offset, uint64_t length)
+{
+	uint64_t zone_size_bytes;
+	uint32_t block_size;
+	int rc;
+
+	assert(fio_thread);
+	assert(target);
+
+	block_size = spdk_bdev_get_block_size(target->bdev);
+	zone_size_bytes = spdk_bdev_get_zone_size(target->bdev) * block_size;
+
+	for (uint64_t cur = offset; cur < offset + length; cur += zone_size_bytes) {
+		struct spdk_fio_zone_cb_arg cb_arg = {
+			.target = target,
+			.completed = 0,
+			.offset_blocks = cur / block_size,
+		};
+
+		spdk_thread_send_msg(fio_thread->thread, spdk_fio_bdev_zone_reset, &cb_arg);
+		do {
+			spdk_fio_poll_thread(fio_thread);
+		} while (!cb_arg.completed);
+
+		rc = cb_arg.completed;
+		if (rc < 0) {
+			SPDK_ERRLOG("Failed to reset zone: %d\n", rc);
+			return rc;
+		}
+	}
+
+	return 0;
+}
+
+static int
+spdk_fio_reset_wp(struct thread_data *td, struct fio_file *f, uint64_t offset, uint64_t length)
+{
+	return spdk_fio_reset_zones(td->io_ops_data, f->engine_data, offset, length);
+}
+#endif
+
+#if FIO_IOOPS_VERSION >= 30
+static int spdk_fio_get_max_open_zones(struct thread_data *td, struct fio_file *f,
+				       unsigned int *max_open_zones)
+{
+	struct spdk_bdev *bdev;
+
+	bdev = spdk_bdev_get_by_name(f->file_name);
+	if (!bdev) {
+		SPDK_ERRLOG("Cannot get max open zones, no bdev with name: %s\n", f->file_name);
+		return -ENODEV;
+	}
+
+	*max_open_zones = spdk_bdev_get_max_open_zones(bdev);
+
+	return 0;
+}
+#endif
+
+static int
+spdk_fio_handle_options(struct thread_data *td, struct fio_file *f, struct spdk_bdev *bdev)
+{
+	struct spdk_fio_options *fio_options = td->eo;
+
+	if (fio_options->initial_zone_reset && spdk_bdev_is_zoned(bdev)) {
+#if FIO_HAS_ZBD
+		int rc = spdk_fio_init(td);
+		if (rc) {
+			return rc;
+		}
+		rc = spdk_fio_reset_zones(td->io_ops_data, f->engine_data, 0, f->real_file_size);
+		if (rc) {
+			spdk_fio_cleanup(td);
+			return rc;
+		}
+#else
+		SPDK_ERRLOG("fio version is too old to support zoned block devices\n");
+#endif
+	}
+
+	return 0;
+}
+
+static int
+spdk_fio_handle_options_per_target(struct thread_data *td, struct fio_file *f)
+{
+	struct spdk_fio_target *target = f->engine_data;
+	struct spdk_fio_options *fio_options = td->eo;
+
+	if (fio_options->zone_append && spdk_bdev_is_zoned(target->bdev)) {
+		if (spdk_bdev_io_type_supported(target->bdev, SPDK_BDEV_IO_TYPE_ZONE_APPEND)) {
+			SPDK_DEBUGLOG(fio_bdev, "Using zone appends instead of writes on: '%s'\n",
+				      f->file_name);
+			target->zone_append_enabled = true;
+		} else {
+			SPDK_WARNLOG("Falling back to writes on: '%s' - bdev lacks zone append cmd\n",
+				     f->file_name);
+		}
+	}
+
+	return 0;
+}
+
 static struct fio_option options[] = {
 	{
 		.name		= "spdk_conf",
@@ -793,6 +1175,26 @@ static struct fio_option options[] = {
 		.category	= FIO_OPT_C_ENGINE,
 		.group		= FIO_OPT_G_INVALID,
 	},
+	{
+		.name		= "initial_zone_reset",
+		.lname		= "Reset Zones on initialization",
+		.type		= FIO_OPT_INT,
+		.off1		= offsetof(struct spdk_fio_options, initial_zone_reset),
+		.def		= "0",
+		.help		= "Reset Zones on initialization (0=disable, 1=Reset All Zones)",
+		.category	= FIO_OPT_C_ENGINE,
+		.group		= FIO_OPT_G_INVALID,
+	},
+	{
+		.name		= "zone_append",
+		.lname		= "Use zone append instead of write",
+		.type		= FIO_OPT_INT,
+		.off1		= offsetof(struct spdk_fio_options, zone_append),
+		.def		= "0",
+		.help		= "Use zone append instead of write (1=zone append, 0=write)",
+		.category	= FIO_OPT_C_ENGINE,
+		.group		= FIO_OPT_G_INVALID,
+	},
 	{
 		.name		= NULL,
 	},
@@ -823,6 +1225,14 @@ struct ioengine_ops ioengine = {
 	.iomem_free		= spdk_fio_iomem_free,
 	.io_u_init		= spdk_fio_io_u_init,
 	.io_u_free		= spdk_fio_io_u_free,
+#if FIO_HAS_ZBD
+	.get_zoned_model	= spdk_fio_get_zoned_model,
+	.report_zones		= spdk_fio_report_zones,
+	.reset_wp		= spdk_fio_reset_wp,
+#endif
+#if FIO_IOOPS_VERSION >= 30
+	.get_max_open_zones	= spdk_fio_get_max_open_zones,
+#endif
 	.option_struct_size	= sizeof(struct spdk_fio_options),
 	.options		= options,
 };
@@ -852,3 +1262,5 @@ static void fio_exit spdk_fio_unregister(void)
 	}
 	unregister_ioengine(&ioengine);
 }
+
+SPDK_LOG_REGISTER_COMPONENT(fio_bdev)
diff --git a/examples/bdev/fio_plugin/zbd_example.fio b/examples/bdev/fio_plugin/zbd_example.fio
new file mode 100644
index 000000000..f026564e8
--- /dev/null
+++ b/examples/bdev/fio_plugin/zbd_example.fio
@@ -0,0 +1,18 @@
+[global]
+ioengine=spdk_bdev
+spdk_json_conf=./examples/bdev/fio_plugin/bdev_zoned.json
+thread=1
+direct=1
+time_based
+runtime=10
+rw=randwrite
+bs=16K
+zonemode=zbd
+max_open_zones=8
+initial_zone_reset=1
+zone_append=1
+iodepth=64
+
+[test]
+filename=Zone0
+numjobs=1
diff --git a/examples/bdev/hello_world/Makefile b/examples/bdev/hello_world/Makefile
index e2848a7ce..ae413fdf3 100644
--- a/examples/bdev/hello_world/Makefile
+++ b/examples/bdev/hello_world/Makefile
@@ -37,6 +37,6 @@ APP = hello_bdev
 
 C_SRCS := hello_bdev.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/examples/blob/cli/Makefile b/examples/blob/cli/Makefile
index e658ebc87..e072ca6d7 100644
--- a/examples/blob/cli/Makefile
+++ b/examples/blob/cli/Makefile
@@ -38,6 +38,6 @@ APP = blobcli
 C_SRCS := blobcli.c
 
 # Don't link bdev_lvol in blobcli - otherwise this utility cannot operate on an lvolstore
-SPDK_LIB_LIST = $(filter-out bdev_lvol,$(ALL_MODULES_LIST)) event_bdev
+SPDK_LIB_LIST = $(filter-out bdev_lvol,$(ALL_MODULES_LIST)) event event_bdev
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/examples/blob/hello_world/Makefile b/examples/blob/hello_world/Makefile
index ad6c814cc..a93d47755 100644
--- a/examples/blob/hello_world/Makefile
+++ b/examples/blob/hello_world/Makefile
@@ -37,6 +37,6 @@ APP = hello_blob
 
 C_SRCS := hello_blob.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/examples/interrupt_tgt/Makefile b/examples/interrupt_tgt/Makefile
index 90a2b8a4d..e84eb4509 100644
--- a/examples/interrupt_tgt/Makefile
+++ b/examples/interrupt_tgt/Makefile
@@ -39,7 +39,7 @@ APP = interrupt_tgt
 
 C_SRCS := interrupt_tgt.c
 
-SPDK_LIB_LIST = $(INTR_BLOCKDEV_MODULES_LIST) event_bdev conf
+SPDK_LIB_LIST = $(INTR_BLOCKDEV_MODULES_LIST) event event_bdev conf
 
 SPDK_LIB_LIST += event_nbd
 SPDK_LIB_LIST += event_vhost
diff --git a/examples/nvme/fio_plugin/README.md b/examples/nvme/fio_plugin/README.md
index 445573ae2..4242c7404 100644
--- a/examples/nvme/fio_plugin/README.md
+++ b/examples/nvme/fio_plugin/README.md
@@ -125,7 +125,8 @@ Zoned Namespaces has a resource constraint on the amount of zones which can be i
 any point in time. You can control how many zones fio will keep in an open state by using the
 ``--max_open_zones`` option.
 
-The SPDK/NVMe fio io-engine will set a default value if you do not provide one.
+If you use a fio version newer than 3.26, fio will automatically detect and set the proper value.
+If you use an old version of fio, make sure to provide the proper --max_open_zones value yourself.
 
 ## Maximum Active Zones
 
diff --git a/examples/nvme/fio_plugin/fio_plugin.c b/examples/nvme/fio_plugin/fio_plugin.c
index 14f5b9f10..4d590c6a3 100644
--- a/examples/nvme/fio_plugin/fio_plugin.c
+++ b/examples/nvme/fio_plugin/fio_plugin.c
@@ -241,7 +241,6 @@ get_fio_ctrlr(const struct spdk_nvme_transport_id *trid)
 	return NULL;
 }
 
-#if FIO_HAS_ZBD
 /**
  * Returns the fio_qpair matching the given fio_file and has an associated ns
  */
@@ -259,6 +258,7 @@ get_fio_qpair(struct spdk_fio_thread *fio_thread, struct fio_file *f)
 	return NULL;
 }
 
+#if FIO_HAS_ZBD
 /**
  * Callback function to use while processing completions until completion-indicator turns non-zero
  */
@@ -426,12 +426,12 @@ attach_cb(void *cb_ctx, const struct spdk_nvme_transport_id *trid,
 
 	if (fio_options->zone_append && spdk_nvme_ns_get_csi(ns) == SPDK_NVME_CSI_ZNS) {
 		if (spdk_nvme_ctrlr_get_flags(ctrlr) & SPDK_NVME_CTRLR_ZONE_APPEND_SUPPORTED) {
-			fprintf(stdout, "Using zone appends instead of writes on: '%s'\n",
-				fio_qpair->f->file_name);
+			SPDK_DEBUGLOG(fio_nvme, "Using zone appends instead of writes on: '%s'\n",
+				      f->file_name);
 			fio_qpair->zone_append_enabled = true;
 		} else {
 			SPDK_WARNLOG("Falling back to writes on: '%s' - ns lacks zone append cmd\n",
-				     fio_qpair->f->file_name);
+				     f->file_name);
 		}
 	}
 
@@ -988,16 +988,12 @@ spdk_fio_queue(struct thread_data *td, struct io_u *io_u)
 	uint64_t		lba;
 	uint32_t		lba_count;
 
-	/* Find the namespace that corresponds to the file in the io_u */
-	TAILQ_FOREACH(fio_qpair, &fio_thread->fio_qpair, link) {
-		if (fio_qpair->f == io_u->file) {
-			ns = fio_qpair->ns;
-			break;
-		}
-	}
-	if (fio_qpair == NULL || ns == NULL) {
+	fio_qpair = get_fio_qpair(fio_thread, io_u->file);
+	if (fio_qpair == NULL) {
 		return -ENXIO;
 	}
+	ns = fio_qpair->ns;
+
 	if (fio_qpair->nvme_pi_enabled && !fio_qpair->extended_lba) {
 		md_buf = fio_req->md_buf;
 	}
@@ -1041,7 +1037,7 @@ spdk_fio_queue(struct thread_data *td, struct io_u *io_u)
 								    spdk_fio_completion_cb, fio_req,
 								    fio_qpair->io_flags, dif_ctx->apptag_mask, dif_ctx->app_tag);
 			} else {
-				uint64_t zslba = fio_offset_to_zslba(io_u->offset, fio_qpair->ns);
+				uint64_t zslba = fio_offset_to_zslba(io_u->offset, ns);
 				rc = spdk_nvme_zns_zone_append_with_md(ns, fio_qpair->qpair, io_u->buf, md_buf, zslba,
 								       lba_count,
 								       spdk_fio_completion_cb, fio_req,
@@ -1054,7 +1050,7 @@ spdk_fio_queue(struct thread_data *td, struct io_u *io_u)
 								     spdk_nvme_io_reset_sgl, spdk_nvme_io_next_sge, md_buf,
 								     dif_ctx->apptag_mask, dif_ctx->app_tag);
 			} else {
-				uint64_t zslba = fio_offset_to_zslba(io_u->offset, fio_qpair->ns);
+				uint64_t zslba = fio_offset_to_zslba(io_u->offset, ns);
 				rc = spdk_nvme_zns_zone_appendv_with_md(ns, fio_qpair->qpair, zslba,
 									lba_count, spdk_fio_completion_cb, fio_req, fio_qpair->io_flags,
 									spdk_nvme_io_reset_sgl, spdk_nvme_io_next_sge, md_buf,
@@ -1157,13 +1153,9 @@ spdk_fio_get_zoned_model(struct thread_data *td, struct fio_file *f, enum zbd_zo
 	struct spdk_fio_qpair *fio_qpair = NULL;
 	const struct spdk_nvme_zns_ns_data *zns_data = NULL;
 
-	*model = ZBD_IGNORE;
-
-	if (f->filetype != FIO_TYPE_FILE && \
-	    f->filetype != FIO_TYPE_BLOCK && \
-	    f->filetype != FIO_TYPE_CHAR) {
-		log_info("spdk/nvme: ignoring filetype: %d\n", f->filetype);
-		return 0;
+	if (f->filetype != FIO_TYPE_BLOCK) {
+		log_info("spdk/nvme: unsupported filetype: %d\n", f->filetype);
+		return -EINVAL;
 	}
 
 	fio_qpair = get_fio_qpair(fio_thread, f);
@@ -1190,29 +1182,6 @@ spdk_fio_get_zoned_model(struct thread_data *td, struct fio_file *f, enum zbd_zo
 
 		*model = ZBD_HOST_MANAGED;
 
-		/** Unlimited open resources, skip checking 'max_open_zones' */
-		if (0xFFFFFFFF == zns_data->mor) {
-			return 0;
-		}
-
-		if (!td->o.max_open_zones) {
-			td->o.max_open_zones = spdk_min(ZBD_MAX_OPEN_ZONES, zns_data->mor + 1);
-			log_info("spdk/nvme: parameter 'max_open_zones' was unset; assigned: %d.\n",
-				 td->o.max_open_zones);
-		} else if (td->o.max_open_zones < 0) {
-			log_err("spdk/nvme: invalid parameter 'max_open_zones': %d\n",
-				td->o.max_open_zones);
-			return -EINVAL;
-		} else if (td->o.max_open_zones > ZBD_MAX_OPEN_ZONES) {
-			log_err("spdk/nvme: parameter 'max_open_zones': %d exceeds fio-limit: %d\n",
-				td->o.max_open_zones, ZBD_MAX_OPEN_ZONES);
-			return -EINVAL;
-		} else if ((uint32_t)td->o.max_open_zones > (zns_data->mor + 1)) {
-			log_err("spdk/nvme: parameter 'max_open_zones': %d exceeds dev-limit: %u\n",
-				td->o.max_open_zones, zns_data->mor + 1);
-			return -EINVAL;
-		}
-
 		return 0;
 	}
 
@@ -1382,6 +1351,25 @@ spdk_fio_reset_wp(struct thread_data *td, struct fio_file *f, uint64_t offset, u
 }
 #endif
 
+#if FIO_IOOPS_VERSION >= 30
+static int spdk_fio_get_max_open_zones(struct thread_data *td, struct fio_file *f,
+				       unsigned int *max_open_zones)
+{
+	struct spdk_fio_thread *fio_thread = td->io_ops_data;
+	struct spdk_fio_qpair *fio_qpair = NULL;
+
+	fio_qpair = get_fio_qpair(fio_thread, f);
+	if (!fio_qpair) {
+		log_err("spdk/nvme: no ns/qpair or file_name: '%s'\n", f->file_name);
+		return -ENODEV;
+	}
+
+	*max_open_zones = spdk_nvme_zns_ns_get_max_open_zones(fio_qpair->ns);
+
+	return 0;
+}
+#endif
+
 static void spdk_fio_cleanup(struct thread_data *td)
 {
 	struct spdk_fio_thread	*fio_thread = td->io_ops_data;
@@ -1634,7 +1622,7 @@ static struct fio_option options[] = {
 		.type		= FIO_OPT_INT,
 		.off1		= offsetof(struct spdk_fio_options, zone_append),
 		.def		= "0",
-		.help		= "Use zone append instead of write (zone_append=1 or zone_append=0)",
+		.help		= "Use zone append instead of write (1=zone append, 0=write)",
 		.category	= FIO_OPT_C_ENGINE,
 		.group		= FIO_OPT_G_INVALID,
 	},
@@ -1673,6 +1661,9 @@ struct ioengine_ops ioengine = {
 	.get_zoned_model	= spdk_fio_get_zoned_model,
 	.report_zones		= spdk_fio_report_zones,
 	.reset_wp		= spdk_fio_reset_wp,
+#endif
+#if FIO_IOOPS_VERSION >= 30
+	.get_max_open_zones	= spdk_fio_get_max_open_zones,
 #endif
 	.flags			= FIO_RAWIO | FIO_NOEXTEND | FIO_NODISKUTIL | FIO_MEMALIGN,
 	.options		= options,
@@ -1688,3 +1679,5 @@ static void fio_exit fio_spdk_unregister(void)
 {
 	unregister_ioengine(&ioengine);
 }
+
+SPDK_LOG_REGISTER_COMPONENT(fio_nvme)
diff --git a/examples/nvme/identify/identify.c b/examples/nvme/identify/identify.c
index f6ce6b73d..8da8f5fc9 100644
--- a/examples/nvme/identify/identify.c
+++ b/examples/nvme/identify/identify.c
@@ -201,7 +201,7 @@ get_zns_zone_report_completion(void *cb_arg, const struct spdk_nvme_cpl *cpl)
 }
 
 static int
-get_feature(struct spdk_nvme_ctrlr *ctrlr, uint8_t fid)
+get_feature(struct spdk_nvme_ctrlr *ctrlr, uint8_t fid, uint32_t nsid)
 {
 	struct spdk_nvme_cmd cmd = {};
 	struct feature *feature = &features[fid];
@@ -210,35 +210,28 @@ get_feature(struct spdk_nvme_ctrlr *ctrlr, uint8_t fid)
 
 	cmd.opc = SPDK_NVME_OPC_GET_FEATURES;
 	cmd.cdw10_bits.get_features.fid = fid;
+	cmd.nsid = nsid;
 
 	return spdk_nvme_ctrlr_cmd_admin_raw(ctrlr, &cmd, NULL, 0, get_feature_completion, feature);
 }
 
 static void
-get_features(struct spdk_nvme_ctrlr *ctrlr)
+get_features(struct spdk_nvme_ctrlr *ctrlr, uint8_t *features_to_get, size_t num_features,
+	     uint32_t nsid)
 {
 	size_t i;
 
-	uint8_t features_to_get[] = {
-		SPDK_NVME_FEAT_ARBITRATION,
-		SPDK_NVME_FEAT_POWER_MANAGEMENT,
-		SPDK_NVME_FEAT_TEMPERATURE_THRESHOLD,
-		SPDK_NVME_FEAT_ERROR_RECOVERY,
-		SPDK_NVME_FEAT_NUMBER_OF_QUEUES,
-		SPDK_OCSSD_FEAT_MEDIA_FEEDBACK,
-	};
-
 	/* Submit only one GET FEATURES at a time. There is a known issue #1799
 	 * with Google Cloud Platform NVMe SSDs that do not handle overlapped
 	 * GET FEATURES commands correctly.
 	 */
 	outstanding_commands = 0;
-	for (i = 0; i < SPDK_COUNTOF(features_to_get); i++) {
+	for (i = 0; i < num_features; i++) {
 		if (!spdk_nvme_ctrlr_is_ocssd_supported(ctrlr) &&
 		    features_to_get[i] == SPDK_OCSSD_FEAT_MEDIA_FEEDBACK) {
 			continue;
 		}
-		if (get_feature(ctrlr, features_to_get[i]) == 0) {
+		if (get_feature(ctrlr, features_to_get[i], nsid) == 0) {
 			outstanding_commands++;
 		} else {
 			printf("get_feature(0x%02X) failed to submit command\n", features_to_get[i]);
@@ -251,6 +244,30 @@ get_features(struct spdk_nvme_ctrlr *ctrlr)
 
 }
 
+static void
+get_ctrlr_features(struct spdk_nvme_ctrlr *ctrlr)
+{
+	uint8_t features_to_get[] = {
+		SPDK_NVME_FEAT_ARBITRATION,
+		SPDK_NVME_FEAT_POWER_MANAGEMENT,
+		SPDK_NVME_FEAT_TEMPERATURE_THRESHOLD,
+		SPDK_NVME_FEAT_NUMBER_OF_QUEUES,
+		SPDK_OCSSD_FEAT_MEDIA_FEEDBACK,
+	};
+
+	get_features(ctrlr, features_to_get, SPDK_COUNTOF(features_to_get), 0);
+}
+
+static void
+get_ns_features(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid)
+{
+	uint8_t features_to_get[] = {
+		SPDK_NVME_FEAT_ERROR_RECOVERY,
+	};
+
+	get_features(ctrlr, features_to_get, SPDK_COUNTOF(features_to_get), nsid);
+}
+
 static int
 get_error_log_page(struct spdk_nvme_ctrlr *ctrlr)
 {
@@ -942,6 +959,16 @@ print_namespace(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns *ns)
 	/* This function is only called for active namespaces. */
 	assert(spdk_nvme_ns_is_active(ns));
 
+	if (features[SPDK_NVME_FEAT_ERROR_RECOVERY].valid) {
+		unsigned tler = features[SPDK_NVME_FEAT_ERROR_RECOVERY].result & 0xFFFF;
+		printf("Error Recovery Timeout:                ");
+		if (tler == 0) {
+			printf("Unlimited\n");
+		} else {
+			printf("%u milliseconds\n", tler * 100);
+		}
+	}
+
 	printf("Command Set Identifier:                %s (%02Xh)\n",
 	       csi_name(spdk_nvme_ns_get_csi(ns)), spdk_nvme_ns_get_csi(ns));
 	printf("Deallocate:                            %s\n",
@@ -1014,6 +1041,10 @@ print_namespace(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns *ns)
 			printf("  Atomic Write Unit (PFail):           %d\n", nsdata->nawupf + 1);
 		}
 
+		if (nsdata->npwg) {
+			printf("  Preferred Write Granularity:         %d\n", nsdata->npwg + 1);
+		}
+
 		if (nsdata->nacwu) {
 			printf("  Atomic Compare & Write Unit:         %d\n", nsdata->nacwu + 1);
 		}
@@ -1023,6 +1054,12 @@ print_namespace(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns *ns)
 		printf("  Atomic Boundary Offset:              %d\n", nsdata->nabo);
 	}
 
+	if (cdata->oncs.copy) {
+		printf("Maximum Single Source Range Length:    %d\n", nsdata->mssrl);
+		printf("Maximum Copy Length:                   %d\n", nsdata->mcl);
+		printf("Maximum Source Range Count:            %d\n", nsdata->msrc + 1);
+	}
+
 	printf("NGUID/EUI64 Never Reused:              %s\n",
 	       nsdata->nsfeat.guid_never_reused ? "Yes" : "No");
 
@@ -1194,7 +1231,7 @@ print_controller(struct spdk_nvme_ctrlr *ctrlr, const struct spdk_nvme_transport
 		 * attempt GET_FEATURES when NOT targeting a
 		 * Discovery Controller.
 		 */
-		get_features(ctrlr);
+		get_ctrlr_features(ctrlr);
 	}
 	get_log_pages(ctrlr);
 
@@ -1255,15 +1292,6 @@ print_controller(struct spdk_nvme_ctrlr *ctrlr, const struct spdk_nvme_transport
 		printf("%" PRIu64 "\n", (uint64_t)1 << (12 + cap.bits.mpsmin + cdata->mdts));
 	}
 	printf("Max Number of Namespaces:              %d\n", cdata->nn);
-	if (features[SPDK_NVME_FEAT_ERROR_RECOVERY].valid) {
-		unsigned tler = features[SPDK_NVME_FEAT_ERROR_RECOVERY].result & 0xFFFF;
-		printf("Error Recovery Timeout:                ");
-		if (tler == 0) {
-			printf("Unlimited\n");
-		} else {
-			printf("%u milliseconds\n", tler * 100);
-		}
-	}
 	printf("NVMe Specification Version (VS):       %u.%u", vs.bits.mjr, vs.bits.mnr);
 	if (vs.bits.ter) {
 		printf(".%u", vs.bits.ter);
@@ -1459,6 +1487,8 @@ print_controller(struct spdk_nvme_ctrlr *ctrlr, const struct spdk_nvme_transport
 	       cdata->oncs.reservations ? "Supported" : "Not Supported");
 	printf("Timestamp:                   %s\n",
 	       cdata->oncs.timestamp ? "Supported" : "Not Supported");
+	printf("Copy:                        %s\n",
+	       cdata->oncs.copy ? "Supported" : "Not Supported");
 	printf("Volatile Write Cache:        %s\n",
 	       cdata->vwc.present ? "Present" : "Not Present");
 	printf("Atomic Write Unit (Normal):  %d\n", cdata->awun + 1);
@@ -1647,18 +1677,18 @@ print_controller(struct spdk_nvme_ctrlr *ctrlr, const struct spdk_nvme_transport
 		printf("Number of Power States:      %u\n", cdata->npss + 1);
 		printf("Current Power State:         Power State #%u\n", ps);
 		for (i = 0; i <= cdata->npss; i++) {
-			const struct spdk_nvme_power_state *psd = &cdata->psd[i];
+			const struct spdk_nvme_power_state psd = cdata->psd[i];
 			printf("Power State #%u:  ", i);
-			if (psd->mps) {
+			if (psd.mps) {
 				/* MP scale is 0.0001 W */
 				printf("Max Power: %u.%04u W\n",
-				       psd->mp / 10000,
-				       psd->mp % 10000);
+				       psd.mp / 10000,
+				       psd.mp % 10000);
 			} else {
 				/* MP scale is 0.01 W */
 				printf("Max Power: %3u.%02u W\n",
-				       psd->mp / 100,
-				       psd->mp % 100);
+				       psd.mp / 100,
+				       psd.mp % 100);
 			}
 			/* TODO: print other power state descriptor fields */
 		}
@@ -1943,6 +1973,7 @@ print_controller(struct spdk_nvme_ctrlr *ctrlr, const struct spdk_nvme_transport
 	printf("=================\n");
 	for (nsid = spdk_nvme_ctrlr_get_first_active_ns(ctrlr);
 	     nsid != 0; nsid = spdk_nvme_ctrlr_get_next_active_ns(ctrlr, nsid)) {
+		get_ns_features(ctrlr, nsid);
 		print_namespace(ctrlr, spdk_nvme_ctrlr_get_ns(ctrlr, nsid));
 	}
 
diff --git a/examples/nvme/perf/perf.c b/examples/nvme/perf/perf.c
index 844d6908f..5e62e801c 100644
--- a/examples/nvme/perf/perf.c
+++ b/examples/nvme/perf/perf.c
@@ -49,6 +49,7 @@
 #include "spdk/log.h"
 #include "spdk/likely.h"
 #include "spdk/sock.h"
+#include "spdk/zipf.h"
 
 #ifdef SPDK_CONFIG_URING
 #include <liburing.h>
@@ -105,6 +106,8 @@ struct ns_entry {
 	uint32_t		block_size;
 	uint32_t		md_size;
 	bool			md_interleave;
+	unsigned int		seed;
+	struct spdk_zipf	*zipf;
 	bool			pi_loc;
 	enum spdk_nvme_pi_type	pi_type;
 	uint32_t		io_flags;
@@ -267,6 +270,7 @@ static bool g_exit;
 /* Default to 10 seconds for the keep alive value. This value is arbitrary. */
 static uint32_t g_keep_alive_timeout_in_ms = 10000;
 static uint32_t g_quiet_count = 1;
+static double g_zipf_theta;
 
 /* When user specifies -Q, some error messages are rate limited.  When rate
  * limited, we only print the error message every g_quiet_count times the
@@ -726,6 +730,10 @@ register_file(const char *path)
 	entry->size_in_ios = size / g_io_size_bytes;
 	entry->io_size_blocks = g_io_size_bytes / blklen;
 
+	if (g_is_random && g_zipf_theta > 0) {
+		entry->zipf = spdk_zipf_create(entry->size_in_ios, g_zipf_theta, 0);
+	}
+
 	snprintf(entry->name, sizeof(entry->name), "%s", path);
 
 	g_num_namespaces++;
@@ -1213,6 +1221,10 @@ register_ns(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns *ns)
 	entry->size_in_ios = ns_size / g_io_size_bytes;
 	entry->io_size_blocks = g_io_size_bytes / sector_size;
 
+	if (g_is_random && g_zipf_theta > 0) {
+		entry->zipf = spdk_zipf_create(entry->size_in_ios, g_zipf_theta, 0);
+	}
+
 	entry->block_size = spdk_nvme_ns_get_extended_sector_size(ns);
 	entry->md_size = spdk_nvme_ns_get_md_size(ns);
 	entry->md_interleave = spdk_nvme_ns_supports_extended_lba(ns);
@@ -1253,6 +1265,7 @@ unregister_namespaces(void)
 
 	TAILQ_FOREACH_SAFE(entry, &g_namespaces, link, tmp) {
 		TAILQ_REMOVE(&g_namespaces, entry, link);
+		spdk_zipf_free(&entry->zipf);
 		free(entry);
 	}
 }
@@ -1341,8 +1354,6 @@ register_ctrlr(struct spdk_nvme_ctrlr *ctrlr, struct trid_entry *trid_entry)
 	}
 }
 
-static __thread unsigned int seed = 0;
-
 static inline void
 submit_single_io(struct perf_task *task)
 {
@@ -1351,8 +1362,10 @@ submit_single_io(struct perf_task *task)
 	struct ns_worker_ctx	*ns_ctx = task->ns_ctx;
 	struct ns_entry		*entry = ns_ctx->entry;
 
-	if (g_is_random) {
-		offset_in_ios = rand_r(&seed) % entry->size_in_ios;
+	if (entry->zipf) {
+		offset_in_ios = spdk_zipf_generate(entry->zipf);
+	} else if (g_is_random) {
+		offset_in_ios = rand_r(&entry->seed) % entry->size_in_ios;
 	} else {
 		offset_in_ios = ns_ctx->offset_in_ios++;
 		if (ns_ctx->offset_in_ios == entry->size_in_ios) {
@@ -1363,7 +1376,7 @@ submit_single_io(struct perf_task *task)
 	task->submit_tsc = spdk_get_ticks();
 
 	if ((g_rw_percentage == 100) ||
-	    (g_rw_percentage != 0 && ((rand_r(&seed) % 100) < g_rw_percentage))) {
+	    (g_rw_percentage != 0 && ((rand_r(&entry->seed) % 100) < g_rw_percentage))) {
 		task->is_read = true;
 	} else {
 		task->is_read = false;
@@ -1527,13 +1540,13 @@ print_periodic_performance(bool warmup)
 		if (g_monitor_perf_cores) {
 			core_busy_tsc += busy_tsc;
 			core_idle_tsc += idle_tsc;
-			core_busy_perc += (double)core_busy_tsc / (core_idle_tsc + core_busy_tsc) * 100;
 		}
 	}
 	mb_this_second = (double)io_this_second * g_io_size_bytes / (1024 * 1024);
 
 	printf("%s%9ju IOPS, %8.2f MiB/s", warmup ? "[warmup] " : "", io_this_second, mb_this_second);
 	if (g_monitor_perf_cores) {
+		core_busy_perc = (double)core_busy_tsc / (core_idle_tsc + core_busy_tsc) * 100;
 		printf("%3d Core(s): %6.2f%% Busy", g_num_workers, core_busy_perc);
 	}
 	printf("\r");
@@ -1699,6 +1712,7 @@ static void usage(char *program_name)
 	printf("\t[-w, --io-pattern <pattern> io pattern type, must be one of\n");
 	printf("\t\t(read, write, randread, randwrite, rw, randrw)]\n");
 	printf("\t[-M, --rwmixread <0-100> rwmixread (100 for reads, 0 for writes)]\n");
+	printf("\t[-F, --zipf <theta> use zipf distribution for random I/O\n");
 	printf("\t[-L, --enable-sw-latency-tracking enable latency tracking via sw, default: disabled]\n");
 	printf("\t\t-L for latency summary, -LL for detailed histogram\n");
 	printf("\t[-l, --enable-ssd-latency-tracking enable latency tracking via ssd (if supported), default: disabled]\n");
@@ -2171,7 +2185,7 @@ parse_metadata(const char *metacfg_str)
 	return 0;
 }
 
-#define PERF_GETOPT_SHORT "a:b:c:e:gi:lmo:q:r:k:s:t:w:z:A:C:DGHILM:NO:P:Q:RS:T:U:VZ:"
+#define PERF_GETOPT_SHORT "a:b:c:e:gi:lmo:q:r:k:s:t:w:z:A:C:DF:GHILM:NO:P:Q:RS:T:U:VZ:"
 
 static const struct option g_perf_cmdline_opts[] = {
 #define PERF_WARMUP_TIME	'a'
@@ -2212,6 +2226,8 @@ static const struct option g_perf_cmdline_opts[] = {
 	{"max-completion-per-poll",			required_argument,	NULL, PERF_MAX_COMPLETIONS_PER_POLL},
 #define PERF_DISABLE_SQ_CMB	'D'
 	{"disable-sq-cmb",			no_argument,	NULL, PERF_DISABLE_SQ_CMB},
+#define PERF_ZIPF		'F'
+	{"zipf",				required_argument,	NULL, PERF_ZIPF},
 #define PERF_ENABLE_DEBUG	'G'
 	{"enable-debug",			no_argument,	NULL, PERF_ENABLE_DEBUG},
 #define PERF_ENABLE_TCP_HDGST	'H'
@@ -2256,6 +2272,7 @@ parse_args(int argc, char **argv, struct spdk_env_opts *env_opts)
 	int op, long_idx;
 	long int val;
 	int rc;
+	char *endptr;
 
 	while ((op = getopt_long(argc, argv, PERF_GETOPT_SHORT, g_perf_cmdline_opts, &long_idx)) != -1) {
 		switch (op) {
@@ -2331,6 +2348,14 @@ parse_args(int argc, char **argv, struct spdk_env_opts *env_opts)
 				break;
 			}
 			break;
+		case PERF_ZIPF:
+			errno = 0;
+			g_zipf_theta = strtod(optarg, &endptr);
+			if (errno || optarg == endptr || g_zipf_theta < 0) {
+				fprintf(stderr, "Illegal zipf theta value %s\n", optarg);
+				return 1;
+			}
+			break;
 		case PERF_ALLOWED_PCI_ADDR:
 			if (add_allowed_pci_device(optarg, env_opts)) {
 				usage(argv[0]);
diff --git a/examples/nvmf/nvmf/Makefile b/examples/nvmf/nvmf/Makefile
index 4db21ca82..d83d278cc 100644
--- a/examples/nvmf/nvmf/Makefile
+++ b/examples/nvmf/nvmf/Makefile
@@ -38,6 +38,6 @@ include $(SPDK_ROOT_DIR)/mk/spdk.modules.mk
 APP := nvmf
 
 C_SRCS := nvmf.c
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev nvmf
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev nvmf
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/examples/nvmf/nvmf/nvmf.c b/examples/nvmf/nvmf/nvmf.c
index 8f7170980..5529968ae 100644
--- a/examples/nvmf/nvmf/nvmf.c
+++ b/examples/nvmf/nvmf/nvmf.c
@@ -32,8 +32,10 @@
  */
 
 #include "spdk/stdinc.h"
+
 #include "spdk/env.h"
 #include "spdk/event.h"
+#include "spdk/init.h"
 #include "spdk/string.h"
 #include "spdk/thread.h"
 #include "spdk/bdev.h"
diff --git a/examples/sock/hello_world/Makefile b/examples/sock/hello_world/Makefile
index f86df44cc..350c30570 100644
--- a/examples/sock/hello_world/Makefile
+++ b/examples/sock/hello_world/Makefile
@@ -38,6 +38,6 @@ APP = hello_sock
 C_SRCS := hello_sock.c
 
 SPDK_LIB_LIST = $(SOCK_MODULES_LIST)
-SPDK_LIB_LIST += event_net sock
+SPDK_LIB_LIST += event event_net sock
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/examples/util/Makefile b/examples/util/Makefile
new file mode 100644
index 000000000..40ddb1e2c
--- /dev/null
+++ b/examples/util/Makefile
@@ -0,0 +1,47 @@
+#
+#  BSD LICENSE
+#
+#  Copyright (c) Intel Corporation.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Intel Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
+include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
+
+DIRS-y += zipf
+
+.PHONY: all clean $(DIRS-y)
+
+all: $(DIRS-y)
+	@:
+
+clean: $(DIRS-y)
+	@:
+
+include $(SPDK_ROOT_DIR)/mk/spdk.subdirs.mk
diff --git a/examples/util/zipf/Makefile b/examples/util/zipf/Makefile
new file mode 100644
index 000000000..66528d9a9
--- /dev/null
+++ b/examples/util/zipf/Makefile
@@ -0,0 +1,42 @@
+#
+#  Copyright (c) Intel Corporation.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Intel Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../..)
+include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
+include $(SPDK_ROOT_DIR)/mk/spdk.modules.mk
+
+APP = zipf
+
+C_SRCS := zipf.c
+
+SPDK_LIB_LIST = util
+
+include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/examples/util/zipf/zipf.c b/examples/util/zipf/zipf.c
new file mode 100644
index 000000000..d62ca2daf
--- /dev/null
+++ b/examples/util/zipf/zipf.c
@@ -0,0 +1,102 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/stdinc.h"
+#include "spdk/zipf.h"
+#include "spdk/histogram_data.h"
+#include "spdk/string.h"
+
+static void
+usage(const char *prog)
+{
+	printf("usage: %s <theta> <range> <count>\n", prog);
+}
+
+static void
+print_bucket(void *ctx, uint64_t start, uint64_t end, uint64_t count,
+	     uint64_t total, uint64_t so_far)
+{
+	double so_far_pct;
+	char range[64];
+
+	if (count == 0) {
+		return;
+	}
+
+	so_far_pct = (double)so_far * 100 / total;
+	snprintf(range, sizeof(range), "[%ju, %ju)", start, end);
+	printf("%24s: %9.4f%%  (%9ju)\n", range, so_far_pct, count);
+}
+
+int
+main(int argc, char **argv)
+{
+	struct spdk_zipf *zipf;
+	struct spdk_histogram_data *h;
+	float theta;
+	int range, count, i;
+
+	if (argc < 4) {
+		usage(argv[0]);
+		return 1;
+	}
+
+	theta = atof(argv[1]);
+	range = spdk_strtol(argv[2], 10);
+	count = spdk_strtol(argv[3], 10);
+
+	if (range <= 0 || count <= 0) {
+		printf("range and count must be positive integers\n");
+		usage(argv[0]);
+		return 1;
+	}
+
+	zipf = spdk_zipf_create(range, theta, time(NULL));
+	h = spdk_histogram_data_alloc();
+	if (zipf == NULL || h == NULL) {
+		spdk_zipf_free(&zipf);
+		spdk_histogram_data_free(h);
+		printf("out of resource\n");
+		return 1;
+	}
+
+	for (i = 0; i < count; i++) {
+		spdk_histogram_data_tally(h, spdk_zipf_generate(zipf));
+	}
+
+	spdk_histogram_data_iterate(h, print_bucket, NULL);
+	spdk_histogram_data_free(h);
+	spdk_zipf_free(&zipf);
+
+	return 0;
+}
diff --git a/include/spdk/accel_engine.h b/include/spdk/accel_engine.h
index 205877958..051e2aadf 100644
--- a/include/spdk/accel_engine.h
+++ b/include/spdk/accel_engine.h
@@ -51,6 +51,7 @@ enum accel_capability {
 	ACCEL_COMPARE		= 1 << 3,
 	ACCEL_CRC32C		= 1 << 4,
 	ACCEL_DIF		= 1 << 5,
+	ACCEL_COPY_CRC32C	= 1 << 6,
 };
 
 /**
@@ -336,6 +337,28 @@ int spdk_accel_batch_prep_crc32cv(struct spdk_io_channel *ch, struct spdk_accel_
 				  uint32_t *dst, struct iovec *iovs, uint32_t iovcnt, uint32_t seed,
 				  spdk_accel_completion_cb cb_fn, void *cb_arg);
 
+/**
+ * Synchronous call to prepare a copy + crc32c request into a previously initialized batch
+ *  created with spdk_accel_batch_create(). The callback will be called when the operation
+ *  completes after the batch has been submitted by an asynchronous call to
+ *  spdk_accel_batch_submit().
+ *
+ * \param ch I/O channel associated with this call.
+ * \param batch Handle provided when the batch was started with spdk_accel_batch_create().
+ * \param dst Destination to write the data to.
+ * \param src The source address for the data.
+ * \param crc_dst Destination to write the CRC-32C to.
+ * \param seed Four byte seed value.
+ * \param nbytes Length in bytes.
+ * \param cb_fn Called when this operation completes.
+ * \param cb_arg Callback argument.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_accel_batch_prep_copy_crc32c(struct spdk_io_channel *ch, struct spdk_accel_batch *batch,
+				      void *dst, void *src, uint32_t *crc_dst, uint32_t seed, uint64_t nbytes,
+				      spdk_accel_completion_cb cb_fn, void *cb_arg);
+
 /**
  * Submit a CRC-32C calculation request.
  *
@@ -372,6 +395,26 @@ int spdk_accel_submit_crc32c(struct spdk_io_channel *ch, uint32_t *dst, void *sr
 int spdk_accel_submit_crc32cv(struct spdk_io_channel *ch, uint32_t *dst, struct iovec *iovs,
 			      uint32_t iovcnt, uint32_t seed, spdk_accel_completion_cb cb_fn, void *cb_arg);
 
+/**
+ * Submit a copy with CRC-32C calculation request.
+ *
+ * This operation will copy data and calculate the 4 byte CRC32-C for the given data.
+ *
+ * \param ch I/O channel associated with this call.
+ * \param dst Destination to write the data to.
+ * \param src The source address for the data.
+ * \param crc_dst Destination to write the CRC-32C to.
+ * \param seed Four byte seed value.
+ * \param nbytes Length in bytes.
+ * \param cb_fn Called when this CRC-32C operation completes.
+ * \param cb_arg Callback argument.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_accel_submit_copy_crc32c(struct spdk_io_channel *ch, void *dst, void *src,
+				  uint32_t *crc_dst, uint32_t seed, uint64_t nbytes,
+				  spdk_accel_completion_cb cb_fn, void *cb_arg);
+
 struct spdk_json_write_ctx;
 
 /**
diff --git a/include/spdk/bdev.h b/include/spdk/bdev.h
index 72f673b04..a7fabc790 100644
--- a/include/spdk/bdev.h
+++ b/include/spdk/bdev.h
@@ -590,6 +590,14 @@ bool spdk_bdev_is_zoned(const struct spdk_bdev *bdev);
  */
 uint32_t spdk_bdev_get_data_block_size(const struct spdk_bdev *bdev);
 
+/**
+ * Get block device physical block size.
+ *
+ * \param bdev Block device to query.
+ * \return Size of physical block size for this bdev in bytes.
+ */
+uint32_t spdk_bdev_get_physical_block_size(const struct spdk_bdev *bdev);
+
 /**
  * Get DIF type of the block device.
  *
@@ -795,7 +803,7 @@ int spdk_bdev_read_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *c
  *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
  */
 int spdk_bdev_read_blocks_with_md(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
-				  void *buf, void *md, int64_t offset_blocks, uint64_t num_blocks,
+				  void *buf, void *md, uint64_t offset_blocks, uint64_t num_blocks,
 				  spdk_bdev_io_completion_cb cb, void *cb_arg);
 
 /**
@@ -1218,6 +1226,8 @@ int spdk_bdev_comparev_and_writev_blocks(struct spdk_bdev_desc *desc, struct spd
  *
  * \param desc Block device descriptor
  * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param iov A scatter gather list to be populated with the buffers
+ * \param iovcnt The maximum number of elements in iov.
  * \param offset_blocks The offset, in blocks, from the start of the block device.
  * \param num_blocks The number of blocks.
  * \param populate Whether the data buffer should be populated with the
@@ -1231,11 +1241,11 @@ int spdk_bdev_comparev_and_writev_blocks(struct spdk_bdev_desc *desc, struct spd
  * negated errno on failure, in which case the callback will not be called.
  */
 int spdk_bdev_zcopy_start(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			  struct iovec *iov, int iovcnt,
 			  uint64_t offset_blocks, uint64_t num_blocks,
 			  bool populate,
 			  spdk_bdev_io_completion_cb cb, void *cb_arg);
 
-
 /**
  * Submit a request to release a data buffer representing a range of blocks.
  *
diff --git a/include/spdk/bdev_module.h b/include/spdk/bdev_module.h
index ac7739625..1b44011f1 100644
--- a/include/spdk/bdev_module.h
+++ b/include/spdk/bdev_module.h
@@ -47,6 +47,7 @@
 #include "spdk/queue.h"
 #include "spdk/scsi_spec.h"
 #include "spdk/thread.h"
+#include "spdk/tree.h"
 #include "spdk/util.h"
 #include "spdk/uuid.h"
 
@@ -161,6 +162,36 @@ struct spdk_bdev_module {
 	} internal;
 };
 
+/**
+ * Called by a bdev module to lay exclusive write claim to a bdev.
+ *
+ * Also upgrades that bdev's descriptor to have write access.
+ *
+ * \param bdev Block device to be claimed.
+ * \param desc Descriptor for the above block device.
+ * \param module Bdev module attempting to claim bdev.
+ *
+ * \return 0 on success
+ * \return -EPERM if the bdev is already claimed by another module.
+ */
+int spdk_bdev_module_claim_bdev(struct spdk_bdev *bdev, struct spdk_bdev_desc *desc,
+				struct spdk_bdev_module *module);
+
+/**
+ * Called to release a write claim on a block device.
+ *
+ * \param bdev Block device to be released.
+ */
+void spdk_bdev_module_release_bdev(struct spdk_bdev *bdev);
+
+/* Libraries may define __SPDK_BDEV_MODULE_ONLY so that they include
+ * only the struct spdk_bdev_module definition, and the relevant APIs
+ * to claim/release a bdev. This may be useful in some cases to avoid
+ * abidiff errors related to including the struct spdk_bdev structure
+ * unnecessarily.
+ */
+#ifndef __SPDK_BDEV_MODULE_ONLY
+
 typedef void (*spdk_bdev_unregister_cb)(void *cb_arg, int rc);
 
 /**
@@ -237,8 +268,14 @@ enum spdk_bdev_io_status {
 	SPDK_BDEV_IO_STATUS_SUCCESS = 1,
 };
 
+struct spdk_bdev_name {
+	char *name;
+	struct spdk_bdev *bdev;
+	RB_ENTRY(spdk_bdev_name) node;
+};
+
 struct spdk_bdev_alias {
-	char *alias;
+	struct spdk_bdev_name alias;
 	TAILQ_ENTRY(spdk_bdev_alias) tailq;
 };
 
@@ -265,6 +302,9 @@ struct spdk_bdev {
 	/** Size in bytes of a logical block for the backend */
 	uint32_t blocklen;
 
+	/** Size in bytes of a physical block for the backend */
+	uint32_t phys_blocklen;
+
 	/** Number of blocks */
 	uint64_t blockcnt;
 
@@ -310,6 +350,15 @@ struct spdk_bdev {
 	/* Maximum number of segments in a I/O */
 	uint32_t max_num_segments;
 
+	/* Maximum unmap in unit of logical block */
+	uint32_t max_unmap;
+
+	/* Maximum unmap block segments */
+	uint32_t max_unmap_segments;
+
+	/* Maximum write zeroes in unit of logical block */
+	uint32_t max_write_zeroes;
+
 	/**
 	 * UUID for this bdev.
 	 *
@@ -462,6 +511,9 @@ struct spdk_bdev {
 		 *  locked due to overlapping with another locked range.
 		 */
 		lba_range_tailq_t pending_locked_ranges;
+
+		/** Bdev name used for quick lookup */
+		struct spdk_bdev_name bdev_name;
 	} internal;
 };
 
@@ -766,28 +818,6 @@ void spdk_bdev_module_init_done(struct spdk_bdev_module *module);
  */
 void spdk_bdev_module_finish_done(void);
 
-/**
- * Called by a bdev module to lay exclusive write claim to a bdev.
- *
- * Also upgrades that bdev's descriptor to have write access.
- *
- * \param bdev Block device to be claimed.
- * \param desc Descriptor for the above block device.
- * \param module Bdev module attempting to claim bdev.
- *
- * \return 0 on success
- * \return -EPERM if the bdev is already claimed by another module.
- */
-int spdk_bdev_module_claim_bdev(struct spdk_bdev *bdev, struct spdk_bdev_desc *desc,
-				struct spdk_bdev_module *module);
-
-/**
- * Called to release a write claim on a block device.
- *
- * \param bdev Block device to be released.
- */
-void spdk_bdev_module_release_bdev(struct spdk_bdev *bdev);
-
 /**
  * Add alias to block device names list.
  * Aliases can be add only to registered bdev.
@@ -1227,4 +1257,6 @@ static void __attribute__((constructor)) _spdk_bdev_module_register_##name(void)
 	spdk_bdev_module_list_add(module); \
 } \
 
+#endif /* __SPDK_BDEV_MODULE_ONLY */
+
 #endif /* SPDK_BDEV_MODULE_H */
diff --git a/include/spdk/crc32.h b/include/spdk/crc32.h
index a2032a25e..5b6dbb4b7 100644
--- a/include/spdk/crc32.h
+++ b/include/spdk/crc32.h
@@ -66,6 +66,16 @@ uint32_t spdk_crc32_ieee_update(const void *buf, size_t len, uint32_t crc);
  */
 uint32_t spdk_crc32c_update(const void *buf, size_t len, uint32_t crc);
 
+/**
+ * Calculate a partial CRC-32C checksum.
+ *
+ * \param iov Data buffer vectors to checksum.
+ * \param iovcnt size of iov parameter.
+ * \param crc32c Previous CRC-32C value.
+ * \return Updated CRC-32C value.
+ */
+uint32_t spdk_crc32c_iov_update(struct iovec *iov, int iovcnt, uint32_t crc32c);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/include/spdk/env.h b/include/spdk/env.h
index c8fbad1c2..3aaf42e65 100644
--- a/include/spdk/env.h
+++ b/include/spdk/env.h
@@ -1343,7 +1343,7 @@ struct spdk_pci_event {
 	struct spdk_pci_addr traddr;
 };
 
-typedef void (*spdk_pci_error_handler)(siginfo_t *info, void *ctx);
+typedef void (*spdk_pci_error_handler)(const void *failure_addr, void *ctx);
 
 /**
  * Begin listening for PCI bus events. This is used to detect hot-insert and
diff --git a/include/spdk/event.h b/include/spdk/event.h
index d65c29172..74c946728 100644
--- a/include/spdk/event.h
+++ b/include/spdk/event.h
@@ -44,6 +44,7 @@
 #include "spdk/stdinc.h"
 
 #include "spdk/cpuset.h"
+#include "spdk/init.h"
 #include "spdk/queue.h"
 #include "spdk/log.h"
 #include "spdk/thread.h"
@@ -82,8 +83,6 @@ typedef void (*spdk_app_shutdown_cb)(void);
  */
 typedef void (*spdk_sighandler_t)(int signal);
 
-#define SPDK_DEFAULT_RPC_ADDR "/var/tmp/spdk.sock"
-
 /**
  * \brief Event framework initialization options
  */
diff --git a/include/spdk/histogram_data.h b/include/spdk/histogram_data.h
index 5f114fe69..de9ef08e7 100644
--- a/include/spdk/histogram_data.h
+++ b/include/spdk/histogram_data.h
@@ -123,9 +123,7 @@ __spdk_histogram_data_get_bucket_range(struct spdk_histogram_data *h, uint64_t d
 {
 	uint32_t clz, range;
 
-	assert(datapoint != 0);
-
-	clz = __builtin_clzll(datapoint);
+	clz = datapoint > 0 ? __builtin_clzll(datapoint) : 64;
 
 	if (clz <= SPDK_HISTOGRAM_BUCKET_LSB(h)) {
 		range = SPDK_HISTOGRAM_BUCKET_LSB(h) - clz;
diff --git a/include/spdk/idxd.h b/include/spdk/idxd.h
index b7674627e..80120844f 100644
--- a/include/spdk/idxd.h
+++ b/include/spdk/idxd.h
@@ -69,14 +69,6 @@ struct idxd_batch;
  */
 int spdk_idxd_configure_chan(struct spdk_idxd_io_channel *chan);
 
-/**
- * Reconfigures this channel based on how many current channels there are.
- *
- * \param chan IDXD channel to be set.
- * \return 0 on success, negative errno on failure.
- */
-int spdk_idxd_reconfigure_chan(struct spdk_idxd_io_channel *chan);
-
 /**
  * Signature for callback function invoked when a request is completed.
  *
@@ -86,26 +78,14 @@ int spdk_idxd_reconfigure_chan(struct spdk_idxd_io_channel *chan);
  */
 typedef void (*spdk_idxd_req_cb)(void *arg, int status);
 
-/**
- * Callback for spdk_idxd_probe() enumeration.
- *
- * \param cb_ctx User-specified opaque value corresponding to cb_ctx from spdk_idxd_probe().
- * \param pci_dev PCI device that is being probed.
- *
- * \return true to attach to this device.
- */
-typedef bool (*spdk_idxd_probe_cb)(void *cb_ctx, struct spdk_pci_device *pci_dev);
-
 /**
  * Callback for spdk_idxd_probe() to report a device that has been attached to
  * the userspace IDXD driver.
  *
  * \param cb_ctx User-specified opaque value corresponding to cb_ctx from spdk_idxd_probe().
- * \param pci_dev PCI device that was attached to the driver.
  * \param idxd IDXD device that was attached to the driver.
  */
-typedef void (*spdk_idxd_attach_cb)(void *cb_ctx, struct spdk_pci_device *pci_dev,
-				    struct spdk_idxd_device *idxd);
+typedef void (*spdk_idxd_attach_cb)(void *cb_ctx, struct spdk_idxd_device *idxd);
 
 /**
  * Enumerate the IDXD devices attached to the system and attach the userspace
@@ -119,13 +99,12 @@ typedef void (*spdk_idxd_attach_cb)(void *cb_ctx, struct spdk_pci_device *pci_de
  *
  * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of
  * the callbacks.
- * \param probe_cb will be called once per IDXD device found in the system.
  * \param attach_cb will be called for devices for which probe_cb returned true
  * once the IDXD controller has been attached to the userspace driver.
  *
  * \return 0 on success, -1 on failure.
  */
-int spdk_idxd_probe(void *cb_ctx, spdk_idxd_probe_cb probe_cb, spdk_idxd_attach_cb attach_cb);
+int spdk_idxd_probe(void *cb_ctx, spdk_idxd_attach_cb attach_cb);
 
 /**
  * Detach specified device returned by spdk_idxd_probe() from the IDXD driver.
@@ -352,7 +331,7 @@ int spdk_idxd_submit_fill(struct spdk_idxd_io_channel *chan,
  *
  * \param chan IDXD channel to submit request.
  * \param batch Handle provided when the batch was started with spdk_idxd_batch_create().
- * \param dst Resulting calculation.
+ * \param crc_dst Resulting calculation.
  * \param src Source virtual address.
  * \param seed Four byte CRC-32C seed value.
  * \param nbytes Number of bytes to calculate on.
@@ -363,7 +342,7 @@ int spdk_idxd_submit_fill(struct spdk_idxd_io_channel *chan,
  * \return 0 on success, negative errno on failure.
  */
 int spdk_idxd_batch_prep_crc32c(struct spdk_idxd_io_channel *chan, struct idxd_batch *batch,
-				uint32_t *dst, void *src, uint32_t seed, uint64_t nbytes,
+				uint32_t *crc_dst, void *src, uint32_t seed, uint64_t nbytes,
 				spdk_idxd_req_cb cb_fn, void *cb_arg);
 
 /**
@@ -373,7 +352,7 @@ int spdk_idxd_batch_prep_crc32c(struct spdk_idxd_io_channel *chan, struct idxd_b
  * by writing to the proper device portal.
  *
  * \param chan IDXD channel to submit request.
- * \param dst Resulting calculation.
+ * \param crc_dst Resulting calculation.
  * \param src Source virtual address.
  * \param seed Four byte CRC-32C seed value.
  * \param nbytes Number of bytes to calculate on.
@@ -383,10 +362,55 @@ int spdk_idxd_batch_prep_crc32c(struct spdk_idxd_io_channel *chan, struct idxd_b
  *
  * \return 0 on success, negative errno on failure.
  */
-int spdk_idxd_submit_crc32c(struct spdk_idxd_io_channel *chan, uint32_t *dst, void *src,
+int spdk_idxd_submit_crc32c(struct spdk_idxd_io_channel *chan, uint32_t *crc_dst, void *src,
 			    uint32_t seed, uint64_t nbytes,
 			    spdk_idxd_req_cb cb_fn, void *cb_arg);
 
+/**
+ * Synchronous call to prepare a copy combined with crc32c request into a previously
+ *  initialized batch created with spdk_idxd_batch_create(). The callback will be called
+ *  when the copy + crc32c completes after the batch has been submitted by an asynchronous
+ *  call to spdk_idxd_batch_submit().
+ *
+ * \param chan IDXD channel to submit request.
+ * \param batch Handle provided when the batch was started with spdk_idxd_batch_create().
+ * \param dst Destination virtual address.
+ * \param src Source virtual address.
+ * \param crc_dst Resulting calculation.
+ * \param seed Four byte CRC-32C seed value.
+ * \param nbytes Number of bytes to calculate on.
+ * \param cb_fn Callback function which will be called when the request is complete.
+ * \param cb_arg Opaque value which will be passed back as the arg parameter in
+ * the completion callback.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_idxd_batch_prep_copy_crc32c(struct spdk_idxd_io_channel *chan, struct idxd_batch *batch,
+				     void *dst, void *src, uint32_t *crc_dst, uint32_t seed, uint64_t nbytes,
+				     spdk_idxd_req_cb cb_fn, void *cb_arg);
+
+/**
+ * Build and submit a copy combined with CRC32-C request.
+ *
+ * This function will build the descriptor for copy plus CRC32-C and then immediately
+ * submit by writing to the proper device portal.
+ *
+ * \param chan IDXD channel to submit request.
+ * \param dst Destination virtual address.
+ * \param src Source virtual address.
+ * \param crc_dst Resulting calculation.
+ * \param seed Four byte CRC-32C seed value.
+ * \param nbytes Number of bytes to calculate on.
+ * \param cb_fn Callback function which will be called when the request is complete.
+ * \param cb_arg Opaque value which will be passed back as the cb_arg parameter
+ * in the completion callback.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_idxd_submit_copy_crc32c(struct spdk_idxd_io_channel *chan, void *dst, void *src,
+				 uint32_t *crc_dst, uint32_t seed, uint64_t nbytes,
+				 spdk_idxd_req_cb cb_fn, void *cb_arg);
+
 /**
  * Check for completed requests on an IDXD channel.
  *
@@ -399,7 +423,6 @@ int spdk_idxd_process_events(struct spdk_idxd_io_channel *chan);
  * Returns an IDXD channel for a given IDXD device.
  *
  * \param idxd IDXD device to get a channel for.
- *
  * \return pointer to an IDXD channel.
  */
 struct spdk_idxd_io_channel *spdk_idxd_get_channel(struct spdk_idxd_device *idxd);
@@ -408,17 +431,16 @@ struct spdk_idxd_io_channel *spdk_idxd_get_channel(struct spdk_idxd_device *idxd
  * Free an IDXD channel.
  *
  * \param chan IDXD channel to free.
- * \return true if the underlying device needs a rebalance
  */
-bool spdk_idxd_put_channel(struct spdk_idxd_io_channel *chan);
+void spdk_idxd_put_channel(struct spdk_idxd_io_channel *chan);
 
 /**
- * Determine if the idxd device needs rebalancing.
+ * Get the max number of outstanding operations supported by this channel.
  *
- * \param idxd IDXD device.
- * \return true if rebalance is needed, false if not.
+ * \param chan IDXD channel to communicate on.
+ * \return max number of operations supported.
  */
-bool spdk_idxd_device_needs_rebalance(struct spdk_idxd_device *idxd);
+int spdk_idxd_chan_get_max_operations(struct spdk_idxd_io_channel *chan);
 
 #ifdef __cplusplus
 }
diff --git a/include/spdk/init.h b/include/spdk/init.h
new file mode 100644
index 000000000..aa8abc174
--- /dev/null
+++ b/include/spdk/init.h
@@ -0,0 +1,107 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.  All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * SPDK Initialization Helper
+ */
+
+#ifndef SPDK_INIT_H
+#define SPDK_INIT_H
+
+#include "spdk/stdinc.h"
+#include "spdk/queue.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define SPDK_DEFAULT_RPC_ADDR "/var/tmp/spdk.sock"
+
+/**
+ * Create the SPDK JSON-RPC server and listen at the provided address. The RPC server is optional and is
+ * independent of subsystem initialization. The RPC server can be started and stopped at any time.
+ *
+ * \param listen_addr Path to a unix domain socket to listen on
+ *
+ * \return Negated errno on failure. 0 on success.
+ */
+int spdk_rpc_initialize(const char *listen_addr);
+
+/**
+ * Shut down the SPDK JSON-RPC target
+ */
+void spdk_rpc_finish(void);
+
+typedef void (*spdk_subsystem_init_fn)(int rc, void *ctx);
+
+/**
+ * Begin the initialization process for all SPDK subsystems. SPDK is divided into subsystems at a macro-level
+ * and each subsystem automatically registers itself with this library at start up using a C
+ * constructor. Further, each subsystem can declare other subsystems that it depends on.
+ * Calling this function will correctly initialize all subsystems that are present, in the
+ * required order.
+ *
+ * \param cb_fn Function called when the process is complete.
+ * \param cb_arg User context passed to cb_fn.
+ */
+void spdk_subsystem_init(spdk_subsystem_init_fn cb_fn, void *cb_arg);
+
+/**
+ * Like spdk_subsystem_init, but additionally configure each subsystem using the provided JSON config
+ * file. This will automatically start a JSON RPC server and then stop it.
+ *
+ * \param json_config_file Path to a JSON config file.
+ * \param rpc_addr Path to a unix domain socket to send configuration RPCs to.
+ * \param cb_fn Function called when the process is complete.
+ * \param cb_arg User context passed to cb_fn.
+ * \param stop_on_error Whether to stop initialization if one of the JSON RPCs fails.
+ */
+void spdk_subsystem_init_from_json_config(const char *json_config_file, const char *rpc_addr,
+		spdk_subsystem_init_fn cb_fn, void *cb_arg,
+		bool stop_on_error);
+
+typedef void (*spdk_subsystem_fini_fn)(void *ctx);
+
+/**
+ * Tear down all of the subsystems in the correct order.
+ *
+ * \param cb_fn Function called when the process is complete.
+ * \param cb_arg User context passed to cb_fn
+ */
+void spdk_subsystem_fini(spdk_subsystem_fini_fn cb_fn, void *cb_arg);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/include/spdk/nvme.h b/include/spdk/nvme.h
index f5a8f73c9..a2d44a2bd 100644
--- a/include/spdk/nvme.h
+++ b/include/spdk/nvme.h
@@ -297,6 +297,15 @@ struct spdk_nvme_accel_fn_table {
  */
 bool spdk_nvme_ctrlr_is_discovery(struct spdk_nvme_ctrlr *ctrlr);
 
+/**
+ * Indicate whether a ctrlr handle is associated with a fabrics controller.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return true if a fabrics controller, else false.
+ */
+bool spdk_nvme_ctrlr_is_fabrics(struct spdk_nvme_ctrlr *ctrlr);
+
 /**
  * Get the default options for the creation of a specific NVMe controller.
  *
@@ -3062,6 +3071,40 @@ int spdk_nvme_ns_cmd_dataset_management(struct spdk_nvme_ns *ns, struct spdk_nvm
 					spdk_nvme_cmd_cb cb_fn,
 					void *cb_arg);
 
+/**
+ * Submit a simple copy command request to the specified NVMe namespace.
+ *
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * This is a convenience wrapper that will automatically allocate and construct
+ * the correct data buffers. Therefore, ranges does not need to be allocated from
+ * pinned memory and can be placed on the stack. If a higher performance, zero-copy
+ * version of SCC is required, simply build and submit a raw command using
+ * spdk_nvme_ctrlr_cmd_io_raw().
+ *
+ * \param ns NVMe namespace to submit the SCC request
+ * \param qpair I/O queue pair to submit the request
+ * \param ranges An array of \ref spdk_nvme_scc_source_range elements describing the LBAs
+ * to operate on.
+ * \param num_ranges The number of elements in the ranges array.
+ * \param dest_lba Destination LBA to copy the data.
+ * \param cb_fn Callback function to invoke when the I/O is completed
+ * \param cb_arg Argument to pass to the callback function
+ *
+ * \return 0 if successfully submitted, negated errnos on the following error conditions:
+ * -ENOMEM: The request cannot be allocated.
+ * -EINVAL: Invalid ranges.
+ * -ENXIO: The qpair is failed at the transport level.
+ */
+int spdk_nvme_ns_cmd_copy(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+			  const struct spdk_nvme_scc_source_range *ranges,
+			  uint16_t num_ranges,
+			  uint64_t dest_lba,
+			  spdk_nvme_cmd_cb cb_fn,
+			  void *cb_arg);
+
 /**
  * Submit a flush request to the specified NVMe namespace.
  *
diff --git a/include/spdk/nvme_spec.h b/include/spdk/nvme_spec.h
index b715b41f5..d44755819 100644
--- a/include/spdk/nvme_spec.h
+++ b/include/spdk/nvme_spec.h
@@ -72,6 +72,11 @@ extern "C" {
  */
 #define SPDK_NVME_DATASET_MANAGEMENT_RANGE_MAX_BLOCKS	0xFFFFFFFFu
 
+/**
+ * Maximum number of entries in the log page of Changed Namespace List.
+ */
+#define SPDK_NVME_MAX_CHANGED_NAMESPACES 1024
+
 union spdk_nvme_cap_register {
 	uint64_t	raw;
 	struct {
@@ -1338,6 +1343,21 @@ struct spdk_nvme_dsm_range {
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_dsm_range) == 16, "Incorrect size");
 
+/**
+ * Simple Copy Command source range
+ */
+struct spdk_nvme_scc_source_range {
+	uint64_t reserved0;
+	uint64_t slba;
+	uint16_t nlb;
+	uint16_t reserved18;
+	uint32_t reserved20;
+	uint32_t eilbrt;
+	uint16_t elbat;
+	uint16_t elbatm;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_scc_source_range) == 32, "Incorrect size");
+
 /**
  * Status code types
  */
@@ -1386,6 +1406,9 @@ enum spdk_nvme_generic_command_status_code {
 	SPDK_NVME_SC_SANITIZE_IN_PROGRESS		= 0x1d,
 	SPDK_NVME_SC_SGL_DATA_BLOCK_GRANULARITY_INVALID	= 0x1e,
 	SPDK_NVME_SC_COMMAND_INVALID_IN_CMB		= 0x1f,
+	SPDK_NVME_SC_COMMAND_NAMESPACE_IS_PROTECTED	= 0x20,
+	SPDK_NVME_SC_COMMAND_INTERRUPTED		= 0x21,
+	SPDK_NVME_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR	= 0x22,
 
 	SPDK_NVME_SC_LBA_OUT_OF_RANGE			= 0x80,
 	SPDK_NVME_SC_CAPACITY_EXCEEDED			= 0x81,
@@ -1443,6 +1466,7 @@ enum spdk_nvme_command_specific_status_code {
 	SPDK_NVME_SC_CONFLICTING_ATTRIBUTES		= 0x80,
 	SPDK_NVME_SC_INVALID_PROTECTION_INFO		= 0x81,
 	SPDK_NVME_SC_ATTEMPTED_WRITE_TO_RO_RANGE	= 0x82,
+	SPDK_NVME_SC_CMD_SIZE_LIMIT_SIZE_EXCEEDED	= 0x83,
 };
 
 /**
@@ -1539,6 +1563,8 @@ enum spdk_nvme_nvm_opcode {
 
 	SPDK_NVME_OPC_RESERVATION_ACQUIRE		= 0x11,
 	SPDK_NVME_OPC_RESERVATION_RELEASE		= 0x15,
+
+	SPDK_NVME_OPC_COPY				= 0x19,
 };
 
 /**
@@ -1822,7 +1848,7 @@ struct spdk_nvme_cdata_sgls {
 	uint32_t	reserved2 : 10;
 };
 
-struct __attribute__((packed)) __attribute__((aligned)) spdk_nvme_ctrlr_data {
+struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 	/* bytes 0-255: controller capabilities and features */
 
 	/** pci vendor id */
@@ -2149,7 +2175,9 @@ struct __attribute__((packed)) __attribute__((aligned)) spdk_nvme_ctrlr_data {
 		uint16_t	set_features_save: 1;
 		uint16_t	reservations: 1;
 		uint16_t	timestamp: 1;
-		uint16_t	reserved: 9;
+		uint16_t	verify: 1;
+		uint16_t	copy: 1;
+		uint16_t	reserved9: 7;
 	} oncs;
 
 	/** fused operation support */
@@ -2187,7 +2215,12 @@ struct __attribute__((packed)) __attribute__((aligned)) spdk_nvme_ctrlr_data {
 	/** atomic compare & write unit */
 	uint16_t		acwu;
 
-	uint16_t		reserved534;
+	/** optional copy formats supported */
+	struct {
+		uint16_t	copy_format0 : 1;
+		uint16_t	reserved1: 15;
+	} ocfs;
+
 
 	struct spdk_nvme_cdata_sgls sgls;
 
@@ -2479,7 +2512,16 @@ struct spdk_nvme_ns_data {
 	/** Namespace Optimal Write Size */
 	uint16_t                nows;
 
-	uint8_t			reserved64[18];
+	/** Maximum Single Source Range Length */
+	uint16_t                mssrl;
+
+	/** Maximum Copy Length */
+	uint32_t                mcl;
+
+	/** Maximum Source Range Count */
+	uint8_t	                msrc;
+
+	uint8_t			reserved64[11];
 
 	/** ANA group identifier */
 	uint32_t		anagrpid;
@@ -2794,8 +2836,12 @@ enum spdk_nvme_log_page {
 	/* 0x81-0xBF - I/O command set specific */
 
 	/* 0xC0-0xFF - vendor specific */
+	SPDK_NVME_LOG_VENDOR_SPECIFIC_START	= 0xc0,
+	SPDK_NVME_LOG_VENDOR_SPECIFIC_END	= 0xff,
 };
 
+#define spdk_nvme_log_page_is_vendor_specific(lid) ((lid) >= SPDK_NVME_LOG_VENDOR_SPECIFIC_START)
+
 /**
  * Error information log page (\ref SPDK_NVME_LOG_ERROR)
  */
diff --git a/include/spdk/nvmf.h b/include/spdk/nvmf.h
index 89435622b..06ca5d9f7 100644
--- a/include/spdk/nvmf.h
+++ b/include/spdk/nvmf.h
@@ -70,6 +70,7 @@ struct spdk_nvmf_target_opts {
 	char		name[NVMF_TGT_NAME_MAX_LENGTH];
 	uint32_t	max_subsystems;
 	uint32_t	acceptor_poll_rate;
+	uint16_t	crdt[3];
 };
 
 struct spdk_nvmf_transport_opts {
@@ -120,8 +121,14 @@ struct spdk_nvmf_listen_opts {
 void spdk_nvmf_listen_opts_init(struct spdk_nvmf_listen_opts *opts, size_t opts_size);
 
 struct spdk_nvmf_poll_group_stat {
+	/* cumulative admin qpair count */
 	uint32_t admin_qpairs;
+	/* cumulative io qpair count */
 	uint32_t io_qpairs;
+	/* current admin qpair count */
+	uint32_t current_admin_qpairs;
+	/* current io qpair count */
+	uint32_t current_io_qpairs;
 	uint64_t pending_bdev_io;
 };
 
@@ -844,6 +851,24 @@ struct spdk_nvmf_ns *spdk_nvmf_subsystem_get_ns(struct spdk_nvmf_subsystem *subs
  */
 uint32_t spdk_nvmf_subsystem_get_max_namespaces(const struct spdk_nvmf_subsystem *subsystem);
 
+/**
+ * Get the minimum controller ID allowed in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return Minimum controller ID allowed in the subsystem.
+ */
+uint16_t spdk_nvmf_subsystem_get_min_cntlid(const struct spdk_nvmf_subsystem *subsystem);
+
+/**
+ * Get the maximum controller ID allowed in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return Maximum controller ID allowed in the subsystem.
+ */
+uint16_t spdk_nvmf_subsystem_get_max_cntlid(const struct spdk_nvmf_subsystem *subsystem);
+
 /**
  * Get a namespace's NSID.
  *
diff --git a/include/spdk/nvmf_transport.h b/include/spdk/nvmf_transport.h
index f3af783fd..10952f560 100644
--- a/include/spdk/nvmf_transport.h
+++ b/include/spdk/nvmf_transport.h
@@ -2,7 +2,7 @@
  *   BSD LICENSE
  *
  *   Copyright (c) Intel Corporation. All rights reserved.
- *   Copyright (c) 2019 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2019, 2021 Mellanox Technologies LTD. All rights reserved.
  *
  *   Redistribution and use in source and binary forms, with or without
  *   modification, are permitted provided that the following conditions
@@ -74,7 +74,6 @@ SPDK_STATIC_ASSERT(sizeof(union nvmf_c2h_msg) == 16, "Incorrect size");
 
 struct spdk_nvmf_dif_info {
 	struct spdk_dif_ctx			dif_ctx;
-	bool					dif_insert_or_strip;
 	uint32_t				elba_length;
 	uint32_t				orig_length;
 };
@@ -82,23 +81,27 @@ struct spdk_nvmf_dif_info {
 struct spdk_nvmf_request {
 	struct spdk_nvmf_qpair		*qpair;
 	uint32_t			length;
-	enum spdk_nvme_data_transfer	xfer;
+	uint8_t				xfer; /* type enum spdk_nvme_data_transfer */
+	bool				data_from_pool;
+	bool				dif_enabled;
 	void				*data;
 	union nvmf_h2c_msg		*cmd;
 	union nvmf_c2h_msg		*rsp;
-	void				*buffers[NVMF_REQ_MAX_BUFFERS];
-	struct iovec			iov[NVMF_REQ_MAX_BUFFERS];
+	STAILQ_ENTRY(spdk_nvmf_request)	buf_link;
+	uint64_t			timeout_tsc;
+
 	uint32_t			iovcnt;
-	bool				data_from_pool;
-	struct spdk_bdev_io_wait_entry	bdev_io_wait;
+	struct iovec			iov[NVMF_REQ_MAX_BUFFERS];
+	void				*buffers[NVMF_REQ_MAX_BUFFERS];
+
 	struct spdk_nvmf_dif_info	dif;
+
+	struct spdk_bdev_io_wait_entry	bdev_io_wait;
 	spdk_nvmf_nvme_passthru_cmd_cb	cmd_cb_fn;
 	struct spdk_nvmf_request	*first_fused_req;
 	struct spdk_nvmf_request	*req_to_abort;
 	struct spdk_poller		*poller;
-	uint64_t			timeout_tsc;
 
-	STAILQ_ENTRY(spdk_nvmf_request)	buf_link;
 	TAILQ_ENTRY(spdk_nvmf_request)	link;
 };
 
@@ -429,10 +432,6 @@ int spdk_nvmf_request_get_buffers(struct spdk_nvmf_request *req,
 				  struct spdk_nvmf_transport_poll_group *group,
 				  struct spdk_nvmf_transport *transport,
 				  uint32_t length);
-int spdk_nvmf_request_get_buffers_multi(struct spdk_nvmf_request *req,
-					struct spdk_nvmf_transport_poll_group *group,
-					struct spdk_nvmf_transport *transport,
-					uint32_t *lengths, uint32_t num_lengths);
 
 bool spdk_nvmf_request_get_dif_ctx(struct spdk_nvmf_request *req, struct spdk_dif_ctx *dif_ctx);
 
diff --git a/include/spdk/pci_ids.h b/include/spdk/pci_ids.h
index 816eb0a84..0791b75fc 100644
--- a/include/spdk/pci_ids.h
+++ b/include/spdk/pci_ids.h
@@ -52,6 +52,7 @@ extern "C" {
 #define SPDK_PCI_VID_VIRTIO		0x1af4
 #define SPDK_PCI_VID_CNEXLABS		0x1d1d
 #define SPDK_PCI_VID_VMWARE		0x15ad
+#define SPDK_PCI_VID_REDHAT		0x1b36
 
 #define SPDK_PCI_CLASS_ANY_ID		0xffffff
 /**
diff --git a/include/spdk/stdinc.h b/include/spdk/stdinc.h
index 65820d58e..f7501f820 100644
--- a/include/spdk/stdinc.h
+++ b/include/spdk/stdinc.h
@@ -50,6 +50,7 @@ extern "C" {
 #include <errno.h>
 #include <inttypes.h>
 #include <limits.h>
+#include <math.h>
 #include <stdarg.h>
 #include <stdbool.h>
 #include <stddef.h>
diff --git a/include/spdk/thread.h b/include/spdk/thread.h
index 3f0459670..c602b6ec0 100644
--- a/include/spdk/thread.h
+++ b/include/spdk/thread.h
@@ -39,9 +39,7 @@
 #define SPDK_THREAD_H_
 
 #include "spdk/stdinc.h"
-
 #include "spdk/cpuset.h"
-#include "spdk/queue.h"
 
 #ifdef __cplusplus
 extern "C" {
@@ -210,30 +208,7 @@ typedef void (*spdk_channel_msg)(struct spdk_io_channel_iter *i);
  */
 typedef void (*spdk_channel_for_each_cpl)(struct spdk_io_channel_iter *i, int status);
 
-/**
- * \brief Represents a per-thread channel for accessing an I/O device.
- *
- * An I/O device may be a physical entity (i.e. NVMe controller) or a software
- *  entity (i.e. a blobstore).
- *
- * This structure is not part of the API - all accesses should be done through
- *  spdk_io_channel function calls.
- */
-struct spdk_io_channel {
-	struct spdk_thread		*thread;
-	struct io_device		*dev;
-	uint32_t			ref;
-	uint32_t			destroy_ref;
-	TAILQ_ENTRY(spdk_io_channel)	tailq;
-	spdk_io_channel_destroy_cb	destroy_cb;
-
-	/*
-	 * Modules will allocate extra memory off the end of this structure
-	 *  to store references to hardware-specific references (i.e. NVMe queue
-	 *  pairs, or references to child device spdk_io_channels (i.e.
-	 *  virtual bdevs).
-	 */
-};
+#define SPDK_IO_CHANNEL_STRUCT_SIZE	96
 
 /**
  * Initialize the threading library. Must be called once prior to allocating any threads.
@@ -688,7 +663,7 @@ void spdk_put_io_channel(struct spdk_io_channel *ch);
 static inline void *
 spdk_io_channel_get_ctx(struct spdk_io_channel *ch)
 {
-	return (uint8_t *)ch + sizeof(*ch);
+	return (uint8_t *)ch + SPDK_IO_CHANNEL_STRUCT_SIZE;
 }
 
 /**
diff --git a/include/spdk/trace.h b/include/spdk/trace.h
index 1ed6c8b31..06a99efc3 100644
--- a/include/spdk/trace.h
+++ b/include/spdk/trace.h
@@ -53,7 +53,7 @@ struct spdk_trace_entry {
 	uint16_t	poller_id;
 	uint32_t	size;
 	uint64_t	object_id;
-	uint64_t	arg1;
+	uint8_t		args[40];
 };
 
 /* If type changes from a uint8_t, change this value. */
@@ -80,15 +80,22 @@ struct spdk_trace_object {
 #define SPDK_TRACE_ARG_TYPE_PTR 1
 #define SPDK_TRACE_ARG_TYPE_STR 2
 
+#define SPDK_TRACE_MAX_ARGS_COUNT 5
+
+struct spdk_trace_argument {
+	char	name[14];
+	uint8_t	type;
+	uint8_t	size;
+};
+
 struct spdk_trace_tpoint {
-	char		name[24];
-	uint16_t	tpoint_id;
-	uint8_t		owner_type;
-	uint8_t		object_type;
-	uint8_t		new_object;
-	uint8_t		arg1_type;
-	uint8_t		reserved;
-	char		arg1_name[8];
+	char				name[24];
+	uint16_t			tpoint_id;
+	uint8_t				owner_type;
+	uint8_t				object_type;
+	uint8_t				new_object;
+	uint8_t				num_args;
+	struct spdk_trace_argument	args[SPDK_TRACE_MAX_ARGS_COUNT];
 };
 
 struct spdk_trace_history {
@@ -175,67 +182,57 @@ spdk_get_per_lcore_history(struct spdk_trace_histories *trace_histories, unsigne
 }
 
 void _spdk_trace_record(uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-			uint32_t size, uint64_t object_id, uint64_t arg1);
+			uint32_t size, uint64_t object_id, int num_args, ...);
+
+#define _spdk_trace_record_tsc(tsc, tpoint_id, poller_id, size, object_id, num_args, ...)	\
+	do {											\
+		assert(tpoint_id < SPDK_TRACE_MAX_TPOINT_ID);					\
+		if (g_trace_histories == NULL ||						\
+		    !((1ULL << (tpoint_id & 0x3F)) &						\
+		      g_trace_histories->flags.tpoint_mask[tpoint_id >> 6])) {			\
+			break;									\
+		}										\
+		_spdk_trace_record(tsc, tpoint_id, poller_id, size, object_id,			\
+				   num_args, ## __VA_ARGS__);					\
+	} while (0)
+
+/* Return the number of variable arguments. */
+#define spdk_trace_num_args(...) _spdk_trace_num_args(, ## __VA_ARGS__)
+#define _spdk_trace_num_args(...) __spdk_trace_num_args(__VA_ARGS__, 8, 7, 6, 5, 4, 3, 2, 1, 0)
+#define __spdk_trace_num_args(v, a1, a2, a3, a4, a5, a6, a7, a8, count, ...) count
 
 /**
  * Record the current trace state for tracing tpoints. Debug tool can read the
  * information from shared memory to post-process the tpoint entries and display
- * in a human-readable format. This function will call spdk_get_ticks() to get
- * the current tsc to save in the tracepoint.
+ * in a human-readable format.
  *
+ * \param tsc Current tsc.
  * \param tpoint_id Tracepoint id to record.
  * \param poller_id Poller id to record.
  * \param size Size to record.
  * \param object_id Object id to record.
- * \param arg1 Argument to record.
+ * \param ... Extra tracepoint arguments. The number, types, and order of the arguments
+ *	      must match the definition of the tracepoint.
  */
-static inline
-void spdk_trace_record(uint16_t tpoint_id, uint16_t poller_id, uint32_t size,
-		       uint64_t object_id, uint64_t arg1)
-{
-	/*
-	 * Tracepoint group ID is encoded in the tpoint_id.  Lower 6 bits determine the tracepoint
-	 *  within the group, the remaining upper bits determine the tracepoint group.  Each
-	 *  tracepoint group has its own tracepoint mask.
-	 */
-	assert(tpoint_id < SPDK_TRACE_MAX_TPOINT_ID);
-	if (g_trace_histories == NULL ||
-	    !((1ULL << (tpoint_id & 0x3F)) & g_trace_histories->flags.tpoint_mask[tpoint_id >> 6])) {
-		return;
-	}
-
-	_spdk_trace_record(0, tpoint_id, poller_id, size, object_id, arg1);
-}
+#define spdk_trace_record_tsc(tsc, tpoint_id, poller_id, size, object_id, ...)	\
+	_spdk_trace_record_tsc(tsc, tpoint_id, poller_id, size, object_id,	\
+			       spdk_trace_num_args(__VA_ARGS__), ## __VA_ARGS__)
 
 /**
  * Record the current trace state for tracing tpoints. Debug tool can read the
  * information from shared memory to post-process the tpoint entries and display
- * in a human-readable format.
+ * in a human-readable format. This macro will call spdk_get_ticks() to get
+ * the current tsc to save in the tracepoint.
  *
- * \param tsc Current tsc.
  * \param tpoint_id Tracepoint id to record.
  * \param poller_id Poller id to record.
  * \param size Size to record.
  * \param object_id Object id to record.
- * \param arg1 Argument to record.
+ * \param ... Extra tracepoint arguments. The number, types, and order of the arguments
+ *	      must match the definition of the tracepoint.
  */
-static inline
-void spdk_trace_record_tsc(uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-			   uint32_t size, uint64_t object_id, uint64_t arg1)
-{
-	/*
-	 * Tracepoint group ID is encoded in the tpoint_id.  Lower 6 bits determine the tracepoint
-	 *  within the group, the remaining upper bits determine the tracepoint group.  Each
-	 *  tracepoint group has its own tracepoint mask.
-	 */
-	assert(tpoint_id < SPDK_TRACE_MAX_TPOINT_ID);
-	if (g_trace_histories == NULL ||
-	    !((1ULL << (tpoint_id & 0x3F)) & g_trace_histories->flags.tpoint_mask[tpoint_id >> 6])) {
-		return;
-	}
-
-	_spdk_trace_record(tsc, tpoint_id, poller_id, size, object_id, arg1);
-}
+#define spdk_trace_record(tpoint_id, poller_id, size, object_id, ...) \
+	spdk_trace_record_tsc(0, tpoint_id, poller_id, size, object_id, ## __VA_ARGS__)
 
 /**
  * Get the current tpoint mask of the given tpoint group.
@@ -326,7 +323,7 @@ void spdk_trace_register_owner(uint8_t type, char id_prefix);
 void spdk_trace_register_object(uint8_t type, char id_prefix);
 
 /**
- * Register the description for the tpoint.
+ * Register the description for a tpoint with a single argument.
  *
  * \param name Name for the tpoint.
  * \param tpoint_id Id for the tpoint.
@@ -340,6 +337,29 @@ void spdk_trace_register_description(const char *name, uint16_t tpoint_id, uint8
 				     uint8_t object_type, uint8_t new_object,
 				     uint8_t arg1_type, const char *arg1_name);
 
+struct spdk_trace_tpoint_opts {
+	const char	*name;
+	uint16_t	tpoint_id;
+	uint8_t		owner_type;
+	uint8_t		object_type;
+	uint8_t		new_object;
+	struct {
+		const char	*name;
+		uint8_t		type;
+		uint8_t		size;
+	} args[SPDK_TRACE_MAX_ARGS_COUNT];
+};
+
+/**
+ * Register the description for a number of tpoints. This function allows the user to register
+ * tracepoints with multiple arguments (up to 5).
+ *
+ * \param opts Array of structures describing tpoints and their arguments.
+ * \param num_opts Number of tpoints to register (size of the opts array).
+ */
+void spdk_trace_register_description_ext(const struct spdk_trace_tpoint_opts *opts,
+		size_t num_opts);
+
 struct spdk_trace_register_fn *spdk_trace_get_first_register_fn(void);
 
 struct spdk_trace_register_fn *spdk_trace_get_next_register_fn(struct spdk_trace_register_fn
diff --git a/include/spdk/tree.h b/include/spdk/tree.h
new file mode 100644
index 000000000..6da45995a
--- /dev/null
+++ b/include/spdk/tree.h
@@ -0,0 +1,834 @@
+/*-
+ * SPDX-License-Identifier: BSD-2-Clause-FreeBSD
+ *
+ * Copyright 2002 Niels Provos <provos@citi.umich.edu>
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef	SPDK_TREE_H
+#define SPDK_TREE_H
+
+#include <sys/cdefs.h>
+
+/*
+ * This file defines data structures for different types of trees:
+ * splay trees and rank-balanced trees.
+ *
+ * A splay tree is a self-organizing data structure.  Every operation
+ * on the tree causes a splay to happen.  The splay moves the requested
+ * node to the root of the tree and partly rebalances it.
+ *
+ * This has the benefit that request locality causes faster lookups as
+ * the requested nodes move to the top of the tree.  On the other hand,
+ * every lookup causes memory writes.
+ *
+ * The Balance Theorem bounds the total access time for m operations
+ * and n inserts on an initially empty tree as O((m + n)lg n).  The
+ * amortized cost for a sequence of m accesses to a splay tree is O(lg n);
+ *
+ * A rank-balanced tree is a binary search tree with an integer
+ * rank-difference as an attribute of each pointer from parent to child.
+ * The sum of the rank-differences on any path from a node down to null is
+ * the same, and defines the rank of that node. The rank of the null node
+ * is -1.
+ *
+ * Different additional conditions define different sorts of balanced
+ * trees, including "red-black" and "AVL" trees.  The set of conditions
+ * applied here are the "weak-AVL" conditions of Haeupler, Sen and Tarjan:
+ *	- every rank-difference is 1 or 2.
+ *	- the rank of any leaf is 1.
+ *
+ * For historical reasons, rank differences that are even are associated
+ * with the color red (Rank-Even-Difference), and the child that a red edge
+ * points to is called a red child.
+ *
+ * Every operation on a rank-balanced tree is bounded as O(lg n).
+ * The maximum height of a rank-balanced tree is 2lg (n+1).
+ */
+
+#define SPLAY_HEAD(name, type)						\
+struct name {								\
+	struct type *sph_root; /* root of the tree */			\
+}
+
+#define SPLAY_INITIALIZER(root)						\
+	{ NULL }
+
+#define SPLAY_INIT(root) do {						\
+	(root)->sph_root = NULL;					\
+} while (/* CONSTCOND */ 0)
+
+#define SPLAY_ENTRY(type)						\
+struct {								\
+	struct type *spe_left; /* left element */			\
+	struct type *spe_right; /* right element */			\
+}
+
+#define SPLAY_LEFT(elm, field)		(elm)->field.spe_left
+#define SPLAY_RIGHT(elm, field)		(elm)->field.spe_right
+#define SPLAY_ROOT(head)		(head)->sph_root
+#define SPLAY_EMPTY(head)		(SPLAY_ROOT(head) == NULL)
+
+/* SPLAY_ROTATE_{LEFT,RIGHT} expect that tmp hold SPLAY_{RIGHT,LEFT} */
+#define SPLAY_ROTATE_RIGHT(head, tmp, field) do {			\
+	SPLAY_LEFT((head)->sph_root, field) = SPLAY_RIGHT(tmp, field);	\
+	SPLAY_RIGHT(tmp, field) = (head)->sph_root;			\
+	(head)->sph_root = tmp;						\
+} while (/* CONSTCOND */ 0)
+
+#define SPLAY_ROTATE_LEFT(head, tmp, field) do {			\
+	SPLAY_RIGHT((head)->sph_root, field) = SPLAY_LEFT(tmp, field);	\
+	SPLAY_LEFT(tmp, field) = (head)->sph_root;			\
+	(head)->sph_root = tmp;						\
+} while (/* CONSTCOND */ 0)
+
+#define SPLAY_LINKLEFT(head, tmp, field) do {				\
+	SPLAY_LEFT(tmp, field) = (head)->sph_root;			\
+	tmp = (head)->sph_root;						\
+	(head)->sph_root = SPLAY_LEFT((head)->sph_root, field);		\
+} while (/* CONSTCOND */ 0)
+
+#define SPLAY_LINKRIGHT(head, tmp, field) do {				\
+	SPLAY_RIGHT(tmp, field) = (head)->sph_root;			\
+	tmp = (head)->sph_root;						\
+	(head)->sph_root = SPLAY_RIGHT((head)->sph_root, field);	\
+} while (/* CONSTCOND */ 0)
+
+#define SPLAY_ASSEMBLE(head, node, left, right, field) do {		\
+	SPLAY_RIGHT(left, field) = SPLAY_LEFT((head)->sph_root, field);	\
+	SPLAY_LEFT(right, field) = SPLAY_RIGHT((head)->sph_root, field);\
+	SPLAY_LEFT((head)->sph_root, field) = SPLAY_RIGHT(node, field);	\
+	SPLAY_RIGHT((head)->sph_root, field) = SPLAY_LEFT(node, field);	\
+} while (/* CONSTCOND */ 0)
+
+/* Generates prototypes and inline functions */
+
+#define SPLAY_PROTOTYPE(name, type, field, cmp)				\
+void name##_SPLAY(struct name *, struct type *);			\
+void name##_SPLAY_MINMAX(struct name *, int);				\
+struct type *name##_SPLAY_INSERT(struct name *, struct type *);		\
+struct type *name##_SPLAY_REMOVE(struct name *, struct type *);		\
+									\
+/* Finds the node with the same key as elm */				\
+static __attribute__((unused)) __inline struct type *					\
+name##_SPLAY_FIND(struct name *head, struct type *elm)			\
+{									\
+	if (SPLAY_EMPTY(head))						\
+		return(NULL);						\
+	name##_SPLAY(head, elm);					\
+	if ((cmp)(elm, (head)->sph_root) == 0)				\
+		return (head->sph_root);				\
+	return (NULL);							\
+}									\
+									\
+static __attribute__((unused)) __inline struct type *					\
+name##_SPLAY_NEXT(struct name *head, struct type *elm)			\
+{									\
+	name##_SPLAY(head, elm);					\
+	if (SPLAY_RIGHT(elm, field) != NULL) {				\
+		elm = SPLAY_RIGHT(elm, field);				\
+		while (SPLAY_LEFT(elm, field) != NULL) {		\
+			elm = SPLAY_LEFT(elm, field);			\
+		}							\
+	} else								\
+		elm = NULL;						\
+	return (elm);							\
+}									\
+									\
+static __attribute__((unused)) __inline struct type *					\
+name##_SPLAY_MIN_MAX(struct name *head, int val)			\
+{									\
+	name##_SPLAY_MINMAX(head, val);					\
+        return (SPLAY_ROOT(head));					\
+}
+
+/* Main splay operation.
+ * Moves node close to the key of elm to top
+ */
+#define SPLAY_GENERATE(name, type, field, cmp)				\
+struct type *								\
+name##_SPLAY_INSERT(struct name *head, struct type *elm)		\
+{									\
+    if (SPLAY_EMPTY(head)) {						\
+	    SPLAY_LEFT(elm, field) = SPLAY_RIGHT(elm, field) = NULL;	\
+    } else {								\
+	    int __comp;							\
+	    name##_SPLAY(head, elm);					\
+	    __comp = (cmp)(elm, (head)->sph_root);			\
+	    if (__comp < 0) {						\
+		    SPLAY_LEFT(elm, field) = SPLAY_LEFT((head)->sph_root, field);\
+		    SPLAY_RIGHT(elm, field) = (head)->sph_root;		\
+		    SPLAY_LEFT((head)->sph_root, field) = NULL;		\
+	    } else if (__comp > 0) {					\
+		    SPLAY_RIGHT(elm, field) = SPLAY_RIGHT((head)->sph_root, field);\
+		    SPLAY_LEFT(elm, field) = (head)->sph_root;		\
+		    SPLAY_RIGHT((head)->sph_root, field) = NULL;	\
+	    } else							\
+		    return ((head)->sph_root);				\
+    }									\
+    (head)->sph_root = (elm);						\
+    return (NULL);							\
+}									\
+									\
+struct type *								\
+name##_SPLAY_REMOVE(struct name *head, struct type *elm)		\
+{									\
+	struct type *__tmp;						\
+	if (SPLAY_EMPTY(head))						\
+		return (NULL);						\
+	name##_SPLAY(head, elm);					\
+	if ((cmp)(elm, (head)->sph_root) == 0) {			\
+		if (SPLAY_LEFT((head)->sph_root, field) == NULL) {	\
+			(head)->sph_root = SPLAY_RIGHT((head)->sph_root, field);\
+		} else {						\
+			__tmp = SPLAY_RIGHT((head)->sph_root, field);	\
+			(head)->sph_root = SPLAY_LEFT((head)->sph_root, field);\
+			name##_SPLAY(head, elm);			\
+			SPLAY_RIGHT((head)->sph_root, field) = __tmp;	\
+		}							\
+		return (elm);						\
+	}								\
+	return (NULL);							\
+}									\
+									\
+void									\
+name##_SPLAY(struct name *head, struct type *elm)			\
+{									\
+	struct type __node, *__left, *__right, *__tmp;			\
+	int __comp;							\
+\
+	SPLAY_LEFT(&__node, field) = SPLAY_RIGHT(&__node, field) = NULL;\
+	__left = __right = &__node;					\
+\
+	while ((__comp = (cmp)(elm, (head)->sph_root)) != 0) {		\
+		if (__comp < 0) {					\
+			__tmp = SPLAY_LEFT((head)->sph_root, field);	\
+			if (__tmp == NULL)				\
+				break;					\
+			if ((cmp)(elm, __tmp) < 0){			\
+				SPLAY_ROTATE_RIGHT(head, __tmp, field);	\
+				if (SPLAY_LEFT((head)->sph_root, field) == NULL)\
+					break;				\
+			}						\
+			SPLAY_LINKLEFT(head, __right, field);		\
+		} else if (__comp > 0) {				\
+			__tmp = SPLAY_RIGHT((head)->sph_root, field);	\
+			if (__tmp == NULL)				\
+				break;					\
+			if ((cmp)(elm, __tmp) > 0){			\
+				SPLAY_ROTATE_LEFT(head, __tmp, field);	\
+				if (SPLAY_RIGHT((head)->sph_root, field) == NULL)\
+					break;				\
+			}						\
+			SPLAY_LINKRIGHT(head, __left, field);		\
+		}							\
+	}								\
+	SPLAY_ASSEMBLE(head, &__node, __left, __right, field);		\
+}									\
+									\
+/* Splay with either the minimum or the maximum element			\
+ * Used to find minimum or maximum element in tree.			\
+ */									\
+void name##_SPLAY_MINMAX(struct name *head, int __comp) \
+{									\
+	struct type __node, *__left, *__right, *__tmp;			\
+\
+	SPLAY_LEFT(&__node, field) = SPLAY_RIGHT(&__node, field) = NULL;\
+	__left = __right = &__node;					\
+\
+	while (1) {							\
+		if (__comp < 0) {					\
+			__tmp = SPLAY_LEFT((head)->sph_root, field);	\
+			if (__tmp == NULL)				\
+				break;					\
+			if (__comp < 0){				\
+				SPLAY_ROTATE_RIGHT(head, __tmp, field);	\
+				if (SPLAY_LEFT((head)->sph_root, field) == NULL)\
+					break;				\
+			}						\
+			SPLAY_LINKLEFT(head, __right, field);		\
+		} else if (__comp > 0) {				\
+			__tmp = SPLAY_RIGHT((head)->sph_root, field);	\
+			if (__tmp == NULL)				\
+				break;					\
+			if (__comp > 0) {				\
+				SPLAY_ROTATE_LEFT(head, __tmp, field);	\
+				if (SPLAY_RIGHT((head)->sph_root, field) == NULL)\
+					break;				\
+			}						\
+			SPLAY_LINKRIGHT(head, __left, field);		\
+		}							\
+	}								\
+	SPLAY_ASSEMBLE(head, &__node, __left, __right, field);		\
+}
+
+#define SPLAY_NEGINF	-1
+#define SPLAY_INF	1
+
+#define SPLAY_INSERT(name, x, y)	name##_SPLAY_INSERT(x, y)
+#define SPLAY_REMOVE(name, x, y)	name##_SPLAY_REMOVE(x, y)
+#define SPLAY_FIND(name, x, y)		name##_SPLAY_FIND(x, y)
+#define SPLAY_NEXT(name, x, y)		name##_SPLAY_NEXT(x, y)
+#define SPLAY_MIN(name, x)		(SPLAY_EMPTY(x) ? NULL	\
+					: name##_SPLAY_MIN_MAX(x, SPLAY_NEGINF))
+#define SPLAY_MAX(name, x)		(SPLAY_EMPTY(x) ? NULL	\
+					: name##_SPLAY_MIN_MAX(x, SPLAY_INF))
+
+#define SPLAY_FOREACH(x, name, head)					\
+	for ((x) = SPLAY_MIN(name, head);				\
+	     (x) != NULL;						\
+	     (x) = SPLAY_NEXT(name, head, x))
+
+/* Macros that define a rank-balanced tree */
+#define RB_HEAD(name, type)						\
+struct name {								\
+	struct type *rbh_root; /* root of the tree */			\
+}
+
+#define RB_INITIALIZER(root)						\
+	{ NULL }
+
+#define RB_INIT(root) do {						\
+	(root)->rbh_root = NULL;					\
+} while (/* CONSTCOND */ 0)
+
+#define RB_ENTRY(type)							\
+struct {								\
+	struct type *rbe_left;		/* left element */		\
+	struct type *rbe_right;		/* right element */		\
+	struct type *rbe_parent;	/* parent element */		\
+}
+
+#define RB_LEFT(elm, field)		(elm)->field.rbe_left
+#define RB_RIGHT(elm, field)		(elm)->field.rbe_right
+
+/*
+ * With the expectation that any object of struct type has an
+ * address that is a multiple of 4, and that therefore the
+ * 2 least significant bits of a pointer to struct type are
+ * always zero, this implementation sets those bits to indicate
+ * that the left or right child of the tree node is "red".
+ */
+#define RB_UP(elm, field)		(elm)->field.rbe_parent
+#define RB_BITS(elm, field)		(*(uintptr_t *)&RB_UP(elm, field))
+#define RB_RED_L			((uintptr_t)1)
+#define RB_RED_R			((uintptr_t)2)
+#define RB_RED_MASK			((uintptr_t)3)
+#define RB_FLIP_LEFT(elm, field)	(RB_BITS(elm, field) ^= RB_RED_L)
+#define RB_FLIP_RIGHT(elm, field)	(RB_BITS(elm, field) ^= RB_RED_R)
+#define RB_RED_LEFT(elm, field)		((RB_BITS(elm, field) & RB_RED_L) != 0)
+#define RB_RED_RIGHT(elm, field)	((RB_BITS(elm, field) & RB_RED_R) != 0)
+#define RB_PARENT(elm, field)		((__typeof(RB_UP(elm, field)))	\
+					 (RB_BITS(elm, field) & ~RB_RED_MASK))
+/*
+ * _RB_ROOT starts with an underscore. This is a workaround for the issue that
+ * RB_ROOT() had a name conflict with the SPDK FIO plugin. The SPDK FIO plugin
+ * includes FIO and FIO defines RB_ROOT() itself.
+ */
+#define _RB_ROOT(head)			(head)->rbh_root
+#define RB_EMPTY(head)			(_RB_ROOT(head) == NULL)
+
+#define RB_SET_PARENT(dst, src, field) do {				\
+	RB_BITS(dst, field) &= RB_RED_MASK;				\
+	RB_BITS(dst, field) |= (uintptr_t)src;			\
+} while (/* CONSTCOND */ 0)
+
+#define RB_SET(elm, parent, field) do {					\
+	RB_UP(elm, field) = parent;					\
+	RB_LEFT(elm, field) = RB_RIGHT(elm, field) = NULL;		\
+} while (/* CONSTCOND */ 0)
+
+#define RB_COLOR(elm, field)	(RB_PARENT(elm, field) == NULL ? 0 :	\
+				RB_LEFT(RB_PARENT(elm, field), field) == elm ? \
+				RB_RED_LEFT(RB_PARENT(elm, field), field) : \
+				RB_RED_RIGHT(RB_PARENT(elm, field), field))
+
+/*
+ * Something to be invoked in a loop at the root of every modified subtree,
+ * from the bottom up to the root, to update augmented node data.
+ */
+#ifndef RB_AUGMENT
+#define RB_AUGMENT(x)	break
+#endif
+
+#define RB_SWAP_CHILD(head, out, in, field) do {			\
+	if (RB_PARENT(out, field) == NULL)				\
+		_RB_ROOT(head) = (in);					\
+	else if ((out) == RB_LEFT(RB_PARENT(out, field), field))	\
+		RB_LEFT(RB_PARENT(out, field), field) = (in);		\
+	else								\
+		RB_RIGHT(RB_PARENT(out, field), field) = (in);		\
+} while (/* CONSTCOND */ 0)
+
+#define RB_ROTATE_LEFT(head, elm, tmp, field) do {			\
+	(tmp) = RB_RIGHT(elm, field);					\
+	if ((RB_RIGHT(elm, field) = RB_LEFT(tmp, field)) != NULL) {	\
+		RB_SET_PARENT(RB_RIGHT(elm, field), elm, field);	\
+	}								\
+	RB_SET_PARENT(tmp, RB_PARENT(elm, field), field);		\
+	RB_SWAP_CHILD(head, elm, tmp, field);				\
+	RB_LEFT(tmp, field) = (elm);					\
+	RB_SET_PARENT(elm, tmp, field);					\
+	RB_AUGMENT(elm);						\
+} while (/* CONSTCOND */ 0)
+
+#define RB_ROTATE_RIGHT(head, elm, tmp, field) do {			\
+	(tmp) = RB_LEFT(elm, field);					\
+	if ((RB_LEFT(elm, field) = RB_RIGHT(tmp, field)) != NULL) {	\
+		RB_SET_PARENT(RB_LEFT(elm, field), elm, field);		\
+	}								\
+	RB_SET_PARENT(tmp, RB_PARENT(elm, field), field);		\
+	RB_SWAP_CHILD(head, elm, tmp, field);				\
+	RB_RIGHT(tmp, field) = (elm);					\
+	RB_SET_PARENT(elm, tmp, field);					\
+	RB_AUGMENT(elm);						\
+} while (/* CONSTCOND */ 0)
+
+/* Generates prototypes and inline functions */
+#define	RB_PROTOTYPE(name, type, field, cmp)				\
+	RB_PROTOTYPE_INTERNAL(name, type, field, cmp,)
+#define	RB_PROTOTYPE_STATIC(name, type, field, cmp)			\
+	RB_PROTOTYPE_INTERNAL(name, type, field, cmp, __attribute__((unused)) static)
+#define RB_PROTOTYPE_INTERNAL(name, type, field, cmp, attr)		\
+	RB_PROTOTYPE_INSERT_COLOR(name, type, attr);			\
+	RB_PROTOTYPE_REMOVE_COLOR(name, type, attr);			\
+	RB_PROTOTYPE_INSERT(name, type, attr);				\
+	RB_PROTOTYPE_REMOVE(name, type, attr);				\
+	RB_PROTOTYPE_FIND(name, type, attr);				\
+	RB_PROTOTYPE_NFIND(name, type, attr);				\
+	RB_PROTOTYPE_NEXT(name, type, attr);				\
+	RB_PROTOTYPE_PREV(name, type, attr);				\
+	RB_PROTOTYPE_MINMAX(name, type, attr);				\
+	RB_PROTOTYPE_REINSERT(name, type, attr);
+#define RB_PROTOTYPE_INSERT_COLOR(name, type, attr)			\
+	attr void name##_RB_INSERT_COLOR(struct name *, struct type *)
+#define RB_PROTOTYPE_REMOVE_COLOR(name, type, attr)			\
+	attr void name##_RB_REMOVE_COLOR(struct name *,			\
+	    struct type *, struct type *)
+#define RB_PROTOTYPE_REMOVE(name, type, attr)				\
+	attr struct type *name##_RB_REMOVE(struct name *, struct type *)
+#define RB_PROTOTYPE_INSERT(name, type, attr)				\
+	attr struct type *name##_RB_INSERT(struct name *, struct type *)
+#define RB_PROTOTYPE_FIND(name, type, attr)				\
+	attr struct type *name##_RB_FIND(struct name *, struct type *)
+#define RB_PROTOTYPE_NFIND(name, type, attr)				\
+	attr struct type *name##_RB_NFIND(struct name *, struct type *)
+#define RB_PROTOTYPE_NEXT(name, type, attr)				\
+	attr struct type *name##_RB_NEXT(struct type *)
+#define RB_PROTOTYPE_PREV(name, type, attr)				\
+	attr struct type *name##_RB_PREV(struct type *)
+#define RB_PROTOTYPE_MINMAX(name, type, attr)				\
+	attr struct type *name##_RB_MINMAX(struct name *, int)
+#define RB_PROTOTYPE_REINSERT(name, type, attr)			\
+	attr struct type *name##_RB_REINSERT(struct name *, struct type *)
+
+/* Main rb operation.
+ * Moves node close to the key of elm to top
+ */
+#define	RB_GENERATE(name, type, field, cmp)				\
+	RB_GENERATE_INTERNAL(name, type, field, cmp,)
+#define	RB_GENERATE_STATIC(name, type, field, cmp)			\
+	RB_GENERATE_INTERNAL(name, type, field, cmp, __attribute__((unused)) static)
+#define RB_GENERATE_INTERNAL(name, type, field, cmp, attr)		\
+	RB_GENERATE_INSERT_COLOR(name, type, field, attr)		\
+	RB_GENERATE_REMOVE_COLOR(name, type, field, attr)		\
+	RB_GENERATE_INSERT(name, type, field, cmp, attr)		\
+	RB_GENERATE_REMOVE(name, type, field, attr)			\
+	RB_GENERATE_FIND(name, type, field, cmp, attr)			\
+	RB_GENERATE_NFIND(name, type, field, cmp, attr)			\
+	RB_GENERATE_NEXT(name, type, field, attr)			\
+	RB_GENERATE_PREV(name, type, field, attr)			\
+	RB_GENERATE_MINMAX(name, type, field, attr)			\
+	RB_GENERATE_REINSERT(name, type, field, cmp, attr)
+
+#define RB_GENERATE_INSERT_COLOR(name, type, field, attr)		\
+attr void								\
+name##_RB_INSERT_COLOR(struct name *head, struct type *elm)		\
+{									\
+	struct type *child, *parent;					\
+	while ((parent = RB_PARENT(elm, field)) != NULL) {		\
+		if (RB_LEFT(parent, field) == elm) {			\
+			if (RB_RED_LEFT(parent, field)) {		\
+				RB_FLIP_LEFT(parent, field);		\
+				return;					\
+			}						\
+			RB_FLIP_RIGHT(parent, field);			\
+			if (RB_RED_RIGHT(parent, field)) {		\
+				elm = parent;				\
+				continue;				\
+			}						\
+			if (!RB_RED_RIGHT(elm, field)) {		\
+				RB_FLIP_LEFT(elm, field);		\
+				RB_ROTATE_LEFT(head, elm, child, field);\
+				if (RB_RED_LEFT(child, field))		\
+					RB_FLIP_RIGHT(elm, field);	\
+				else if (RB_RED_RIGHT(child, field))	\
+					RB_FLIP_LEFT(parent, field);	\
+				elm = child;				\
+			}						\
+			RB_ROTATE_RIGHT(head, parent, elm, field);	\
+		} else {						\
+			if (RB_RED_RIGHT(parent, field)) {		\
+				RB_FLIP_RIGHT(parent, field);		\
+				return;					\
+			}						\
+			RB_FLIP_LEFT(parent, field);			\
+			if (RB_RED_LEFT(parent, field)) {		\
+				elm = parent;				\
+				continue;				\
+			}						\
+			if (!RB_RED_LEFT(elm, field)) {			\
+				RB_FLIP_RIGHT(elm, field);		\
+				RB_ROTATE_RIGHT(head, elm, child, field);\
+				if (RB_RED_RIGHT(child, field))		\
+					RB_FLIP_LEFT(elm, field);	\
+				else if (RB_RED_LEFT(child, field))	\
+					RB_FLIP_RIGHT(parent, field);	\
+				elm = child;				\
+			}						\
+			RB_ROTATE_LEFT(head, parent, elm, field);	\
+		}							\
+		RB_BITS(elm, field) &= ~RB_RED_MASK;			\
+		break;							\
+	}								\
+}
+
+#define RB_GENERATE_REMOVE_COLOR(name, type, field, attr)		\
+attr void								\
+name##_RB_REMOVE_COLOR(struct name *head,				\
+    struct type *parent, struct type *elm)				\
+{									\
+	struct type *sib;						\
+	if (RB_LEFT(parent, field) == elm &&				\
+	    RB_RIGHT(parent, field) == elm) {				\
+		RB_BITS(parent, field) &= ~RB_RED_MASK;			\
+		elm = parent;						\
+		parent = RB_PARENT(elm, field);				\
+		if (parent == NULL)					\
+			return;						\
+	}								\
+	do  {								\
+		if (RB_LEFT(parent, field) == elm) {			\
+			if (!RB_RED_LEFT(parent, field)) {		\
+				RB_FLIP_LEFT(parent, field);		\
+				return;					\
+			}						\
+			if (RB_RED_RIGHT(parent, field)) {		\
+				RB_FLIP_RIGHT(parent, field);		\
+				elm = parent;				\
+				continue;				\
+			}						\
+			sib = RB_RIGHT(parent, field);			\
+			if ((~RB_BITS(sib, field) & RB_RED_MASK) == 0) {\
+				RB_BITS(sib, field) &= ~RB_RED_MASK;	\
+				elm = parent;				\
+				continue;				\
+			}						\
+			RB_FLIP_RIGHT(sib, field);			\
+			if (RB_RED_LEFT(sib, field))			\
+				RB_FLIP_LEFT(parent, field);		\
+			else if (!RB_RED_RIGHT(sib, field)) {		\
+				RB_FLIP_LEFT(parent, field);		\
+				RB_ROTATE_RIGHT(head, sib, elm, field);	\
+				if (RB_RED_RIGHT(elm, field))		\
+					RB_FLIP_LEFT(sib, field);	\
+				if (RB_RED_LEFT(elm, field))		\
+					RB_FLIP_RIGHT(parent, field);	\
+				RB_BITS(elm, field) |= RB_RED_MASK;	\
+				sib = elm;				\
+			}						\
+			RB_ROTATE_LEFT(head, parent, sib, field);	\
+		} else {						\
+			if (!RB_RED_RIGHT(parent, field)) {		\
+				RB_FLIP_RIGHT(parent, field);		\
+				return;					\
+			}						\
+			if (RB_RED_LEFT(parent, field)) {		\
+				RB_FLIP_LEFT(parent, field);		\
+				elm = parent;				\
+				continue;				\
+			}						\
+			sib = RB_LEFT(parent, field);			\
+			if ((~RB_BITS(sib, field) & RB_RED_MASK) == 0) {\
+				RB_BITS(sib, field) &= ~RB_RED_MASK;	\
+				elm = parent;				\
+				continue;				\
+			}						\
+			RB_FLIP_LEFT(sib, field);			\
+			if (RB_RED_RIGHT(sib, field))			\
+				RB_FLIP_RIGHT(parent, field);		\
+			else if (!RB_RED_LEFT(sib, field)) {		\
+				RB_FLIP_RIGHT(parent, field);		\
+				RB_ROTATE_LEFT(head, sib, elm, field);	\
+				if (RB_RED_LEFT(elm, field))		\
+					RB_FLIP_RIGHT(sib, field);	\
+				if (RB_RED_RIGHT(elm, field))		\
+					RB_FLIP_LEFT(parent, field);	\
+				RB_BITS(elm, field) |= RB_RED_MASK;	\
+				sib = elm;				\
+			}						\
+			RB_ROTATE_RIGHT(head, parent, sib, field);	\
+		}							\
+		break;							\
+	} while ((parent = RB_PARENT(elm, field)) != NULL);		\
+}
+
+#define RB_GENERATE_REMOVE(name, type, field, attr)			\
+attr struct type *							\
+name##_RB_REMOVE(struct name *head, struct type *elm)			\
+{									\
+	struct type *child, *old, *parent, *right;			\
+									\
+	old = elm;							\
+	parent = RB_PARENT(elm, field);					\
+	right = RB_RIGHT(elm, field);					\
+	if (RB_LEFT(elm, field) == NULL)				\
+		elm = child = right;					\
+	else if (right == NULL)						\
+		elm = child = RB_LEFT(elm, field);			\
+	else {								\
+		if ((child = RB_LEFT(right, field)) == NULL) {		\
+			child = RB_RIGHT(right, field);			\
+			RB_RIGHT(old, field) = child;			\
+			parent = elm = right;				\
+		} else {						\
+			do						\
+				elm = child;				\
+			while ((child = RB_LEFT(elm, field)) != NULL);	\
+			child = RB_RIGHT(elm, field);			\
+			parent = RB_PARENT(elm, field);			\
+			RB_LEFT(parent, field) = child;			\
+			RB_SET_PARENT(RB_RIGHT(old, field), elm, field);\
+		}							\
+		RB_SET_PARENT(RB_LEFT(old, field), elm, field);		\
+		elm->field = old->field;				\
+	}								\
+	RB_SWAP_CHILD(head, old, elm, field);				\
+	if (child != NULL)						\
+		RB_SET_PARENT(child, parent, field);			\
+	if (parent != NULL)						\
+		name##_RB_REMOVE_COLOR(head, parent, child);		\
+	while (parent != NULL) {					\
+		RB_AUGMENT(parent);					\
+		parent = RB_PARENT(parent, field);			\
+	}								\
+	return (old);							\
+}
+
+#define RB_GENERATE_INSERT(name, type, field, cmp, attr)		\
+/* Inserts a node into the RB tree */					\
+attr struct type *							\
+name##_RB_INSERT(struct name *head, struct type *elm)			\
+{									\
+	struct type *tmp;						\
+	struct type *parent = NULL;					\
+	int comp = 0;							\
+	tmp = _RB_ROOT(head);						\
+	while (tmp) {							\
+		parent = tmp;						\
+		comp = (cmp)(elm, parent);				\
+		if (comp < 0)						\
+			tmp = RB_LEFT(tmp, field);			\
+		else if (comp > 0)					\
+			tmp = RB_RIGHT(tmp, field);			\
+		else							\
+			return (tmp);					\
+	}								\
+	RB_SET(elm, parent, field);					\
+	if (parent == NULL)						\
+		_RB_ROOT(head) = elm;					\
+	else if (comp < 0)						\
+		RB_LEFT(parent, field) = elm;				\
+	else								\
+		RB_RIGHT(parent, field) = elm;				\
+	name##_RB_INSERT_COLOR(head, elm);				\
+	while (elm != NULL) {						\
+		RB_AUGMENT(elm);					\
+		elm = RB_PARENT(elm, field);				\
+	}								\
+	return (NULL);							\
+}
+
+#define RB_GENERATE_FIND(name, type, field, cmp, attr)			\
+/* Finds the node with the same key as elm */				\
+attr struct type *							\
+name##_RB_FIND(struct name *head, struct type *elm)			\
+{									\
+	struct type *tmp = _RB_ROOT(head);				\
+	int comp;							\
+	while (tmp) {							\
+		comp = cmp(elm, tmp);					\
+		if (comp < 0)						\
+			tmp = RB_LEFT(tmp, field);			\
+		else if (comp > 0)					\
+			tmp = RB_RIGHT(tmp, field);			\
+		else							\
+			return (tmp);					\
+	}								\
+	return (NULL);							\
+}
+
+#define RB_GENERATE_NFIND(name, type, field, cmp, attr)			\
+/* Finds the first node greater than or equal to the search key */	\
+attr struct type *							\
+name##_RB_NFIND(struct name *head, struct type *elm)			\
+{									\
+	struct type *tmp = _RB_ROOT(head);				\
+	struct type *res = NULL;					\
+	int comp;							\
+	while (tmp) {							\
+		comp = cmp(elm, tmp);					\
+		if (comp < 0) {						\
+			res = tmp;					\
+			tmp = RB_LEFT(tmp, field);			\
+		}							\
+		else if (comp > 0)					\
+			tmp = RB_RIGHT(tmp, field);			\
+		else							\
+			return (tmp);					\
+	}								\
+	return (res);							\
+}
+
+#define RB_GENERATE_NEXT(name, type, field, attr)			\
+/* ARGSUSED */								\
+attr struct type *							\
+name##_RB_NEXT(struct type *elm)					\
+{									\
+	if (RB_RIGHT(elm, field)) {					\
+		elm = RB_RIGHT(elm, field);				\
+		while (RB_LEFT(elm, field))				\
+			elm = RB_LEFT(elm, field);			\
+	} else {							\
+		if (RB_PARENT(elm, field) &&				\
+		    (elm == RB_LEFT(RB_PARENT(elm, field), field)))	\
+			elm = RB_PARENT(elm, field);			\
+		else {							\
+			while (RB_PARENT(elm, field) &&			\
+			    (elm == RB_RIGHT(RB_PARENT(elm, field), field)))\
+				elm = RB_PARENT(elm, field);		\
+			elm = RB_PARENT(elm, field);			\
+		}							\
+	}								\
+	return (elm);							\
+}
+
+#define RB_GENERATE_PREV(name, type, field, attr)			\
+/* ARGSUSED */								\
+attr struct type *							\
+name##_RB_PREV(struct type *elm)					\
+{									\
+	if (RB_LEFT(elm, field)) {					\
+		elm = RB_LEFT(elm, field);				\
+		while (RB_RIGHT(elm, field))				\
+			elm = RB_RIGHT(elm, field);			\
+	} else {							\
+		if (RB_PARENT(elm, field) &&				\
+		    (elm == RB_RIGHT(RB_PARENT(elm, field), field)))	\
+			elm = RB_PARENT(elm, field);			\
+		else {							\
+			while (RB_PARENT(elm, field) &&			\
+			    (elm == RB_LEFT(RB_PARENT(elm, field), field)))\
+				elm = RB_PARENT(elm, field);		\
+			elm = RB_PARENT(elm, field);			\
+		}							\
+	}								\
+	return (elm);							\
+}
+
+#define RB_GENERATE_MINMAX(name, type, field, attr)			\
+attr struct type *							\
+name##_RB_MINMAX(struct name *head, int val)				\
+{									\
+	struct type *tmp = _RB_ROOT(head);				\
+	struct type *parent = NULL;					\
+	while (tmp) {							\
+		parent = tmp;						\
+		if (val < 0)						\
+			tmp = RB_LEFT(tmp, field);			\
+		else							\
+			tmp = RB_RIGHT(tmp, field);			\
+	}								\
+	return (parent);						\
+}
+
+#define	RB_GENERATE_REINSERT(name, type, field, cmp, attr)		\
+attr struct type *							\
+name##_RB_REINSERT(struct name *head, struct type *elm)			\
+{									\
+	struct type *cmpelm;						\
+	if (((cmpelm = RB_PREV(name, head, elm)) != NULL &&		\
+	    cmp(cmpelm, elm) >= 0) ||					\
+	    ((cmpelm = RB_NEXT(name, head, elm)) != NULL &&		\
+	    cmp(elm, cmpelm) >= 0)) {					\
+		/* XXXLAS: Remove/insert is heavy handed. */		\
+		RB_REMOVE(name, head, elm);				\
+		return (RB_INSERT(name, head, elm));			\
+	}								\
+	return (NULL);							\
+}									\
+
+#define RB_NEGINF	-1
+#define RB_INF	1
+
+#define RB_INSERT(name, x, y)	name##_RB_INSERT(x, y)
+#define RB_REMOVE(name, x, y)	name##_RB_REMOVE(x, y)
+#define RB_FIND(name, x, y)	name##_RB_FIND(x, y)
+#define RB_NFIND(name, x, y)	name##_RB_NFIND(x, y)
+#define RB_NEXT(name, x, y)	name##_RB_NEXT(y)
+#define RB_PREV(name, x, y)	name##_RB_PREV(y)
+#define RB_MIN(name, x)		name##_RB_MINMAX(x, RB_NEGINF)
+#define RB_MAX(name, x)		name##_RB_MINMAX(x, RB_INF)
+#define RB_REINSERT(name, x, y)	name##_RB_REINSERT(x, y)
+
+#define RB_FOREACH(x, name, head)					\
+	for ((x) = RB_MIN(name, head);					\
+	     (x) != NULL;						\
+	     (x) = name##_RB_NEXT(x))
+
+#define RB_FOREACH_FROM(x, name, y)					\
+	for ((x) = (y);							\
+	    ((x) != NULL) && ((y) = name##_RB_NEXT(x), (x) != NULL);	\
+	     (x) = (y))
+
+#define RB_FOREACH_SAFE(x, name, head, y)				\
+	for ((x) = RB_MIN(name, head);					\
+	    ((x) != NULL) && ((y) = name##_RB_NEXT(x), (x) != NULL);	\
+	     (x) = (y))
+
+#define RB_FOREACH_REVERSE(x, name, head)				\
+	for ((x) = RB_MAX(name, head);					\
+	     (x) != NULL;						\
+	     (x) = name##_RB_PREV(x))
+
+#define RB_FOREACH_REVERSE_FROM(x, name, y)				\
+	for ((x) = (y);							\
+	    ((x) != NULL) && ((y) = name##_RB_PREV(x), (x) != NULL);	\
+	     (x) = (y))
+
+#define RB_FOREACH_REVERSE_SAFE(x, name, head, y)			\
+	for ((x) = RB_MAX(name, head);					\
+	    ((x) != NULL) && ((y) = name##_RB_PREV(x), (x) != NULL);	\
+	     (x) = (y))
+
+#endif	/* SPDK_TREE_H */
diff --git a/include/spdk/util.h b/include/spdk/util.h
index 6358524fa..363aebdf4 100644
--- a/include/spdk/util.h
+++ b/include/spdk/util.h
@@ -66,7 +66,7 @@ extern "C" {
  * power-of-two value.
  */
 #define SPDK_ALIGN_FLOOR(val, align) \
-	(typeof(val))((val) & (~((typeof(val))((align) - 1))))
+	(__typeof__(val))((val) & (~((__typeof__(val))((align) - 1))))
 /**
  * Macro to align a value to a given power-of-two. The resultant value
  * will be of the same type as the first parameter, and will be no lower
@@ -74,7 +74,7 @@ extern "C" {
  * value.
  */
 #define SPDK_ALIGN_CEIL(val, align) \
-	SPDK_ALIGN_FLOOR(((val) + ((typeof(val)) (align) - 1)), align)
+	SPDK_ALIGN_FLOOR(((val) + ((__typeof__(val)) (align) - 1)), align)
 
 uint32_t spdk_u32log2(uint32_t x);
 
diff --git a/include/spdk/version.h b/include/spdk/version.h
index 43b904330..39c47d369 100644
--- a/include/spdk/version.h
+++ b/include/spdk/version.h
@@ -46,7 +46,7 @@
 /**
  * Minor version number (month of original release).
  */
-#define SPDK_VERSION_MINOR	4
+#define SPDK_VERSION_MINOR	7
 
 /**
  * Patch level.
@@ -59,7 +59,7 @@
 /**
  * Version string suffix.
  */
-#define SPDK_VERSION_SUFFIX	""
+#define SPDK_VERSION_SUFFIX	"-pre"
 
 /**
  * Single numeric value representing a version number for compile-time comparisons.
diff --git a/include/spdk/vfio_user_spec.h b/include/spdk/vfio_user_spec.h
index ac87ba181..fa3e259d4 100644
--- a/include/spdk/vfio_user_spec.h
+++ b/include/spdk/vfio_user_spec.h
@@ -36,29 +36,34 @@
 
 #include "spdk/stdinc.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
 enum vfio_user_command {
 	VFIO_USER_VERSION			= 1,
 	VFIO_USER_DMA_MAP			= 2,
 	VFIO_USER_DMA_UNMAP			= 3,
 	VFIO_USER_DEVICE_GET_INFO		= 4,
 	VFIO_USER_DEVICE_GET_REGION_INFO	= 5,
-	VFIO_USER_DEVICE_GET_IRQ_INFO		= 6,
-	VFIO_USER_DEVICE_SET_IRQS		= 7,
-	VFIO_USER_REGION_READ			= 8,
-	VFIO_USER_REGION_WRITE			= 9,
-	VFIO_USER_DMA_READ			= 10,
-	VFIO_USER_DMA_WRITE			= 11,
-	VFIO_USER_VM_INTERRUPT			= 12,
+	VFIO_USER_DEVICE_GET_REGION_IO_FDS	= 6,
+	VFIO_USER_DEVICE_GET_IRQ_INFO		= 7,
+	VFIO_USER_DEVICE_SET_IRQS		= 8,
+	VFIO_USER_REGION_READ			= 9,
+	VFIO_USER_REGION_WRITE			= 10,
+	VFIO_USER_DMA_READ			= 11,
+	VFIO_USER_DMA_WRITE			= 12,
 	VFIO_USER_DEVICE_RESET			= 13,
+	VFIO_USER_DIRTY_PAGES			= 14,
 	VFIO_USER_MAX,
 };
 
 enum vfio_user_message_type {
-	VFIO_USER_MESSAGE_COMMAND		= 0,
-	VFIO_USER_MESSAGE_REPLY			= 1,
+	VFIO_USER_MESSAGE_COMMAND	= 0,
+	VFIO_USER_MESSAGE_REPLY		= 1,
 };
 
-#define VFIO_USER_FLAGS_NO_REPLY		(0x1)
+#define VFIO_USER_FLAGS_NO_REPLY	(0x1)
 
 struct vfio_user_header {
 	uint16_t	msg_id;
@@ -66,8 +71,8 @@ struct vfio_user_header {
 	uint32_t	msg_size;
 	struct {
 		uint32_t	type     : 4;
-#define VFIO_USER_F_TYPE_COMMAND    0
-#define VFIO_USER_F_TYPE_REPLY      1
+#define VFIO_USER_F_TYPE_COMMAND	0
+#define VFIO_USER_F_TYPE_REPLY		1
 		uint32_t	no_reply : 1;
 		uint32_t	error    : 1;
 		uint32_t	resvd    : 26;
@@ -81,15 +86,47 @@ struct vfio_user_version {
 	uint8_t		data[];
 } __attribute__((packed));
 
-struct vfio_user_dma_region {
-	uint64_t	addr;
+/*
+ * Similar to vfio_device_info, but without caps (yet).
+ */
+struct vfio_user_device_info {
+	uint32_t	argsz;
+	/* VFIO_DEVICE_FLAGS_* */
+	uint32_t	flags;
+	uint32_t	num_regions;
+	uint32_t	num_irqs;
+} __attribute__((packed));
+
+/* based on struct vfio_bitmap */
+struct vfio_user_bitmap {
+	uint64_t	pgsize;
 	uint64_t	size;
-	uint64_t	offset;
-	uint32_t	prot;
+	char		data[];
+} __attribute__((packed));
+
+/* based on struct vfio_iommu_type1_dma_map */
+struct vfio_user_dma_map {
+	uint32_t	argsz;
+#define VFIO_USER_F_DMA_REGION_READ	(1 << 0)
+#define VFIO_USER_F_DMA_REGION_WRITE	(1 << 1)
 	uint32_t	flags;
-#define VFIO_USER_F_DMA_REGION_MAPPABLE (1 << 0)
+	uint64_t	offset;
+	uint64_t	addr;
+	uint64_t	size;
 } __attribute__((packed));
 
+/* based on struct vfio_iommu_type1_dma_unmap */
+struct vfio_user_dma_unmap {
+	uint32_t	argsz;
+#ifndef VFIO_DMA_UNMAP_FLAG_GET_DIRTY_BITMAP
+#define VFIO_DMA_UNMAP_FLAG_GET_DIRTY_BITMAP	(1 << 0)
+#endif
+	uint32_t	flags;
+	uint64_t	addr;
+	uint64_t	size;
+	struct vfio_user_bitmap	bitmap[];
+};
+
 struct vfio_user_region_access {
 	uint64_t	offset;
 	uint32_t	region;
@@ -98,13 +135,25 @@ struct vfio_user_region_access {
 } __attribute__((packed));
 
 struct vfio_user_dma_region_access {
-	uint64_t    addr;
-	uint32_t    count;
-	uint8_t     data[];
+	uint64_t	addr;
+	uint64_t	count;
+	uint8_t		data[];
 } __attribute__((packed));
 
 struct vfio_user_irq_info {
-	uint32_t    subindex;
+	uint32_t	subindex;
 } __attribute__((packed));
 
+/* based on struct vfio_iommu_type1_dirty_bitmap_get */
+struct vfio_user_bitmap_range {
+	uint64_t	iova;
+	uint64_t	size;
+	struct vfio_user_bitmap	bitmap;
+} __attribute__((packed));
+
+
+#ifdef __cplusplus
+}
+#endif
+
 #endif
diff --git a/include/spdk/zipf.h b/include/spdk/zipf.h
new file mode 100644
index 000000000..d68c417b7
--- /dev/null
+++ b/include/spdk/zipf.h
@@ -0,0 +1,83 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * Zipf random number distribution
+ */
+
+#ifndef SPDK_ZIPF_H
+#define SPDK_ZIPF_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_zipf;
+
+/**
+ * Create a zipf random number generator.
+ *
+ * Numbers from [0, range) will be returned by the generator when
+ * calling \ref spdk_zipf_generate.
+ *
+ * \param range Range of values for the zipf distribution.
+ * \param theta Theta distribution parameter.
+ * \param seed Seed value for the random number generator.
+ *
+ * \return a pointer to the new zipf generator.
+ */
+struct spdk_zipf *spdk_zipf_create(uint64_t range, double theta, uint32_t seed);
+
+/**
+ * Free a zipf generator and set the pointer to NULL.
+ *
+ * \param zipfp Zipf generator to free.
+ */
+void spdk_zipf_free(struct spdk_zipf **zipfp);
+
+/**
+ * Generate a value from the zipf generator.
+ *
+ * \param zipf Zipf generator to generate the value from.
+ *
+ * \return value in the range [0, range)
+ */
+uint64_t spdk_zipf_generate(struct spdk_zipf *zipf);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/include/spdk_internal/accel_engine.h b/include/spdk_internal/accel_engine.h
index 12d6abc95..a3cb56306 100644
--- a/include/spdk_internal/accel_engine.h
+++ b/include/spdk_internal/accel_engine.h
@@ -67,12 +67,13 @@ struct spdk_accel_batch {
 };
 
 enum accel_opcode {
-	ACCEL_OPCODE_MEMMOVE	= 0,
-	ACCEL_OPCODE_MEMFILL	= 1,
-	ACCEL_OPCODE_COMPARE	= 2,
-	ACCEL_OPCODE_BATCH	= 3,
-	ACCEL_OPCODE_CRC32C	= 4,
-	ACCEL_OPCODE_DUALCAST	= 5,
+	ACCEL_OPCODE_MEMMOVE		= 0,
+	ACCEL_OPCODE_MEMFILL		= 1,
+	ACCEL_OPCODE_COMPARE		= 2,
+	ACCEL_OPCODE_BATCH		= 3,
+	ACCEL_OPCODE_CRC32C		= 4,
+	ACCEL_OPCODE_DUALCAST		= 5,
+	ACCEL_OPCODE_COPY_CRC32C	= 6,
 };
 
 struct spdk_accel_task {
@@ -80,6 +81,10 @@ struct spdk_accel_task {
 	struct spdk_accel_batch		*batch;
 	spdk_accel_completion_cb	cb_fn;
 	void				*cb_arg;
+	struct {
+		spdk_accel_completion_cb	cb_fn;
+		void				*cb_arg;
+	}				chained;
 	union {
 		struct {
 			struct iovec			*iovs; /* iovs passed by the caller */
@@ -92,14 +97,11 @@ struct spdk_accel_task {
 		void			*src2;
 	};
 	union {
-		struct {
-			spdk_accel_completion_cb	cb_fn;
-			void				*cb_arg;
-		} chained;
 		void				*dst2;
 		uint32_t			seed;
 		uint64_t			fill_pattern;
 	};
+	uint32_t			*crc_dst;
 	enum accel_opcode		op_code;
 	uint64_t			nbytes;
 	TAILQ_ENTRY(spdk_accel_task)	link;
diff --git a/include/spdk_internal/event.h b/include/spdk_internal/event.h
index 9ecc4209d..7d5088a56 100644
--- a/include/spdk_internal/event.h
+++ b/include/spdk_internal/event.h
@@ -64,11 +64,11 @@ struct spdk_lw_thread {
 	TAILQ_ENTRY(spdk_lw_thread)	link;
 	uint64_t			tsc_start;
 	uint32_t                        lcore;
-	uint32_t                        new_lcore;
 	bool				resched;
+	/* stats over a lifetime of a thread */
+	struct spdk_thread_stats	total_stats;
+	/* stats during the last scheduling period */
 	struct spdk_thread_stats	current_stats;
-	struct spdk_thread_stats	snapshot_stats;
-	struct spdk_thread_stats	last_stats;
 };
 
 /**
@@ -164,59 +164,6 @@ int spdk_reactor_set_interrupt_mode(uint32_t lcore, bool new_in_interrupt,
  */
 struct spdk_thread *_spdk_get_app_thread(void);
 
-struct spdk_subsystem {
-	const char *name;
-	/* User must call spdk_subsystem_init_next() when they are done with their initialization. */
-	void (*init)(void);
-	void (*fini)(void);
-
-	/**
-	 * Write JSON configuration handler.
-	 *
-	 * \param w JSON write context
-	 */
-	void (*write_config_json)(struct spdk_json_write_ctx *w);
-	TAILQ_ENTRY(spdk_subsystem) tailq;
-};
-
-struct spdk_subsystem *spdk_subsystem_find(const char *name);
-struct spdk_subsystem *spdk_subsystem_get_first(void);
-struct spdk_subsystem *spdk_subsystem_get_next(struct spdk_subsystem *cur_subsystem);
-
-struct spdk_subsystem_depend {
-	const char *name;
-	const char *depends_on;
-	TAILQ_ENTRY(spdk_subsystem_depend) tailq;
-};
-
-struct spdk_subsystem_depend *spdk_subsystem_get_first_depend(void);
-struct spdk_subsystem_depend *spdk_subsystem_get_next_depend(struct spdk_subsystem_depend
-		*cur_depend);
-
-void spdk_add_subsystem(struct spdk_subsystem *subsystem);
-void spdk_add_subsystem_depend(struct spdk_subsystem_depend *depend);
-
-typedef void (*spdk_subsystem_init_fn)(int rc, void *ctx);
-void spdk_subsystem_init(spdk_subsystem_init_fn cb_fn, void *cb_arg);
-void spdk_subsystem_fini(spdk_msg_fn cb_fn, void *cb_arg);
-void spdk_subsystem_init_next(int rc);
-void spdk_subsystem_fini_next(void);
-void spdk_app_json_config_load(const char *json_config_file, const char *rpc_addr,
-			       spdk_subsystem_init_fn cb_fn, void *cb_arg,
-			       bool stop_on_error);
-
-/**
- * Save pointed \c subsystem configuration to the JSON write context \c w. In case of
- * error \c null is written to the JSON context.
- *
- * \param w JSON write context
- * \param subsystem the subsystem to query
- */
-void spdk_subsystem_config_json(struct spdk_json_write_ctx *w, struct spdk_subsystem *subsystem);
-
-int spdk_rpc_initialize(const char *listen_addr);
-void spdk_rpc_finish(void);
-
 struct spdk_governor_capabilities {
 	bool freq_change;
 	bool freq_getset;
@@ -312,11 +259,15 @@ struct spdk_governor *_spdk_governor_get(void);
  * A list of cores and threads which is used for scheduling.
  */
 struct spdk_scheduler_core_info {
-	uint64_t core_idle_tsc;
-	uint64_t core_busy_tsc;
+	/* stats over a lifetime of a core */
+	uint64_t total_idle_tsc;
+	uint64_t total_busy_tsc;
+	/* stats during the last scheduling period */
+	uint64_t current_idle_tsc;
+	uint64_t current_busy_tsc;
+
 	uint32_t lcore;
 	uint32_t threads_count;
-	uint32_t pending_threads_count;
 	bool interrupt_mode;
 	struct spdk_lw_thread **threads;
 };
@@ -408,37 +359,6 @@ static void __attribute__((constructor)) _spdk_scheduler_register_ ## scheduler
  */
 void _spdk_lw_thread_set_core(struct spdk_lw_thread *thread, uint32_t lcore);
 
-/**
- * Get threads stats
- *
- * \param thread thread that stats regards to.
- * \param stats Output parameter for accumulated TSC counts while the thread was busy.
- */
-void _spdk_lw_thread_get_current_stats(struct spdk_lw_thread *thread,
-				       struct spdk_thread_stats *stats);
-
-/**
- * \brief Register a new subsystem
- */
-#define SPDK_SUBSYSTEM_REGISTER(_name) \
-	__attribute__((constructor)) static void _name ## _register(void)	\
-	{									\
-		spdk_add_subsystem(&_name);					\
-	}
-
-/**
- * \brief Declare that a subsystem depends on another subsystem.
- */
-#define SPDK_SUBSYSTEM_DEPEND(_name, _depends_on)						\
-	static struct spdk_subsystem_depend __subsystem_ ## _name ## _depend_on ## _depends_on = { \
-	.name = #_name,										\
-	.depends_on = #_depends_on,								\
-	};											\
-	__attribute__((constructor)) static void _name ## _depend_on ## _depends_on(void)	\
-	{											\
-		spdk_add_subsystem_depend(&__subsystem_ ## _name ## _depend_on ## _depends_on); \
-	}
-
 #ifdef __cplusplus
 }
 #endif
diff --git a/include/spdk_internal/init.h b/include/spdk_internal/init.h
new file mode 100644
index 000000000..0a43beb7f
--- /dev/null
+++ b/include/spdk_internal/init.h
@@ -0,0 +1,90 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.  All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_INIT_INTERNAL_H
+#define SPDK_INIT_INTERNAL_H
+
+#include "spdk/stdinc.h"
+#include "spdk/queue.h"
+
+struct spdk_json_write_ctx;
+
+struct spdk_subsystem {
+	const char *name;
+	/* User must call spdk_subsystem_init_next() when they are done with their initialization. */
+	void (*init)(void);
+	void (*fini)(void);
+
+	/**
+	 * Write JSON configuration handler.
+	 *
+	 * \param w JSON write context
+	 */
+	void (*write_config_json)(struct spdk_json_write_ctx *w);
+	TAILQ_ENTRY(spdk_subsystem) tailq;
+};
+
+struct spdk_subsystem_depend {
+	const char *name;
+	const char *depends_on;
+	TAILQ_ENTRY(spdk_subsystem_depend) tailq;
+};
+
+void spdk_add_subsystem(struct spdk_subsystem *subsystem);
+void spdk_add_subsystem_depend(struct spdk_subsystem_depend *depend);
+
+void spdk_subsystem_init_next(int rc);
+void spdk_subsystem_fini_next(void);
+
+/**
+ * \brief Register a new subsystem
+ */
+#define SPDK_SUBSYSTEM_REGISTER(_name) \
+	__attribute__((constructor)) static void _name ## _register(void)	\
+	{									\
+		spdk_add_subsystem(&_name);					\
+	}
+
+/**
+ * \brief Declare that a subsystem depends on another subsystem.
+ */
+#define SPDK_SUBSYSTEM_DEPEND(_name, _depends_on)						\
+	static struct spdk_subsystem_depend __subsystem_ ## _name ## _depend_on ## _depends_on = { \
+	.name = #_name,										\
+	.depends_on = #_depends_on,								\
+	};											\
+	__attribute__((constructor)) static void _name ## _depend_on ## _depends_on(void)	\
+	{											\
+		spdk_add_subsystem_depend(&__subsystem_ ## _name ## _depend_on ## _depends_on); \
+	}
+
+#endif
diff --git a/include/spdk_internal/lvolstore.h b/include/spdk_internal/lvolstore.h
index f82157e53..1607d8330 100644
--- a/include/spdk_internal/lvolstore.h
+++ b/include/spdk_internal/lvolstore.h
@@ -36,8 +36,8 @@
 
 #include "spdk/blob.h"
 #include "spdk/lvol.h"
+#include "spdk/queue.h"
 #include "spdk/uuid.h"
-#include "spdk/bdev_module.h"
 
 /* Default size of blobstore cluster */
 #define SPDK_LVS_OPTS_CLUSTER_SZ (4 * 1024 * 1024)
diff --git a/include/spdk_internal/nvme_tcp.h b/include/spdk_internal/nvme_tcp.h
index 8d2e20515..bdd68c094 100644
--- a/include/spdk_internal/nvme_tcp.h
+++ b/include/spdk_internal/nvme_tcp.h
@@ -38,6 +38,8 @@
 #include "spdk/sock.h"
 #include "spdk/dif.h"
 
+#include "sgl.h"
+
 #define SPDK_CRC32C_XOR				0xffffffffUL
 #define SPDK_NVME_TCP_DIGEST_LEN		4
 #define SPDK_NVME_TCP_DIGEST_ALIGNMENT		4
@@ -76,13 +78,6 @@
 
 typedef void (*nvme_tcp_qpair_xfer_complete_cb)(void *cb_arg);
 
-struct _nvme_tcp_sgl {
-	struct iovec	*iov;
-	int		iovcnt;
-	uint32_t	iov_offset;
-	uint32_t	total_size;
-};
-
 struct nvme_tcp_pdu {
 	union {
 		/* to hold error pdu data */
@@ -101,7 +96,6 @@ struct nvme_tcp_pdu {
 
 	bool						has_hdgst;
 	bool						ddgst_enable;
-	uint32_t					header_digest_crc32;
 	uint32_t					data_digest_crc32;
 	uint8_t						data_digest[SPDK_NVME_TCP_DIGEST_LEN];
 
@@ -126,7 +120,7 @@ struct nvme_tcp_pdu {
 	TAILQ_ENTRY(nvme_tcp_pdu)			tailq;
 	uint32_t					remaining;
 	uint32_t					padding_len;
-	struct _nvme_tcp_sgl				sgl;
+	struct spdk_iov_sgl				sgl;
 
 	struct spdk_dif_ctx				*dif_ctx;
 
@@ -206,20 +200,6 @@ nvme_tcp_pdu_calc_header_digest(struct nvme_tcp_pdu *pdu)
 	return crc32c;
 }
 
-static uint32_t
-_update_crc32c_iov(struct iovec *iov, int iovcnt, uint32_t crc32c)
-{
-	int i;
-
-	for (i = 0; i < iovcnt; i++) {
-		assert(iov[i].iov_base != NULL);
-		assert(iov[i].iov_len != 0);
-		crc32c = spdk_crc32c_update(iov[i].iov_base, iov[i].iov_len, crc32c);
-	}
-
-	return crc32c;
-}
-
 static uint32_t
 nvme_tcp_pdu_calc_data_digest(struct nvme_tcp_pdu *pdu)
 {
@@ -229,7 +209,7 @@ nvme_tcp_pdu_calc_data_digest(struct nvme_tcp_pdu *pdu)
 	assert(pdu->data_len != 0);
 
 	if (spdk_likely(!pdu->dif_ctx)) {
-		crc32c = _update_crc32c_iov(pdu->data_iov, pdu->data_iovcnt, crc32c);
+		crc32c = spdk_crc32c_iov_update(pdu->data_iov, pdu->data_iovcnt, crc32c);
 	} else {
 		spdk_dif_update_crc32c_stream(pdu->data_iov, pdu->data_iovcnt,
 					      0, pdu->data_len, &crc32c, pdu->dif_ctx);
@@ -249,32 +229,7 @@ nvme_tcp_pdu_calc_data_digest(struct nvme_tcp_pdu *pdu)
 }
 
 static inline void
-_nvme_tcp_sgl_init(struct _nvme_tcp_sgl *s, struct iovec *iov, int iovcnt,
-		   uint32_t iov_offset)
-{
-	s->iov = iov;
-	s->iovcnt = iovcnt;
-	s->iov_offset = iov_offset;
-	s->total_size = 0;
-}
-
-static inline void
-_nvme_tcp_sgl_advance(struct _nvme_tcp_sgl *s, uint32_t step)
-{
-	s->iov_offset += step;
-	while (s->iovcnt > 0) {
-		if (s->iov_offset < s->iov->iov_len) {
-			break;
-		}
-
-		s->iov_offset -= s->iov->iov_len;
-		s->iov++;
-		s->iovcnt--;
-	}
-}
-
-static inline void
-_nvme_tcp_sgl_get_buf(struct _nvme_tcp_sgl *s, void **_buf, uint32_t *_buf_len)
+_nvme_tcp_sgl_get_buf(struct spdk_iov_sgl *s, void **_buf, uint32_t *_buf_len)
 {
 	if (_buf != NULL) {
 		*_buf = s->iov->iov_base + s->iov_offset;
@@ -285,33 +240,12 @@ _nvme_tcp_sgl_get_buf(struct _nvme_tcp_sgl *s, void **_buf, uint32_t *_buf_len)
 }
 
 static inline bool
-_nvme_tcp_sgl_append(struct _nvme_tcp_sgl *s, uint8_t *data, uint32_t data_len)
-{
-	if (s->iov_offset >= data_len) {
-		s->iov_offset -= data_len;
-	} else {
-		assert(s->iovcnt > 0);
-		s->iov->iov_base = data + s->iov_offset;
-		s->iov->iov_len = data_len - s->iov_offset;
-		s->total_size += data_len - s->iov_offset;
-		s->iov_offset = 0;
-		s->iov++;
-		s->iovcnt--;
-		if (s->iovcnt == 0) {
-			return false;
-		}
-	}
-
-	return true;
-}
-
-static inline bool
-_nvme_tcp_sgl_append_multi(struct _nvme_tcp_sgl *s, struct iovec *iov, int iovcnt)
+_nvme_tcp_sgl_append_multi(struct spdk_iov_sgl *s, struct iovec *iov, int iovcnt)
 {
 	int i;
 
 	for (i = 0; i < iovcnt; i++) {
-		if (!_nvme_tcp_sgl_append(s, iov[i].iov_base, iov[i].iov_len)) {
+		if (!spdk_iov_sgl_append(s, iov[i].iov_base, iov[i].iov_len)) {
 			return false;
 		}
 	}
@@ -333,7 +267,7 @@ _get_iov_array_size(struct iovec *iov, int iovcnt)
 }
 
 static inline bool
-_nvme_tcp_sgl_append_multi_with_md(struct _nvme_tcp_sgl *s, struct iovec *iov, int iovcnt,
+_nvme_tcp_sgl_append_multi_with_md(struct spdk_iov_sgl *s, struct iovec *iov, int iovcnt,
 				   uint32_t data_len, const struct spdk_dif_ctx *dif_ctx)
 {
 	int rc;
@@ -369,14 +303,14 @@ nvme_tcp_build_iovs(struct iovec *iov, int iovcnt, struct nvme_tcp_pdu *pdu,
 		    bool hdgst_enable, bool ddgst_enable, uint32_t *_mapped_length)
 {
 	uint32_t hlen, plen;
-	struct _nvme_tcp_sgl *sgl;
+	struct spdk_iov_sgl *sgl;
 
 	if (iovcnt == 0) {
 		return 0;
 	}
 
 	sgl = &pdu->sgl;
-	_nvme_tcp_sgl_init(sgl, iov, iovcnt, 0);
+	spdk_iov_sgl_init(sgl, iov, iovcnt, 0);
 	hlen = pdu->hdr.common.hlen;
 
 	/* Header Digest */
@@ -387,7 +321,7 @@ nvme_tcp_build_iovs(struct iovec *iov, int iovcnt, struct nvme_tcp_pdu *pdu,
 	plen = hlen;
 	if (!pdu->data_len) {
 		/* PDU header + possible header digest */
-		_nvme_tcp_sgl_append(sgl, (uint8_t *)&pdu->hdr.raw, hlen);
+		spdk_iov_sgl_append(sgl, (uint8_t *)&pdu->hdr.raw, hlen);
 		goto end;
 	}
 
@@ -397,7 +331,7 @@ nvme_tcp_build_iovs(struct iovec *iov, int iovcnt, struct nvme_tcp_pdu *pdu,
 		plen = hlen;
 	}
 
-	if (!_nvme_tcp_sgl_append(sgl, (uint8_t *)&pdu->hdr.raw, hlen)) {
+	if (!spdk_iov_sgl_append(sgl, (uint8_t *)&pdu->hdr.raw, hlen)) {
 		goto end;
 	}
 
@@ -417,7 +351,7 @@ nvme_tcp_build_iovs(struct iovec *iov, int iovcnt, struct nvme_tcp_pdu *pdu,
 	/* Data Digest */
 	if (g_nvme_tcp_ddgst[pdu->hdr.common.pdu_type] && ddgst_enable) {
 		plen += SPDK_NVME_TCP_DIGEST_LEN;
-		_nvme_tcp_sgl_append(sgl, pdu->data_digest, SPDK_NVME_TCP_DIGEST_LEN);
+		spdk_iov_sgl_append(sgl, pdu->data_digest, SPDK_NVME_TCP_DIGEST_LEN);
 	}
 
 	assert(plen == pdu->hdr.common.plen);
@@ -434,14 +368,14 @@ static int
 nvme_tcp_build_payload_iovs(struct iovec *iov, int iovcnt, struct nvme_tcp_pdu *pdu,
 			    bool ddgst_enable, uint32_t *_mapped_length)
 {
-	struct _nvme_tcp_sgl *sgl;
+	struct spdk_iov_sgl *sgl;
 
 	if (iovcnt == 0) {
 		return 0;
 	}
 
 	sgl = &pdu->sgl;
-	_nvme_tcp_sgl_init(sgl, iov, iovcnt, pdu->rw_offset);
+	spdk_iov_sgl_init(sgl, iov, iovcnt, pdu->rw_offset);
 
 	if (spdk_likely(!pdu->dif_ctx)) {
 		if (!_nvme_tcp_sgl_append_multi(sgl, pdu->data_iov, pdu->data_iovcnt)) {
@@ -456,7 +390,7 @@ nvme_tcp_build_payload_iovs(struct iovec *iov, int iovcnt, struct nvme_tcp_pdu *
 
 	/* Data Digest */
 	if (ddgst_enable) {
-		_nvme_tcp_sgl_append(sgl, pdu->data_digest, SPDK_NVME_TCP_DIGEST_LEN);
+		spdk_iov_sgl_append(sgl, pdu->data_digest, SPDK_NVME_TCP_DIGEST_LEN);
 	}
 
 end:
@@ -566,7 +500,7 @@ nvme_tcp_pdu_set_data_buf(struct nvme_tcp_pdu *pdu,
 {
 	uint32_t buf_offset, buf_len, remain_len, len;
 	uint8_t *buf;
-	struct _nvme_tcp_sgl *pdu_sgl, buf_sgl;
+	struct spdk_iov_sgl *pdu_sgl, buf_sgl;
 
 	pdu->data_len = data_len;
 
@@ -584,20 +518,20 @@ nvme_tcp_pdu_set_data_buf(struct nvme_tcp_pdu *pdu,
 	} else {
 		pdu_sgl = &pdu->sgl;
 
-		_nvme_tcp_sgl_init(pdu_sgl, pdu->data_iov, NVME_TCP_MAX_SGL_DESCRIPTORS, 0);
-		_nvme_tcp_sgl_init(&buf_sgl, iov, iovcnt, 0);
+		spdk_iov_sgl_init(pdu_sgl, pdu->data_iov, NVME_TCP_MAX_SGL_DESCRIPTORS, 0);
+		spdk_iov_sgl_init(&buf_sgl, iov, iovcnt, 0);
 
-		_nvme_tcp_sgl_advance(&buf_sgl, buf_offset);
+		spdk_iov_sgl_advance(&buf_sgl, buf_offset);
 		remain_len = buf_len;
 
 		while (remain_len > 0) {
 			_nvme_tcp_sgl_get_buf(&buf_sgl, (void *)&buf, &len);
 			len = spdk_min(len, remain_len);
 
-			_nvme_tcp_sgl_advance(&buf_sgl, len);
+			spdk_iov_sgl_advance(&buf_sgl, len);
 			remain_len -= len;
 
-			if (!_nvme_tcp_sgl_append(pdu_sgl, buf, len)) {
+			if (!spdk_iov_sgl_append(pdu_sgl, buf, len)) {
 				break;
 			}
 		}
diff --git a/include/spdk_internal/sgl.h b/include/spdk_internal/sgl.h
new file mode 100644
index 000000000..9fee65d42
--- /dev/null
+++ b/include/spdk_internal/sgl.h
@@ -0,0 +1,127 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SGL_INTERNAL_H__
+#define __SGL_INTERNAL_H__
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_iov_sgl {
+	struct iovec    *iov;
+	int             iovcnt;
+	uint32_t        iov_offset;
+	uint32_t        total_size;
+};
+
+/**
+ * Initialize struct spdk_iov_sgl with iov, iovcnt and iov_offset.
+ *
+ * \param s the spdk_iov_sgl to be filled.
+ * \param iov the io vector to fill the s
+ * \param iovcnt the size the iov
+ * \param iov_offset the current filled iov_offset for s.
+ */
+
+static inline void
+spdk_iov_sgl_init(struct spdk_iov_sgl *s, struct iovec *iov, int iovcnt,
+		  uint32_t iov_offset)
+{
+	s->iov = iov;
+	s->iovcnt = iovcnt;
+	s->iov_offset = iov_offset;
+	s->total_size = 0;
+}
+
+/**
+ * Consume the iovs in spdk_iov_sgl with passed bytes
+ *
+ * \param s the spdk_iov_sgl which contains the iov
+ * \param step the bytes_size consumed.
+ */
+
+static inline void
+spdk_iov_sgl_advance(struct spdk_iov_sgl *s, uint32_t step)
+{
+	s->iov_offset += step;
+	while (s->iovcnt > 0) {
+		assert(s->iov != NULL);
+		if (s->iov_offset < s->iov->iov_len) {
+			break;
+		}
+
+		s->iov_offset -= s->iov->iov_len;
+		s->iov++;
+		s->iovcnt--;
+	}
+}
+
+/**
+ * Append the data to the struct spdk_iov_sgl pointed by s
+ *
+ * \param s the address of the struct spdk_iov_sgl
+ * \param data the data buffer to be appended
+ * \param data_len the length of the data.
+ *
+ * \return true if all the data is appended.
+ */
+
+static inline bool
+spdk_iov_sgl_append(struct spdk_iov_sgl *s, uint8_t *data, uint32_t data_len)
+{
+	if (s->iov_offset >= data_len) {
+		s->iov_offset -= data_len;
+	} else {
+		assert(s->iovcnt > 0);
+		s->iov->iov_base = data + s->iov_offset;
+		s->iov->iov_len = data_len - s->iov_offset;
+		s->total_size += data_len - s->iov_offset;
+		s->iov_offset = 0;
+		s->iov++;
+		s->iovcnt--;
+		if (s->iovcnt == 0) {
+			return false;
+		}
+	}
+
+	return true;
+}
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* __SGL_INTERNAL_H__ */
diff --git a/include/spdk_internal/thread.h b/include/spdk_internal/thread.h
index 6d9e830d4..752d45dd4 100644
--- a/include/spdk_internal/thread.h
+++ b/include/spdk_internal/thread.h
@@ -31,115 +31,40 @@
  *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-#ifndef SPDK_THREAD_INTERNAL_H_
-#define SPDK_THREAD_INTERNAL_H_
+#ifndef SPDK_INTERNAL_THREAD_H_
+#define SPDK_INTERNAL_THREAD_H_
 
 #include "spdk/stdinc.h"
 #include "spdk/thread.h"
 
-#define SPDK_MAX_POLLER_NAME_LEN	256
-#define SPDK_MAX_THREAD_NAME_LEN	256
+struct spdk_poller;
 
-enum spdk_poller_state {
-	/* The poller is registered with a thread but not currently executing its fn. */
-	SPDK_POLLER_STATE_WAITING,
-
-	/* The poller is currently running its fn. */
-	SPDK_POLLER_STATE_RUNNING,
-
-	/* The poller was unregistered during the execution of its fn. */
-	SPDK_POLLER_STATE_UNREGISTERED,
-
-	/* The poller is in the process of being paused.  It will be paused
-	 * during the next time it's supposed to be executed.
-	 */
-	SPDK_POLLER_STATE_PAUSING,
-
-	/* The poller is registered but currently paused.  It's on the
-	 * paused_pollers list.
-	 */
-	SPDK_POLLER_STATE_PAUSED,
+struct spdk_poller_stats {
+	uint64_t	run_count;
+	uint64_t	busy_count;
 };
 
-struct spdk_poller {
-	TAILQ_ENTRY(spdk_poller)	tailq;
+struct io_device;
+struct spdk_thread;
 
-	/* Current state of the poller; should only be accessed from the poller's thread. */
-	enum spdk_poller_state		state;
+const char *spdk_poller_get_name(struct spdk_poller *poller);
+const char *spdk_poller_get_state_str(struct spdk_poller *poller);
+uint64_t spdk_poller_get_period_ticks(struct spdk_poller *poller);
+void spdk_poller_get_stats(struct spdk_poller *poller, struct spdk_poller_stats *stats);
 
-	uint64_t			period_ticks;
-	uint64_t			next_run_tick;
-	uint64_t			run_count;
-	uint64_t			busy_count;
-	spdk_poller_fn			fn;
-	void				*arg;
-	struct spdk_thread		*thread;
-	int				interruptfd;
-	spdk_poller_set_interrupt_mode_cb set_intr_cb_fn;
-	void				*set_intr_cb_arg;
+const char *spdk_io_channel_get_io_device_name(struct spdk_io_channel *ch);
+int spdk_io_channel_get_ref_count(struct spdk_io_channel *ch);
 
-	char				name[SPDK_MAX_POLLER_NAME_LEN + 1];
-};
-
-enum spdk_thread_state {
-	/* The thread is pocessing poller and message by spdk_thread_poll(). */
-	SPDK_THREAD_STATE_RUNNING,
-
-	/* The thread is in the process of termination. It reaps unregistering
-	 * poller are releasing I/O channel.
-	 */
-	SPDK_THREAD_STATE_EXITING,
-
-	/* The thread is exited. It is ready to call spdk_thread_destroy(). */
-	SPDK_THREAD_STATE_EXITED,
-};
-
-struct spdk_thread {
-	uint64_t			tsc_last;
-	struct spdk_thread_stats	stats;
-	/*
-	 * Contains pollers actively running on this thread.  Pollers
-	 *  are run round-robin. The thread takes one poller from the head
-	 *  of the ring, executes it, then puts it back at the tail of
-	 *  the ring.
-	 */
-	TAILQ_HEAD(active_pollers_head, spdk_poller)	active_pollers;
-	/**
-	 * Contains pollers running on this thread with a periodic timer.
-	 */
-	TAILQ_HEAD(timed_pollers_head, spdk_poller)	timed_pollers;
-	/*
-	 * Contains paused pollers.  Pollers on this queue are waiting until
-	 * they are resumed (in which case they're put onto the active/timer
-	 * queues) or unregistered.
-	 */
-	TAILQ_HEAD(paused_pollers_head, spdk_poller)	paused_pollers;
-	struct spdk_ring		*messages;
-	int				msg_fd;
-	SLIST_HEAD(, spdk_msg)		msg_cache;
-	size_t				msg_cache_count;
-	spdk_msg_fn			critical_msg;
-	uint64_t			id;
-	enum spdk_thread_state		state;
-	int				pending_unregister_count;
-
-	TAILQ_HEAD(, spdk_io_channel)	io_channels;
-	TAILQ_ENTRY(spdk_thread)	tailq;
-
-	char				name[SPDK_MAX_THREAD_NAME_LEN + 1];
-	struct spdk_cpuset		cpumask;
-	uint64_t			exit_timeout_tsc;
-
-	/* Indicates whether this spdk_thread currently runs in interrupt. */
-	bool				in_interrupt;
-	struct spdk_fd_group		*fgrp;
-
-	/* User context allocated at the end */
-	uint8_t				ctx[0];
-};
+const char *spdk_io_device_get_name(struct io_device *dev);
 
-const char *spdk_poller_state_str(enum spdk_poller_state state);
+struct spdk_poller *spdk_thread_get_first_active_poller(struct spdk_thread *thread);
+struct spdk_poller *spdk_thread_get_next_active_poller(struct spdk_poller *prev);
+struct spdk_poller *spdk_thread_get_first_timed_poller(struct spdk_thread *thread);
+struct spdk_poller *spdk_thread_get_next_timed_poller(struct spdk_poller *prev);
+struct spdk_poller *spdk_thread_get_first_paused_poller(struct spdk_thread *thread);
+struct spdk_poller *spdk_thread_get_next_paused_poller(struct spdk_poller *prev);
 
-const char *spdk_io_device_get_name(struct io_device *dev);
+struct spdk_io_channel *spdk_thread_get_first_io_channel(struct spdk_thread *thread);
+struct spdk_io_channel *spdk_thread_get_next_io_channel(struct spdk_io_channel *prev);
 
-#endif /* SPDK_THREAD_INTERNAL_H_ */
+#endif /* SPDK_INTERNAL_THREAD_H_ */
diff --git a/include/spdk_internal/usdt.h b/include/spdk_internal/usdt.h
new file mode 100644
index 000000000..3da9fa54b
--- /dev/null
+++ b/include/spdk_internal/usdt.h
@@ -0,0 +1,60 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_INTERNAL_USDT_H
+#define SPDK_INTERNAL_USDT_H
+
+#include "spdk/config.h"
+#include "spdk/env.h"
+
+#ifdef SPDK_CONFIG_USDT
+
+#include <sys/sdt.h>
+
+#define SPDK_DTRACE_PROBE(name)			DTRACE_PROBE1(spdk,name,spdk_get_ticks())
+#define SPDK_DTRACE_PROBE1(name,a1)		DTRACE_PROBE2(spdk,name,spdk_get_ticks(),a1)
+#define SPDK_DTRACE_PROBE2(name,a1,a2)		DTRACE_PROBE3(spdk,name,spdk_get_ticks(),a1,a2)
+#define SPDK_DTRACE_PROBE3(name,a1,a2,a3)	DTRACE_PROBE4(spdk,name,spdk_get_ticks(),a1,a2,a3)
+#define SPDK_DTRACE_PROBE4(name,a1,a2,a3,a4)	DTRACE_PROBE5(spdk,name,spdk_get_ticks(),a1,a2,a3,a4)
+
+#else
+
+#define SPDK_DTRACE_PROBE(...)
+#define SPDK_DTRACE_PROBE1(...)
+#define SPDK_DTRACE_PROBE2(...)
+#define SPDK_DTRACE_PROBE3(...)
+#define SPDK_DTRACE_PROBE4(...)
+
+#endif
+
+#endif /* SPDK_INTERNAL_USDT_H */
diff --git a/lib/Makefile b/lib/Makefile
index eab297ed0..58b0017b9 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -37,7 +37,7 @@ include $(SPDK_ROOT_DIR)/mk/spdk.lib_deps.mk
 
 DIRS-y += bdev blob blobfs conf accel event json jsonrpc \
           log lvol net rpc sock thread trace util nvme vmd nvmf scsi \
-          ioat ut_mock iscsi notify
+          ioat ut_mock iscsi notify init
 ifeq ($(OS),Linux)
 DIRS-y += nbd ftl
 endif
diff --git a/lib/accel/Makefile b/lib/accel/Makefile
index 7b11d8c3b..4492949ad 100644
--- a/lib/accel/Makefile
+++ b/lib/accel/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 5
+SO_VER := 6
 SO_MINOR := 0
 SO_SUFFIX := $(SO_VER).$(SO_MINOR)
 
diff --git a/lib/accel/accel_engine.c b/lib/accel/accel_engine.c
index 450595120..906ff4b17 100644
--- a/lib/accel/accel_engine.c
+++ b/lib/accel/accel_engine.c
@@ -382,6 +382,7 @@ spdk_accel_submit_crc32cv(struct spdk_io_channel *ch, uint32_t *dst, struct iove
 	accel_task->v.iovs = iov;
 	accel_task->v.iovcnt = iov_cnt;
 	accel_task->dst = (void *)dst;
+	accel_task->seed = seed;
 	accel_task->op_code = ACCEL_OPCODE_CRC32C;
 
 	if (_is_supported(accel_ch->engine, ACCEL_CRC32C)) {
@@ -400,6 +401,38 @@ spdk_accel_submit_crc32cv(struct spdk_io_channel *ch, uint32_t *dst, struct iove
 	}
 }
 
+/* Accel framework public API for copy with CRC-32C function */
+int
+spdk_accel_submit_copy_crc32c(struct spdk_io_channel *ch, void *dst, void *src,
+			      uint32_t *crc_dst, uint32_t seed, uint64_t nbytes,
+			      spdk_accel_completion_cb cb_fn, void *cb_arg)
+{
+	struct accel_io_channel *accel_ch = spdk_io_channel_get_ctx(ch);
+	struct spdk_accel_task *accel_task;
+
+	accel_task = _get_task(accel_ch, NULL, cb_fn, cb_arg);
+	if (accel_task == NULL) {
+		return -ENOMEM;
+	}
+
+	accel_task->dst = dst;
+	accel_task->src = src;
+	accel_task->crc_dst = crc_dst;
+	accel_task->v.iovcnt = 0;
+	accel_task->seed = seed;
+	accel_task->nbytes = nbytes;
+	accel_task->op_code = ACCEL_OPCODE_COPY_CRC32C;
+
+	if (_is_supported(accel_ch->engine, ACCEL_COPY_CRC32C)) {
+		return accel_ch->engine->submit_tasks(accel_ch->engine_ch, accel_task);
+	} else {
+		_sw_accel_copy(dst, src, nbytes);
+		_sw_accel_crc32c(crc_dst, src, seed, nbytes);
+		spdk_accel_task_complete(accel_task, 0);
+		return 0;
+	}
+}
+
 /* Accel framework public API for getting max operations for a batch. */
 uint32_t
 spdk_accel_batch_get_max(struct spdk_io_channel *ch)
@@ -628,6 +661,36 @@ spdk_accel_batch_prep_crc32cv(struct spdk_io_channel *ch, struct spdk_accel_batc
 	return 0;
 }
 
+int
+spdk_accel_batch_prep_copy_crc32c(struct spdk_io_channel *ch, struct spdk_accel_batch *batch,
+				  void *dst, void *src, uint32_t *crc_dst,  uint32_t seed, uint64_t nbytes,
+				  spdk_accel_completion_cb cb_fn, void *cb_arg)
+{
+	struct spdk_accel_task *accel_task;
+	struct accel_io_channel *accel_ch = spdk_io_channel_get_ctx(ch);
+
+	accel_task = _get_task(accel_ch, batch, cb_fn, cb_arg);
+	if (accel_task == NULL) {
+		return -ENOMEM;
+	}
+
+	accel_task->dst = dst;
+	accel_task->src = src;
+	accel_task->crc_dst = crc_dst;
+	accel_task->v.iovcnt = 0;
+	accel_task->seed = seed;
+	accel_task->nbytes = nbytes;
+	accel_task->op_code = ACCEL_OPCODE_COPY_CRC32C;
+
+	if (_is_supported(accel_ch->engine, ACCEL_COPY_CRC32C)) {
+		TAILQ_INSERT_TAIL(&batch->hw_tasks, accel_task, link);
+	} else {
+		TAILQ_INSERT_TAIL(&batch->sw_tasks, accel_task, link);
+	}
+
+	return 0;
+}
+
 /* Accel framework public API for batch_create function. */
 struct spdk_accel_batch *
 spdk_accel_batch_create(struct spdk_io_channel *ch)
@@ -715,6 +778,11 @@ spdk_accel_batch_submit(struct spdk_io_channel *ch, struct spdk_accel_batch *bat
 			}
 			spdk_accel_task_complete(accel_task, 0);
 			break;
+		case ACCEL_OPCODE_COPY_CRC32C:
+			_sw_accel_copy(accel_task->dst, accel_task->src, accel_task->nbytes);
+			_sw_accel_crc32c(accel_task->crc_dst, accel_task->src, accel_task->seed, accel_task->nbytes);
+			spdk_accel_task_complete(accel_task, 0);
+			break;
 		case ACCEL_OPCODE_DUALCAST:
 			_sw_accel_dualcast(accel_task->dst, accel_task->dst2, accel_task->src,
 					   accel_task->nbytes);
@@ -967,15 +1035,7 @@ _sw_accel_crc32c(uint32_t *dst, void *src, uint32_t seed, uint64_t nbytes)
 static void
 _sw_accel_crc32cv(uint32_t *dst, struct iovec *iov, uint32_t iovcnt, uint32_t seed)
 {
-	uint32_t i, crc32c = ~seed;
-
-	for (i = 0; i < iovcnt; i++) {
-		assert(iov[i].iov_base != NULL);
-		assert(iov[i].iov_len != 0);
-		crc32c = spdk_crc32c_update(iov[i].iov_base, iov[i].iov_len, crc32c);
-	}
-
-	*dst = crc32c;
+	*dst = spdk_crc32c_iov_update(iov, iovcnt, ~seed);
 }
 
 static struct spdk_io_channel *sw_accel_get_io_channel(void);
diff --git a/lib/accel/spdk_accel.map b/lib/accel/spdk_accel.map
index 781fa9b70..35699f7fa 100644
--- a/lib/accel/spdk_accel.map
+++ b/lib/accel/spdk_accel.map
@@ -15,6 +15,7 @@
 	spdk_accel_batch_prep_fill;
 	spdk_accel_batch_prep_crc32c;
 	spdk_accel_batch_prep_crc32cv;
+	spdk_accel_batch_prep_copy_crc32c;
 	spdk_accel_batch_submit;
 	spdk_accel_batch_cancel;
 	spdk_accel_submit_copy;
@@ -23,6 +24,7 @@
 	spdk_accel_submit_fill;
 	spdk_accel_submit_crc32c;
 	spdk_accel_submit_crc32cv;
+	spdk_accel_submit_copy_crc32c;
 	spdk_accel_write_config_json;
 
 	# functions needed by modules
diff --git a/lib/bdev/Makefile b/lib/bdev/Makefile
index 4b9f3f098..383332262 100644
--- a/lib/bdev/Makefile
+++ b/lib/bdev/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 6
+SO_VER := 7
 SO_MINOR := 0
 
 ifeq ($(CONFIG_VTUNE),y)
diff --git a/lib/bdev/bdev.c b/lib/bdev/bdev.c
index 9c741d001..a9ae6a78c 100644
--- a/lib/bdev/bdev.c
+++ b/lib/bdev/bdev.c
@@ -84,12 +84,27 @@ int __itt_init_ittlib(const char *, __itt_group_id);
 
 #define SPDK_BDEV_POOL_ALIGNMENT 512
 
+/* The maximum number of children requests for a UNMAP or WRITE ZEROES command
+ * when splitting into children requests at a time.
+ */
+#define SPDK_BDEV_MAX_CHILDREN_UNMAP_WRITE_ZEROES_REQS (8)
+
 static const char *qos_rpc_type[] = {"rw_ios_per_sec",
 				     "rw_mbytes_per_sec", "r_mbytes_per_sec", "w_mbytes_per_sec"
 				    };
 
 TAILQ_HEAD(spdk_bdev_list, spdk_bdev);
 
+RB_HEAD(bdev_name_tree, spdk_bdev_name);
+
+static int
+bdev_name_cmp(struct spdk_bdev_name *name1, struct spdk_bdev_name *name2)
+{
+	return strcmp(name1->name, name2->name);
+}
+
+RB_GENERATE_STATIC(bdev_name_tree, spdk_bdev_name, node, bdev_name_cmp);
+
 struct spdk_bdev_mgr {
 	struct spdk_mempool *bdev_io_pool;
 
@@ -101,6 +116,7 @@ struct spdk_bdev_mgr {
 	TAILQ_HEAD(bdev_module_list, spdk_bdev_module) bdev_modules;
 
 	struct spdk_bdev_list bdevs;
+	struct bdev_name_tree bdev_names;
 
 	bool init_complete;
 	bool module_init_complete;
@@ -115,6 +131,7 @@ struct spdk_bdev_mgr {
 static struct spdk_bdev_mgr g_bdev_mgr = {
 	.bdev_modules = TAILQ_HEAD_INITIALIZER(g_bdev_mgr.bdev_modules),
 	.bdevs = TAILQ_HEAD_INITIALIZER(g_bdev_mgr.bdevs),
+	.bdev_names = RB_INITIALIZER(g_bdev_mgr.bdev_names),
 	.init_complete = false,
 	.module_init_complete = false,
 	.mutex = PTHREAD_MUTEX_INITIALIZER,
@@ -541,7 +558,7 @@ bdev_in_examine_allowlist(struct spdk_bdev *bdev)
 		return true;
 	}
 	TAILQ_FOREACH(tmp, &bdev->aliases, tailq) {
-		if (bdev_examine_allowlist_check(tmp->alias)) {
+		if (bdev_examine_allowlist_check(tmp->alias.name)) {
 			return true;
 		}
 	}
@@ -711,21 +728,13 @@ spdk_bdev_next_leaf(struct spdk_bdev *prev)
 struct spdk_bdev *
 spdk_bdev_get_by_name(const char *bdev_name)
 {
-	struct spdk_bdev_alias *tmp;
-	struct spdk_bdev *bdev = spdk_bdev_first();
-
-	while (bdev != NULL) {
-		if (strcmp(bdev_name, bdev->name) == 0) {
-			return bdev;
-		}
-
-		TAILQ_FOREACH(tmp, &bdev->aliases, tailq) {
-			if (strcmp(bdev_name, tmp->alias) == 0) {
-				return bdev;
-			}
-		}
+	struct spdk_bdev_name find;
+	struct spdk_bdev_name *res;
 
-		bdev = spdk_bdev_next(bdev);
+	find.name = (char *)bdev_name;
+	res = RB_FIND(bdev_name_tree, &g_bdev_mgr.bdev_names, &find);
+	if (res != NULL) {
+		return res->bdev;
 	}
 
 	return NULL;
@@ -1953,26 +1962,7 @@ bdev_queue_io_wait_with_cb(struct spdk_bdev_io *bdev_io, spdk_bdev_io_wait_cb cb
 }
 
 static bool
-bdev_io_type_can_split(uint8_t type)
-{
-	assert(type != SPDK_BDEV_IO_TYPE_INVALID);
-	assert(type < SPDK_BDEV_NUM_IO_TYPES);
-
-	/* Only split READ and WRITE I/O.  Theoretically other types of I/O like
-	 * UNMAP could be split, but these types of I/O are typically much larger
-	 * in size (sometimes the size of the entire block device), and the bdev
-	 * module can more efficiently split these types of I/O.  Plus those types
-	 * of I/O do not have a payload, which makes the splitting process simpler.
-	 */
-	if (type == SPDK_BDEV_IO_TYPE_READ || type == SPDK_BDEV_IO_TYPE_WRITE) {
-		return true;
-	} else {
-		return false;
-	}
-}
-
-static bool
-bdev_io_should_split(struct spdk_bdev_io *bdev_io)
+bdev_rw_should_split(struct spdk_bdev_io *bdev_io)
 {
 	uint32_t io_boundary = bdev_io->bdev->optimal_io_boundary;
 	uint32_t max_size = bdev_io->bdev->max_segment_size;
@@ -1984,10 +1974,6 @@ bdev_io_should_split(struct spdk_bdev_io *bdev_io)
 		return false;
 	}
 
-	if (!bdev_io_type_can_split(bdev_io->type)) {
-		return false;
-	}
-
 	if (io_boundary) {
 		uint64_t start_stripe, end_stripe;
 
@@ -2024,6 +2010,52 @@ bdev_io_should_split(struct spdk_bdev_io *bdev_io)
 	return false;
 }
 
+static bool
+bdev_unmap_should_split(struct spdk_bdev_io *bdev_io)
+{
+	uint32_t num_unmap_segments;
+
+	if (!bdev_io->bdev->max_unmap || !bdev_io->bdev->max_unmap_segments) {
+		return false;
+	}
+	num_unmap_segments = spdk_divide_round_up(bdev_io->u.bdev.num_blocks, bdev_io->bdev->max_unmap);
+	if (num_unmap_segments > bdev_io->bdev->max_unmap_segments) {
+		return true;
+	}
+
+	return false;
+}
+
+static bool
+bdev_write_zeroes_should_split(struct spdk_bdev_io *bdev_io)
+{
+	if (!bdev_io->bdev->max_write_zeroes) {
+		return false;
+	}
+
+	if (bdev_io->u.bdev.num_blocks > bdev_io->bdev->max_write_zeroes) {
+		return true;
+	}
+
+	return false;
+}
+
+static bool
+bdev_io_should_split(struct spdk_bdev_io *bdev_io)
+{
+	switch (bdev_io->type) {
+	case SPDK_BDEV_IO_TYPE_READ:
+	case SPDK_BDEV_IO_TYPE_WRITE:
+		return bdev_rw_should_split(bdev_io);
+	case SPDK_BDEV_IO_TYPE_UNMAP:
+		return bdev_unmap_should_split(bdev_io);
+	case SPDK_BDEV_IO_TYPE_WRITE_ZEROES:
+		return bdev_write_zeroes_should_split(bdev_io);
+	default:
+		return false;
+	}
+}
+
 static uint32_t
 _to_next_boundary(uint64_t offset, uint32_t boundary)
 {
@@ -2034,7 +2066,104 @@ static void
 bdev_io_split_done(struct spdk_bdev_io *bdev_io, bool success, void *cb_arg);
 
 static void
-_bdev_io_split(void *_bdev_io)
+_bdev_rw_split(void *_bdev_io);
+
+static void
+bdev_unmap_split(struct spdk_bdev_io *bdev_io);
+
+static void
+_bdev_unmap_split(void *_bdev_io)
+{
+	return bdev_unmap_split((struct spdk_bdev_io *)_bdev_io);
+}
+
+static void
+bdev_write_zeroes_split(struct spdk_bdev_io *bdev_io);
+
+static void
+_bdev_write_zeroes_split(void *_bdev_io)
+{
+	return bdev_write_zeroes_split((struct spdk_bdev_io *)_bdev_io);
+}
+
+static int
+bdev_io_split_submit(struct spdk_bdev_io *bdev_io, struct iovec *iov, int iovcnt, void *md_buf,
+		     uint64_t num_blocks, uint64_t *offset, uint64_t *remaining)
+{
+	int rc;
+	uint64_t current_offset, current_remaining;
+	spdk_bdev_io_wait_cb io_wait_fn;
+
+	current_offset = *offset;
+	current_remaining = *remaining;
+
+	bdev_io->u.bdev.split_outstanding++;
+
+	io_wait_fn = _bdev_rw_split;
+	switch (bdev_io->type) {
+	case SPDK_BDEV_IO_TYPE_READ:
+		rc = bdev_readv_blocks_with_md(bdev_io->internal.desc,
+					       spdk_io_channel_from_ctx(bdev_io->internal.ch),
+					       iov, iovcnt, md_buf, current_offset,
+					       num_blocks,
+					       bdev_io_split_done, bdev_io);
+		break;
+	case SPDK_BDEV_IO_TYPE_WRITE:
+		rc = bdev_writev_blocks_with_md(bdev_io->internal.desc,
+						spdk_io_channel_from_ctx(bdev_io->internal.ch),
+						iov, iovcnt, md_buf, current_offset,
+						num_blocks,
+						bdev_io_split_done, bdev_io);
+		break;
+	case SPDK_BDEV_IO_TYPE_UNMAP:
+		io_wait_fn = _bdev_unmap_split;
+		rc = spdk_bdev_unmap_blocks(bdev_io->internal.desc,
+					    spdk_io_channel_from_ctx(bdev_io->internal.ch),
+					    current_offset, num_blocks,
+					    bdev_io_split_done, bdev_io);
+		break;
+	case SPDK_BDEV_IO_TYPE_WRITE_ZEROES:
+		io_wait_fn = _bdev_write_zeroes_split;
+		rc = spdk_bdev_write_zeroes_blocks(bdev_io->internal.desc,
+						   spdk_io_channel_from_ctx(bdev_io->internal.ch),
+						   current_offset, num_blocks,
+						   bdev_io_split_done, bdev_io);
+		break;
+	default:
+		assert(false);
+		rc = -EINVAL;
+		break;
+	}
+
+	if (rc == 0) {
+		current_offset += num_blocks;
+		current_remaining -= num_blocks;
+		bdev_io->u.bdev.split_current_offset_blocks = current_offset;
+		bdev_io->u.bdev.split_remaining_num_blocks = current_remaining;
+		*offset = current_offset;
+		*remaining = current_remaining;
+	} else {
+		bdev_io->u.bdev.split_outstanding--;
+		if (rc == -ENOMEM) {
+			if (bdev_io->u.bdev.split_outstanding == 0) {
+				/* No I/O is outstanding. Hence we should wait here. */
+				bdev_queue_io_wait_with_cb(bdev_io, io_wait_fn);
+			}
+		} else {
+			bdev_io->internal.status = SPDK_BDEV_IO_STATUS_FAILED;
+			if (bdev_io->u.bdev.split_outstanding == 0) {
+				spdk_trace_record(TRACE_BDEV_IO_DONE, 0, 0, (uintptr_t)bdev_io);
+				TAILQ_REMOVE(&bdev_io->internal.ch->io_submitted, bdev_io, internal.ch_link);
+				bdev_io->internal.cb(bdev_io, false, bdev_io->internal.caller_ctx);
+			}
+		}
+	}
+
+	return rc;
+}
+
+static void
+_bdev_rw_split(void *_bdev_io)
 {
 	struct iovec *parent_iov, *iov;
 	struct spdk_bdev_io *bdev_io = _bdev_io;
@@ -2138,8 +2267,7 @@ _bdev_io_split(void *_bdev_io)
 							if (bdev_io->u.bdev.split_outstanding == 0) {
 								SPDK_ERRLOG("The first child io was less than a block size\n");
 								bdev_io->internal.status = SPDK_BDEV_IO_STATUS_FAILED;
-								spdk_trace_record_tsc(spdk_get_ticks(), TRACE_BDEV_IO_DONE, 0, 0,
-										      (uintptr_t)bdev_io, 0);
+								spdk_trace_record(TRACE_BDEV_IO_DONE, 0, 0, (uintptr_t)bdev_io);
 								TAILQ_REMOVE(&bdev_io->internal.ch->io_submitted, bdev_io, internal.ch_link);
 								bdev_io->internal.cb(bdev_io, false, bdev_io->internal.caller_ctx);
 							}
@@ -2162,44 +2290,56 @@ _bdev_io_split(void *_bdev_io)
 			to_next_boundary -= to_next_boundary_bytes / blocklen;
 		}
 
-		bdev_io->u.bdev.split_outstanding++;
+		rc = bdev_io_split_submit(bdev_io, iov, iovcnt, md_buf, to_next_boundary,
+					  &current_offset, &remaining);
+		if (spdk_unlikely(rc)) {
+			return;
+		}
+	}
+}
+
+static void
+bdev_unmap_split(struct spdk_bdev_io *bdev_io)
+{
+	uint64_t offset, unmap_blocks, remaining, max_unmap_blocks;
+	uint32_t num_children_reqs = 0;
+	int rc;
+
+	offset = bdev_io->u.bdev.split_current_offset_blocks;
+	remaining = bdev_io->u.bdev.split_remaining_num_blocks;
+	max_unmap_blocks = bdev_io->bdev->max_unmap * bdev_io->bdev->max_unmap_segments;
+
+	while (remaining && (num_children_reqs < SPDK_BDEV_MAX_CHILDREN_UNMAP_WRITE_ZEROES_REQS)) {
+		unmap_blocks = spdk_min(remaining, max_unmap_blocks);
 
-		if (bdev_io->type == SPDK_BDEV_IO_TYPE_READ) {
-			rc = bdev_readv_blocks_with_md(bdev_io->internal.desc,
-						       spdk_io_channel_from_ctx(bdev_io->internal.ch),
-						       iov, iovcnt, md_buf, current_offset,
-						       to_next_boundary,
-						       bdev_io_split_done, bdev_io);
+		rc = bdev_io_split_submit(bdev_io, NULL, 0, NULL, unmap_blocks,
+					  &offset, &remaining);
+		if (spdk_likely(rc == 0)) {
+			num_children_reqs++;
 		} else {
-			rc = bdev_writev_blocks_with_md(bdev_io->internal.desc,
-							spdk_io_channel_from_ctx(bdev_io->internal.ch),
-							iov, iovcnt, md_buf, current_offset,
-							to_next_boundary,
-							bdev_io_split_done, bdev_io);
+			return;
 		}
+	}
+}
 
-		if (rc == 0) {
-			current_offset += to_next_boundary;
-			remaining -= to_next_boundary;
-			bdev_io->u.bdev.split_current_offset_blocks = current_offset;
-			bdev_io->u.bdev.split_remaining_num_blocks = remaining;
-		} else {
-			bdev_io->u.bdev.split_outstanding--;
-			if (rc == -ENOMEM) {
-				if (bdev_io->u.bdev.split_outstanding == 0) {
-					/* No I/O is outstanding. Hence we should wait here. */
-					bdev_queue_io_wait_with_cb(bdev_io, _bdev_io_split);
-				}
-			} else {
-				bdev_io->internal.status = SPDK_BDEV_IO_STATUS_FAILED;
-				if (bdev_io->u.bdev.split_outstanding == 0) {
-					spdk_trace_record_tsc(spdk_get_ticks(), TRACE_BDEV_IO_DONE, 0, 0,
-							      (uintptr_t)bdev_io, 0);
-					TAILQ_REMOVE(&bdev_io->internal.ch->io_submitted, bdev_io, internal.ch_link);
-					bdev_io->internal.cb(bdev_io, false, bdev_io->internal.caller_ctx);
-				}
-			}
+static void
+bdev_write_zeroes_split(struct spdk_bdev_io *bdev_io)
+{
+	uint64_t offset, write_zeroes_blocks, remaining;
+	uint32_t num_children_reqs = 0;
+	int rc;
+
+	offset = bdev_io->u.bdev.split_current_offset_blocks;
+	remaining = bdev_io->u.bdev.split_remaining_num_blocks;
+
+	while (remaining && (num_children_reqs < SPDK_BDEV_MAX_CHILDREN_UNMAP_WRITE_ZEROES_REQS)) {
+		write_zeroes_blocks = spdk_min(remaining, bdev_io->bdev->max_write_zeroes);
 
+		rc = bdev_io_split_submit(bdev_io, NULL, 0, NULL, write_zeroes_blocks,
+					  &offset, &remaining);
+		if (spdk_likely(rc == 0)) {
+			num_children_reqs++;
+		} else {
 			return;
 		}
 	}
@@ -2228,8 +2368,7 @@ bdev_io_split_done(struct spdk_bdev_io *bdev_io, bool success, void *cb_arg)
 	 */
 	if (parent_io->u.bdev.split_remaining_num_blocks == 0) {
 		assert(parent_io->internal.cb != bdev_io_split_done);
-		spdk_trace_record_tsc(spdk_get_ticks(), TRACE_BDEV_IO_DONE, 0, 0,
-				      (uintptr_t)parent_io, 0);
+		spdk_trace_record(TRACE_BDEV_IO_DONE, 0, 0, (uintptr_t)parent_io);
 		TAILQ_REMOVE(&parent_io->internal.ch->io_submitted, parent_io, internal.ch_link);
 		parent_io->internal.cb(parent_io, parent_io->internal.status == SPDK_BDEV_IO_STATUS_SUCCESS,
 				       parent_io->internal.caller_ctx);
@@ -2240,40 +2379,66 @@ bdev_io_split_done(struct spdk_bdev_io *bdev_io, bool success, void *cb_arg)
 	 * Continue with the splitting process.  This function will complete the parent I/O if the
 	 * splitting is done.
 	 */
-	_bdev_io_split(parent_io);
+	switch (parent_io->type) {
+	case SPDK_BDEV_IO_TYPE_READ:
+	case SPDK_BDEV_IO_TYPE_WRITE:
+		_bdev_rw_split(parent_io);
+		break;
+	case SPDK_BDEV_IO_TYPE_UNMAP:
+		bdev_unmap_split(parent_io);
+		break;
+	case SPDK_BDEV_IO_TYPE_WRITE_ZEROES:
+		bdev_write_zeroes_split(parent_io);
+		break;
+	default:
+		assert(false);
+		break;
+	}
 }
 
 static void
-bdev_io_split_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io, bool success);
+bdev_rw_split_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io, bool success);
 
 static void
 bdev_io_split(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
 {
-	assert(bdev_io_type_can_split(bdev_io->type));
-
 	bdev_io->u.bdev.split_current_offset_blocks = bdev_io->u.bdev.offset_blocks;
 	bdev_io->u.bdev.split_remaining_num_blocks = bdev_io->u.bdev.num_blocks;
 	bdev_io->u.bdev.split_outstanding = 0;
 	bdev_io->internal.status = SPDK_BDEV_IO_STATUS_SUCCESS;
 
-	if (_is_buf_allocated(bdev_io->u.bdev.iovs)) {
-		_bdev_io_split(bdev_io);
-	} else {
-		assert(bdev_io->type == SPDK_BDEV_IO_TYPE_READ);
-		spdk_bdev_io_get_buf(bdev_io, bdev_io_split_get_buf_cb,
-				     bdev_io->u.bdev.num_blocks * bdev_io->bdev->blocklen);
+	switch (bdev_io->type) {
+	case SPDK_BDEV_IO_TYPE_READ:
+	case SPDK_BDEV_IO_TYPE_WRITE:
+		if (_is_buf_allocated(bdev_io->u.bdev.iovs)) {
+			_bdev_rw_split(bdev_io);
+		} else {
+			assert(bdev_io->type == SPDK_BDEV_IO_TYPE_READ);
+			spdk_bdev_io_get_buf(bdev_io, bdev_rw_split_get_buf_cb,
+					     bdev_io->u.bdev.num_blocks * bdev_io->bdev->blocklen);
+		}
+		break;
+	case SPDK_BDEV_IO_TYPE_UNMAP:
+		bdev_unmap_split(bdev_io);
+		break;
+	case SPDK_BDEV_IO_TYPE_WRITE_ZEROES:
+		bdev_write_zeroes_split(bdev_io);
+		break;
+	default:
+		assert(false);
+		break;
 	}
 }
 
 static void
-bdev_io_split_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io, bool success)
+bdev_rw_split_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io, bool success)
 {
 	if (!success) {
 		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
 		return;
 	}
 
-	_bdev_io_split(bdev_io);
+	_bdev_rw_split(bdev_io);
 }
 
 /* Explicitly mark this inline, since it's used as a function pointer and otherwise won't
@@ -2467,11 +2632,6 @@ spdk_bdev_io_type_supported(struct spdk_bdev *bdev, enum spdk_bdev_io_type io_ty
 			/* The bdev layer will emulate write zeroes as long as write is supported. */
 			supported = bdev_io_type_supported(bdev, SPDK_BDEV_IO_TYPE_WRITE);
 			break;
-		case SPDK_BDEV_IO_TYPE_ZCOPY:
-			/* Zero copy can be emulated with regular read and write */
-			supported = bdev_io_type_supported(bdev, SPDK_BDEV_IO_TYPE_READ) &&
-				    bdev_io_type_supported(bdev, SPDK_BDEV_IO_TYPE_WRITE);
-			break;
 		default:
 			break;
 		}
@@ -3073,10 +3233,32 @@ bdev_channel_destroy(void *io_device, void *ctx_buf)
 	bdev_channel_destroy_resource(ch);
 }
 
+static int
+bdev_name_add(struct spdk_bdev_name *bdev_name, struct spdk_bdev *bdev, const char *name)
+{
+	bdev_name->name = strdup(name);
+	if (bdev_name->name == NULL) {
+		SPDK_ERRLOG("Unable to allocate bdev name\n");
+		return -ENOMEM;
+	}
+
+	bdev_name->bdev = bdev;
+	RB_INSERT(bdev_name_tree, &g_bdev_mgr.bdev_names, bdev_name);
+	return 0;
+}
+
+static void
+bdev_name_del(struct spdk_bdev_name *bdev_name)
+{
+	RB_REMOVE(bdev_name_tree, &g_bdev_mgr.bdev_names, bdev_name);
+	free(bdev_name->name);
+}
+
 int
 spdk_bdev_alias_add(struct spdk_bdev *bdev, const char *alias)
 {
 	struct spdk_bdev_alias *tmp;
+	int ret;
 
 	if (alias == NULL) {
 		SPDK_ERRLOG("Empty alias passed\n");
@@ -3094,11 +3276,10 @@ spdk_bdev_alias_add(struct spdk_bdev *bdev, const char *alias)
 		return -ENOMEM;
 	}
 
-	tmp->alias = strdup(alias);
-	if (tmp->alias == NULL) {
+	ret = bdev_name_add(&tmp->alias, bdev, alias);
+	if (ret != 0) {
 		free(tmp);
-		SPDK_ERRLOG("Unable to allocate alias\n");
-		return -ENOMEM;
+		return ret;
 	}
 
 	TAILQ_INSERT_TAIL(&bdev->aliases, tmp, tailq);
@@ -3112,9 +3293,9 @@ spdk_bdev_alias_del(struct spdk_bdev *bdev, const char *alias)
 	struct spdk_bdev_alias *tmp;
 
 	TAILQ_FOREACH(tmp, &bdev->aliases, tailq) {
-		if (strcmp(alias, tmp->alias) == 0) {
+		if (strcmp(alias, tmp->alias.name) == 0) {
 			TAILQ_REMOVE(&bdev->aliases, tmp, tailq);
-			free(tmp->alias);
+			bdev_name_del(&tmp->alias);
 			free(tmp);
 			return 0;
 		}
@@ -3132,7 +3313,7 @@ spdk_bdev_alias_del_all(struct spdk_bdev *bdev)
 
 	TAILQ_FOREACH_SAFE(p, &bdev->aliases, tailq, tmp) {
 		TAILQ_REMOVE(&bdev->aliases, p, tailq);
-		free(p->alias);
+		bdev_name_del(&p->alias);
 		free(p);
 	}
 }
@@ -3291,6 +3472,12 @@ spdk_bdev_get_data_block_size(const struct spdk_bdev *bdev)
 	}
 }
 
+uint32_t
+spdk_bdev_get_physical_block_size(const struct spdk_bdev *bdev)
+{
+	return bdev->phys_blocklen;
+}
+
 static uint32_t
 _bdev_get_block_size_with_md(const struct spdk_bdev *bdev)
 {
@@ -3524,7 +3711,7 @@ _bdev_io_check_md_buf(const struct iovec *iovs, const void *md_buf)
 
 static int
 bdev_read_blocks_with_md(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch, void *buf,
-			 void *md_buf, int64_t offset_blocks, uint64_t num_blocks,
+			 void *md_buf, uint64_t offset_blocks, uint64_t num_blocks,
 			 spdk_bdev_io_completion_cb cb, void *cb_arg)
 {
 	struct spdk_bdev *bdev = spdk_bdev_desc_get_bdev(desc);
@@ -3581,7 +3768,7 @@ spdk_bdev_read_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
 
 int
 spdk_bdev_read_blocks_with_md(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
-			      void *buf, void *md_buf, int64_t offset_blocks, uint64_t num_blocks,
+			      void *buf, void *md_buf, uint64_t offset_blocks, uint64_t num_blocks,
 			      spdk_bdev_io_completion_cb cb, void *cb_arg)
 {
 	struct iovec iov = {
@@ -4184,31 +4371,9 @@ spdk_bdev_comparev_and_writev_blocks(struct spdk_bdev_desc *desc, struct spdk_io
 				   bdev_comparev_and_writev_blocks_locked, bdev_io);
 }
 
-static void
-bdev_zcopy_get_buf(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io, bool success)
-{
-	if (!success) {
-		/* Don't use spdk_bdev_io_complete here - this bdev_io was never actually submitted. */
-		bdev_io->internal.status = SPDK_BDEV_IO_STATUS_NOMEM;
-		bdev_io->internal.cb(bdev_io, success, bdev_io->internal.caller_ctx);
-		return;
-	}
-
-	if (bdev_io->u.bdev.zcopy.populate) {
-		/* Read the real data into the buffer */
-		bdev_io->type = SPDK_BDEV_IO_TYPE_READ;
-		bdev_io->internal.status = SPDK_BDEV_IO_STATUS_PENDING;
-		bdev_io_submit(bdev_io);
-		return;
-	}
-
-	/* Don't use spdk_bdev_io_complete here - this bdev_io was never actually submitted. */
-	bdev_io->internal.status = SPDK_BDEV_IO_STATUS_SUCCESS;
-	bdev_io->internal.cb(bdev_io, success, bdev_io->internal.caller_ctx);
-}
-
 int
 spdk_bdev_zcopy_start(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		      struct iovec *iov, int iovcnt,
 		      uint64_t offset_blocks, uint64_t num_blocks,
 		      bool populate,
 		      spdk_bdev_io_completion_cb cb, void *cb_arg)
@@ -4239,21 +4404,15 @@ spdk_bdev_zcopy_start(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
 	bdev_io->type = SPDK_BDEV_IO_TYPE_ZCOPY;
 	bdev_io->u.bdev.num_blocks = num_blocks;
 	bdev_io->u.bdev.offset_blocks = offset_blocks;
-	bdev_io->u.bdev.iovs = NULL;
-	bdev_io->u.bdev.iovcnt = 0;
+	bdev_io->u.bdev.iovs = iov;
+	bdev_io->u.bdev.iovcnt = iovcnt;
 	bdev_io->u.bdev.md_buf = NULL;
 	bdev_io->u.bdev.zcopy.populate = populate ? 1 : 0;
 	bdev_io->u.bdev.zcopy.commit = 0;
 	bdev_io->u.bdev.zcopy.start = 1;
 	bdev_io_init(bdev_io, bdev, cb_arg, cb);
 
-	if (bdev_io_type_supported(bdev, SPDK_BDEV_IO_TYPE_ZCOPY)) {
-		bdev_io_submit(bdev_io);
-	} else {
-		/* Emulate zcopy by allocating a buffer */
-		spdk_bdev_io_get_buf(bdev_io, bdev_zcopy_get_buf,
-				     bdev_io->u.bdev.num_blocks * bdev->blocklen);
-	}
+	bdev_io_submit(bdev_io);
 
 	return 0;
 }
@@ -4262,16 +4421,6 @@ int
 spdk_bdev_zcopy_end(struct spdk_bdev_io *bdev_io, bool commit,
 		    spdk_bdev_io_completion_cb cb, void *cb_arg)
 {
-	struct spdk_bdev *bdev = bdev_io->bdev;
-
-	if (bdev_io->type == SPDK_BDEV_IO_TYPE_READ) {
-		/* This can happen if the zcopy was emulated in start */
-		if (bdev_io->u.bdev.zcopy.start != 1) {
-			return -EINVAL;
-		}
-		bdev_io->type = SPDK_BDEV_IO_TYPE_ZCOPY;
-	}
-
 	if (bdev_io->type != SPDK_BDEV_IO_TYPE_ZCOPY) {
 		return -EINVAL;
 	}
@@ -4282,19 +4431,6 @@ spdk_bdev_zcopy_end(struct spdk_bdev_io *bdev_io, bool commit,
 	bdev_io->internal.cb = cb;
 	bdev_io->internal.status = SPDK_BDEV_IO_STATUS_PENDING;
 
-	if (bdev_io_type_supported(bdev, SPDK_BDEV_IO_TYPE_ZCOPY)) {
-		bdev_io_submit(bdev_io);
-		return 0;
-	}
-
-	if (!bdev_io->u.bdev.zcopy.commit) {
-		/* Don't use spdk_bdev_io_complete here - this bdev_io was never actually submitted. */
-		bdev_io->internal.status = SPDK_BDEV_IO_STATUS_SUCCESS;
-		bdev_io->internal.cb(bdev_io, true, bdev_io->internal.caller_ctx);
-		return 0;
-	}
-
-	bdev_io->type = SPDK_BDEV_IO_TYPE_WRITE;
 	bdev_io_submit(bdev_io);
 
 	return 0;
@@ -5062,7 +5198,7 @@ bdev_io_complete(void *ctx)
 
 	tsc = spdk_get_ticks();
 	tsc_diff = tsc - bdev_io->internal.submit_tsc;
-	spdk_trace_record_tsc(tsc, TRACE_BDEV_IO_DONE, 0, 0, (uintptr_t)bdev_io, 0);
+	spdk_trace_record_tsc(tsc, TRACE_BDEV_IO_DONE, 0, 0, (uintptr_t)bdev_io);
 
 	TAILQ_REMOVE(&bdev_ch->io_submitted, bdev_io, internal.ch_link);
 
@@ -5404,9 +5540,10 @@ spdk_bdev_io_get_io_channel(struct spdk_bdev_io *bdev_io)
 }
 
 static int
-bdev_init(struct spdk_bdev *bdev)
+bdev_register(struct spdk_bdev *bdev)
 {
 	char *bdev_name;
+	int ret;
 
 	assert(bdev->module != NULL);
 
@@ -5439,6 +5576,12 @@ bdev_init(struct spdk_bdev *bdev)
 	bdev->internal.qd_poller = NULL;
 	bdev->internal.qos = NULL;
 
+	ret = bdev_name_add(&bdev->internal.bdev_name, bdev, bdev->name);
+	if (ret != 0) {
+		free(bdev_name);
+		return ret;
+	}
+
 	/* If the user didn't specify a uuid, generate one. */
 	if (spdk_mem_all_zero(&bdev->uuid, sizeof(bdev->uuid))) {
 		spdk_uuid_generate(&bdev->uuid);
@@ -5464,6 +5607,10 @@ bdev_init(struct spdk_bdev *bdev)
 		bdev->acwu = 1;
 	}
 
+	if (bdev->phys_blocklen == 0) {
+		bdev->phys_blocklen = spdk_bdev_get_data_block_size(bdev);
+	}
+
 	TAILQ_INIT(&bdev->internal.open_descs);
 	TAILQ_INIT(&bdev->internal.locked_ranges);
 	TAILQ_INIT(&bdev->internal.pending_locked_ranges);
@@ -5480,6 +5627,10 @@ bdev_init(struct spdk_bdev *bdev)
 	free(bdev_name);
 
 	pthread_mutex_init(&bdev->internal.mutex, NULL);
+
+	SPDK_DEBUGLOG(bdev, "Inserting bdev %s into list\n", bdev->name);
+	TAILQ_INSERT_TAIL(&g_bdev_mgr.bdevs, bdev, internal.link);
+
 	return 0;
 }
 
@@ -5508,32 +5659,23 @@ bdev_destroy_cb(void *io_device)
 }
 
 static void
-bdev_start_finished(void *arg)
+bdev_register_finished(void *arg)
 {
 	struct spdk_bdev *bdev = arg;
 
 	spdk_notify_send("bdev_register", spdk_bdev_get_name(bdev));
 }
 
-static void
-bdev_start(struct spdk_bdev *bdev)
-{
-	SPDK_DEBUGLOG(bdev, "Inserting bdev %s into list\n", bdev->name);
-	TAILQ_INSERT_TAIL(&g_bdev_mgr.bdevs, bdev, internal.link);
-
-	/* Examine configuration before initializing I/O */
-	bdev_examine(bdev);
-
-	spdk_bdev_wait_for_examine(bdev_start_finished, bdev);
-}
-
 int
 spdk_bdev_register(struct spdk_bdev *bdev)
 {
-	int rc = bdev_init(bdev);
+	int rc = bdev_register(bdev);
 
 	if (rc == 0) {
-		bdev_start(bdev);
+		/* Examine configuration before initializing I/O */
+		bdev_examine(bdev);
+
+		spdk_bdev_wait_for_examine(bdev_register_finished, bdev);
 	}
 
 	return rc;
@@ -5599,6 +5741,7 @@ bdev_unregister_unsafe(struct spdk_bdev *bdev)
 	if (rc == 0) {
 		TAILQ_REMOVE(&g_bdev_mgr.bdevs, bdev, internal.link);
 		SPDK_DEBUGLOG(bdev, "Removing bdev %s from list done\n", bdev->name);
+		bdev_name_del(&bdev->internal.bdev_name);
 		spdk_notify_send("bdev_unregister", spdk_bdev_get_name(bdev));
 	}
 
@@ -6637,7 +6780,7 @@ bdev_lock_lba_range_get_channel(struct spdk_io_channel_iter *i)
 static void
 bdev_lock_lba_range_ctx(struct spdk_bdev *bdev, struct locked_lba_range_ctx *ctx)
 {
-	assert(spdk_get_thread() == ctx->range.owner_ch->channel->thread);
+	assert(spdk_get_thread() == spdk_io_channel_get_thread(ctx->range.owner_ch->channel));
 
 	/* We will add a copy of this range to each channel now. */
 	spdk_for_each_channel(__bdev_to_io_dev(bdev), bdev_lock_lba_range_get_channel, ctx,
@@ -6728,7 +6871,7 @@ bdev_unlock_lba_range_cb(struct spdk_io_channel_iter *i, int status)
 			TAILQ_REMOVE(&bdev->internal.pending_locked_ranges, range, tailq);
 			pending_ctx = SPDK_CONTAINEROF(range, struct locked_lba_range_ctx, range);
 			TAILQ_INSERT_TAIL(&bdev->internal.locked_ranges, range, tailq);
-			spdk_thread_send_msg(pending_ctx->range.owner_ch->channel->thread,
+			spdk_thread_send_msg(spdk_io_channel_get_thread(pending_ctx->range.owner_ch->channel),
 					     bdev_lock_lba_range_ctx_msg, pending_ctx);
 		}
 	}
@@ -6846,7 +6989,9 @@ SPDK_TRACE_REGISTER_FN(bdev_trace, "bdev", TRACE_GROUP_BDEV)
 	spdk_trace_register_owner(OWNER_BDEV, 'b');
 	spdk_trace_register_object(OBJECT_BDEV_IO, 'i');
 	spdk_trace_register_description("BDEV_IO_START", TRACE_BDEV_IO_START, OWNER_BDEV,
-					OBJECT_BDEV_IO, 1, 0, "type:   ");
+					OBJECT_BDEV_IO, 1,
+					SPDK_TRACE_ARG_TYPE_INT, "type");
 	spdk_trace_register_description("BDEV_IO_DONE", TRACE_BDEV_IO_DONE, OWNER_BDEV,
-					OBJECT_BDEV_IO, 0, 0, "");
+					OBJECT_BDEV_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 }
diff --git a/lib/bdev/bdev_rpc.c b/lib/bdev/bdev_rpc.c
index 9055c77dd..40985b2a8 100644
--- a/lib/bdev/bdev_rpc.c
+++ b/lib/bdev/bdev_rpc.c
@@ -363,7 +363,7 @@ rpc_dump_bdev_info(struct spdk_json_write_ctx *w,
 	spdk_json_write_named_array_begin(w, "aliases");
 
 	TAILQ_FOREACH(tmp, spdk_bdev_get_aliases(bdev), tailq) {
-		spdk_json_write_string(w, tmp->alias);
+		spdk_json_write_string(w, tmp->alias.name);
 	}
 
 	spdk_json_write_array_end(w);
diff --git a/lib/bdev/part.c b/lib/bdev/part.c
index 917874e12..1922e89e0 100644
--- a/lib/bdev/part.c
+++ b/lib/bdev/part.c
@@ -374,7 +374,7 @@ spdk_bdev_part_submit_request(struct spdk_bdev_part_channel *ch, struct spdk_bde
 				     bdev_part_complete_io, bdev_io);
 		break;
 	case SPDK_BDEV_IO_TYPE_ZCOPY:
-		rc = spdk_bdev_zcopy_start(base_desc, base_ch, remapped_offset,
+		rc = spdk_bdev_zcopy_start(base_desc, base_ch, NULL, 0, remapped_offset,
 					   bdev_io->u.bdev.num_blocks, bdev_io->u.bdev.zcopy.populate,
 					   bdev_part_complete_zcopy_io, bdev_io);
 		break;
diff --git a/lib/bdev/spdk_bdev.map b/lib/bdev/spdk_bdev.map
index aec215d58..2e3df5fd2 100644
--- a/lib/bdev/spdk_bdev.map
+++ b/lib/bdev/spdk_bdev.map
@@ -15,7 +15,6 @@
 	spdk_bdev_next;
 	spdk_bdev_first_leaf;
 	spdk_bdev_next_leaf;
-	spdk_bdev_open;
 	spdk_bdev_open_ext;
 	spdk_bdev_close;
 	spdk_bdev_desc_get_bdev;
@@ -40,6 +39,7 @@
 	spdk_bdev_is_md_separate;
 	spdk_bdev_is_zoned;
 	spdk_bdev_get_data_block_size;
+	spdk_bdev_get_physical_block_size;
 	spdk_bdev_get_dif_type;
 	spdk_bdev_is_dif_head_of_md;
 	spdk_bdev_is_dif_check_enabled;
diff --git a/lib/blob/blobstore.c b/lib/blob/blobstore.c
index 0278eab80..57a336acc 100644
--- a/lib/blob/blobstore.c
+++ b/lib/blob/blobstore.c
@@ -300,6 +300,7 @@ blob_alloc(struct spdk_blob_store *bs, spdk_blob_id id)
 	TAILQ_INIT(&blob->xattrs);
 	TAILQ_INIT(&blob->xattrs_internal);
 	TAILQ_INIT(&blob->pending_persists);
+	TAILQ_INIT(&blob->persists_to_complete);
 
 	return blob;
 }
@@ -322,6 +323,7 @@ blob_free(struct spdk_blob *blob)
 {
 	assert(blob != NULL);
 	assert(TAILQ_EMPTY(&blob->pending_persists));
+	assert(TAILQ_EMPTY(&blob->persists_to_complete));
 
 	free(blob->active.extent_pages);
 	free(blob->clean.extent_pages);
@@ -874,26 +876,28 @@ blob_serialize_add_page(const struct spdk_blob *blob,
 			uint32_t *page_count,
 			struct spdk_blob_md_page **last_page)
 {
-	struct spdk_blob_md_page *page;
+	struct spdk_blob_md_page *page, *tmp_pages;
 
 	assert(pages != NULL);
 	assert(page_count != NULL);
 
+	*last_page = NULL;
 	if (*page_count == 0) {
 		assert(*pages == NULL);
-		*page_count = 1;
 		*pages = spdk_malloc(SPDK_BS_PAGE_SIZE, 0,
 				     NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_DMA);
+		if (*pages == NULL) {
+			return -ENOMEM;
+		}
+		*page_count = 1;
 	} else {
 		assert(*pages != NULL);
+		tmp_pages = spdk_realloc(*pages, SPDK_BS_PAGE_SIZE * (*page_count + 1), 0);
+		if (tmp_pages == NULL) {
+			return -ENOMEM;
+		}
 		(*page_count)++;
-		*pages = spdk_realloc(*pages, SPDK_BS_PAGE_SIZE * (*page_count), 0);
-	}
-
-	if (*pages == NULL) {
-		*page_count = 0;
-		*last_page = NULL;
-		return -ENOMEM;
+		*pages = tmp_pages;
 	}
 
 	page = &(*pages)[*page_count - 1];
@@ -1490,16 +1494,18 @@ blob_load_cpl(spdk_bs_sequence_t *seq, void *cb_arg, int bserrno)
 	}
 
 	if (page->next != SPDK_INVALID_MD_PAGE) {
+		struct spdk_blob_md_page *tmp_pages;
 		uint32_t next_page = page->next;
 		uint64_t next_lba = bs_md_page_to_lba(blob->bs, next_page);
 
 		/* Read the next page */
-		ctx->num_pages++;
-		ctx->pages = spdk_realloc(ctx->pages, (sizeof(*page) * ctx->num_pages), 0);
-		if (ctx->pages == NULL) {
+		tmp_pages = spdk_realloc(ctx->pages, (sizeof(*page) * (ctx->num_pages + 1)), 0);
+		if (tmp_pages == NULL) {
 			blob_load_final(ctx, -ENOMEM);
 			return;
 		}
+		ctx->num_pages++;
+		ctx->pages = tmp_pages;
 
 		bs_sequence_read_dev(seq, &ctx->pages[ctx->num_pages - 1],
 				     next_lba,
@@ -1619,32 +1625,47 @@ bs_batch_clear_dev(struct spdk_blob_persist_ctx *ctx, spdk_bs_batch_t *batch, ui
 
 static void blob_persist_check_dirty(struct spdk_blob_persist_ctx *ctx);
 
+static void
+blob_persist_complete_cb(void *arg)
+{
+	struct spdk_blob_persist_ctx *ctx = arg;
+
+	/* Call user callback */
+	ctx->cb_fn(ctx->seq, ctx->cb_arg, 0);
+
+	/* Free the memory */
+	spdk_free(ctx->pages);
+	free(ctx);
+}
+
 static void
 blob_persist_complete(spdk_bs_sequence_t *seq, struct spdk_blob_persist_ctx *ctx, int bserrno)
 {
-	struct spdk_blob_persist_ctx	*next_persist;
+	struct spdk_blob_persist_ctx	*next_persist, *tmp;
 	struct spdk_blob		*blob = ctx->blob;
 
 	if (bserrno == 0) {
 		blob_mark_clean(blob);
 	}
 
-	assert(ctx == TAILQ_FIRST(&blob->pending_persists));
-	TAILQ_REMOVE(&blob->pending_persists, ctx, link);
+	assert(ctx == TAILQ_FIRST(&blob->persists_to_complete));
 
-	next_persist = TAILQ_FIRST(&blob->pending_persists);
+	/* Complete all persists that were pending when the current persist started */
+	TAILQ_FOREACH_SAFE(next_persist, &blob->persists_to_complete, link, tmp) {
+		TAILQ_REMOVE(&blob->persists_to_complete, next_persist, link);
+		spdk_thread_send_msg(spdk_get_thread(), blob_persist_complete_cb, next_persist);
+	}
 
-	/* Call user callback */
-	ctx->cb_fn(seq, ctx->cb_arg, bserrno);
+	if (TAILQ_EMPTY(&blob->pending_persists)) {
+		return;
+	}
 
-	/* Free the memory */
-	spdk_free(ctx->pages);
-	free(ctx);
+	/* Queue up all pending persists for completion and start blob persist with first one */
+	TAILQ_SWAP(&blob->persists_to_complete, &blob->pending_persists, spdk_blob_persist_ctx, link);
+	next_persist = TAILQ_FIRST(&blob->persists_to_complete);
 
-	if (next_persist != NULL) {
-		blob->state = SPDK_BLOB_STATE_DIRTY;
-		blob_persist_check_dirty(next_persist);
-	}
+	blob->state = SPDK_BLOB_STATE_DIRTY;
+	blob_persist_check_dirty(next_persist);
 }
 
 static void
@@ -2264,7 +2285,7 @@ blob_persist(spdk_bs_sequence_t *seq, struct spdk_blob *blob,
 
 	blob_verify_md_op(blob);
 
-	if (blob->state == SPDK_BLOB_STATE_CLEAN && TAILQ_EMPTY(&blob->pending_persists)) {
+	if (blob->state == SPDK_BLOB_STATE_CLEAN && TAILQ_EMPTY(&blob->persists_to_complete)) {
 		cb_fn(seq, cb_arg, 0);
 		return;
 	}
@@ -2281,11 +2302,11 @@ blob_persist(spdk_bs_sequence_t *seq, struct spdk_blob *blob,
 
 	/* Multiple blob persists can affect one another, via blob->state or
 	 * blob mutable data changes. To prevent it, queue up the persists. */
-	if (!TAILQ_EMPTY(&blob->pending_persists)) {
+	if (!TAILQ_EMPTY(&blob->persists_to_complete)) {
 		TAILQ_INSERT_TAIL(&blob->pending_persists, ctx, link);
 		return;
 	}
-	TAILQ_INSERT_HEAD(&blob->pending_persists, ctx, link);
+	TAILQ_INSERT_HEAD(&blob->persists_to_complete, ctx, link);
 
 	blob_persist_check_dirty(ctx);
 }
@@ -2612,8 +2633,6 @@ blob_request_submit_op_single(struct spdk_io_channel *_ch, struct spdk_blob *blo
 	cpl.u.blob_basic.cb_fn = cb_fn;
 	cpl.u.blob_basic.cb_arg = cb_arg;
 
-	is_allocated = blob_calculate_lba_and_lba_count(blob, offset, length, &lba, &lba_count);
-
 	if (blob->frozen_refcnt) {
 		/* This blob I/O is frozen */
 		spdk_bs_user_op_t *op;
@@ -2630,6 +2649,8 @@ blob_request_submit_op_single(struct spdk_io_channel *_ch, struct spdk_blob *blo
 		return;
 	}
 
+	is_allocated = blob_calculate_lba_and_lba_count(blob, offset, length, &lba, &lba_count);
+
 	switch (op_type) {
 	case SPDK_BLOB_READ: {
 		spdk_bs_batch_t *batch;
@@ -4349,6 +4370,12 @@ bs_load_super_cpl(spdk_bs_sequence_t *seq, void *cb_arg, int bserrno)
 	}
 	ctx->bs->md_start = ctx->super->md_start;
 	ctx->bs->md_len = ctx->super->md_len;
+	rc = spdk_bit_array_resize(&ctx->bs->open_blobids, ctx->bs->md_len);
+	if (rc < 0) {
+		bs_load_ctx_fail(ctx, -ENOMEM);
+		return;
+	}
+
 	ctx->bs->total_data_clusters = ctx->bs->total_clusters - spdk_divide_round_up(
 					       ctx->bs->md_start + ctx->bs->md_len, ctx->bs->pages_per_cluster);
 	ctx->bs->super_blob = ctx->super->super_blob;
diff --git a/lib/blob/blobstore.h b/lib/blob/blobstore.h
index 0c308ebed..950536c2e 100644
--- a/lib/blob/blobstore.h
+++ b/lib/blob/blobstore.h
@@ -168,6 +168,7 @@ struct spdk_blob {
 
 	/* A list of pending metadata pending_persists */
 	TAILQ_HEAD(, spdk_blob_persist_ctx) pending_persists;
+	TAILQ_HEAD(, spdk_blob_persist_ctx) persists_to_complete;
 
 	/* Number of data clusters retrived from extent table,
 	 * that many have to be read from extent pages. */
diff --git a/lib/blobfs/blobfs.c b/lib/blobfs/blobfs.c
index c9bcde899..6c58c8ae0 100644
--- a/lib/blobfs/blobfs.c
+++ b/lib/blobfs/blobfs.c
@@ -74,36 +74,40 @@ static pthread_mutex_t g_cache_init_lock = PTHREAD_MUTEX_INITIALIZER;
 
 SPDK_TRACE_REGISTER_FN(blobfs_trace, "blobfs", TRACE_GROUP_BLOBFS)
 {
-	spdk_trace_register_description("BLOBFS_XATTR_START",
-					TRACE_BLOBFS_XATTR_START,
-					OWNER_NONE, OBJECT_NONE, 0,
-					SPDK_TRACE_ARG_TYPE_STR,
-					"file:    ");
-	spdk_trace_register_description("BLOBFS_XATTR_END",
-					TRACE_BLOBFS_XATTR_END,
-					OWNER_NONE, OBJECT_NONE, 0,
-					SPDK_TRACE_ARG_TYPE_STR,
-					"file:    ");
-	spdk_trace_register_description("BLOBFS_OPEN",
-					TRACE_BLOBFS_OPEN,
-					OWNER_NONE, OBJECT_NONE, 0,
-					SPDK_TRACE_ARG_TYPE_STR,
-					"file:    ");
-	spdk_trace_register_description("BLOBFS_CLOSE",
-					TRACE_BLOBFS_CLOSE,
-					OWNER_NONE, OBJECT_NONE, 0,
-					SPDK_TRACE_ARG_TYPE_STR,
-					"file:    ");
-	spdk_trace_register_description("BLOBFS_DELETE_START",
-					TRACE_BLOBFS_DELETE_START,
-					OWNER_NONE, OBJECT_NONE, 0,
-					SPDK_TRACE_ARG_TYPE_STR,
-					"file:    ");
-	spdk_trace_register_description("BLOBFS_DELETE_DONE",
-					TRACE_BLOBFS_DELETE_DONE,
-					OWNER_NONE, OBJECT_NONE, 0,
-					SPDK_TRACE_ARG_TYPE_STR,
-					"file:    ");
+	struct spdk_trace_tpoint_opts opts[] = {
+		{
+			"BLOBFS_XATTR_START", TRACE_BLOBFS_XATTR_START,
+			OWNER_NONE, OBJECT_NONE, 0,
+			{{ "file", SPDK_TRACE_ARG_TYPE_STR, 40 }},
+		},
+		{
+			"BLOBFS_XATTR_END", TRACE_BLOBFS_XATTR_END,
+			OWNER_NONE, OBJECT_NONE, 0,
+			{{ "file", SPDK_TRACE_ARG_TYPE_STR, 40 }},
+		},
+		{
+			"BLOBFS_OPEN", TRACE_BLOBFS_OPEN,
+			OWNER_NONE, OBJECT_NONE, 0,
+			{{ "file", SPDK_TRACE_ARG_TYPE_STR, 40 }},
+		},
+		{
+			"BLOBFS_CLOSE", TRACE_BLOBFS_CLOSE,
+			OWNER_NONE, OBJECT_NONE, 0,
+			{{ "file", SPDK_TRACE_ARG_TYPE_STR, 40 }},
+		},
+		{
+			"BLOBFS_DELETE_START", TRACE_BLOBFS_DELETE_START,
+			OWNER_NONE, OBJECT_NONE, 0,
+			{{ "file", SPDK_TRACE_ARG_TYPE_STR, 40 }},
+		},
+		{
+			"BLOBFS_DELETE_DONE", TRACE_BLOBFS_DELETE_DONE,
+			OWNER_NONE, OBJECT_NONE, 0,
+			{{ "file", SPDK_TRACE_ARG_TYPE_STR, 40 }},
+		}
+	};
+
+	spdk_trace_register_description_ext(opts, SPDK_COUNTOF(opts));
 }
 
 void
@@ -119,7 +123,6 @@ struct spdk_file {
 	struct spdk_filesystem	*fs;
 	struct spdk_blob	*blob;
 	char			*name;
-	uint64_t		trace_arg_name;
 	uint64_t		length;
 	bool                    is_deleted;
 	bool			open_for_writing;
@@ -706,14 +709,6 @@ fs_load_done(void *ctx, int bserrno)
 
 }
 
-static void
-_file_build_trace_arg_name(struct spdk_file *f)
-{
-	f->trace_arg_name = 0;
-	memcpy(&f->trace_arg_name, f->name,
-	       spdk_min(sizeof(f->trace_arg_name), strlen(f->name)));
-}
-
 static void
 iter_cb(void *ctx, struct spdk_blob *blob, int rc)
 {
@@ -761,7 +756,6 @@ iter_cb(void *ctx, struct spdk_blob *blob, int rc)
 		}
 
 		f->name = strdup(name);
-		_file_build_trace_arg_name(f);
 		f->blobid = spdk_blob_get_id(blob);
 		f->length = *length;
 		f->length_flushed = *length;
@@ -1100,6 +1094,8 @@ spdk_fs_create_file_async(struct spdk_filesystem *fs, const char *name,
 	req = alloc_fs_request(fs->md_target.md_fs_channel);
 	if (req == NULL) {
 		SPDK_ERRLOG("Cannot allocate create async req for file=%s\n", name);
+		TAILQ_REMOVE(&fs->files, file, tailq);
+		file_free(file);
 		cb_fn(cb_arg, -ENOMEM);
 		return;
 	}
@@ -1110,7 +1106,14 @@ spdk_fs_create_file_async(struct spdk_filesystem *fs, const char *name,
 	args->arg = cb_arg;
 
 	file->name = strdup(name);
-	_file_build_trace_arg_name(file);
+	if (!file->name) {
+		SPDK_ERRLOG("Cannot allocate file->name for file=%s\n", name);
+		free_fs_request(req);
+		TAILQ_REMOVE(&fs->files, file, tailq);
+		file_free(file);
+		cb_fn(cb_arg, -ENOMEM);
+		return;
+	}
 	spdk_bs_create_blob(fs->bs, fs_create_blob_create_cb, args);
 }
 
@@ -1174,7 +1177,7 @@ fs_open_blob_done(void *ctx, struct spdk_blob *blob, int bserrno)
 		req = TAILQ_FIRST(&f->open_requests);
 		args = &req->args;
 		TAILQ_REMOVE(&f->open_requests, req, args.op.open.tailq);
-		spdk_trace_record(TRACE_BLOBFS_OPEN, 0, 0, 0, f->trace_arg_name);
+		spdk_trace_record(TRACE_BLOBFS_OPEN, 0, 0, 0, f->name);
 		args->fn.file_op_with_handle(args->arg, f, bserrno);
 		free_fs_request(req);
 	}
@@ -1353,7 +1356,6 @@ _fs_md_rename_file(struct spdk_fs_request *req)
 
 	free(f->name);
 	f->name = strdup(args->op.rename.new_name);
-	_file_build_trace_arg_name(f);
 	args->file = f;
 	spdk_bs_open_blob(args->fs->bs, f->blobid, fs_rename_blob_open_cb, req);
 }
@@ -1514,21 +1516,13 @@ spdk_fs_delete_file_async(struct spdk_filesystem *fs, const char *name,
 	spdk_bs_delete_blob(fs->bs, blobid, blob_delete_cb, req);
 }
 
-static uint64_t
-fs_name_to_uint64(const char *name)
-{
-	uint64_t result = 0;
-	memcpy(&result, name, spdk_min(sizeof(result), strlen(name)));
-	return result;
-}
-
 static void
 __fs_delete_file_done(void *arg, int fserrno)
 {
 	struct spdk_fs_request *req = arg;
 	struct spdk_fs_cb_args *args = &req->args;
 
-	spdk_trace_record(TRACE_BLOBFS_DELETE_DONE, 0, 0, 0, fs_name_to_uint64(args->op.delete.name));
+	spdk_trace_record(TRACE_BLOBFS_DELETE_DONE, 0, 0, 0, args->op.delete.name);
 	__wake_caller(args, fserrno);
 }
 
@@ -1538,7 +1532,7 @@ __fs_delete_file(void *arg)
 	struct spdk_fs_request *req = arg;
 	struct spdk_fs_cb_args *args = &req->args;
 
-	spdk_trace_record(TRACE_BLOBFS_DELETE_START, 0, 0, 0, fs_name_to_uint64(args->op.delete.name));
+	spdk_trace_record(TRACE_BLOBFS_DELETE_START, 0, 0, 0, args->op.delete.name);
 	spdk_fs_delete_file_async(args->fs, args->op.delete.name, __fs_delete_file_done, req);
 }
 
@@ -2211,7 +2205,7 @@ __file_cache_finish_sync(void *ctx, int bserrno)
 	file->length_xattr = sync_args->op.sync.length;
 	assert(sync_args->op.sync.offset <= file->length_flushed);
 	spdk_trace_record(TRACE_BLOBFS_XATTR_END, 0, sync_args->op.sync.offset,
-			  0, file->trace_arg_name);
+			  0, file->name);
 	BLOBFS_TRACE(file, "sync done offset=%jx\n", sync_args->op.sync.offset);
 	TAILQ_REMOVE(&file->sync_requests, sync_req, args.op.sync.tailq);
 	pthread_spin_unlock(&file->lock);
@@ -2244,7 +2238,7 @@ __check_sync_reqs(struct spdk_file *file)
 
 		pthread_spin_unlock(&file->lock);
 		spdk_trace_record(TRACE_BLOBFS_XATTR_START, 0, file->length_flushed,
-				  0, file->trace_arg_name);
+				  0, file->name);
 		spdk_blob_sync_md(file->blob, __file_cache_finish_sync, sync_req);
 	} else {
 		pthread_spin_unlock(&file->lock);
@@ -2822,7 +2816,7 @@ __file_close_async_done(void *ctx, int bserrno)
 	struct spdk_fs_cb_args *args = &req->args;
 	struct spdk_file *file = args->file;
 
-	spdk_trace_record(TRACE_BLOBFS_CLOSE, 0, 0, 0, file->trace_arg_name);
+	spdk_trace_record(TRACE_BLOBFS_CLOSE, 0, 0, 0, file->name);
 
 	if (file->is_deleted) {
 		spdk_fs_delete_file_async(file->fs, file->name, blob_delete_cb, ctx);
diff --git a/lib/env_dpdk/Makefile b/lib/env_dpdk/Makefile
index f869a0b48..19b0e7be6 100644
--- a/lib/env_dpdk/Makefile
+++ b/lib/env_dpdk/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 6
+SO_VER := 7
 SO_MINOR := 0
 
 CFLAGS += $(ENV_CFLAGS)
@@ -51,7 +51,7 @@ LIBDPDK_PKGCONFIG = $(call pkgconfig_filename,spdk_dpdklibs)
 
 $(LIBDPDK_PKGCONFIG): $(PKGCONFIG) $(PKGCONFIG_INST)
 	$(Q)$(SPDK_ROOT_DIR)/scripts/pc_libs.sh \
-		"-L$(DPDK_ABS_DIR)/lib $(DPDK_LIB_LIST:%=-l%)" "" DPDK spdk_dpdklibs > $@
+		"-L$(DPDK_LIB_DIR) $(DPDK_LIB_LIST:%=-l%)" "" DPDK spdk_dpdklibs > $@
 	$(Q)sed -i.bak '5s,.*,Requires: $(DEPDIRS-$(LIBNAME):%=spdk_%) spdk_dpdklibs,' $(PKGCONFIG) ; rm $(PKGCONFIG).bak
 	$(Q)sed -i.bak '5s,.*,Requires: $(DEPDIRS-$(LIBNAME):%=spdk_%) spdk_dpdklibs,' $(PKGCONFIG_INST) ; rm $(PKGCONFIG_INST).bak
 
diff --git a/lib/env_dpdk/env.c b/lib/env_dpdk/env.c
index c2801da2e..b51020c7e 100644
--- a/lib/env_dpdk/env.c
+++ b/lib/env_dpdk/env.c
@@ -71,7 +71,7 @@ spdk_malloc(size_t size, size_t align, uint64_t *phys_addr, int socket_id, uint3
 	buf = rte_malloc_socket(NULL, size, align, socket_id);
 	if (buf && phys_addr) {
 #ifdef DEBUG
-		SPDK_ERRLOG("phys_addr param in spdk_*malloc() is deprecated\n");
+		SPDK_ERRLOG("phys_addr param in spdk_malloc() is deprecated\n");
 #endif
 		*phys_addr = virt_to_phys(buf);
 	}
@@ -81,9 +81,19 @@ spdk_malloc(size_t size, size_t align, uint64_t *phys_addr, int socket_id, uint3
 void *
 spdk_zmalloc(size_t size, size_t align, uint64_t *phys_addr, int socket_id, uint32_t flags)
 {
-	void *buf = spdk_malloc(size, align, phys_addr, socket_id, flags);
-	if (buf) {
-		memset(buf, 0, size);
+	void *buf;
+
+	if (flags == 0) {
+		return NULL;
+	}
+
+	align = spdk_max(align, RTE_CACHE_LINE_SIZE);
+	buf = rte_zmalloc_socket(NULL, size, align, socket_id);
+	if (buf && phys_addr) {
+#ifdef DEBUG
+		SPDK_ERRLOG("phys_addr param in spdk_zmalloc() is deprecated\n");
+#endif
+		*phys_addr = virt_to_phys(buf);
 	}
 	return buf;
 }
diff --git a/lib/env_dpdk/env.mk b/lib/env_dpdk/env.mk
index 8edaf8558..7d256e06f 100644
--- a/lib/env_dpdk/env.mk
+++ b/lib/env_dpdk/env.mk
@@ -41,11 +41,22 @@ DPDK_DIR = $(CONFIG_DPDK_DIR)
 
 export DPDK_ABS_DIR = $(abspath $(DPDK_DIR))
 
+ifneq ($(CONFIG_DPDK_LIB_DIR),)
+DPDK_LIB_DIR = $(CONFIG_DPDK_LIB_DIR)
+else
+DPDK_LIB_DIR = $(DPDK_ABS_DIR)/lib
+endif
+
+ifneq ($(CONFIG_DPDK_INC_DIR),)
+DPDK_INC_DIR = $(CONFIG_DPDK_INC_DIR)
+else
 ifneq (, $(wildcard $(DPDK_ABS_DIR)/include/rte_config.h))
 DPDK_INC_DIR := $(DPDK_ABS_DIR)/include
 else
 DPDK_INC_DIR := $(DPDK_ABS_DIR)/include/dpdk
 endif
+endif
+
 DPDK_INC := -I$(DPDK_INC_DIR)
 
 DPDK_LIB_LIST = rte_eal rte_mempool rte_ring rte_mbuf rte_bus_pci rte_pci rte_mempool_ring
@@ -55,7 +66,7 @@ DPDK_LIB_LIST += rte_power rte_ethdev rte_net
 endif
 
 # DPDK 20.05 eal dependency
-ifneq (, $(wildcard $(DPDK_ABS_DIR)/lib/librte_telemetry.*))
+ifneq (, $(wildcard $(DPDK_LIB_DIR)/librte_telemetry.*))
 DPDK_LIB_LIST += rte_telemetry
 endif
 
@@ -66,7 +77,7 @@ DPDK_FRAMEWORK=n
 ifeq ($(CONFIG_CRYPTO),y)
 DPDK_FRAMEWORK=y
 DPDK_LIB_LIST += rte_reorder
-ifneq (, $(wildcard $(DPDK_ABS_DIR)/lib/librte_crypto_aesni_mb.*))
+ifneq (, $(wildcard $(DPDK_LIB_DIR)/librte_crypto_aesni_mb.*))
 DPDK_LIB_LIST += rte_crypto_aesni_mb
 else
 # PMD name for DPDK 20.08 and earlier
@@ -76,7 +87,7 @@ endif
 
 ifeq ($(CONFIG_REDUCE),y)
 DPDK_FRAMEWORK=y
-ifneq (, $(wildcard $(DPDK_ABS_DIR)/lib/librte_compress_isal.*))
+ifneq (, $(wildcard $(DPDK_LIB_DIR)/librte_compress_isal.*))
 DPDK_LIB_LIST += rte_compress_isal
 else
 # PMD name for DPDK 20.08 and earlier
@@ -86,7 +97,7 @@ endif
 
 ifeq ($(DPDK_FRAMEWORK),y)
 DPDK_LIB_LIST += rte_cryptodev rte_compressdev rte_bus_vdev
-ifneq (, $(wildcard $(DPDK_ABS_DIR)/lib/librte_common_qat.*))
+ifneq (, $(wildcard $(DPDK_LIB_DIR)/librte_common_qat.*))
 DPDK_LIB_LIST += rte_common_qat
 else
 # PMD name for DPDK 20.08 and earlier
@@ -94,7 +105,7 @@ DPDK_LIB_LIST += rte_pmd_qat
 endif
 endif
 
-ifneq (, $(wildcard $(DPDK_ABS_DIR)/lib/librte_kvargs.*))
+ifneq (, $(wildcard $(DPDK_LIB_DIR)/librte_kvargs.*))
 DPDK_LIB_LIST += rte_kvargs
 endif
 
@@ -114,13 +125,13 @@ endif
 
 ifeq ($(LINK_HASH),y)
 DPDK_LIB_LIST += rte_hash
-ifneq (, $(wildcard $(DPDK_ABS_DIR)/lib/librte_rcu.*))
+ifneq (, $(wildcard $(DPDK_LIB_DIR)/librte_rcu.*))
 DPDK_LIB_LIST += rte_rcu
 endif
 endif
 
-DPDK_SHARED_LIB = $(DPDK_LIB_LIST:%=$(DPDK_ABS_DIR)/lib/lib%.so)
-DPDK_STATIC_LIB = $(DPDK_LIB_LIST:%=$(DPDK_ABS_DIR)/lib/lib%.a)
+DPDK_SHARED_LIB = $(DPDK_LIB_LIST:%=$(DPDK_LIB_DIR)/lib%.so)
+DPDK_STATIC_LIB = $(DPDK_LIB_LIST:%=$(DPDK_LIB_DIR)/lib%.a)
 DPDK_SHARED_LIB_LINKER_ARGS = $(call add_no_as_needed,$(DPDK_SHARED_LIB))
 DPDK_STATIC_LIB_LINKER_ARGS = $(call add_whole_archive,$(DPDK_STATIC_LIB))
 
@@ -160,12 +171,12 @@ endif
 ifeq ($(CONFIG_SHARED),y)
 ENV_DPDK_FILE = $(call spdk_lib_list_to_shared_libs,env_dpdk)
 ENV_LIBS = $(ENV_DPDK_FILE) $(DPDK_SHARED_LIB)
-DPDK_LINKER_ARGS = -Wl,-rpath-link $(DPDK_ABS_DIR)/lib $(DPDK_SHARED_LIB_LINKER_ARGS)
+DPDK_LINKER_ARGS = -Wl,-rpath-link $(DPDK_LIB_DIR) $(DPDK_SHARED_LIB_LINKER_ARGS)
 ENV_LINKER_ARGS = $(ENV_DPDK_FILE) $(DPDK_LINKER_ARGS)
 else
 ENV_DPDK_FILE = $(call spdk_lib_list_to_static_libs,env_dpdk)
 ENV_LIBS = $(ENV_DPDK_FILE) $(DPDK_STATIC_LIB)
-DPDK_LINKER_ARGS = -Wl,-rpath-link $(DPDK_ABS_DIR)/lib $(DPDK_STATIC_LIB_LINKER_ARGS)
+DPDK_LINKER_ARGS = -Wl,-rpath-link $(DPDK_LIB_DIR) $(DPDK_STATIC_LIB_LINKER_ARGS)
 ENV_LINKER_ARGS = $(ENV_DPDK_FILE) $(DPDK_LINKER_ARGS)
 ENV_LINKER_ARGS += $(DPDK_PRIVATE_LINKER_ARGS)
 endif
diff --git a/lib/env_dpdk/init.c b/lib/env_dpdk/init.c
index e6464c93c..9c8962eaa 100644
--- a/lib/env_dpdk/init.c
+++ b/lib/env_dpdk/init.c
@@ -267,10 +267,21 @@ build_eal_cmdline(const struct spdk_env_opts *opts)
 		}
 	}
 
-	/* set the coremask */
-	/* NOTE: If coremask starts with '[' and ends with ']' it is a core list
+	/*
+	 * Set the coremask:
+	 *
+	 * - if it starts with '-', we presume it's literal EAL arguments such
+	 *   as --lcores.
+	 *
+	 * - if it starts with '[', we presume it's a core list to use with the
+	 *   -l option.
+	 *
+	 * - otherwise, it's a CPU mask of the form "0xff.." as expected by the
+	 *   -c option.
 	 */
-	if (opts->core_mask[0] == '[') {
+	if (opts->core_mask[0] == '-') {
+		args = push_arg(args, &argcount, _sprintf_alloc("%s", opts->core_mask));
+	} else if (opts->core_mask[0] == '[') {
 		char *l_arg = _sprintf_alloc("-l %s", opts->core_mask + 1);
 
 		if (l_arg != NULL) {
diff --git a/lib/env_dpdk/sigbus_handler.c b/lib/env_dpdk/sigbus_handler.c
index 098e7647c..0a1e2eeba 100644
--- a/lib/env_dpdk/sigbus_handler.c
+++ b/lib/env_dpdk/sigbus_handler.c
@@ -53,7 +53,7 @@ sigbus_fault_sighandler(int signum, siginfo_t *info, void *ctx)
 
 	pthread_mutex_lock(&g_sighandler_mutex);
 	TAILQ_FOREACH(sigbus_handler, &g_sigbus_handler, tailq) {
-		sigbus_handler->func(info, sigbus_handler->ctx);
+		sigbus_handler->func(info->si_addr, sigbus_handler->ctx);
 	}
 	pthread_mutex_unlock(&g_sighandler_mutex);
 }
diff --git a/lib/event/Makefile b/lib/event/Makefile
index 359bfe81f..84640b789 100644
--- a/lib/event/Makefile
+++ b/lib/event/Makefile
@@ -34,14 +34,14 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 8
+SO_VER := 9
 SO_MINOR := 0
 
 CFLAGS += $(ENV_CFLAGS)
 
 LIBNAME = event
-C_SRCS = app.c reactor.c rpc.c subsystem.c json_config.c log_rpc.c \
-	 app_rpc.c subsystem_rpc.c scheduler_static.c
+C_SRCS = app.c reactor.c log_rpc.c \
+	 app_rpc.c scheduler_static.c
 
 # Do not compile schedulers and governors based on DPDK env
 # if non-DPDK env is used.
diff --git a/lib/event/app.c b/lib/event/app.c
index 3eb31da30..090b1692f 100644
--- a/lib/event/app.c
+++ b/lib/event/app.c
@@ -38,6 +38,7 @@
 
 #include "spdk/assert.h"
 #include "spdk/env.h"
+#include "spdk/init.h"
 #include "spdk/log.h"
 #include "spdk/thread.h"
 #include "spdk/trace.h"
@@ -402,8 +403,9 @@ bootstrap_fn(void *arg1)
 
 	if (g_spdk_app.json_config_file) {
 		g_delay_subsystem_init = false;
-		spdk_app_json_config_load(g_spdk_app.json_config_file, g_spdk_app.rpc_addr, app_start_rpc,
-					  NULL, !g_spdk_app.json_config_ignore_errors);
+		spdk_subsystem_init_from_json_config(g_spdk_app.json_config_file, g_spdk_app.rpc_addr,
+						     app_start_rpc,
+						     NULL, !g_spdk_app.json_config_ignore_errors);
 	} else {
 		if (!g_delay_subsystem_init) {
 			spdk_subsystem_init(app_start_rpc, NULL);
diff --git a/lib/event/app_rpc.c b/lib/event/app_rpc.c
index b07bda12f..380c712a5 100644
--- a/lib/event/app_rpc.c
+++ b/lib/event/app_rpc.c
@@ -202,13 +202,18 @@ _rpc_thread_get_stats(void *arg)
 	uint64_t timed_pollers_count = 0;
 	uint64_t paused_pollers_count = 0;
 
-	TAILQ_FOREACH(poller, &thread->active_pollers, tailq) {
+	for (poller = spdk_thread_get_first_active_poller(thread); poller != NULL;
+	     poller = spdk_thread_get_next_active_poller(poller)) {
 		active_pollers_count++;
 	}
-	TAILQ_FOREACH(poller, &thread->timed_pollers, tailq) {
+
+	for (poller = spdk_thread_get_first_timed_poller(thread); poller != NULL;
+	     poller = spdk_thread_get_next_timed_poller(poller)) {
 		timed_pollers_count++;
 	}
-	TAILQ_FOREACH(poller, &thread->paused_pollers, tailq) {
+
+	for (poller = spdk_thread_get_first_paused_poller(thread); poller != NULL;
+	     poller = spdk_thread_get_next_paused_poller(poller)) {
 		paused_pollers_count++;
 	}
 
@@ -245,13 +250,19 @@ SPDK_RPC_REGISTER("thread_get_stats", rpc_thread_get_stats, SPDK_RPC_RUNTIME)
 static void
 rpc_get_poller(struct spdk_poller *poller, struct spdk_json_write_ctx *w)
 {
+	struct spdk_poller_stats stats;
+	uint64_t period_ticks;
+
+	period_ticks = spdk_poller_get_period_ticks(poller);
+	spdk_poller_get_stats(poller, &stats);
+
 	spdk_json_write_object_begin(w);
-	spdk_json_write_named_string(w, "name", poller->name);
-	spdk_json_write_named_string(w, "state", spdk_poller_state_str(poller->state));
-	spdk_json_write_named_uint64(w, "run_count", poller->run_count);
-	spdk_json_write_named_uint64(w, "busy_count", poller->busy_count);
-	if (poller->period_ticks) {
-		spdk_json_write_named_uint64(w, "period_ticks", poller->period_ticks);
+	spdk_json_write_named_string(w, "name", spdk_poller_get_name(poller));
+	spdk_json_write_named_string(w, "state", spdk_poller_get_state_str(poller));
+	spdk_json_write_named_uint64(w, "run_count", stats.run_count);
+	spdk_json_write_named_uint64(w, "busy_count", stats.busy_count);
+	if (period_ticks) {
+		spdk_json_write_named_uint64(w, "period_ticks", period_ticks);
 	}
 	spdk_json_write_object_end(w);
 }
@@ -268,19 +279,22 @@ _rpc_thread_get_pollers(void *arg)
 	spdk_json_write_named_uint64(ctx->w, "id", spdk_thread_get_id(thread));
 
 	spdk_json_write_named_array_begin(ctx->w, "active_pollers");
-	TAILQ_FOREACH(poller, &thread->active_pollers, tailq) {
+	for (poller = spdk_thread_get_first_active_poller(thread); poller != NULL;
+	     poller = spdk_thread_get_next_active_poller(poller)) {
 		rpc_get_poller(poller, ctx->w);
 	}
 	spdk_json_write_array_end(ctx->w);
 
 	spdk_json_write_named_array_begin(ctx->w, "timed_pollers");
-	TAILQ_FOREACH(poller, &thread->timed_pollers, tailq) {
+	for (poller = spdk_thread_get_first_timed_poller(thread); poller != NULL;
+	     poller = spdk_thread_get_next_timed_poller(poller)) {
 		rpc_get_poller(poller, ctx->w);
 	}
 	spdk_json_write_array_end(ctx->w);
 
 	spdk_json_write_named_array_begin(ctx->w, "paused_pollers");
-	TAILQ_FOREACH(poller, &thread->paused_pollers, tailq) {
+	for (poller = spdk_thread_get_first_paused_poller(thread); poller != NULL;
+	     poller = spdk_thread_get_next_paused_poller(poller)) {
 		rpc_get_poller(poller, ctx->w);
 	}
 	spdk_json_write_array_end(ctx->w);
@@ -307,8 +321,8 @@ static void
 rpc_get_io_channel(struct spdk_io_channel *ch, struct spdk_json_write_ctx *w)
 {
 	spdk_json_write_object_begin(w);
-	spdk_json_write_named_string(w, "name", spdk_io_device_get_name(ch->dev));
-	spdk_json_write_named_uint32(w, "ref", ch->ref);
+	spdk_json_write_named_string(w, "name", spdk_io_channel_get_io_device_name(ch));
+	spdk_json_write_named_uint32(w, "ref", spdk_io_channel_get_ref_count(ch));
 	spdk_json_write_object_end(w);
 }
 
@@ -323,7 +337,8 @@ _rpc_thread_get_io_channels(void *arg)
 	spdk_json_write_named_string(ctx->w, "name", spdk_thread_get_name(thread));
 
 	spdk_json_write_named_array_begin(ctx->w, "io_channels");
-	TAILQ_FOREACH(ch, &thread->io_channels, tailq) {
+	for (ch = spdk_thread_get_first_io_channel(thread); ch != NULL;
+	     ch = spdk_thread_get_next_io_channel(ch)) {
 		rpc_get_io_channel(ch, ctx->w);
 	}
 	spdk_json_write_array_end(ctx->w);
diff --git a/lib/event/gscheduler.c b/lib/event/gscheduler.c
index 4f46d304c..251eaa7d4 100644
--- a/lib/event/gscheduler.c
+++ b/lib/event/gscheduler.c
@@ -35,7 +35,7 @@
 #include "spdk/likely.h"
 
 #include "spdk_internal/event.h"
-#include "spdk_internal/thread.h"
+#include "spdk/thread.h"
 
 #include "spdk/log.h"
 #include "spdk/env.h"
@@ -72,9 +72,8 @@ static void
 balance(struct spdk_scheduler_core_info *cores, int core_count, struct spdk_governor *governor)
 {
 	struct spdk_scheduler_core_info *core;
-	struct spdk_lw_thread *thread;
 	struct spdk_governor_capabilities capabilities;
-	uint32_t i, j;
+	uint32_t i;
 	int rc;
 	bool turbo_available = false;
 
@@ -82,13 +81,6 @@ balance(struct spdk_scheduler_core_info *cores, int core_count, struct spdk_gove
 	SPDK_ENV_FOREACH_CORE(i) {
 		core = &cores[i];
 
-		for (j = 0; j < core->threads_count; j++) {
-			thread = core->threads[j];
-
-			/* do not change thread lcore */
-			thread->new_lcore = thread->lcore;
-		}
-
 		rc = governor->get_core_capabilities(core->lcore, &capabilities);
 		if (rc < 0) {
 			SPDK_ERRLOG("failed to get capabilities for core: %u\n", core->lcore);
@@ -97,7 +89,7 @@ balance(struct spdk_scheduler_core_info *cores, int core_count, struct spdk_gove
 
 		turbo_available = (capabilities.turbo_available && capabilities.turbo_set) ? true : false;
 
-		if (core->core_busy_tsc < (core->core_idle_tsc / 1000)) {
+		if (core->total_busy_tsc < (core->total_idle_tsc / 1000)) {
 			rc = governor->set_core_freq_min(core->lcore);
 			if (rc < 0) {
 				SPDK_ERRLOG("setting to minimal frequency for core %u failed\n", core->lcore);
@@ -111,7 +103,7 @@ balance(struct spdk_scheduler_core_info *cores, int core_count, struct spdk_gove
 			}
 
 			SPDK_DEBUGLOG(reactor, "setting to minimal frequency for core: %u\n", core->lcore);
-		} else if (core->core_idle_tsc > core->core_busy_tsc) {
+		} else if (core->total_idle_tsc > core->total_busy_tsc) {
 			rc = governor->core_freq_down(core->lcore);
 			if (rc < 0) {
 				SPDK_ERRLOG("lowering frequency for core %u failed\n", core->lcore);
@@ -125,7 +117,7 @@ balance(struct spdk_scheduler_core_info *cores, int core_count, struct spdk_gove
 			}
 
 			SPDK_DEBUGLOG(reactor, "lowering frequency for core: %u\n", core->lcore);
-		} else if (core->core_idle_tsc < (core->core_busy_tsc / 1000)) {
+		} else if (core->total_idle_tsc < (core->total_busy_tsc / 1000)) {
 			rc = governor->set_core_freq_max(core->lcore);
 			if (rc < 0) {
 				SPDK_ERRLOG("setting to maximal frequency for core %u failed\n", core->lcore);
diff --git a/lib/event/log_rpc.c b/lib/event/log_rpc.c
index 9666dbd56..a56952e99 100644
--- a/lib/event/log_rpc.c
+++ b/lib/event/log_rpc.c
@@ -110,7 +110,7 @@ rpc_log_set_print_level(struct spdk_jsonrpc_request *request,
 		SPDK_DEBUGLOG(log_rpc, "spdk_json_decode_object failed\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
 						 "spdk_json_decode_object failed");
-		goto invalid;
+		goto end;
 	}
 
 	level = _parse_log_level(req.level);
@@ -118,16 +118,12 @@ rpc_log_set_print_level(struct spdk_jsonrpc_request *request,
 		SPDK_DEBUGLOG(log_rpc, "tried to set invalid log level\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS,
 						 "invalid log level");
-		goto invalid;
+		goto end;
 	}
 
 	spdk_log_set_print_level(level);
-	free_rpc_log_level(&req);
-
 	spdk_jsonrpc_send_bool_response(request, true);
-	return;
-
-invalid:
+end:
 	free_rpc_log_level(&req);
 }
 SPDK_RPC_REGISTER("log_set_print_level", rpc_log_set_print_level,
@@ -177,7 +173,7 @@ rpc_log_set_level(struct spdk_jsonrpc_request *request,
 		SPDK_DEBUGLOG(log_rpc, "spdk_json_decode_object failed\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
 						 "spdk_json_decode_object failed");
-		goto invalid;
+		goto end;
 	}
 
 	level = _parse_log_level(req.level);
@@ -185,17 +181,13 @@ rpc_log_set_level(struct spdk_jsonrpc_request *request,
 		SPDK_DEBUGLOG(log_rpc, "tried to set invalid log level\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS,
 						 "invalid log level");
-		goto invalid;
+		goto end;
 	}
 
 
 	spdk_log_set_level(level);
-	free_rpc_log_level(&req);
-
 	spdk_jsonrpc_send_bool_response(request, true);
-	return;
-
-invalid:
+end:
 	free_rpc_log_level(&req);
 }
 SPDK_RPC_REGISTER("log_set_level", rpc_log_set_level, SPDK_RPC_STARTUP | SPDK_RPC_RUNTIME)
@@ -242,23 +234,19 @@ rpc_log_set_flag(struct spdk_jsonrpc_request *request,
 		SPDK_DEBUGLOG(log_rpc, "spdk_json_decode_object failed\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
 						 "spdk_json_decode_object failed");
-		goto invalid;
+		goto end;
 	}
 
 	if (req.flag == 0) {
 		SPDK_DEBUGLOG(log_rpc, "invalid flag 0\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS,
 						 "invalid flag 0");
-		goto invalid;
+		goto end;
 	}
 
 	spdk_log_set_flag(req.flag);
-	free_rpc_log_flag(&req);
-
 	spdk_jsonrpc_send_bool_response(request, true);
-	return;
-
-invalid:
+end:
 	free_rpc_log_flag(&req);
 }
 SPDK_RPC_REGISTER("log_set_flag", rpc_log_set_flag, SPDK_RPC_STARTUP | SPDK_RPC_RUNTIME)
@@ -275,23 +263,19 @@ rpc_log_clear_flag(struct spdk_jsonrpc_request *request,
 		SPDK_DEBUGLOG(log_rpc, "spdk_json_decode_object failed\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
 						 "spdk_json_decode_object failed");
-		goto invalid;
+		goto end;
 	}
 
 	if (req.flag == 0) {
 		SPDK_DEBUGLOG(log_rpc, "Invalid flag 0\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS,
 						 "invalid flag 0");
-		goto invalid;
+		goto end;
 	}
 
 	spdk_log_clear_flag(req.flag);
-	free_rpc_log_flag(&req);
-
 	spdk_jsonrpc_send_bool_response(request, true);
-	return;
-
-invalid:
+end:
 	free_rpc_log_flag(&req);
 }
 SPDK_RPC_REGISTER("log_clear_flag", rpc_log_clear_flag,
diff --git a/lib/event/reactor.c b/lib/event/reactor.c
index 4b058ab32..9f2606a8b 100644
--- a/lib/event/reactor.c
+++ b/lib/event/reactor.c
@@ -394,7 +394,7 @@ _reactor_set_interrupt_mode(void *arg1, void *arg2)
 	assert(target != NULL);
 	assert(target->in_interrupt != target->new_in_interrupt);
 	SPDK_DEBUGLOG(reactor, "Do reactor set on core %u from %s to state %s\n",
-		      target->lcore, !target->in_interrupt ? "intr" : "poll", target->new_in_interrupt ? "intr" : "poll");
+		      target->lcore, target->in_interrupt ? "intr" : "poll", target->new_in_interrupt ? "intr" : "poll");
 
 	target->in_interrupt = target->new_in_interrupt;
 
@@ -405,6 +405,9 @@ _reactor_set_interrupt_mode(void *arg1, void *arg2)
 	}
 
 	if (target->new_in_interrupt == false) {
+		/* Reactor is no longer in interrupt mode. Refresh the tsc_last to accurately
+		 * track reactor stats. */
+		target->tsc_last = spdk_get_ticks();
 		spdk_for_each_reactor(_reactor_set_notify_cpuset, target, NULL, _reactor_set_notify_cpuset_cpl);
 	} else {
 		uint64_t notify = 1;
@@ -602,17 +605,15 @@ event_queue_run_batch(struct spdk_reactor *reactor)
 		thread = NULL;
 	}
 
-	spdk_set_thread(thread);
-
 	for (i = 0; i < count; i++) {
 		struct spdk_event *event = events[i];
 
 		assert(event != NULL);
+		spdk_set_thread(thread);
 		event->fn(event->arg1, event->arg2);
+		spdk_set_thread(NULL);
 	}
 
-	spdk_set_thread(NULL);
-
 	spdk_mempool_put_bulk(g_spdk_event_mempool, events, count);
 
 	return count;
@@ -673,11 +674,17 @@ static void
 _init_thread_stats(struct spdk_reactor *reactor, struct spdk_lw_thread *lw_thread)
 {
 	struct spdk_thread *thread = spdk_thread_get_from_ctx(lw_thread);
+	struct spdk_thread_stats prev_total_stats;
 
-	lw_thread->lcore = reactor->lcore;
+	/* Read total_stats before updating it to calculate stats during the last scheduling period. */
+	prev_total_stats = lw_thread->total_stats;
 
 	spdk_set_thread(thread);
-	spdk_thread_get_stats(&lw_thread->current_stats);
+	spdk_thread_get_stats(&lw_thread->total_stats);
+	spdk_set_thread(NULL);
+
+	lw_thread->current_stats.busy_tsc = lw_thread->total_stats.busy_tsc - prev_total_stats.busy_tsc;
+	lw_thread->current_stats.idle_tsc = lw_thread->total_stats.idle_tsc - prev_total_stats.idle_tsc;
 }
 
 static void
@@ -691,8 +698,8 @@ _threads_reschedule(struct spdk_scheduler_core_info *cores_info)
 		core = &cores_info[i];
 		for (j = 0; j < core->threads_count; j++) {
 			lw_thread = core->threads[j];
-			if (lw_thread->lcore != lw_thread->new_lcore) {
-				_spdk_lw_thread_set_core(lw_thread, lw_thread->new_lcore);
+			if (lw_thread->lcore != i) {
+				lw_thread->resched = true;
 			}
 		}
 	}
@@ -785,8 +792,10 @@ _reactors_scheduler_gather_metrics(void *arg1, void *arg2)
 	reactor->flags.is_scheduling = true;
 	core_info = &g_core_infos[reactor->lcore];
 	core_info->lcore = reactor->lcore;
-	core_info->core_idle_tsc = reactor->idle_tsc;
-	core_info->core_busy_tsc = reactor->busy_tsc;
+	core_info->current_idle_tsc = reactor->idle_tsc - core_info->total_idle_tsc;
+	core_info->total_idle_tsc = reactor->idle_tsc;
+	core_info->current_busy_tsc = reactor->busy_tsc - core_info->total_busy_tsc;
+	core_info->total_busy_tsc = reactor->busy_tsc;
 	core_info->interrupt_mode = reactor->in_interrupt;
 
 	SPDK_DEBUGLOG(reactor, "Gathering metrics on %u\n", reactor->lcore);
@@ -817,7 +826,6 @@ _reactors_scheduler_gather_metrics(void *arg1, void *arg2)
 		i = 0;
 		TAILQ_FOREACH(lw_thread, &reactor->threads, link) {
 			core_info->threads[i] = lw_thread;
-			_spdk_lw_thread_get_current_stats(lw_thread, &lw_thread->snapshot_stats);
 			i++;
 		}
 	}
@@ -901,6 +909,16 @@ _reactor_run(struct spdk_reactor *reactor)
 
 	event_queue_run_batch(reactor);
 
+	/* If no threads are present on the reactor,
+	 * tsc_last gets outdated. Update it to track
+	 * thread execution time correctly. */
+	if (spdk_unlikely(TAILQ_EMPTY(&reactor->threads))) {
+		now = spdk_get_ticks();
+		reactor->idle_tsc += now - reactor->tsc_last;
+		reactor->tsc_last = now;
+		return;
+	}
+
 	TAILQ_FOREACH_SAFE(lw_thread, &reactor->threads, link, tmp) {
 		thread = spdk_thread_get_from_ctx(lw_thread);
 		rc = spdk_thread_poll(thread, 0, reactor->tsc_last);
@@ -1121,6 +1139,7 @@ static void
 _schedule_thread(void *arg1, void *arg2)
 {
 	struct spdk_lw_thread *lw_thread = arg1;
+	struct spdk_thread *thread;
 	struct spdk_reactor *reactor;
 	uint32_t current_core;
 	int efd;
@@ -1129,15 +1148,22 @@ _schedule_thread(void *arg1, void *arg2)
 	reactor = spdk_reactor_get(current_core);
 	assert(reactor != NULL);
 
+	/* Update total_stats to reflect state of thread
+	* at the end of the move. */
+	thread = spdk_thread_get_from_ctx(lw_thread);
+	spdk_set_thread(thread);
+	spdk_thread_get_stats(&lw_thread->total_stats);
+	spdk_set_thread(NULL);
+
+	lw_thread->lcore = current_core;
+
 	TAILQ_INSERT_TAIL(&reactor->threads, lw_thread, link);
 	reactor->thread_count++;
 
 	/* Operate thread intr if running with full interrupt ability */
 	if (spdk_interrupt_mode_is_enabled()) {
 		int rc;
-		struct spdk_thread *thread;
 
-		thread = spdk_thread_get_from_ctx(lw_thread);
 		efd = spdk_thread_get_interrupt_fd(thread);
 		rc = spdk_fd_group_add(reactor->fgrp, efd, thread_process_interrupts, thread);
 		if (rc < 0) {
@@ -1154,7 +1180,6 @@ _reactor_schedule_thread(struct spdk_thread *thread)
 {
 	uint32_t core;
 	struct spdk_lw_thread *lw_thread;
-	struct spdk_thread_stats last_stats;
 	struct spdk_event *evt = NULL;
 	struct spdk_cpuset *cpumask;
 	uint32_t i;
@@ -1168,9 +1193,7 @@ _reactor_schedule_thread(struct spdk_thread *thread)
 	lw_thread = spdk_thread_get_ctx(thread);
 	assert(lw_thread != NULL);
 	core = lw_thread->lcore;
-	last_stats = lw_thread->last_stats;
 	memset(lw_thread, 0, sizeof(*lw_thread));
-	lw_thread->last_stats = last_stats;
 
 	if (current_lcore != SPDK_ENV_LCORE_ID_ANY) {
 		local_reactor = spdk_reactor_get(current_lcore);
@@ -1255,7 +1278,9 @@ _reactor_request_thread_reschedule(struct spdk_thread *thread)
 
 	lw_thread = spdk_thread_get_ctx(thread);
 
-	_spdk_lw_thread_set_core(lw_thread, SPDK_ENV_LCORE_ID_ANY);
+	assert(lw_thread != NULL);
+	lw_thread->resched = true;
+	lw_thread->lcore = SPDK_ENV_LCORE_ID_ANY;
 
 	current_core = spdk_env_get_current_core();
 	reactor = spdk_reactor_get(current_core);
@@ -1462,21 +1487,6 @@ reactor_interrupt_fini(struct spdk_reactor *reactor)
 	reactor->fgrp = NULL;
 }
 
-void
-_spdk_lw_thread_set_core(struct spdk_lw_thread *thread, uint32_t lcore)
-{
-	assert(thread != NULL);
-	thread->lcore = lcore;
-	thread->resched = true;
-}
-
-void
-_spdk_lw_thread_get_current_stats(struct spdk_lw_thread *thread, struct spdk_thread_stats *stats)
-{
-	assert(thread != NULL);
-	*stats = thread->current_stats;
-}
-
 static int
 _governor_get_capabilities(uint32_t lcore_id, struct spdk_governor_capabilities *capabilities)
 {
diff --git a/lib/event/scheduler_dynamic.c b/lib/event/scheduler_dynamic.c
index ab685b410..463aae7d8 100644
--- a/lib/event/scheduler_dynamic.c
+++ b/lib/event/scheduler_dynamic.c
@@ -37,13 +37,20 @@
 #include "spdk/log.h"
 #include "spdk/env.h"
 
-#include "spdk_internal/thread.h"
+#include "spdk/thread.h"
 #include "spdk_internal/event.h"
 
 static uint32_t g_next_lcore = SPDK_ENV_LCORE_ID_ANY;
 static uint32_t g_main_lcore;
 static bool g_core_mngmnt_available;
-uint64_t g_last_main_core_busy, g_last_main_core_idle;
+
+struct core_stats {
+	uint64_t busy;
+	uint64_t idle;
+	uint32_t thread_count;
+};
+
+static struct core_stats *g_cores;
 
 #define SCHEDULER_THREAD_BUSY 100
 #define SCHEDULER_LOAD_LIMIT 50
@@ -68,16 +75,114 @@ _get_thread_load(struct spdk_lw_thread *lw_thread)
 {
 	uint64_t busy, idle;
 
-	busy = lw_thread->snapshot_stats.busy_tsc - lw_thread->last_stats.busy_tsc;
-	idle = lw_thread->snapshot_stats.idle_tsc - lw_thread->last_stats.idle_tsc;
-
-	lw_thread->last_stats.busy_tsc = lw_thread->snapshot_stats.busy_tsc;
-	lw_thread->last_stats.idle_tsc = lw_thread->snapshot_stats.idle_tsc;
+	busy = lw_thread->current_stats.busy_tsc;
+	idle = lw_thread->current_stats.idle_tsc;
 
+	if (busy == 0) {
+		/* No work was done, exit before possible division by 0. */
+		return 0;
+	}
 	/* return percentage of time thread was busy */
 	return busy  * 100 / (busy + idle);
 }
 
+typedef void (*_foreach_fn)(struct spdk_lw_thread *lw_thread);
+
+static void
+_foreach_thread(struct spdk_scheduler_core_info *cores_info, _foreach_fn fn)
+{
+	struct spdk_scheduler_core_info *core;
+	uint32_t i, j;
+
+	SPDK_ENV_FOREACH_CORE(i) {
+		core = &cores_info[i];
+		for (j = 0; j < core->threads_count; j++) {
+			fn(core->threads[j]);
+		}
+	}
+}
+
+static void
+_move_thread(struct spdk_lw_thread *lw_thread, uint32_t dst_core)
+{
+	struct core_stats *dst = &g_cores[dst_core];
+	struct core_stats *src = &g_cores[lw_thread->lcore];
+	uint64_t busy_tsc = lw_thread->current_stats.busy_tsc;
+
+	if (src == dst) {
+		/* Don't modify stats if thread is already on that core. */
+		return;
+	}
+
+	dst->busy += spdk_min(UINT64_MAX - dst->busy, busy_tsc);
+	dst->idle -= spdk_min(dst->idle, busy_tsc);
+	dst->thread_count++;
+
+	src->busy -= spdk_min(src->busy, busy_tsc);
+	src->idle += spdk_min(UINT64_MAX - src->busy, busy_tsc);
+	assert(src->thread_count > 0);
+	src->thread_count--;
+
+	lw_thread->lcore = dst_core;
+}
+
+static bool
+_can_core_fit_thread(struct spdk_lw_thread *lw_thread, uint32_t dst_core)
+{
+	struct core_stats *dst = &g_cores[dst_core];
+
+	/* Thread can always fit on the core it's currently on. */
+	if (lw_thread->lcore == dst_core) {
+		return true;
+	}
+
+	/* Reactors in interrupt mode do not update stats,
+	 * a thread can always fit into reactor in interrupt mode. */
+	if (dst->busy + dst->idle == 0) {
+		return true;
+	}
+
+	/* Core has no threads. */
+	if (dst->thread_count == 0) {
+		return true;
+	}
+
+	if (lw_thread->current_stats.busy_tsc <= dst->idle) {
+		return true;
+	}
+	return false;
+}
+
+static uint32_t
+_find_optimal_core(struct spdk_lw_thread *lw_thread)
+{
+	uint32_t i;
+	uint32_t target_lcore;
+	uint32_t current_lcore = lw_thread->lcore;
+	struct spdk_thread *thread = spdk_thread_get_from_ctx(lw_thread);
+	struct spdk_cpuset *cpumask = spdk_thread_get_cpumask(thread);
+
+	/* Find a core that can fit the thread. */
+	for (i = 0; i < spdk_env_get_core_count(); i++) {
+		target_lcore = _get_next_target_core();
+
+		/* Ignore cores outside cpumask. */
+		if (!spdk_cpuset_get_cpu(cpumask, target_lcore)) {
+			continue;
+		}
+
+		/* Skip cores that cannot fit the thread and current one. */
+		if (!_can_core_fit_thread(lw_thread, target_lcore) || target_lcore == current_lcore) {
+			continue;
+		}
+
+		return target_lcore;
+	}
+
+	/* If no better core is found, remain on the same one. */
+	return current_lcore;
+}
+
 static int
 init(struct spdk_governor *governor)
 {
@@ -88,8 +193,11 @@ init(struct spdk_governor *governor)
 	rc = _spdk_governor_set("dpdk_governor");
 	g_core_mngmnt_available = !rc;
 
-	g_last_main_core_busy = 0;
-	g_last_main_core_idle = 0;
+	g_cores = calloc(spdk_env_get_last_core() + 1, sizeof(struct core_stats));
+	if (g_cores == NULL) {
+		SPDK_ERRLOG("Failed to allocate memory for dynamic scheduler core stats.\n");
+		return -ENOMEM;
+	}
 
 	return 0;
 }
@@ -100,6 +208,9 @@ deinit(struct spdk_governor *governor)
 	uint32_t i;
 	int rc = 0;
 
+	free(g_cores);
+	g_cores = NULL;
+
 	if (!g_core_mngmnt_available) {
 		return 0;
 	}
@@ -120,116 +231,53 @@ deinit(struct spdk_governor *governor)
 	return rc;
 }
 
+static void
+_balance_idle(struct spdk_lw_thread *lw_thread)
+{
+	if (_get_thread_load(lw_thread) >= SCHEDULER_LOAD_LIMIT) {
+		return;
+	}
+	/* This thread is idle, move it to the main core. */
+	_move_thread(lw_thread, g_main_lcore);
+}
+
+static void
+_balance_active(struct spdk_lw_thread *lw_thread)
+{
+	uint32_t target_lcore;
+
+	if (_get_thread_load(lw_thread) < SCHEDULER_LOAD_LIMIT) {
+		return;
+	}
+
+	/* This thread is active. */
+	target_lcore = _find_optimal_core(lw_thread);
+	_move_thread(lw_thread, target_lcore);
+}
+
 static void
 balance(struct spdk_scheduler_core_info *cores_info, int cores_count,
 	struct spdk_governor *governor)
 {
 	struct spdk_reactor *reactor;
-	struct spdk_lw_thread *lw_thread;
-	struct spdk_thread *thread;
 	struct spdk_scheduler_core_info *core;
-	struct spdk_cpuset *cpumask;
-	uint64_t main_core_busy;
-	uint64_t main_core_idle;
-	uint64_t thread_busy;
-	uint32_t target_lcore;
-	uint32_t i, j, k;
+	struct core_stats *main_core;
+	uint32_t i;
 	int rc;
-	uint8_t load;
 	bool busy_threads_present = false;
 
-	main_core_busy = cores_info[g_main_lcore].core_busy_tsc - g_last_main_core_busy;
-	main_core_idle = cores_info[g_main_lcore].core_idle_tsc - g_last_main_core_idle;
-	g_last_main_core_busy = cores_info[g_main_lcore].core_busy_tsc;
-	g_last_main_core_idle = cores_info[g_main_lcore].core_idle_tsc;
-
 	SPDK_ENV_FOREACH_CORE(i) {
-		cores_info[i].pending_threads_count = cores_info[i].threads_count;
+		g_cores[i].thread_count = cores_info[i].threads_count;
+		g_cores[i].busy = cores_info[i].current_busy_tsc;
+		g_cores[i].idle = cores_info[i].current_idle_tsc;
 	}
+	main_core = &g_cores[g_main_lcore];
 
-	/* Distribute active threads across all cores and move idle threads to main core */
-	SPDK_ENV_FOREACH_CORE(i) {
-		core = &cores_info[i];
-
-		for (j = 0; j < core->threads_count; j++) {
-			lw_thread = core->threads[j];
-			lw_thread->new_lcore = lw_thread->lcore;
-			thread = spdk_thread_get_from_ctx(lw_thread);
-			cpumask = spdk_thread_get_cpumask(thread);
-
-			if (lw_thread->last_stats.busy_tsc + lw_thread->last_stats.idle_tsc == 0) {
-				lw_thread->last_stats.busy_tsc = lw_thread->snapshot_stats.busy_tsc;
-				lw_thread->last_stats.idle_tsc = lw_thread->snapshot_stats.idle_tsc;
-
-				if (i != g_main_lcore) {
-					busy_threads_present = true;
-				}
-
-				continue;
-			}
-
-			thread_busy = lw_thread->snapshot_stats.busy_tsc - lw_thread->last_stats.busy_tsc;
-
-			load = _get_thread_load(lw_thread);
-
-			if (i == g_main_lcore && load >= SCHEDULER_LOAD_LIMIT) {
-				/* This thread is active and on the main core, we need to pick a core to move it to */
-				for (k = 0; k < spdk_env_get_core_count(); k++) {
-					target_lcore = _get_next_target_core();
-
-					/* Do not use main core if it is too busy for new thread */
-					if (target_lcore == g_main_lcore && thread_busy > main_core_idle) {
-						continue;
-					}
-
-					if (spdk_cpuset_get_cpu(cpumask, target_lcore)) {
-						lw_thread->new_lcore = target_lcore;
-						cores_info[target_lcore].pending_threads_count++;
-						core->pending_threads_count--;
-
-						if (target_lcore != g_main_lcore) {
-							busy_threads_present = true;
-							main_core_idle += spdk_min(UINT64_MAX - main_core_idle, thread_busy);
-							main_core_busy -= spdk_min(main_core_busy, thread_busy);
-						}
-
-						break;
-					}
-				}
-			} else if (i != g_main_lcore && load < SCHEDULER_LOAD_LIMIT) {
-				/* This thread is idle but not on the main core, so we need to move it to the main core */
-				lw_thread->new_lcore = g_main_lcore;
-				cores_info[g_main_lcore].pending_threads_count++;
-				core->pending_threads_count--;
-
-				main_core_busy += spdk_min(UINT64_MAX - main_core_busy, thread_busy);
-				main_core_idle -= spdk_min(main_core_idle, thread_busy);
-			} else {
-				/* Move busy thread only if cpumask does not match current core (except main core) */
-				if (i != g_main_lcore) {
-					if (!spdk_cpuset_get_cpu(cpumask, i)) {
-						for (k = 0; k < spdk_env_get_core_count(); k++) {
-							target_lcore = _get_next_target_core();
-
-							if (spdk_cpuset_get_cpu(cpumask, target_lcore)) {
-								lw_thread->new_lcore = target_lcore;
-								cores_info[target_lcore].pending_threads_count++;
-								core->pending_threads_count--;
-
-								if (target_lcore == g_main_lcore) {
-									main_core_busy += spdk_min(UINT64_MAX - main_core_busy, thread_busy);
-									main_core_idle -= spdk_min(main_core_idle, thread_busy);
-								}
-								break;
-							}
-						}
-					}
-
-					busy_threads_present = true;
-				}
-			}
-		}
-	}
+	/* Distribute threads in two passes, to make sure updated core stats are considered on each pass.
+	 * 1) Move all idle threads to main core. */
+	_foreach_thread(cores_info, _balance_idle);
+	/* 2) Distribute active threads across all cores. */
+	_foreach_thread(cores_info, _balance_active);
 
 	/* Switch unused cores to interrupt mode and switch cores to polled mode
 	 * if they will be used after rebalancing */
@@ -237,10 +285,15 @@ balance(struct spdk_scheduler_core_info *cores_info, int cores_count,
 		reactor = spdk_reactor_get(i);
 		core = &cores_info[i];
 		/* We can switch mode only if reactor already does not have any threads */
-		if (core->pending_threads_count == 0 && TAILQ_EMPTY(&reactor->threads)) {
+		if (g_cores[i].thread_count == 0 && TAILQ_EMPTY(&reactor->threads)) {
 			core->interrupt_mode = true;
-		} else if (core->pending_threads_count != 0) {
+		} else if (g_cores[i].thread_count != 0) {
 			core->interrupt_mode = false;
+			if (i != g_main_lcore) {
+				/* If a thread is present on non g_main_lcore,
+				 * it has to be busy. */
+				busy_threads_present = true;
+			}
 		}
 	}
 
@@ -254,7 +307,7 @@ balance(struct spdk_scheduler_core_info *cores_info, int cores_count,
 		if (rc < 0) {
 			SPDK_ERRLOG("setting default frequency for core %u failed\n", g_main_lcore);
 		}
-	} else if (main_core_busy > main_core_idle) {
+	} else if (main_core->busy > main_core->idle) {
 		rc = governor->core_freq_up(g_main_lcore);
 		if (rc < 0) {
 			SPDK_ERRLOG("increasing frequency for core %u failed\n", g_main_lcore);
diff --git a/lib/event/spdk_event.map b/lib/event/spdk_event.map
index 9a4ba5603..b4a30f67c 100644
--- a/lib/event/spdk_event.map
+++ b/lib/event/spdk_event.map
@@ -25,21 +25,6 @@
 	spdk_reactor_get;
 	spdk_for_each_reactor;
 	spdk_reactor_set_interrupt_mode;
-	spdk_subsystem_find;
-	spdk_subsystem_get_first;
-	spdk_subsystem_get_next;
-	spdk_subsystem_get_first_depend;
-	spdk_subsystem_get_next_depend;
-	spdk_add_subsystem;
-	spdk_add_subsystem_depend;
-	spdk_subsystem_init;
-	spdk_subsystem_fini;
-	spdk_subsystem_init_next;
-	spdk_subsystem_fini_next;
-	spdk_app_json_config_load;
-	spdk_subsystem_config_json;
-	spdk_rpc_initialize;
-	spdk_rpc_finish;
 
 	local: *;
 };
diff --git a/lib/ftl/ftl_core.c b/lib/ftl/ftl_core.c
index 18cd7b4a3..bb13c6dd1 100644
--- a/lib/ftl/ftl_core.c
+++ b/lib/ftl/ftl_core.c
@@ -966,11 +966,18 @@ ftl_wptr_pad_band(struct ftl_wptr *wptr)
 	struct spdk_ftl_dev *dev = wptr->dev;
 	struct ftl_batch *batch = dev->current_batch;
 	struct ftl_io_channel *ioch;
+	struct ftl_io *io;
 	size_t size, pad_size, blocks_left;
 
 	size = batch != NULL ? batch->num_entries : 0;
 	TAILQ_FOREACH(ioch, &dev->ioch_queue, tailq) {
 		size += spdk_ring_count(ioch->submit_queue);
+
+		TAILQ_FOREACH(io, &ioch->retry_queue, ioch_entry) {
+			if (io->type == FTL_IO_WRITE) {
+				size += io->num_blocks - io->pos;
+			}
+		}
 	}
 
 	ioch = ftl_io_channel_get_ctx(ftl_get_io_channel(dev));
@@ -989,11 +996,18 @@ ftl_wptr_process_shutdown(struct ftl_wptr *wptr)
 	struct spdk_ftl_dev *dev = wptr->dev;
 	struct ftl_batch *batch = dev->current_batch;
 	struct ftl_io_channel *ioch;
+	struct ftl_io *io;
 	size_t size;
 
 	size = batch != NULL ? batch->num_entries : 0;
 	TAILQ_FOREACH(ioch, &dev->ioch_queue, tailq) {
 		size += spdk_ring_count(ioch->submit_queue);
+
+		TAILQ_FOREACH(io, &ioch->retry_queue, ioch_entry) {
+			if (io->type == FTL_IO_WRITE) {
+				size += io->num_blocks - io->pos;
+			}
+		}
 	}
 
 	if (size >= dev->xfer_size) {
diff --git a/lib/ftl/ftl_trace.c b/lib/ftl/ftl_trace.c
index ba66323ad..f8716a74d 100644
--- a/lib/ftl/ftl_trace.c
+++ b/lib/ftl/ftl_trace.c
@@ -89,72 +89,93 @@ SPDK_TRACE_REGISTER_FN(ftl_trace_func, "ftl", TRACE_GROUP_FTL)
 	for (i = 0; i < FTL_TRACE_SOURCE_MAX; ++i) {
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "band_defrag");
 		spdk_trace_register_description(descbuf, FTL_TRACE_BAND_DEFRAG(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "band: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "band");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "band_write");
 		spdk_trace_register_description(descbuf, FTL_TRACE_BAND_WRITE(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "band: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "band");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "limits");
 		spdk_trace_register_description(descbuf, FTL_TRACE_LIMITS(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "limits: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "limits");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "rwb_pop");
 		spdk_trace_register_description(descbuf, FTL_TRACE_WBUF_POP(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "md_read_sched");
 		spdk_trace_register_description(descbuf, FTL_TRACE_MD_READ_SCHEDULE(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "md_read_submit");
 		spdk_trace_register_description(descbuf, FTL_TRACE_MD_READ_SUBMISSION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "md_read_cmpl");
 		spdk_trace_register_description(descbuf, FTL_TRACE_MD_READ_COMPLETION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "md_write_sched");
 		spdk_trace_register_description(descbuf, FTL_TRACE_MD_WRITE_SCHEDULE(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "md_write_submit");
 		spdk_trace_register_description(descbuf, FTL_TRACE_MD_WRITE_SUBMISSION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "md_write_cmpl");
 		spdk_trace_register_description(descbuf, FTL_TRACE_MD_WRITE_COMPLETION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "read_sched");
 		spdk_trace_register_description(descbuf, FTL_TRACE_READ_SCHEDULE(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "read_submit");
 		spdk_trace_register_description(descbuf, FTL_TRACE_READ_SUBMISSION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "read_cmpl_invld");
 		spdk_trace_register_description(descbuf, FTL_TRACE_READ_COMPLETION_INVALID(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "read_cmpl_cache");
 		spdk_trace_register_description(descbuf, FTL_TRACE_READ_COMPLETION_CACHE(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "read_cmpl_ssd");
 		spdk_trace_register_description(descbuf, FTL_TRACE_READ_COMPLETION_DISK(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "write_sched");
 		spdk_trace_register_description(descbuf, FTL_TRACE_WRITE_SCHEDULE(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "rwb_fill");
 		spdk_trace_register_description(descbuf, FTL_TRACE_WRITE_WBUF_FILL(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "write_submit");
 		spdk_trace_register_description(descbuf, FTL_TRACE_WRITE_SUBMISSION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "write_cmpl");
 		spdk_trace_register_description(descbuf, FTL_TRACE_WRITE_COMPLETION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "lba: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "lba");
 
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "erase_submit");
 		spdk_trace_register_description(descbuf, FTL_TRACE_ERASE_SUBMISSION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 		snprintf(descbuf, sizeof(descbuf), "%c %s", source[i], "erase_cmpl");
 		spdk_trace_register_description(descbuf, FTL_TRACE_ERASE_COMPLETION(i),
-						OWNER_FTL, OBJECT_NONE, 0, 0, "addr: ");
+						OWNER_FTL, OBJECT_NONE, 0,
+						SPDK_TRACE_ARG_TYPE_INT, "addr");
 	}
 }
 
diff --git a/lib/idxd/Makefile b/lib/idxd/Makefile
index 08c19e83d..1c3178d76 100644
--- a/lib/idxd/Makefile
+++ b/lib/idxd/Makefile
@@ -34,10 +34,11 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 4
+SO_VER := 5
 SO_MINOR := 0
 
-C_SRCS = idxd.c
+C_SRCS = idxd.c idxd_user.c
+
 LIBNAME = idxd
 
 SPDK_MAP_FILE = $(abspath $(CURDIR)/spdk_idxd.map)
diff --git a/lib/idxd/idxd.c b/lib/idxd/idxd.c
index f1683d940..45304eb02 100644
--- a/lib/idxd/idxd.c
+++ b/lib/idxd/idxd.c
@@ -45,8 +45,16 @@
 #include "idxd.h"
 
 #define ALIGN_4K 0x1000
+#define USERSPACE_DRIVER_NAME "user"
+#define CHAN_PER_DEVICE(total_wq_size) ((total_wq_size >= 128) ? 8 : 4)
+/*
+ * Need to limit how many completions we reap in one poller to avoid starving
+ * other threads as callers can submit new operations on the polling thread.
+ */
+#define MAX_COMPLETIONS_PER_POLL 16
 
-pthread_mutex_t	g_driver_lock = PTHREAD_MUTEX_INITIALIZER;
+static STAILQ_HEAD(, spdk_idxd_impl) g_idxd_impls = STAILQ_HEAD_INITIALIZER(g_idxd_impls);
+static struct spdk_idxd_impl *g_idxd_impl;
 
 /*
  * g_dev_cfg gives us 2 pre-set configurations of DSA to choose from
@@ -73,34 +81,10 @@ struct device_config g_dev_cfg1 = {
 	.total_engines = 4,
 };
 
-static uint32_t
-_idxd_read_4(struct spdk_idxd_device *idxd, uint32_t offset)
-{
-	return spdk_mmio_read_4((uint32_t *)(idxd->reg_base + offset));
-}
-
-static void
-_idxd_write_4(struct spdk_idxd_device *idxd, uint32_t offset, uint32_t value)
-{
-	spdk_mmio_write_4((uint32_t *)(idxd->reg_base + offset), value);
-}
-
 static uint64_t
-_idxd_read_8(struct spdk_idxd_device *idxd, uint32_t offset)
+idxd_read_8(struct spdk_idxd_device *idxd, void *portal, uint32_t offset)
 {
-	return spdk_mmio_read_8((uint64_t *)(idxd->reg_base + offset));
-}
-
-static void
-_idxd_write_8(struct spdk_idxd_device *idxd, uint32_t offset, uint64_t value)
-{
-	spdk_mmio_write_8((uint64_t *)(idxd->reg_base + offset), value);
-}
-
-bool
-spdk_idxd_device_needs_rebalance(struct spdk_idxd_device *idxd)
-{
-	return idxd->needs_rebalance;
+	return idxd->impl->read_8(idxd, portal, offset);
 }
 
 struct spdk_idxd_io_channel *
@@ -115,17 +99,29 @@ spdk_idxd_get_channel(struct spdk_idxd_device *idxd)
 		SPDK_ERRLOG("Failed to allocate idxd chan\n");
 		return NULL;
 	}
-	chan->idxd = idxd;
-
-	TAILQ_INIT(&chan->batches);
-	TAILQ_INIT(&chan->batch_pool);
-	TAILQ_INIT(&chan->comp_ctx_oustanding);
 
 	chan->batch_base = calloc(NUM_BATCHES_PER_CHANNEL, sizeof(struct idxd_batch));
 	if (chan->batch_base == NULL) {
 		SPDK_ERRLOG("Failed to allocate batch pool\n");
+		free(chan);
+		return NULL;
+	}
+
+	pthread_mutex_lock(&idxd->num_channels_lock);
+	if (idxd->num_channels == CHAN_PER_DEVICE(idxd->total_wq_size)) {
+		/* too many channels sharing this device */
+		pthread_mutex_unlock(&idxd->num_channels_lock);
+		free(chan->batch_base);
+		free(chan);
 		return NULL;
 	}
+	idxd->num_channels++;
+	pthread_mutex_unlock(&idxd->num_channels_lock);
+
+	chan->idxd = idxd;
+	TAILQ_INIT(&chan->batches);
+	TAILQ_INIT(&chan->batch_pool);
+	TAILQ_INIT(&chan->comp_ctx_oustanding);
 
 	batch = chan->batch_base;
 	for (i = 0 ; i < NUM_BATCHES_PER_CHANNEL ; i++) {
@@ -133,30 +129,17 @@ spdk_idxd_get_channel(struct spdk_idxd_device *idxd)
 		batch++;
 	}
 
-	pthread_mutex_lock(&chan->idxd->num_channels_lock);
-	chan->idxd->num_channels++;
-	if (chan->idxd->num_channels > 1) {
-		chan->idxd->needs_rebalance = true;
-	} else {
-		chan->idxd->needs_rebalance = false;
-	}
-	pthread_mutex_unlock(&chan->idxd->num_channels_lock);
-
 	return chan;
 }
 
-bool
+void
 spdk_idxd_put_channel(struct spdk_idxd_io_channel *chan)
 {
 	struct idxd_batch *batch;
-	bool rebalance = false;
 
 	pthread_mutex_lock(&chan->idxd->num_channels_lock);
 	assert(chan->idxd->num_channels > 0);
 	chan->idxd->num_channels--;
-	if (chan->idxd->num_channels > 0) {
-		rebalance = true;
-	}
 	pthread_mutex_unlock(&chan->idxd->num_channels_lock);
 
 	spdk_free(chan->completions);
@@ -169,8 +152,13 @@ spdk_idxd_put_channel(struct spdk_idxd_io_channel *chan)
 	}
 	free(chan->batch_base);
 	free(chan);
+}
 
-	return rebalance;
+/* returns the total max operations for channel. */
+int
+spdk_idxd_chan_get_max_operations(struct spdk_idxd_io_channel *chan)
+{
+	return chan->idxd->total_wq_size / CHAN_PER_DEVICE(chan->idxd->total_wq_size);
 }
 
 int
@@ -185,9 +173,8 @@ spdk_idxd_configure_chan(struct spdk_idxd_io_channel *chan)
 		chan->idxd->wq_id = 0;
 	}
 
-	pthread_mutex_lock(&chan->idxd->num_channels_lock);
-	num_ring_slots = chan->idxd->queues[chan->idxd->wq_id].wqcfg.wq_size / chan->idxd->num_channels;
-	pthread_mutex_unlock(&chan->idxd->num_channels_lock);
+	num_ring_slots = chan->idxd->queues[chan->idxd->wq_id].wqcfg.wq_size / CHAN_PER_DEVICE(
+				 chan->idxd->total_wq_size);
 
 	chan->ring_slots = spdk_bit_array_create(num_ring_slots);
 	if (chan->ring_slots == NULL) {
@@ -195,13 +182,7 @@ spdk_idxd_configure_chan(struct spdk_idxd_io_channel *chan)
 		return -ENOMEM;
 	}
 
-	/*
-	 * max ring slots can change as channels come and go but we
-	 * start off getting all of the slots for this work queue.
-	 */
-	chan->max_ring_slots = num_ring_slots;
-
-	/* Store the original size of the ring. */
+	/* Store the size of the ring. */
 	chan->ring_size = num_ring_slots;
 
 	chan->desc = spdk_zmalloc(num_ring_slots * sizeof(struct idxd_hw_desc),
@@ -230,7 +211,7 @@ spdk_idxd_configure_chan(struct spdk_idxd_io_channel *chan)
 		if (batch->user_desc == NULL) {
 			SPDK_ERRLOG("Failed to allocate batch descriptor memory\n");
 			rc = -ENOMEM;
-			goto err_user_desc;
+			goto err_user_desc_or_comp;
 		}
 
 		batch->user_completions = spdk_zmalloc(DESC_PER_BATCH * sizeof(struct idxd_comp),
@@ -239,118 +220,58 @@ spdk_idxd_configure_chan(struct spdk_idxd_io_channel *chan)
 		if (batch->user_completions == NULL) {
 			SPDK_ERRLOG("Failed to allocate user completion memory\n");
 			rc = -ENOMEM;
-			goto err_user_comp;
+			goto err_user_desc_or_comp;
 		}
 	}
 
-	/* Assign portal based on work queue chosen earlier. */
-	chan->portal = (char *)chan->idxd->portals + chan->idxd->wq_id * PORTAL_SIZE;
+	chan->portal = chan->idxd->impl->portal_get_addr(chan->idxd);
 
 	return 0;
 
-err_user_comp:
+err_user_desc_or_comp:
 	TAILQ_FOREACH(batch, &chan->batch_pool, link) {
 		spdk_free(batch->user_desc);
+		batch->user_desc = NULL;
+		spdk_free(batch->user_completions);
+		batch->user_completions = NULL;
 	}
-err_user_desc:
-	TAILQ_FOREACH(batch, &chan->batch_pool, link) {
-		spdk_free(chan->completions);
-	}
+	spdk_free(chan->completions);
+	chan->completions = NULL;
 err_comp:
 	spdk_free(chan->desc);
+	chan->desc = NULL;
 err_desc:
 	spdk_bit_array_free(&chan->ring_slots);
 
 	return rc;
 }
 
-/* Used for control commands, not for descriptor submission. */
-static int
-idxd_wait_cmd(struct spdk_idxd_device *idxd, int _timeout)
+static inline struct spdk_idxd_impl *
+idxd_get_impl_by_name(const char *impl_name)
 {
-	uint32_t timeout = _timeout;
-	union idxd_cmdsts_reg cmd_status = {};
-
-	cmd_status.raw = _idxd_read_4(idxd, IDXD_CMDSTS_OFFSET);
-	while (cmd_status.active && --timeout) {
-		usleep(1);
-		cmd_status.raw = _idxd_read_4(idxd, IDXD_CMDSTS_OFFSET);
-	}
-
-	/* Check for timeout */
-	if (timeout == 0 && cmd_status.active) {
-		SPDK_ERRLOG("Command timeout, waited %u\n", _timeout);
-		return -EBUSY;
-	}
-
-	/* Check for error */
-	if (cmd_status.err) {
-		SPDK_ERRLOG("Command status reg reports error 0x%x\n", cmd_status.err);
-		return -EINVAL;
-	}
+	struct spdk_idxd_impl *impl;
 
-	return 0;
-}
-
-static void
-_idxd_drain(struct spdk_idxd_io_channel *chan)
-{
-	uint32_t index;
-	int set = 0;
-
-	do {
-		spdk_idxd_process_events(chan);
-		set = 0;
-		for (index = 0; index < chan->max_ring_slots; index++) {
-			set |= spdk_bit_array_get(chan->ring_slots, index);
+	assert(impl_name != NULL);
+	STAILQ_FOREACH(impl, &g_idxd_impls, link) {
+		if (0 == strcmp(impl_name, impl->name)) {
+			return impl;
 		}
-	} while (set);
-}
-
-int
-spdk_idxd_reconfigure_chan(struct spdk_idxd_io_channel *chan)
-{
-	uint32_t num_ring_slots;
-	int rc;
-
-	_idxd_drain(chan);
-
-	assert(spdk_bit_array_count_set(chan->ring_slots) == 0);
-
-	pthread_mutex_lock(&chan->idxd->num_channels_lock);
-	assert(chan->idxd->num_channels > 0);
-	num_ring_slots = chan->ring_size / chan->idxd->num_channels;
-	/* If no change (ie this was a call from another thread doing its for_each_channel,
-	 * then we can just bail now.
-	 */
-	if (num_ring_slots == chan->max_ring_slots) {
-		pthread_mutex_unlock(&chan->idxd->num_channels_lock);
-		return 0;
-	}
-	pthread_mutex_unlock(&chan->idxd->num_channels_lock);
-
-	/* re-allocate our descriptor ring for hw flow control. */
-	rc = spdk_bit_array_resize(&chan->ring_slots, num_ring_slots);
-	if (rc < 0) {
-		SPDK_ERRLOG("Unable to resize channel bit array\n");
-		return -ENOMEM;
 	}
 
-	chan->max_ring_slots = num_ring_slots;
-
-	/*
-	 * Note: The batch descriptor ring does not change with the
-	 * number of channels as descriptors on this ring do not
-	 * "count" for flow control.
-	 */
-
-	return rc;
+	return NULL;
 }
 
 /* Called via RPC to select a pre-defined configuration. */
 void
 spdk_idxd_set_config(uint32_t config_num)
 {
+	g_idxd_impl = idxd_get_impl_by_name(USERSPACE_DRIVER_NAME);
+
+	if (g_idxd_impl == NULL) {
+		SPDK_ERRLOG("Cannot set the idxd implementation");
+		return;
+	}
+
 	switch (config_num) {
 	case 0:
 		g_dev_cfg = &g_dev_cfg0;
@@ -363,378 +284,27 @@ spdk_idxd_set_config(uint32_t config_num)
 		SPDK_ERRLOG("Invalid config, using default\n");
 		break;
 	}
-}
 
-static int
-idxd_unmap_pci_bar(struct spdk_idxd_device *idxd, int bar)
-{
-	int rc = 0;
-	void *addr = NULL;
-
-	if (bar == IDXD_MMIO_BAR) {
-		addr = (void *)idxd->reg_base;
-	} else if (bar == IDXD_WQ_BAR) {
-		addr = (void *)idxd->portals;
-	}
-
-	if (addr) {
-		rc = spdk_pci_device_unmap_bar(idxd->device, 0, addr);
-	}
-	return rc;
-}
-
-static int
-idxd_map_pci_bars(struct spdk_idxd_device *idxd)
-{
-	int rc;
-	void *addr;
-	uint64_t phys_addr, size;
-
-	rc = spdk_pci_device_map_bar(idxd->device, IDXD_MMIO_BAR, &addr, &phys_addr, &size);
-	if (rc != 0 || addr == NULL) {
-		SPDK_ERRLOG("pci_device_map_range failed with error code %d\n", rc);
-		return -1;
-	}
-	idxd->reg_base = addr;
-
-	rc = spdk_pci_device_map_bar(idxd->device, IDXD_WQ_BAR, &addr, &phys_addr, &size);
-	if (rc != 0 || addr == NULL) {
-		SPDK_ERRLOG("pci_device_map_range failed with error code %d\n", rc);
-		rc = idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
-		if (rc) {
-			SPDK_ERRLOG("unable to unmap MMIO bar\n");
-		}
-		return -EINVAL;
-	}
-	idxd->portals = addr;
-
-	return 0;
-}
-
-static int
-idxd_reset_dev(struct spdk_idxd_device *idxd)
-{
-	int rc;
-
-	_idxd_write_4(idxd, IDXD_CMD_OFFSET, IDXD_RESET_DEVICE << IDXD_CMD_SHIFT);
-	rc = idxd_wait_cmd(idxd, IDXD_REGISTER_TIMEOUT_US);
-	if (rc < 0) {
-		SPDK_ERRLOG("Error resetting device %u\n", rc);
-	}
-
-	return rc;
-}
-
-/*
- * Build group config based on getting info from the device combined
- * with the defined configuration. Once built, it is written to the
- * device.
- */
-static int
-idxd_group_config(struct spdk_idxd_device *idxd)
-{
-	int i;
-	uint64_t base_offset;
-	struct idxd_grpcfg *grpcfg;
-
-	assert(g_dev_cfg->num_groups <= idxd->registers.groupcap.num_groups);
-	idxd->groups = calloc(idxd->registers.groupcap.num_groups, sizeof(struct idxd_group));
-	if (idxd->groups == NULL) {
-		SPDK_ERRLOG("Failed to allocate group memory\n");
-		return -ENOMEM;
-	}
-
-	assert(g_dev_cfg->total_engines <= idxd->registers.enginecap.num_engines);
-	for (i = 0; i < g_dev_cfg->total_engines; i++) {
-		idxd->groups[i % g_dev_cfg->num_groups].grpcfg.engines |= (1 << i);
-	}
-
-	assert(g_dev_cfg->total_wqs <= idxd->registers.wqcap.num_wqs);
-	for (i = 0; i < g_dev_cfg->total_wqs; i++) {
-		idxd->groups[i % g_dev_cfg->num_groups].grpcfg.wqs[0] |= (1 << i);
-	}
-
-	for (i = 0; i < g_dev_cfg->num_groups; i++) {
-		idxd->groups[i].idxd = idxd;
-		idxd->groups[i].id = i;
-
-		/* Divide BW tokens evenly */
-		idxd->groups[i].grpcfg.flags.tokens_allowed =
-			idxd->registers.groupcap.total_tokens / g_dev_cfg->num_groups;
-	}
-
-	/*
-	 * Now write the group config to the device for all groups. We write
-	 * to the max number of groups in order to 0 out the ones we didn't
-	 * configure.
-	 */
-	for (i = 0 ; i < idxd->registers.groupcap.num_groups; i++) {
-
-		grpcfg = &idxd->groups[i].grpcfg;
-		if (i < g_dev_cfg->num_groups) {
-			SPDK_DEBUGLOG(idxd, "Group #%u: wqueue_cfg 0x%lx, engine_cfg 0x%lx, flags 0x%x\n", i,
-				      grpcfg->wqs[0], grpcfg->engines, grpcfg->flags.raw);
-		}
-
-		base_offset = idxd->grpcfg_offset + i * 64;
-
-		/* GRPWQCFG, work queues config */
-		_idxd_write_8(idxd, base_offset, grpcfg->wqs[0]);
-
-		/* GRPENGCFG, engine config */
-		_idxd_write_8(idxd, base_offset + CFG_ENGINE_OFFSET, grpcfg->engines);
-
-		/* GRPFLAGS, flags config */
-		_idxd_write_8(idxd, base_offset + CFG_FLAG_OFFSET, grpcfg->flags.raw);
-	}
-
-	return 0;
-}
-
-/*
- * Build work queue (WQ) config based on getting info from the device combined
- * with the defined configuration. Once built, it is written to the device.
- */
-static int
-idxd_wq_config(struct spdk_idxd_device *idxd)
-{
-	int i, j;
-	struct idxd_wq *queue;
-	u_int32_t wq_size = idxd->registers.wqcap.total_wq_size / g_dev_cfg->total_wqs;
-
-	SPDK_NOTICELOG("Total ring slots available space 0x%x, so per work queue is 0x%x\n",
-		       idxd->registers.wqcap.total_wq_size, wq_size);
-	assert(g_dev_cfg->total_wqs <= IDXD_MAX_QUEUES);
-	assert(g_dev_cfg->total_wqs <= idxd->registers.wqcap.num_wqs);
-	assert(LOG2_WQ_MAX_BATCH <= idxd->registers.gencap.max_batch_shift);
-	assert(LOG2_WQ_MAX_XFER <= idxd->registers.gencap.max_xfer_shift);
-
-	idxd->queues = calloc(1, idxd->registers.wqcap.num_wqs * sizeof(struct idxd_wq));
-	if (idxd->queues == NULL) {
-		SPDK_ERRLOG("Failed to allocate queue memory\n");
-		return -ENOMEM;
-	}
-
-	for (i = 0; i < g_dev_cfg->total_wqs; i++) {
-		queue = &idxd->queues[i];
-		queue->wqcfg.wq_size = wq_size;
-		queue->wqcfg.mode = WQ_MODE_DEDICATED;
-		queue->wqcfg.max_batch_shift = LOG2_WQ_MAX_BATCH;
-		queue->wqcfg.max_xfer_shift = LOG2_WQ_MAX_XFER;
-		queue->wqcfg.wq_state = WQ_ENABLED;
-		queue->wqcfg.priority = WQ_PRIORITY_1;
-
-		/* Not part of the config struct */
-		queue->idxd = idxd;
-		queue->group = &idxd->groups[i % g_dev_cfg->num_groups];
-	}
-
-	/*
-	 * Now write the work queue config to the device for all wq space
-	 */
-	for (i = 0 ; i < idxd->registers.wqcap.num_wqs; i++) {
-		queue = &idxd->queues[i];
-		for (j = 0 ; j < WQCFG_NUM_DWORDS; j++) {
-			_idxd_write_4(idxd, idxd->wqcfg_offset + i * 32 + j * 4,
-				      queue->wqcfg.raw[j]);
-		}
-	}
-
-	return 0;
-}
-
-static int
-idxd_device_configure(struct spdk_idxd_device *idxd)
-{
-	int i, rc = 0;
-	union idxd_offsets_register offsets_reg;
-	union idxd_genstatus_register genstatus_reg;
-
-	/*
-	 * Map BAR0 and BAR2
-	 */
-	rc = idxd_map_pci_bars(idxd);
-	if (rc) {
-		return rc;
-	}
-
-	/*
-	 * Reset the device
-	 */
-	rc = idxd_reset_dev(idxd);
-	if (rc) {
-		goto err_reset;
-	}
-
-	/*
-	 * Read in config registers
-	 */
-	idxd->registers.version = _idxd_read_4(idxd, IDXD_VERSION_OFFSET);
-	idxd->registers.gencap.raw = _idxd_read_8(idxd, IDXD_GENCAP_OFFSET);
-	idxd->registers.wqcap.raw = _idxd_read_8(idxd, IDXD_WQCAP_OFFSET);
-	idxd->registers.groupcap.raw = _idxd_read_8(idxd, IDXD_GRPCAP_OFFSET);
-	idxd->registers.enginecap.raw = _idxd_read_8(idxd, IDXD_ENGCAP_OFFSET);
-	for (i = 0; i < IDXD_OPCAP_WORDS; i++) {
-		idxd->registers.opcap.raw[i] =
-			_idxd_read_8(idxd, i * sizeof(uint64_t) + IDXD_OPCAP_OFFSET);
-	}
-	offsets_reg.raw[0] = _idxd_read_8(idxd, IDXD_TABLE_OFFSET);
-	offsets_reg.raw[1] = _idxd_read_8(idxd, IDXD_TABLE_OFFSET + sizeof(uint64_t));
-	idxd->grpcfg_offset = offsets_reg.grpcfg * IDXD_TABLE_OFFSET_MULT;
-	idxd->wqcfg_offset = offsets_reg.wqcfg * IDXD_TABLE_OFFSET_MULT;
-	idxd->ims_offset = offsets_reg.ims * IDXD_TABLE_OFFSET_MULT;
-	idxd->msix_perm_offset = offsets_reg.msix_perm  * IDXD_TABLE_OFFSET_MULT;
-	idxd->perfmon_offset = offsets_reg.perfmon * IDXD_TABLE_OFFSET_MULT;
-
-	/*
-	 * Configure groups and work queues.
-	 */
-	rc = idxd_group_config(idxd);
-	if (rc) {
-		goto err_group_cfg;
-	}
-
-	rc = idxd_wq_config(idxd);
-	if (rc) {
-		goto err_wq_cfg;
-	}
-
-	/*
-	 * Enable the device
-	 */
-	genstatus_reg.raw = _idxd_read_4(idxd, IDXD_GENSTATUS_OFFSET);
-	assert(genstatus_reg.state == IDXD_DEVICE_STATE_DISABLED);
-
-	_idxd_write_4(idxd, IDXD_CMD_OFFSET, IDXD_ENABLE_DEV << IDXD_CMD_SHIFT);
-	rc = idxd_wait_cmd(idxd, IDXD_REGISTER_TIMEOUT_US);
-	genstatus_reg.raw = _idxd_read_4(idxd, IDXD_GENSTATUS_OFFSET);
-	if ((rc < 0) || (genstatus_reg.state != IDXD_DEVICE_STATE_ENABLED)) {
-		rc = -EINVAL;
-		SPDK_ERRLOG("Error enabling device %u\n", rc);
-		goto err_device_enable;
-	}
-
-	genstatus_reg.raw = spdk_mmio_read_4((uint32_t *)(idxd->reg_base + IDXD_GENSTATUS_OFFSET));
-	assert(genstatus_reg.state == IDXD_DEVICE_STATE_ENABLED);
-
-	/*
-	 * Enable the work queues that we've configured
-	 */
-	for (i = 0; i < g_dev_cfg->total_wqs; i++) {
-		_idxd_write_4(idxd, IDXD_CMD_OFFSET,
-			      (IDXD_ENABLE_WQ << IDXD_CMD_SHIFT) | i);
-		rc = idxd_wait_cmd(idxd, IDXD_REGISTER_TIMEOUT_US);
-		if (rc < 0) {
-			SPDK_ERRLOG("Error enabling work queues 0x%x\n", rc);
-			goto err_wq_enable;
-		}
-	}
-
-	if ((rc == 0) && (genstatus_reg.state == IDXD_DEVICE_STATE_ENABLED)) {
-		SPDK_NOTICELOG("Device enabled, version 0x%x gencap: 0x%lx\n",
-			       idxd->registers.version,
-			       idxd->registers.gencap.raw);
-
-	}
-
-	return rc;
-err_wq_enable:
-err_device_enable:
-	free(idxd->queues);
-err_wq_cfg:
-	free(idxd->groups);
-err_group_cfg:
-err_reset:
-	idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
-	idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
-
-	return rc;
+	g_idxd_impl->set_config(g_dev_cfg, config_num);
 }
 
 static void
 idxd_device_destruct(struct spdk_idxd_device *idxd)
 {
-	idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
-	idxd_unmap_pci_bar(idxd, IDXD_WQ_BAR);
-	free(idxd->groups);
-	free(idxd->queues);
-	free(idxd);
-}
-
-/* Caller must hold g_driver_lock */
-static struct spdk_idxd_device *
-idxd_attach(struct spdk_pci_device *device)
-{
-	struct spdk_idxd_device *idxd;
-	uint32_t cmd_reg;
-	int rc;
-
-	idxd = calloc(1, sizeof(struct spdk_idxd_device));
-	if (idxd == NULL) {
-		SPDK_ERRLOG("Failed to allocate memory for idxd device.\n");
-		return NULL;
-	}
-
-	idxd->device = device;
-	pthread_mutex_init(&idxd->num_channels_lock, NULL);
-
-	/* Enable PCI busmaster. */
-	spdk_pci_device_cfg_read32(device, &cmd_reg, 4);
-	cmd_reg |= 0x4;
-	spdk_pci_device_cfg_write32(device, cmd_reg, 4);
-
-	rc = idxd_device_configure(idxd);
-	if (rc) {
-		goto err;
-	}
-
-	return idxd;
-err:
-	idxd_device_destruct(idxd);
-	return NULL;
-}
-
-struct idxd_enum_ctx {
-	spdk_idxd_probe_cb probe_cb;
-	spdk_idxd_attach_cb attach_cb;
-	void *cb_ctx;
-};
-
-/* This function must only be called while holding g_driver_lock */
-static int
-idxd_enum_cb(void *ctx, struct spdk_pci_device *pci_dev)
-{
-	struct idxd_enum_ctx *enum_ctx = ctx;
-	struct spdk_idxd_device *idxd;
-
-	if (enum_ctx->probe_cb(enum_ctx->cb_ctx, pci_dev)) {
-		idxd = idxd_attach(pci_dev);
-		if (idxd == NULL) {
-			SPDK_ERRLOG("idxd_attach() failed\n");
-			return -EINVAL;
-		}
+	assert(idxd->impl != NULL);
 
-		enum_ctx->attach_cb(enum_ctx->cb_ctx, pci_dev, idxd);
-	}
-
-	return 0;
+	idxd->impl->destruct(idxd);
 }
 
 int
-spdk_idxd_probe(void *cb_ctx, spdk_idxd_probe_cb probe_cb, spdk_idxd_attach_cb attach_cb)
+spdk_idxd_probe(void *cb_ctx, spdk_idxd_attach_cb attach_cb)
 {
-	int rc;
-	struct idxd_enum_ctx enum_ctx;
-
-	enum_ctx.probe_cb = probe_cb;
-	enum_ctx.attach_cb = attach_cb;
-	enum_ctx.cb_ctx = cb_ctx;
-
-	pthread_mutex_lock(&g_driver_lock);
-	rc = spdk_pci_enumerate(spdk_pci_idxd_get_driver(), idxd_enum_cb, &enum_ctx);
-	pthread_mutex_unlock(&g_driver_lock);
+	if (g_idxd_impl == NULL) {
+		SPDK_ERRLOG("No idxd impl is selected\n");
+		return -1;
+	}
 
-	return rc;
+	return g_idxd_impl->probe(cb_ctx, attach_cb);
 }
 
 void
@@ -780,9 +350,9 @@ _vtophys(const void *buf, uint64_t *buf_addr, uint64_t size)
 	return 0;
 }
 
-static struct idxd_hw_desc *
+static int
 _idxd_prep_command(struct spdk_idxd_io_channel *chan, spdk_idxd_req_cb cb_fn,
-		   void *cb_arg, struct idxd_batch *batch)
+		   void *cb_arg, struct idxd_hw_desc **_desc, struct idxd_comp **_comp)
 {
 	uint32_t index;
 	struct idxd_hw_desc *desc;
@@ -793,18 +363,18 @@ _idxd_prep_command(struct spdk_idxd_io_channel *chan, spdk_idxd_req_cb cb_fn,
 	index = spdk_bit_array_find_first_clear(chan->ring_slots, 0);
 	if (index == UINT32_MAX) {
 		/* ran out of ring slots */
-		return NULL;
+		return -EBUSY;
 	}
 
 	spdk_bit_array_set(chan->ring_slots, index);
 
-	desc = &chan->desc[index];
-	comp = &chan->completions[index];
+	desc = *_desc = &chan->desc[index];
+	comp = *_comp = &chan->completions[index];
 
 	rc = _vtophys(&comp->hw, &comp_hw_addr, sizeof(struct idxd_hw_comp_record));
 	if (rc) {
 		spdk_bit_array_clear(chan->ring_slots, index);
-		return NULL;
+		return rc;
 	}
 
 	_track_comp(chan, false, index, comp, desc, NULL);
@@ -813,9 +383,8 @@ _idxd_prep_command(struct spdk_idxd_io_channel *chan, spdk_idxd_req_cb cb_fn,
 	desc->completion_addr = comp_hw_addr;
 	comp->cb_arg = cb_arg;
 	comp->cb_fn = cb_fn;
-	comp->batch = batch;
 
-	return desc;
+	return 0;
 }
 
 int
@@ -823,13 +392,14 @@ spdk_idxd_submit_copy(struct spdk_idxd_io_channel *chan, void *dst, const void *
 		      uint64_t nbytes, spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src_addr, dst_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_command(chan, cb_fn, cb_arg, NULL);
-	if (desc == NULL) {
-		return -EBUSY;
+	rc = _idxd_prep_command(chan, cb_fn, cb_arg, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src, &src_addr, nbytes);
@@ -847,6 +417,7 @@ spdk_idxd_submit_copy(struct spdk_idxd_io_channel *chan, void *dst, const void *
 	desc->src_addr = src_addr;
 	desc->dst_addr = dst_addr;
 	desc->xfer_size = nbytes;
+	desc->flags |= IDXD_FLAG_CACHE_CONTROL; /* direct IO to CPU cache instead of mem */
 
 	/* Submit operation. */
 	movdir64b(chan->portal, desc);
@@ -860,6 +431,7 @@ spdk_idxd_submit_dualcast(struct spdk_idxd_io_channel *chan, void *dst1, void *d
 			  const void *src, uint64_t nbytes, spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src_addr, dst1_addr, dst2_addr;
 	int rc;
 
@@ -869,9 +441,9 @@ spdk_idxd_submit_dualcast(struct spdk_idxd_io_channel *chan, void *dst1, void *d
 	}
 
 	/* Common prep. */
-	desc = _idxd_prep_command(chan, cb_fn, cb_arg, NULL);
-	if (desc == NULL) {
-		return -EBUSY;
+	rc = _idxd_prep_command(chan, cb_fn, cb_arg, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src, &src_addr, nbytes);
@@ -895,6 +467,7 @@ spdk_idxd_submit_dualcast(struct spdk_idxd_io_channel *chan, void *dst1, void *d
 	desc->dst_addr = dst1_addr;
 	desc->dest2 = dst2_addr;
 	desc->xfer_size = nbytes;
+	desc->flags |= IDXD_FLAG_CACHE_CONTROL; /* direct IO to CPU cache instead of mem */
 
 	/* Submit operation. */
 	movdir64b(chan->portal, desc);
@@ -907,13 +480,14 @@ spdk_idxd_submit_compare(struct spdk_idxd_io_channel *chan, void *src1, const vo
 			 uint64_t nbytes, spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src1_addr, src2_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_command(chan, cb_fn, cb_arg, NULL);
-	if (desc == NULL) {
-		return -EBUSY;
+	rc = _idxd_prep_command(chan, cb_fn, cb_arg, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src1, &src1_addr, nbytes);
@@ -943,13 +517,14 @@ spdk_idxd_submit_fill(struct spdk_idxd_io_channel *chan, void *dst, uint64_t fil
 		      uint64_t nbytes, spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t dst_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_command(chan, cb_fn, cb_arg, NULL);
-	if (desc == NULL) {
-		return -EBUSY;
+	rc = _idxd_prep_command(chan, cb_fn, cb_arg, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(dst, &dst_addr, nbytes);
@@ -962,6 +537,7 @@ spdk_idxd_submit_fill(struct spdk_idxd_io_channel *chan, void *dst, uint64_t fil
 	desc->pattern = fill_pattern;
 	desc->dst_addr = dst_addr;
 	desc->xfer_size = nbytes;
+	desc->flags |= IDXD_FLAG_CACHE_CONTROL; /* direct IO to CPU cache instead of mem */
 
 	/* Submit operation. */
 	movdir64b(chan->portal, desc);
@@ -970,18 +546,55 @@ spdk_idxd_submit_fill(struct spdk_idxd_io_channel *chan, void *dst, uint64_t fil
 }
 
 int
-spdk_idxd_submit_crc32c(struct spdk_idxd_io_channel *chan, uint32_t *dst, void *src,
+spdk_idxd_submit_crc32c(struct spdk_idxd_io_channel *chan, uint32_t *crc_dst, void *src,
 			uint32_t seed, uint64_t nbytes,
 			spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
+	uint64_t src_addr;
+	int rc;
+
+	/* Common prep. */
+	rc = _idxd_prep_command(chan, cb_fn, cb_arg, &desc, &comp);
+	if (rc) {
+		return rc;
+	}
+
+	rc = _vtophys(src, &src_addr, nbytes);
+	if (rc) {
+		return rc;
+	}
+
+	/* Command specific. */
+	desc->opcode = IDXD_OPCODE_CRC32C_GEN;
+	desc->dst_addr = 0; /* Per spec, needs to be clear. */
+	desc->src_addr = src_addr;
+	desc->flags &= IDXD_CLEAR_CRC_FLAGS;
+	desc->crc32c.seed = seed;
+	desc->xfer_size = nbytes;
+	comp->crc_dst = crc_dst;
+
+	/* Submit operation. */
+	movdir64b(chan->portal, desc);
+
+	return 0;
+}
+
+int
+spdk_idxd_submit_copy_crc32c(struct spdk_idxd_io_channel *chan, void *dst, void *src,
+			     uint32_t *crc_dst, uint32_t seed, uint64_t nbytes,
+			     spdk_idxd_req_cb cb_fn, void *cb_arg)
+{
+	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src_addr, dst_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_command(chan, cb_fn, cb_arg, NULL);
-	if (desc == NULL) {
-		return -EBUSY;
+	rc = _idxd_prep_command(chan, cb_fn, cb_arg, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src, &src_addr, nbytes);
@@ -995,12 +608,13 @@ spdk_idxd_submit_crc32c(struct spdk_idxd_io_channel *chan, uint32_t *dst, void *
 	}
 
 	/* Command specific. */
-	desc->opcode = IDXD_OPCODE_CRC32C_GEN;
+	desc->opcode = IDXD_OPCODE_COPY_CRC;
 	desc->dst_addr = dst_addr;
 	desc->src_addr = src_addr;
 	desc->flags &= IDXD_CLEAR_CRC_FLAGS;
 	desc->crc32c.seed = seed;
 	desc->xfer_size = nbytes;
+	comp->crc_dst = crc_dst;
 
 	/* Submit operation. */
 	movdir64b(chan->portal, desc);
@@ -1083,6 +697,7 @@ spdk_idxd_batch_submit(struct spdk_idxd_io_channel *chan, struct idxd_batch *bat
 		       spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t desc_addr;
 	int i, rc;
 
@@ -1099,10 +714,9 @@ spdk_idxd_batch_submit(struct spdk_idxd_io_channel *chan, struct idxd_batch *bat
 	}
 
 	/* Common prep. */
-	desc = _idxd_prep_command(chan, cb_fn, cb_arg, batch);
-	if (desc == NULL) {
-		SPDK_DEBUGLOG(idxd, "Busy, can't submit batch %p\n", batch);
-		return -EBUSY;
+	rc = _idxd_prep_command(chan, cb_fn, cb_arg, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(batch->user_desc, &desc_addr, batch->remaining * sizeof(struct idxd_hw_desc));
@@ -1114,6 +728,7 @@ spdk_idxd_batch_submit(struct spdk_idxd_io_channel *chan, struct idxd_batch *bat
 	desc->opcode = IDXD_OPCODE_BATCH;
 	desc->desc_list_addr = desc_addr;
 	desc->desc_count = batch->remaining = batch->index;
+	comp->batch = batch;
 	assert(batch->index <= DESC_PER_BATCH);
 
 	/* Add the batch elements completion contexts to the outstanding list to be polled. */
@@ -1134,25 +749,27 @@ spdk_idxd_batch_submit(struct spdk_idxd_io_channel *chan, struct idxd_batch *bat
 	return 0;
 }
 
-static struct idxd_hw_desc *
+static int
 _idxd_prep_batch_cmd(struct spdk_idxd_io_channel *chan, spdk_idxd_req_cb cb_fn,
-		     void *cb_arg, struct idxd_batch *batch)
+		     void *cb_arg, struct idxd_batch *batch,
+		     struct idxd_hw_desc **_desc, struct idxd_comp **_comp)
 {
 	struct idxd_hw_desc *desc;
 	struct idxd_comp *comp;
 
 	if (_is_batch_valid(batch, chan) == false) {
 		SPDK_ERRLOG("Attempt to add to an invalid batch.\n");
-		return NULL;
+		return -EINVAL;
 	}
 
+	assert(batch != NULL); /* suppress scan-build warning. */
 	if (batch->index == DESC_PER_BATCH) {
 		SPDK_ERRLOG("Attempt to add to a batch that is already full.\n");
-		return NULL;
+		return -EINVAL;
 	}
 
-	desc = &batch->user_desc[batch->index];
-	comp = &batch->user_completions[batch->index];
+	desc = *_desc = &batch->user_desc[batch->index];
+	comp = *_comp = &batch->user_completions[batch->index];
 	_track_comp(chan, true, batch->index, comp, desc, batch);
 	SPDK_DEBUGLOG(idxd, "Prep batch %p index %u\n", batch, batch->index);
 
@@ -1164,27 +781,28 @@ _idxd_prep_batch_cmd(struct spdk_idxd_io_channel *chan, spdk_idxd_req_cb cb_fn,
 	comp->cb_fn = cb_fn;
 	comp->batch = batch;
 
-	return desc;
+	return 0;
 }
 
 static int
 _idxd_batch_prep_nop(struct spdk_idxd_io_channel *chan, struct idxd_batch *batch)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
+	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_batch_cmd(chan, NULL, NULL, batch);
-	if (desc == NULL) {
-		return -EINVAL;
+	rc = _idxd_prep_batch_cmd(chan, NULL, NULL, batch, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	/* Command specific. */
 	desc->opcode = IDXD_OPCODE_NOOP;
-	/* TODO: temp workaround for simulator.  Remove when fixed or w/silicon. */
-	if (chan->idxd->registers.gencap.raw == 0x1833f011f) {
+
+	if (chan->idxd->impl->nop_check && chan->idxd->impl->nop_check(chan->idxd)) {
 		desc->xfer_size = 1;
 	}
-
 	return 0;
 }
 
@@ -1193,13 +811,14 @@ spdk_idxd_batch_prep_copy(struct spdk_idxd_io_channel *chan, struct idxd_batch *
 			  void *dst, const void *src, uint64_t nbytes, spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src_addr, dst_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch);
-	if (desc == NULL) {
-		return -EINVAL;
+	rc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src, &src_addr, nbytes);
@@ -1227,13 +846,14 @@ spdk_idxd_batch_prep_fill(struct spdk_idxd_io_channel *chan, struct idxd_batch *
 			  spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t dst_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch);
-	if (desc == NULL) {
-		return -EINVAL;
+	rc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(dst, &dst_addr, nbytes);
@@ -1255,6 +875,7 @@ spdk_idxd_batch_prep_dualcast(struct spdk_idxd_io_channel *chan, struct idxd_bat
 			      void *dst1, void *dst2, const void *src, uint64_t nbytes, spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src_addr, dst1_addr, dst2_addr;
 	int rc;
 
@@ -1264,9 +885,9 @@ spdk_idxd_batch_prep_dualcast(struct spdk_idxd_io_channel *chan, struct idxd_bat
 	}
 
 	/* Common prep. */
-	desc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch);
-	if (desc == NULL) {
-		return -EINVAL;
+	rc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src, &src_addr, nbytes);
@@ -1295,17 +916,51 @@ spdk_idxd_batch_prep_dualcast(struct spdk_idxd_io_channel *chan, struct idxd_bat
 
 int
 spdk_idxd_batch_prep_crc32c(struct spdk_idxd_io_channel *chan, struct idxd_batch *batch,
-			    uint32_t *dst, void *src, uint32_t seed, uint64_t nbytes,
+			    uint32_t *crc_dst, void *src, uint32_t seed, uint64_t nbytes,
 			    spdk_idxd_req_cb cb_fn, void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
+	uint64_t src_addr;
+	int rc;
+
+	/* Common prep. */
+	rc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch, &desc, &comp);
+	if (rc) {
+		return rc;
+	}
+
+	rc = _vtophys(src, &src_addr, nbytes);
+	if (rc) {
+		return rc;
+	}
+
+	/* Command specific. */
+	desc->opcode = IDXD_OPCODE_CRC32C_GEN;
+	desc->dst_addr = 0; /* per specification */
+	desc->src_addr = src_addr;
+	desc->flags &= IDXD_CLEAR_CRC_FLAGS;
+	desc->crc32c.seed = seed;
+	desc->xfer_size = nbytes;
+	comp->crc_dst = crc_dst;
+
+	return 0;
+}
+
+int
+spdk_idxd_batch_prep_copy_crc32c(struct spdk_idxd_io_channel *chan, struct idxd_batch *batch,
+				 void *dst, void *src, uint32_t *crc_dst, uint32_t seed, uint64_t nbytes,
+				 spdk_idxd_req_cb cb_fn, void *cb_arg)
+{
+	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src_addr, dst_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch);
-	if (desc == NULL) {
-		return -EINVAL;
+	rc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src, &src_addr, nbytes);
@@ -1319,12 +974,13 @@ spdk_idxd_batch_prep_crc32c(struct spdk_idxd_io_channel *chan, struct idxd_batch
 	}
 
 	/* Command specific. */
-	desc->opcode = IDXD_OPCODE_CRC32C_GEN;
+	desc->opcode = IDXD_OPCODE_COPY_CRC;
 	desc->dst_addr = dst_addr;
 	desc->src_addr = src_addr;
 	desc->flags &= IDXD_CLEAR_CRC_FLAGS;
 	desc->crc32c.seed = seed;
 	desc->xfer_size = nbytes;
+	comp->crc_dst = crc_dst;
 
 	return 0;
 }
@@ -1335,13 +991,14 @@ spdk_idxd_batch_prep_compare(struct spdk_idxd_io_channel *chan, struct idxd_batc
 			     void *cb_arg)
 {
 	struct idxd_hw_desc *desc;
+	struct idxd_comp *comp;
 	uint64_t src1_addr, src2_addr;
 	int rc;
 
 	/* Common prep. */
-	desc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch);
-	if (desc == NULL) {
-		return -EINVAL;
+	rc = _idxd_prep_batch_cmd(chan, cb_fn, cb_arg, batch, &desc, &comp);
+	if (rc) {
+		return rc;
 	}
 
 	rc = _vtophys(src1, &src1_addr, nbytes);
@@ -1369,7 +1026,7 @@ _dump_error_reg(struct spdk_idxd_io_channel *chan)
 	uint64_t sw_error_0;
 	uint16_t i;
 
-	sw_error_0 = _idxd_read_8(chan->idxd, IDXD_SWERR_OFFSET);
+	sw_error_0 = idxd_read_8(chan->idxd, chan->portal, IDXD_SWERR_OFFSET);
 
 	SPDK_NOTICELOG("SW Error bits set:");
 	for (i = 0; i < CHAR_BIT; i++) {
@@ -1398,22 +1055,22 @@ int
 spdk_idxd_process_events(struct spdk_idxd_io_channel *chan)
 {
 	struct idxd_comp *comp_ctx, *tmp;
-	uint64_t sw_error_0;
 	int status = 0;
 	int rc = 0;
 
 	TAILQ_FOREACH_SAFE(comp_ctx, &chan->comp_ctx_oustanding, link, tmp) {
+		if (rc == MAX_COMPLETIONS_PER_POLL) {
+			break;
+		}
+
 		if (IDXD_COMPLETION(comp_ctx->hw.status)) {
 
 			TAILQ_REMOVE(&chan->comp_ctx_oustanding, comp_ctx, link);
 			rc++;
 
 			if (spdk_unlikely(IDXD_FAILURE(comp_ctx->hw.status))) {
-				sw_error_0 = _idxd_read_8(chan->idxd, IDXD_SWERR_OFFSET);
-				if (IDXD_SW_ERROR(sw_error_0)) {
-					_dump_error_reg(chan);
-					status = -EINVAL;
-				}
+				status = -EINVAL;
+				_dump_error_reg(chan);
 			}
 
 			switch (comp_ctx->desc->opcode) {
@@ -1421,8 +1078,9 @@ spdk_idxd_process_events(struct spdk_idxd_io_channel *chan)
 				SPDK_DEBUGLOG(idxd, "Complete batch %p\n", comp_ctx->batch);
 				break;
 			case IDXD_OPCODE_CRC32C_GEN:
-				*(uint32_t *)comp_ctx->desc->dst_addr = comp_ctx->hw.crc32c_val;
-				*(uint32_t *)comp_ctx->desc->dst_addr ^= ~0;
+			case IDXD_OPCODE_COPY_CRC:
+				*(uint32_t *)comp_ctx->crc_dst = comp_ctx->hw.crc32c_val;
+				*(uint32_t *)comp_ctx->crc_dst ^= ~0;
 				break;
 			case IDXD_OPCODE_COMPARE:
 				if (status == 0) {
@@ -1448,9 +1106,22 @@ spdk_idxd_process_events(struct spdk_idxd_io_channel *chan)
 					_free_batch(comp_ctx->batch, chan);
 				}
 			}
+		} else {
+			/*
+			 * oldest locations are at the head of the list so if
+			 * we've polled a location that hasn't completed, bail
+			 * now as there are unlikely to be any more completions.
+			 */
+			break;
 		}
 	}
 	return rc;
 }
 
+void
+idxd_impl_register(struct spdk_idxd_impl *impl)
+{
+	STAILQ_INSERT_HEAD(&g_idxd_impls, impl, link);
+}
+
 SPDK_LOG_REGISTER_COMPONENT(idxd)
diff --git a/lib/idxd/idxd.h b/lib/idxd/idxd.h
index 10cb5b998..c3d472f62 100644
--- a/lib/idxd/idxd.h
+++ b/lib/idxd/idxd.h
@@ -119,15 +119,8 @@ struct spdk_idxd_io_channel {
 	 * We use one bit array to track ring slots for both
 	 * desc and completions.
 	 *
-	 * TODO: We can get rid of the bit array and just use a uint
-	 * to manage flow control as the current implementation saves
-	 * enough info in comp_ctx that it doesn't need the index. Keeping
-	 * the bit arrays for now as (a) they provide some extra debug benefit
-	 * until we have silicon and (b) they may still be needed depending on
-	 * polling implementation experiments that we need to run with real silicon.
 	 */
 	struct spdk_bit_array		*ring_slots;
-	uint32_t			max_ring_slots;
 
 	/* Lists of batches, free and in use. */
 	TAILQ_HEAD(, idxd_batch)	batch_pool;
@@ -164,9 +157,10 @@ struct idxd_comp {
 	void				*cb_arg;
 	spdk_idxd_req_cb		cb_fn;
 	struct idxd_batch		*batch;
-	bool				batch_op;
 	struct idxd_hw_desc		*desc;
+	uint32_t			*crc_dst;
 	uint32_t			index;
+	bool				batch_op;
 	char				pad[3];
 	TAILQ_ENTRY(idxd_comp)		link;
 };
@@ -178,26 +172,39 @@ struct idxd_wq {
 	union idxd_wqcfg		wqcfg;
 };
 
+struct spdk_idxd_impl {
+	const char *name;
+	void (*set_config)(struct device_config *g_dev_cfg, uint32_t config_num);
+	int (*probe)(void *cb_ctx, spdk_idxd_attach_cb attach_cb);
+	void (*destruct)(struct spdk_idxd_device *idxd);
+	uint64_t (*read_8)(struct spdk_idxd_device *idxd, void *portal, uint32_t offset);
+	char *(*portal_get_addr)(struct spdk_idxd_device *idxd);
+	/* It is a workround for simulator */
+	bool (*nop_check)(struct spdk_idxd_device *idxd);
+
+	STAILQ_ENTRY(spdk_idxd_impl) link;
+};
+
 struct spdk_idxd_device {
-	struct spdk_pci_device		*device;
-	void				*reg_base;
+	struct spdk_idxd_impl		*impl;
 	void				*portals;
-	int				socket_id;
 	int				wq_id;
 	uint32_t			num_channels;
-	bool				needs_rebalance;
+	uint32_t			total_wq_size;
 	pthread_mutex_t			num_channels_lock;
 
-	struct idxd_registers		registers;
-	uint32_t			ims_offset;
-	uint32_t			msix_perm_offset;
-	uint32_t			wqcfg_offset;
-	uint32_t			grpcfg_offset;
-	uint32_t			perfmon_offset;
 	struct idxd_group		*groups;
 	struct idxd_wq			*queues;
 };
 
+void idxd_impl_register(struct spdk_idxd_impl *impl);
+
+#define SPDK_IDXD_IMPL_REGISTER(name, impl) \
+static void __attribute__((constructor)) idxd_impl_register_##name(void) \
+{ \
+	idxd_impl_register(impl); \
+}
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/lib/idxd/idxd_user.c b/lib/idxd/idxd_user.c
new file mode 100644
index 000000000..3f89db5f8
--- /dev/null
+++ b/lib/idxd/idxd_user.c
@@ -0,0 +1,571 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/stdinc.h"
+
+#include "spdk/env.h"
+#include "spdk/util.h"
+#include "spdk/memory.h"
+#include "spdk/likely.h"
+
+#include "spdk/log.h"
+#include "spdk_internal/idxd.h"
+
+#include "idxd.h"
+
+struct spdk_user_idxd_device {
+	struct spdk_idxd_device	idxd;
+	struct spdk_pci_device	*device;
+	int			sock_id;
+	struct idxd_registers	registers;
+	void			*reg_base;
+	uint32_t		wqcfg_offset;
+	uint32_t		grpcfg_offset;
+	uint32_t		ims_offset;
+	uint32_t		msix_perm_offset;
+	uint32_t		perfmon_offset;
+};
+
+typedef bool (*spdk_idxd_probe_cb)(void *cb_ctx, struct spdk_pci_device *pci_dev);
+
+#define __user_idxd(idxd) (struct spdk_user_idxd_device *)idxd
+
+pthread_mutex_t	g_driver_lock = PTHREAD_MUTEX_INITIALIZER;
+static struct device_config g_user_dev_cfg = {};
+
+static struct spdk_idxd_device *idxd_attach(struct spdk_pci_device *device);
+
+static uint32_t
+_idxd_read_4(struct spdk_idxd_device *idxd, uint32_t offset)
+{
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	return spdk_mmio_read_4((uint32_t *)(user_idxd->reg_base + offset));
+}
+
+static void
+_idxd_write_4(struct spdk_idxd_device *idxd, uint32_t offset, uint32_t value)
+{
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	spdk_mmio_write_4((uint32_t *)(user_idxd->reg_base + offset), value);
+}
+
+static uint64_t
+_idxd_read_8(struct spdk_idxd_device *idxd, uint32_t offset)
+{
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	return spdk_mmio_read_8((uint64_t *)(user_idxd->reg_base + offset));
+}
+
+static uint64_t
+idxd_read_8(struct spdk_idxd_device *idxd, void *portal, uint32_t offset)
+{
+	return _idxd_read_8(idxd, offset);
+}
+
+static void
+_idxd_write_8(struct spdk_idxd_device *idxd, uint32_t offset, uint64_t value)
+{
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	spdk_mmio_write_8((uint64_t *)(user_idxd->reg_base + offset), value);
+}
+
+static void
+user_idxd_set_config(struct device_config *dev_cfg, uint32_t config_num)
+{
+	g_user_dev_cfg = *dev_cfg;
+}
+
+/* Used for control commands, not for descriptor submission. */
+static int
+idxd_wait_cmd(struct spdk_idxd_device *idxd, int _timeout)
+{
+	uint32_t timeout = _timeout;
+	union idxd_cmdsts_reg cmd_status = {};
+
+	cmd_status.raw = _idxd_read_4(idxd, IDXD_CMDSTS_OFFSET);
+	while (cmd_status.active && --timeout) {
+		usleep(1);
+		cmd_status.raw = _idxd_read_4(idxd, IDXD_CMDSTS_OFFSET);
+	}
+
+	/* Check for timeout */
+	if (timeout == 0 && cmd_status.active) {
+		SPDK_ERRLOG("Command timeout, waited %u\n", _timeout);
+		return -EBUSY;
+	}
+
+	/* Check for error */
+	if (cmd_status.err) {
+		SPDK_ERRLOG("Command status reg reports error 0x%x\n", cmd_status.err);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
+idxd_unmap_pci_bar(struct spdk_idxd_device *idxd, int bar)
+{
+	int rc = 0;
+	void *addr = NULL;
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	if (bar == IDXD_MMIO_BAR) {
+		addr = (void *)user_idxd->reg_base;
+	} else if (bar == IDXD_WQ_BAR) {
+		addr = (void *)idxd->portals;
+	}
+
+	if (addr) {
+		rc = spdk_pci_device_unmap_bar(user_idxd->device, 0, addr);
+	}
+	return rc;
+}
+
+static int
+idxd_map_pci_bars(struct spdk_idxd_device *idxd)
+{
+	int rc;
+	void *addr;
+	uint64_t phys_addr, size;
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	rc = spdk_pci_device_map_bar(user_idxd->device, IDXD_MMIO_BAR, &addr, &phys_addr, &size);
+	if (rc != 0 || addr == NULL) {
+		SPDK_ERRLOG("pci_device_map_range failed with error code %d\n", rc);
+		return -1;
+	}
+	user_idxd->reg_base = addr;
+
+	rc = spdk_pci_device_map_bar(user_idxd->device, IDXD_WQ_BAR, &addr, &phys_addr, &size);
+	if (rc != 0 || addr == NULL) {
+		SPDK_ERRLOG("pci_device_map_range failed with error code %d\n", rc);
+		rc = idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
+		if (rc) {
+			SPDK_ERRLOG("unable to unmap MMIO bar\n");
+		}
+		return -EINVAL;
+	}
+	idxd->portals = addr;
+
+	return 0;
+}
+
+static int
+idxd_reset_dev(struct spdk_idxd_device *idxd)
+{
+	int rc;
+
+	_idxd_write_4(idxd, IDXD_CMD_OFFSET, IDXD_RESET_DEVICE << IDXD_CMD_SHIFT);
+	rc = idxd_wait_cmd(idxd, IDXD_REGISTER_TIMEOUT_US);
+	if (rc < 0) {
+		SPDK_ERRLOG("Error resetting device %u\n", rc);
+	}
+
+	return rc;
+}
+
+/*
+ * Build group config based on getting info from the device combined
+ * with the defined configuration. Once built, it is written to the
+ * device.
+ */
+static int
+idxd_group_config(struct spdk_idxd_device *idxd)
+{
+	int i;
+	uint64_t base_offset;
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	assert(g_user_dev_cfg.num_groups <= user_idxd->registers.groupcap.num_groups);
+	idxd->groups = calloc(user_idxd->registers.groupcap.num_groups, sizeof(struct idxd_group));
+	if (idxd->groups == NULL) {
+		SPDK_ERRLOG("Failed to allocate group memory\n");
+		return -ENOMEM;
+	}
+
+	assert(g_user_dev_cfg.total_engines <= user_idxd->registers.enginecap.num_engines);
+	for (i = 0; i < g_user_dev_cfg.total_engines; i++) {
+		idxd->groups[i % g_user_dev_cfg.num_groups].grpcfg.engines |= (1 << i);
+	}
+
+	assert(g_user_dev_cfg.total_wqs <= user_idxd->registers.wqcap.num_wqs);
+	for (i = 0; i < g_user_dev_cfg.total_wqs; i++) {
+		idxd->groups[i % g_user_dev_cfg.num_groups].grpcfg.wqs[0] |= (1 << i);
+	}
+
+	for (i = 0; i < g_user_dev_cfg.num_groups; i++) {
+		idxd->groups[i].idxd = idxd;
+		idxd->groups[i].id = i;
+
+		/* Divide BW tokens evenly */
+		idxd->groups[i].grpcfg.flags.tokens_allowed =
+			user_idxd->registers.groupcap.total_tokens / g_user_dev_cfg.num_groups;
+	}
+
+	/*
+	 * Now write the group config to the device for all groups. We write
+	 * to the max number of groups in order to 0 out the ones we didn't
+	 * configure.
+	 */
+	for (i = 0 ; i < user_idxd->registers.groupcap.num_groups; i++) {
+
+		base_offset = user_idxd->grpcfg_offset + i * 64;
+
+		/* GRPWQCFG, work queues config */
+		_idxd_write_8(idxd, base_offset, idxd->groups[i].grpcfg.wqs[0]);
+
+		/* GRPENGCFG, engine config */
+		_idxd_write_8(idxd, base_offset + CFG_ENGINE_OFFSET, idxd->groups[i].grpcfg.engines);
+
+		/* GRPFLAGS, flags config */
+		_idxd_write_8(idxd, base_offset + CFG_FLAG_OFFSET, idxd->groups[i].grpcfg.flags.raw);
+	}
+
+	return 0;
+}
+
+/*
+ * Build work queue (WQ) config based on getting info from the device combined
+ * with the defined configuration. Once built, it is written to the device.
+ */
+static int
+idxd_wq_config(struct spdk_user_idxd_device *user_idxd)
+{
+	int i, j;
+	struct idxd_wq *queue;
+	struct spdk_idxd_device *idxd = &user_idxd->idxd;
+	u_int32_t wq_size = user_idxd->registers.wqcap.total_wq_size / g_user_dev_cfg.total_wqs;
+
+	SPDK_NOTICELOG("Total ring slots available space 0x%x, so per work queue is 0x%x\n",
+		       user_idxd->registers.wqcap.total_wq_size, wq_size);
+	assert(g_user_dev_cfg.total_wqs <= IDXD_MAX_QUEUES);
+	assert(g_user_dev_cfg.total_wqs <= user_idxd->registers.wqcap.num_wqs);
+	assert(LOG2_WQ_MAX_BATCH <= user_idxd->registers.gencap.max_batch_shift);
+	assert(LOG2_WQ_MAX_XFER <= user_idxd->registers.gencap.max_xfer_shift);
+
+	idxd->total_wq_size = user_idxd->registers.wqcap.total_wq_size;
+	idxd->queues = calloc(1, user_idxd->registers.wqcap.num_wqs * sizeof(struct idxd_wq));
+	if (idxd->queues == NULL) {
+		SPDK_ERRLOG("Failed to allocate queue memory\n");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < g_user_dev_cfg.total_wqs; i++) {
+		queue = &user_idxd->idxd.queues[i];
+		queue->wqcfg.wq_size = wq_size;
+		queue->wqcfg.mode = WQ_MODE_DEDICATED;
+		queue->wqcfg.max_batch_shift = LOG2_WQ_MAX_BATCH;
+		queue->wqcfg.max_xfer_shift = LOG2_WQ_MAX_XFER;
+		queue->wqcfg.wq_state = WQ_ENABLED;
+		queue->wqcfg.priority = WQ_PRIORITY_1;
+
+		/* Not part of the config struct */
+		queue->idxd = &user_idxd->idxd;
+		queue->group = &idxd->groups[i % g_user_dev_cfg.num_groups];
+	}
+
+	/*
+	 * Now write the work queue config to the device for all wq space
+	 */
+	for (i = 0 ; i < user_idxd->registers.wqcap.num_wqs; i++) {
+		queue = &idxd->queues[i];
+		for (j = 0 ; j < WQCFG_NUM_DWORDS; j++) {
+			_idxd_write_4(idxd, user_idxd->wqcfg_offset + i * 32 + j * 4,
+				      queue->wqcfg.raw[j]);
+		}
+	}
+
+	return 0;
+}
+
+static int
+idxd_device_configure(struct spdk_user_idxd_device *user_idxd)
+{
+	int i, rc = 0;
+	union idxd_offsets_register offsets_reg;
+	union idxd_genstatus_register genstatus_reg;
+	struct spdk_idxd_device *idxd = &user_idxd->idxd;
+
+	/*
+	 * Map BAR0 and BAR2
+	 */
+	rc = idxd_map_pci_bars(idxd);
+	if (rc) {
+		return rc;
+	}
+
+	/*
+	 * Reset the device
+	 */
+	rc = idxd_reset_dev(idxd);
+	if (rc) {
+		goto err_reset;
+	}
+
+	/*
+	 * Read in config registers
+	 */
+	user_idxd->registers.version = _idxd_read_4(idxd, IDXD_VERSION_OFFSET);
+	user_idxd->registers.gencap.raw = _idxd_read_8(idxd, IDXD_GENCAP_OFFSET);
+	user_idxd->registers.wqcap.raw = _idxd_read_8(idxd, IDXD_WQCAP_OFFSET);
+	user_idxd->registers.groupcap.raw = _idxd_read_8(idxd, IDXD_GRPCAP_OFFSET);
+	user_idxd->registers.enginecap.raw = _idxd_read_8(idxd, IDXD_ENGCAP_OFFSET);
+	for (i = 0; i < IDXD_OPCAP_WORDS; i++) {
+		user_idxd->registers.opcap.raw[i] =
+			_idxd_read_8(idxd, i * sizeof(uint64_t) + IDXD_OPCAP_OFFSET);
+	}
+	offsets_reg.raw[0] = _idxd_read_8(idxd, IDXD_TABLE_OFFSET);
+	offsets_reg.raw[1] = _idxd_read_8(idxd, IDXD_TABLE_OFFSET + sizeof(uint64_t));
+	user_idxd->grpcfg_offset = offsets_reg.grpcfg * IDXD_TABLE_OFFSET_MULT;
+	user_idxd->wqcfg_offset = offsets_reg.wqcfg * IDXD_TABLE_OFFSET_MULT;
+	user_idxd->ims_offset = offsets_reg.ims * IDXD_TABLE_OFFSET_MULT;
+	user_idxd->msix_perm_offset = offsets_reg.msix_perm  * IDXD_TABLE_OFFSET_MULT;
+	user_idxd->perfmon_offset = offsets_reg.perfmon * IDXD_TABLE_OFFSET_MULT;
+
+	/*
+	 * Configure groups and work queues.
+	 */
+	rc = idxd_group_config(idxd);
+	if (rc) {
+		goto err_group_cfg;
+	}
+
+	rc = idxd_wq_config(user_idxd);
+	if (rc) {
+		goto err_wq_cfg;
+	}
+
+	/*
+	 * Enable the device
+	 */
+	genstatus_reg.raw = _idxd_read_4(idxd, IDXD_GENSTATUS_OFFSET);
+	assert(genstatus_reg.state == IDXD_DEVICE_STATE_DISABLED);
+
+	_idxd_write_4(idxd, IDXD_CMD_OFFSET, IDXD_ENABLE_DEV << IDXD_CMD_SHIFT);
+	rc = idxd_wait_cmd(idxd, IDXD_REGISTER_TIMEOUT_US);
+	genstatus_reg.raw = _idxd_read_4(idxd, IDXD_GENSTATUS_OFFSET);
+	if ((rc < 0) || (genstatus_reg.state != IDXD_DEVICE_STATE_ENABLED)) {
+		rc = -EINVAL;
+		SPDK_ERRLOG("Error enabling device %u\n", rc);
+		goto err_device_enable;
+	}
+
+	genstatus_reg.raw = spdk_mmio_read_4((uint32_t *)(user_idxd->reg_base + IDXD_GENSTATUS_OFFSET));
+	assert(genstatus_reg.state == IDXD_DEVICE_STATE_ENABLED);
+
+	/*
+	 * Enable the work queues that we've configured
+	 */
+	for (i = 0; i < g_user_dev_cfg.total_wqs; i++) {
+		_idxd_write_4(idxd, IDXD_CMD_OFFSET,
+			      (IDXD_ENABLE_WQ << IDXD_CMD_SHIFT) | i);
+		rc = idxd_wait_cmd(idxd, IDXD_REGISTER_TIMEOUT_US);
+		if (rc < 0) {
+			SPDK_ERRLOG("Error enabling work queues 0x%x\n", rc);
+			goto err_wq_enable;
+		}
+	}
+
+	if ((rc == 0) && (genstatus_reg.state == IDXD_DEVICE_STATE_ENABLED)) {
+		SPDK_NOTICELOG("Device enabled, version 0x%x gencap: 0x%lx\n",
+			       user_idxd->registers.version,
+			       user_idxd->registers.gencap.raw);
+
+	}
+
+	return rc;
+err_wq_enable:
+err_device_enable:
+	free(idxd->queues);
+err_wq_cfg:
+	free(idxd->groups);
+err_group_cfg:
+err_reset:
+	idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
+	idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
+
+	return rc;
+}
+
+static void
+user_idxd_device_destruct(struct spdk_idxd_device *idxd)
+{
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	idxd_unmap_pci_bar(idxd, IDXD_MMIO_BAR);
+	idxd_unmap_pci_bar(idxd, IDXD_WQ_BAR);
+	free(idxd->groups);
+	free(idxd->queues);
+
+	spdk_pci_device_detach(user_idxd->device);
+	free(user_idxd);
+}
+
+struct idxd_enum_ctx {
+	spdk_idxd_probe_cb probe_cb;
+	spdk_idxd_attach_cb attach_cb;
+	void *cb_ctx;
+};
+
+/* This function must only be called while holding g_driver_lock */
+static int
+idxd_enum_cb(void *ctx, struct spdk_pci_device *pci_dev)
+{
+	struct idxd_enum_ctx *enum_ctx = ctx;
+	struct spdk_idxd_device *idxd;
+
+	if (enum_ctx->probe_cb(enum_ctx->cb_ctx, pci_dev)) {
+		idxd = idxd_attach(pci_dev);
+		if (idxd == NULL) {
+			SPDK_ERRLOG("idxd_attach() failed\n");
+			return -EINVAL;
+		}
+
+		enum_ctx->attach_cb(enum_ctx->cb_ctx, idxd);
+	}
+
+	return 0;
+}
+
+
+static bool
+probe_cb(void *cb_ctx, struct spdk_pci_device *pci_dev)
+{
+	struct spdk_pci_addr pci_addr = spdk_pci_device_get_addr(pci_dev);
+
+	SPDK_NOTICELOG(
+		" Found matching device at %04x:%02x:%02x.%x vendor:0x%04x device:0x%04x\n",
+		pci_addr.domain,
+		pci_addr.bus,
+		pci_addr.dev,
+		pci_addr.func,
+		spdk_pci_device_get_vendor_id(pci_dev),
+		spdk_pci_device_get_device_id(pci_dev));
+
+	/* Claim the device in case conflict with other process */
+	if (spdk_pci_device_claim(pci_dev) < 0) {
+		return false;
+	}
+
+	return true;
+}
+
+static int
+user_idxd_probe(void *cb_ctx, spdk_idxd_attach_cb attach_cb)
+{
+	int rc;
+	struct idxd_enum_ctx enum_ctx;
+
+	enum_ctx.probe_cb = probe_cb;
+	enum_ctx.attach_cb = attach_cb;
+	enum_ctx.cb_ctx = cb_ctx;
+
+	pthread_mutex_lock(&g_driver_lock);
+	rc = spdk_pci_enumerate(spdk_pci_idxd_get_driver(), idxd_enum_cb, &enum_ctx);
+	pthread_mutex_unlock(&g_driver_lock);
+
+	return rc;
+}
+
+static char *
+user_idxd_portal_get_addr(struct spdk_idxd_device *idxd)
+{
+	return (char *)idxd->portals + idxd->wq_id * PORTAL_SIZE;
+}
+
+static bool
+user_idxd_nop_check(struct spdk_idxd_device *idxd)
+{
+	struct spdk_user_idxd_device *user_idxd = __user_idxd(idxd);
+
+	/* TODO: temp workaround for simulator.  Remove this function when fixed or w/silicon. */
+	if (user_idxd->registers.gencap.raw == 0x1833f011f) {
+		return true;
+	}
+
+	return false;
+}
+
+static struct spdk_idxd_impl g_user_idxd_impl = {
+	.name			= "user",
+	.set_config		= user_idxd_set_config,
+	.probe			= user_idxd_probe,
+	.destruct		= user_idxd_device_destruct,
+	.read_8			= idxd_read_8,
+	.portal_get_addr	= user_idxd_portal_get_addr,
+	.nop_check		= user_idxd_nop_check,
+};
+
+/* Caller must hold g_driver_lock */
+static struct spdk_idxd_device *
+idxd_attach(struct spdk_pci_device *device)
+{
+	struct spdk_user_idxd_device *user_idxd;
+	struct spdk_idxd_device *idxd;
+	uint32_t cmd_reg;
+	int rc;
+
+	user_idxd = calloc(1, sizeof(struct spdk_user_idxd_device));
+	if (user_idxd == NULL) {
+		SPDK_ERRLOG("Failed to allocate memory for user_idxd device.\n");
+		return NULL;
+	}
+
+	idxd = &user_idxd->idxd;
+	user_idxd->device = device;
+	idxd->impl = &g_user_idxd_impl;
+	pthread_mutex_init(&idxd->num_channels_lock, NULL);
+
+	/* Enable PCI busmaster. */
+	spdk_pci_device_cfg_read32(device, &cmd_reg, 4);
+	cmd_reg |= 0x4;
+	spdk_pci_device_cfg_write32(device, cmd_reg, 4);
+
+	rc = idxd_device_configure(user_idxd);
+	if (rc) {
+		goto err;
+	}
+
+	return idxd;
+err:
+	user_idxd_device_destruct(idxd);
+	return NULL;
+}
+
+SPDK_IDXD_IMPL_REGISTER(user, &g_user_idxd_impl);
diff --git a/lib/idxd/spdk_idxd.map b/lib/idxd/spdk_idxd.map
index 4f9b7c555..af3755dc3 100644
--- a/lib/idxd/spdk_idxd.map
+++ b/lib/idxd/spdk_idxd.map
@@ -2,15 +2,15 @@
 	global:
 
 	# public functions
-	spdk_idxd_device_needs_rebalance;
+	spdk_idxd_chan_get_max_operations;
 	spdk_idxd_configure_chan;
-	spdk_idxd_reconfigure_chan;
 	spdk_idxd_probe;
 	spdk_idxd_detach;
 	spdk_idxd_batch_prep_copy;
 	spdk_idxd_batch_prep_dualcast;
 	spdk_idxd_batch_prep_fill;
 	spdk_idxd_batch_prep_crc32c;
+	spdk_idxd_batch_prep_copy_crc32c;
 	spdk_idxd_batch_prep_compare;
 	spdk_idxd_batch_submit;
 	spdk_idxd_batch_create;
@@ -19,6 +19,7 @@
 	spdk_idxd_set_config;
 	spdk_idxd_submit_compare;
 	spdk_idxd_submit_crc32c;
+	spdk_idxd_submit_copy_crc32c;
 	spdk_idxd_submit_copy;
 	spdk_idxd_submit_dualcast;
 	spdk_idxd_submit_fill;
diff --git a/lib/init/Makefile b/lib/init/Makefile
new file mode 100644
index 000000000..0e852bd67
--- /dev/null
+++ b/lib/init/Makefile
@@ -0,0 +1,45 @@
+#
+#  BSD LICENSE
+#
+#  Copyright (c) Intel Corporation.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Intel Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
+include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
+
+SO_VER := 1
+SO_MINOR := 0
+
+C_SRCS = json_config.c subsystem.c subsystem_rpc.c rpc.c
+LIBNAME = init
+
+SPDK_MAP_FILE = $(abspath $(CURDIR)/spdk_init.map)
+
+include $(SPDK_ROOT_DIR)/mk/spdk.lib.mk
diff --git a/lib/event/json_config.c b/lib/init/json_config.c
similarity index 98%
rename from lib/event/json_config.c
rename to lib/init/json_config.c
index da085846a..2c942a80f 100644
--- a/lib/event/json_config.c
+++ b/lib/init/json_config.c
@@ -33,6 +33,7 @@
 
 #include "spdk/stdinc.h"
 
+#include "spdk/init.h"
 #include "spdk/util.h"
 #include "spdk/file.h"
 #include "spdk/log.h"
@@ -566,9 +567,9 @@ err:
 }
 
 void
-spdk_app_json_config_load(const char *json_config_file, const char *rpc_addr,
-			  spdk_subsystem_init_fn cb_fn, void *cb_arg,
-			  bool stop_on_error)
+spdk_subsystem_init_from_json_config(const char *json_config_file, const char *rpc_addr,
+				     spdk_subsystem_init_fn cb_fn, void *cb_arg,
+				     bool stop_on_error)
 {
 	struct load_json_config_ctx *ctx = calloc(1, sizeof(*ctx));
 	int rc;
diff --git a/lib/event/rpc.c b/lib/init/rpc.c
similarity index 98%
rename from lib/event/rpc.c
rename to lib/init/rpc.c
index 62c8d5bb2..2adaf8bde 100644
--- a/lib/event/rpc.c
+++ b/lib/init/rpc.c
@@ -34,12 +34,11 @@
 #include "spdk/stdinc.h"
 
 #include "spdk/env.h"
+#include "spdk/init.h"
 #include "spdk/thread.h"
 #include "spdk/log.h"
 #include "spdk/rpc.h"
 
-#include "spdk_internal/event.h"
-
 #define RPC_SELECT_INTERVAL	4000 /* 4ms */
 
 static struct spdk_poller *g_rpc_poller = NULL;
diff --git a/lib/init/spdk_init.map b/lib/init/spdk_init.map
new file mode 100644
index 000000000..c6061c458
--- /dev/null
+++ b/lib/init/spdk_init.map
@@ -0,0 +1,17 @@
+{
+	global:
+
+	# Public functions
+	spdk_add_subsystem;
+	spdk_add_subsystem_depend;
+	spdk_subsystem_init;
+	spdk_subsystem_fini;
+	spdk_subsystem_init_next;
+	spdk_subsystem_fini_next;
+	spdk_subsystem_init_from_json_config;
+
+	spdk_rpc_initialize;
+	spdk_rpc_finish;
+
+	local: *;
+};
diff --git a/lib/event/subsystem.c b/lib/init/subsystem.c
similarity index 93%
rename from lib/event/subsystem.c
rename to lib/init/subsystem.c
index a4f7092ea..60fa8613e 100644
--- a/lib/event/subsystem.c
+++ b/lib/init/subsystem.c
@@ -33,12 +33,18 @@
 
 #include "spdk/stdinc.h"
 
+#include "spdk/init.h"
 #include "spdk/log.h"
+#include "spdk/queue.h"
 #include "spdk/thread.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 #include "spdk/env.h"
 
+#include "spdk/json.h"
+
+#include "subsystem.h"
+
 TAILQ_HEAD(spdk_subsystem_list, spdk_subsystem);
 struct spdk_subsystem_list g_subsystems = TAILQ_HEAD_INITIALIZER(g_subsystems);
 
@@ -80,32 +86,32 @@ _subsystem_find(struct spdk_subsystem_list *list, const char *name)
 }
 
 struct spdk_subsystem *
-spdk_subsystem_find(const char *name)
+subsystem_find(const char *name)
 {
 	return _subsystem_find(&g_subsystems, name);
 }
 
 struct spdk_subsystem *
-spdk_subsystem_get_first(void)
+subsystem_get_first(void)
 {
 	return TAILQ_FIRST(&g_subsystems);
 }
 
 struct spdk_subsystem *
-spdk_subsystem_get_next(struct spdk_subsystem *cur_subsystem)
+subsystem_get_next(struct spdk_subsystem *cur_subsystem)
 {
 	return TAILQ_NEXT(cur_subsystem, tailq);
 }
 
 
 struct spdk_subsystem_depend *
-spdk_subsystem_get_first_depend(void)
+subsystem_get_first_depend(void)
 {
 	return TAILQ_FIRST(&g_subsystems_deps);
 }
 
 struct spdk_subsystem_depend *
-spdk_subsystem_get_next_depend(struct spdk_subsystem_depend *cur_depend)
+subsystem_get_next_depend(struct spdk_subsystem_depend *cur_depend)
 {
 	return TAILQ_NEXT(cur_depend, tailq);
 }
@@ -194,12 +200,12 @@ spdk_subsystem_init(spdk_subsystem_init_fn cb_fn, void *cb_arg)
 
 	/* Verify that all dependency name and depends_on subsystems are registered */
 	TAILQ_FOREACH(dep, &g_subsystems_deps, tailq) {
-		if (!spdk_subsystem_find(dep->name)) {
+		if (!subsystem_find(dep->name)) {
 			SPDK_ERRLOG("subsystem %s is missing\n", dep->name);
 			g_subsystem_start_fn(-1, g_subsystem_start_arg);
 			return;
 		}
-		if (!spdk_subsystem_find(dep->depends_on)) {
+		if (!subsystem_find(dep->depends_on)) {
 			SPDK_ERRLOG("subsystem %s dependency %s is missing\n",
 				    dep->name, dep->depends_on);
 			g_subsystem_start_fn(-1, g_subsystem_start_arg);
@@ -266,7 +272,7 @@ spdk_subsystem_fini(spdk_msg_fn cb_fn, void *cb_arg)
 }
 
 void
-spdk_subsystem_config_json(struct spdk_json_write_ctx *w, struct spdk_subsystem *subsystem)
+subsystem_config_json(struct spdk_json_write_ctx *w, struct spdk_subsystem *subsystem)
 {
 	if (subsystem && subsystem->write_config_json) {
 		subsystem->write_config_json(w);
diff --git a/lib/init/subsystem.h b/lib/init/subsystem.h
new file mode 100644
index 000000000..d30d5b9cf
--- /dev/null
+++ b/lib/init/subsystem.h
@@ -0,0 +1,53 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.  All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_SUBSYSTEM_H
+#define SPDK_SUBSYSTEM_H
+
+struct spdk_subsystem *subsystem_find(const char *name);
+struct spdk_subsystem *subsystem_get_first(void);
+struct spdk_subsystem *subsystem_get_next(struct spdk_subsystem *cur_subsystem);
+
+struct spdk_subsystem_depend *subsystem_get_first_depend(void);
+struct spdk_subsystem_depend *subsystem_get_next_depend(struct spdk_subsystem_depend
+		*cur_depend);
+
+/**
+ * Save pointed \c subsystem configuration to the JSON write context \c w. In case of
+ * error \c null is written to the JSON context.
+ *
+ * \param w JSON write context
+ * \param subsystem the subsystem to query
+ */
+void subsystem_config_json(struct spdk_json_write_ctx *w, struct spdk_subsystem *subsystem);
+
+#endif
diff --git a/lib/event/subsystem_rpc.c b/lib/init/subsystem_rpc.c
similarity index 92%
rename from lib/event/subsystem_rpc.c
rename to lib/init/subsystem_rpc.c
index 293493afa..3ff7fb376 100644
--- a/lib/event/subsystem_rpc.c
+++ b/lib/init/subsystem_rpc.c
@@ -31,12 +31,15 @@
  *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-#include "spdk_internal/event.h"
 #include "spdk/rpc.h"
 #include "spdk/string.h"
 #include "spdk/util.h"
 #include "spdk/env.h"
 
+#include "spdk_internal/init.h"
+
+#include "subsystem.h"
+
 static void
 rpc_framework_get_subsystems(struct spdk_jsonrpc_request *request,
 			     const struct spdk_json_val *params)
@@ -53,22 +56,22 @@ rpc_framework_get_subsystems(struct spdk_jsonrpc_request *request,
 
 	w = spdk_jsonrpc_begin_result(request);
 	spdk_json_write_array_begin(w);
-	subsystem = spdk_subsystem_get_first();
+	subsystem = subsystem_get_first();
 	while (subsystem != NULL) {
 		spdk_json_write_object_begin(w);
 
 		spdk_json_write_named_string(w, "subsystem", subsystem->name);
 		spdk_json_write_named_array_begin(w, "depends_on");
-		deps = spdk_subsystem_get_first_depend();
+		deps = subsystem_get_first_depend();
 		while (deps != NULL) {
 			if (strcmp(subsystem->name, deps->name) == 0) {
 				spdk_json_write_string(w, deps->depends_on);
 			}
-			deps = spdk_subsystem_get_next_depend(deps);
+			deps = subsystem_get_next_depend(deps);
 		}
 		spdk_json_write_array_end(w);
 		spdk_json_write_object_end(w);
-		subsystem = spdk_subsystem_get_next(subsystem);
+		subsystem = subsystem_get_next(subsystem);
 	}
 	spdk_json_write_array_end(w);
 	spdk_jsonrpc_end_result(request, w);
@@ -99,7 +102,7 @@ rpc_framework_get_config(struct spdk_jsonrpc_request *request,
 		return;
 	}
 
-	subsystem = spdk_subsystem_find(req.name);
+	subsystem = subsystem_find(req.name);
 	if (!subsystem) {
 		spdk_jsonrpc_send_error_response_fmt(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS,
 						     "Subsystem '%s' not found", req.name);
@@ -110,7 +113,7 @@ rpc_framework_get_config(struct spdk_jsonrpc_request *request,
 	free(req.name);
 
 	w = spdk_jsonrpc_begin_result(request);
-	spdk_subsystem_config_json(w, subsystem);
+	subsystem_config_json(w, subsystem);
 	spdk_jsonrpc_end_result(request, w);
 }
 
diff --git a/lib/ioat/ioat.c b/lib/ioat/ioat.c
index 27ac0a0ed..af83c42be 100644
--- a/lib/ioat/ioat.c
+++ b/lib/ioat/ioat.c
@@ -429,7 +429,6 @@ ioat_channel_start(struct spdk_ioat_chan *ioat)
 
 	comp_update_bus_addr = spdk_vtophys((void *)ioat->comp_update, NULL);
 	if (comp_update_bus_addr == SPDK_VTOPHYS_ERROR) {
-		spdk_free((void *)ioat->comp_update);
 		return -1;
 	}
 
diff --git a/lib/iscsi/conn.c b/lib/iscsi/conn.c
index 4cbee1358..21ae8b595 100644
--- a/lib/iscsi/conn.c
+++ b/lib/iscsi/conn.c
@@ -1212,7 +1212,7 @@ iscsi_task_cpl(struct spdk_scsi_task *scsi_task)
 	struct spdk_iscsi_conn *conn = task->conn;
 	struct spdk_iscsi_pdu *pdu = task->pdu;
 
-	spdk_trace_record(TRACE_ISCSI_TASK_DONE, conn->id, 0, (uintptr_t)task, 0);
+	spdk_trace_record(TRACE_ISCSI_TASK_DONE, conn->id, 0, (uintptr_t)task);
 
 	task->is_queued = false;
 	primary = iscsi_task_get_primary(task);
@@ -1223,7 +1223,7 @@ iscsi_task_cpl(struct spdk_scsi_task *scsi_task)
 		process_non_read_task_completion(conn, task, primary);
 	}
 	if (!task->parent) {
-		spdk_trace_record(TRACE_ISCSI_PDU_COMPLETED, 0, 0, (uintptr_t)pdu, 0);
+		spdk_trace_record(TRACE_ISCSI_PDU_COMPLETED, 0, 0, (uintptr_t)pdu);
 	}
 }
 
@@ -1321,7 +1321,7 @@ iscsi_conn_read_data(struct spdk_iscsi_conn *conn, int bytes,
 	ret = spdk_sock_recv(conn->sock, buf, bytes);
 
 	if (ret > 0) {
-		spdk_trace_record(TRACE_ISCSI_READ_FROM_SOCKET_DONE, conn->id, ret, 0, 0);
+		spdk_trace_record(TRACE_ISCSI_READ_FROM_SOCKET_DONE, conn->id, ret, 0);
 		return ret;
 	}
 
@@ -1362,7 +1362,7 @@ iscsi_conn_readv_data(struct spdk_iscsi_conn *conn,
 	ret = spdk_sock_readv(conn->sock, iov, iovcnt);
 
 	if (ret > 0) {
-		spdk_trace_record(TRACE_ISCSI_READ_FROM_SOCKET_DONE, conn->id, ret, 0, 0);
+		spdk_trace_record(TRACE_ISCSI_READ_FROM_SOCKET_DONE, conn->id, ret, 0);
 		return ret;
 	}
 
@@ -1439,7 +1439,7 @@ _iscsi_conn_pdu_write_done(void *cb_arg, int err)
 	if (err != 0) {
 		conn->state = ISCSI_CONN_STATE_EXITING;
 	} else {
-		spdk_trace_record(TRACE_ISCSI_FLUSH_WRITEBUF_DONE, conn->id, pdu->mapped_length, (uintptr_t)pdu, 0);
+		spdk_trace_record(TRACE_ISCSI_FLUSH_WRITEBUF_DONE, conn->id, pdu->mapped_length, (uintptr_t)pdu);
 	}
 
 	if ((conn->full_feature) &&
@@ -1648,21 +1648,29 @@ SPDK_TRACE_REGISTER_FN(iscsi_conn_trace, "iscsi_conn", TRACE_GROUP_ISCSI)
 	spdk_trace_register_owner(OWNER_ISCSI_CONN, 'c');
 	spdk_trace_register_object(OBJECT_ISCSI_PDU, 'p');
 	spdk_trace_register_description("ISCSI_READ_DONE", TRACE_ISCSI_READ_FROM_SOCKET_DONE,
-					OWNER_ISCSI_CONN, OBJECT_NONE, 0, 0, "");
+					OWNER_ISCSI_CONN, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("ISCSI_WRITE_START", TRACE_ISCSI_FLUSH_WRITEBUF_START,
-					OWNER_ISCSI_CONN, OBJECT_NONE, 0, 0, "iovec: ");
+					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 1,
+					SPDK_TRACE_ARG_TYPE_INT, "iovec");
 	spdk_trace_register_description("ISCSI_WRITE_DONE", TRACE_ISCSI_FLUSH_WRITEBUF_DONE,
-					OWNER_ISCSI_CONN, OBJECT_NONE, 0, 0, "");
+					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("ISCSI_READ_PDU", TRACE_ISCSI_READ_PDU,
-					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 1, 0, "opc:   ");
+					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 1,
+					SPDK_TRACE_ARG_TYPE_INT, "opc");
 	spdk_trace_register_description("ISCSI_TASK_DONE", TRACE_ISCSI_TASK_DONE,
-					OWNER_ISCSI_CONN, OBJECT_SCSI_TASK, 0, 0, "");
+					OWNER_ISCSI_CONN, OBJECT_SCSI_TASK, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("ISCSI_TASK_QUEUE", TRACE_ISCSI_TASK_QUEUE,
-					OWNER_ISCSI_CONN, OBJECT_SCSI_TASK, 1, 1, "pdu:   ");
+					OWNER_ISCSI_CONN, OBJECT_SCSI_TASK, 1,
+					SPDK_TRACE_ARG_TYPE_PTR, "pdu");
 	spdk_trace_register_description("ISCSI_TASK_EXECUTED", TRACE_ISCSI_TASK_EXECUTED,
-					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 0, 0, "");
+					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("ISCSI_PDU_COMPLETED", TRACE_ISCSI_PDU_COMPLETED,
-					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 0, 0, "");
+					OWNER_ISCSI_CONN, OBJECT_ISCSI_PDU, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 }
 
 void
diff --git a/lib/iscsi/iscsi.c b/lib/iscsi/iscsi.c
index 7e54004bc..2a0b0de21 100644
--- a/lib/iscsi/iscsi.c
+++ b/lib/iscsi/iscsi.c
@@ -57,6 +57,8 @@
 
 #include "spdk/log.h"
 
+#include "spdk_internal/sgl.h"
+
 #define MAX_TMPBUF 1024
 
 #ifdef __FreeBSD__
@@ -387,49 +389,11 @@ iscsi_conn_read_data_segment(struct spdk_iscsi_conn *conn,
 	}
 }
 
-struct _iscsi_sgl {
-	struct iovec	*iov;
-	int		iovcnt;
-	uint32_t	iov_offset;
-	uint32_t	total_size;
-};
-
-static inline void
-_iscsi_sgl_init(struct _iscsi_sgl *s, struct iovec *iovs, int iovcnt,
-		uint32_t iov_offset)
-{
-	s->iov = iovs;
-	s->iovcnt = iovcnt;
-	s->iov_offset = iov_offset;
-	s->total_size = 0;
-}
-
-static inline bool
-_iscsi_sgl_append(struct _iscsi_sgl *s, uint8_t *data, uint32_t data_len)
-{
-	if (s->iov_offset >= data_len) {
-		s->iov_offset -= data_len;
-	} else {
-		assert(s->iovcnt > 0);
-		s->iov->iov_base = data + s->iov_offset;
-		s->iov->iov_len = data_len - s->iov_offset;
-		s->total_size += data_len - s->iov_offset;
-		s->iov_offset = 0;
-		s->iov++;
-		s->iovcnt--;
-		if (s->iovcnt == 0) {
-			return false;
-		}
-	}
-
-	return true;
-}
-
 /* Build iovec array to leave metadata space for every data block
  * when reading data segment from socket.
  */
 static inline bool
-_iscsi_sgl_append_with_md(struct _iscsi_sgl *s,
+_iscsi_sgl_append_with_md(struct spdk_iov_sgl *s,
 			  void *buf, uint32_t buf_len, uint32_t data_len,
 			  struct spdk_dif_ctx *dif_ctx)
 {
@@ -468,7 +432,7 @@ int
 iscsi_build_iovs(struct spdk_iscsi_conn *conn, struct iovec *iovs, int iovcnt,
 		 struct spdk_iscsi_pdu *pdu, uint32_t *_mapped_length)
 {
-	struct _iscsi_sgl sgl;
+	struct spdk_iov_sgl sgl;
 	int enable_digest;
 	uint32_t total_ahs_len;
 	uint32_t data_len;
@@ -487,22 +451,22 @@ iscsi_build_iovs(struct spdk_iscsi_conn *conn, struct iovec *iovs, int iovcnt,
 		enable_digest = 0;
 	}
 
-	_iscsi_sgl_init(&sgl, iovs, iovcnt, pdu->writev_offset);
+	spdk_iov_sgl_init(&sgl, iovs, iovcnt, pdu->writev_offset);
 
 	/* BHS */
-	if (!_iscsi_sgl_append(&sgl, (uint8_t *)&pdu->bhs, ISCSI_BHS_LEN)) {
+	if (!spdk_iov_sgl_append(&sgl, (uint8_t *)&pdu->bhs, ISCSI_BHS_LEN)) {
 		goto end;
 	}
 	/* AHS */
 	if (total_ahs_len > 0) {
-		if (!_iscsi_sgl_append(&sgl, pdu->ahs, 4 * total_ahs_len)) {
+		if (!spdk_iov_sgl_append(&sgl, pdu->ahs, 4 * total_ahs_len)) {
 			goto end;
 		}
 	}
 
 	/* Header Digest */
 	if (enable_digest && conn->header_digest) {
-		if (!_iscsi_sgl_append(&sgl, pdu->header_digest, ISCSI_DIGEST_LEN)) {
+		if (!spdk_iov_sgl_append(&sgl, pdu->header_digest, ISCSI_DIGEST_LEN)) {
 			goto end;
 		}
 	}
@@ -510,7 +474,7 @@ iscsi_build_iovs(struct spdk_iscsi_conn *conn, struct iovec *iovs, int iovcnt,
 	/* Data Segment */
 	if (data_len > 0) {
 		if (!pdu->dif_insert_or_strip) {
-			if (!_iscsi_sgl_append(&sgl, pdu->data, data_len)) {
+			if (!spdk_iov_sgl_append(&sgl, pdu->data, data_len)) {
 				goto end;
 			}
 		} else {
@@ -523,7 +487,7 @@ iscsi_build_iovs(struct spdk_iscsi_conn *conn, struct iovec *iovs, int iovcnt,
 
 	/* Data Digest */
 	if (enable_digest && conn->data_digest && data_len != 0) {
-		_iscsi_sgl_append(&sgl, pdu->data_digest, ISCSI_DIGEST_LEN);
+		spdk_iov_sgl_append(&sgl, pdu->data_digest, ISCSI_DIGEST_LEN);
 	}
 
 end:
@@ -4703,10 +4667,6 @@ iscsi_pdu_payload_read(struct spdk_iscsi_conn *conn, struct spdk_iscsi_pdu *pdu)
 		}
 	}
 
-	/* All data for this PDU has now been read from the socket. */
-	spdk_trace_record(TRACE_ISCSI_READ_PDU, conn->id, pdu->data_valid_bytes,
-			  (uintptr_t)pdu, pdu->bhs.opcode);
-
 	/* check data digest */
 	if (conn->data_digest) {
 		_iscsi_pdu_calc_data_digest(pdu);
@@ -4843,13 +4803,17 @@ iscsi_read_pdu(struct spdk_iscsi_conn *conn)
 				}
 			}
 
+			/* All data for this PDU has now been read from the socket. */
+			spdk_trace_record(TRACE_ISCSI_READ_PDU, conn->id, pdu->data_valid_bytes,
+					  (uintptr_t)pdu, pdu->bhs.opcode);
+
 			if (!pdu->is_rejected) {
 				rc = iscsi_pdu_payload_handle(conn, pdu);
 			} else {
 				rc = 0;
 			}
 			if (rc == 0) {
-				spdk_trace_record(TRACE_ISCSI_TASK_EXECUTED, 0, 0, (uintptr_t)pdu, 0);
+				spdk_trace_record(TRACE_ISCSI_TASK_EXECUTED, 0, 0, (uintptr_t)pdu);
 				iscsi_put_pdu(pdu);
 				conn->pdu_in_progress = NULL;
 				conn->pdu_recv_state = ISCSI_PDU_RECV_STATE_AWAIT_PDU_READY;
diff --git a/lib/iscsi/iscsi.h b/lib/iscsi/iscsi.h
index 09e3c3723..443cf5c18 100644
--- a/lib/iscsi/iscsi.h
+++ b/lib/iscsi/iscsi.h
@@ -339,6 +339,9 @@ struct spdk_iscsi_opts {
 	bool AllowDuplicateIsid;
 	uint32_t MaxLargeDataInPerConnection;
 	uint32_t MaxR2TPerConnection;
+	uint32_t pdu_pool_size;
+	uint32_t immediate_data_pool_size;
+	uint32_t data_out_pool_size;
 };
 
 struct spdk_iscsi_globals {
@@ -372,6 +375,9 @@ struct spdk_iscsi_globals {
 	bool AllowDuplicateIsid;
 	uint32_t MaxLargeDataInPerConnection;
 	uint32_t MaxR2TPerConnection;
+	uint32_t pdu_pool_size;
+	uint32_t immediate_data_pool_size;
+	uint32_t data_out_pool_size;
 
 	struct spdk_mempool *pdu_pool;
 	struct spdk_mempool *pdu_immediate_data_pool;
diff --git a/lib/iscsi/iscsi_rpc.c b/lib/iscsi/iscsi_rpc.c
index 20b4fa57e..595f0a776 100644
--- a/lib/iscsi/iscsi_rpc.c
+++ b/lib/iscsi/iscsi_rpc.c
@@ -1701,6 +1701,9 @@ static const struct spdk_json_object_decoder rpc_set_iscsi_opts_decoders[] = {
 	{"allow_duplicated_isid", offsetof(struct spdk_iscsi_opts, AllowDuplicateIsid), spdk_json_decode_bool, true},
 	{"max_large_datain_per_connection", offsetof(struct spdk_iscsi_opts, MaxLargeDataInPerConnection), spdk_json_decode_uint32, true},
 	{"max_r2t_per_connection", offsetof(struct spdk_iscsi_opts, MaxR2TPerConnection), spdk_json_decode_uint32, true},
+	{"pdu_pool_size", offsetof(struct spdk_iscsi_opts, pdu_pool_size), spdk_json_decode_uint32, true},
+	{"immediate_data_pool_size", offsetof(struct spdk_iscsi_opts, immediate_data_pool_size), spdk_json_decode_uint32, true},
+	{"data_out_pool_size", offsetof(struct spdk_iscsi_opts, data_out_pool_size), spdk_json_decode_uint32, true},
 };
 
 static void
diff --git a/lib/iscsi/iscsi_subsystem.c b/lib/iscsi/iscsi_subsystem.c
index 856ca16f8..03e591a46 100644
--- a/lib/iscsi/iscsi_subsystem.c
+++ b/lib/iscsi/iscsi_subsystem.c
@@ -68,13 +68,6 @@ mobj_ctor(struct spdk_mempool *mp, __attribute__((unused)) void *arg,
 			  ~ISCSI_DATA_BUFFER_MASK);
 }
 
-#define NUM_PDU_PER_CONNECTION(iscsi)	(2 * (iscsi->MaxQueueDepth +	\
-					 iscsi->MaxLargeDataInPerConnection +	\
-					 2 * iscsi->MaxR2TPerConnection + 8))
-#define PDU_POOL_SIZE(iscsi)		(iscsi->MaxConnections * NUM_PDU_PER_CONNECTION(iscsi))
-#define IMMEDIATE_DATA_POOL_SIZE(iscsi)	(iscsi->MaxConnections * 128)
-#define DATA_OUT_POOL_SIZE(iscsi)	(iscsi->MaxConnections * MAX_DATA_OUT_PER_CONNECTION)
-
 static int
 iscsi_initialize_pdu_pool(void)
 {
@@ -86,7 +79,7 @@ iscsi_initialize_pdu_pool(void)
 
 	/* create PDU pool */
 	iscsi->pdu_pool = spdk_mempool_create("PDU_Pool",
-					      PDU_POOL_SIZE(iscsi),
+					      iscsi->pdu_pool_size,
 					      sizeof(struct spdk_iscsi_pdu),
 					      256, SPDK_ENV_SOCKET_ID_ANY);
 	if (!iscsi->pdu_pool) {
@@ -95,7 +88,7 @@ iscsi_initialize_pdu_pool(void)
 	}
 
 	iscsi->pdu_immediate_data_pool = spdk_mempool_create_ctor("PDU_immediate_data_Pool",
-					 IMMEDIATE_DATA_POOL_SIZE(iscsi),
+					 iscsi->immediate_data_pool_size,
 					 imm_mobj_size, 256,
 					 SPDK_ENV_SOCKET_ID_ANY,
 					 mobj_ctor, NULL);
@@ -105,7 +98,7 @@ iscsi_initialize_pdu_pool(void)
 	}
 
 	iscsi->pdu_data_out_pool = spdk_mempool_create_ctor("PDU_data_out_Pool",
-				   DATA_OUT_POOL_SIZE(iscsi),
+				   iscsi->data_out_pool_size,
 				   dout_mobj_size, 256,
 				   SPDK_ENV_SOCKET_ID_ANY,
 				   mobj_ctor, NULL);
@@ -201,10 +194,10 @@ iscsi_check_pools(void)
 {
 	struct spdk_iscsi_globals *iscsi = &g_iscsi;
 
-	iscsi_check_pool(iscsi->pdu_pool, PDU_POOL_SIZE(iscsi));
+	iscsi_check_pool(iscsi->pdu_pool, iscsi->pdu_pool_size);
 	iscsi_check_pool(iscsi->session_pool, SESSION_POOL_SIZE(iscsi));
-	iscsi_check_pool(iscsi->pdu_immediate_data_pool, IMMEDIATE_DATA_POOL_SIZE(iscsi));
-	iscsi_check_pool(iscsi->pdu_data_out_pool, DATA_OUT_POOL_SIZE(iscsi));
+	iscsi_check_pool(iscsi->pdu_immediate_data_pool, iscsi->immediate_data_pool_size);
+	iscsi_check_pool(iscsi->pdu_data_out_pool, iscsi->data_out_pool_size);
 	iscsi_check_pool(iscsi->task_pool, DEFAULT_TASK_POOL_SIZE);
 }
 
@@ -319,6 +312,13 @@ iscsi_log_globals(void)
 		      g_iscsi.MaxR2TPerConnection);
 }
 
+#define NUM_PDU_PER_CONNECTION(opts)	(2 * (opts->MaxQueueDepth +	\
+					 opts->MaxLargeDataInPerConnection +	\
+					 2 * opts->MaxR2TPerConnection + 8))
+#define PDU_POOL_SIZE(opts)		(opts->MaxSessions * NUM_PDU_PER_CONNECTION(opts))
+#define IMMEDIATE_DATA_POOL_SIZE(opts)	(opts->MaxSessions * 128)
+#define DATA_OUT_POOL_SIZE(opts)	(opts->MaxSessions * MAX_DATA_OUT_PER_CONNECTION)
+
 static void
 iscsi_opts_init(struct spdk_iscsi_opts *opts)
 {
@@ -341,6 +341,9 @@ iscsi_opts_init(struct spdk_iscsi_opts *opts)
 	opts->nodebase = NULL;
 	opts->MaxLargeDataInPerConnection = DEFAULT_MAX_LARGE_DATAIN_PER_CONNECTION;
 	opts->MaxR2TPerConnection = DEFAULT_MAXR2T;
+	opts->pdu_pool_size = PDU_POOL_SIZE(opts);
+	opts->immediate_data_pool_size = IMMEDIATE_DATA_POOL_SIZE(opts);
+	opts->data_out_pool_size = DATA_OUT_POOL_SIZE(opts);
 }
 
 struct spdk_iscsi_opts *
@@ -415,6 +418,9 @@ iscsi_opts_copy(struct spdk_iscsi_opts *src)
 	dst->chap_group = src->chap_group;
 	dst->MaxLargeDataInPerConnection = src->MaxLargeDataInPerConnection;
 	dst->MaxR2TPerConnection = src->MaxR2TPerConnection;
+	dst->pdu_pool_size = src->pdu_pool_size;
+	dst->immediate_data_pool_size = src->immediate_data_pool_size;
+	dst->data_out_pool_size = src->data_out_pool_size;
 
 	return dst;
 }
@@ -504,6 +510,21 @@ iscsi_opts_verify(struct spdk_iscsi_opts *opts)
 		return -EINVAL;
 	}
 
+	if (opts->pdu_pool_size == 0) {
+		SPDK_ERRLOG("0 is invalid. pdu_pool_size must be more than 0\n");
+		return -EINVAL;
+	}
+
+	if (opts->immediate_data_pool_size == 0) {
+		SPDK_ERRLOG("0 is invalid. immediate_data_pool_size must be more than 0\n");
+		return -EINVAL;
+	}
+
+	if (opts->data_out_pool_size == 0) {
+		SPDK_ERRLOG("0 is invalid. data_out_pool_size must be more than 0\n");
+		return -EINVAL;
+	}
+
 	return 0;
 }
 
@@ -549,6 +570,9 @@ iscsi_set_global_params(struct spdk_iscsi_opts *opts)
 	g_iscsi.chap_group = opts->chap_group;
 	g_iscsi.MaxLargeDataInPerConnection = opts->MaxLargeDataInPerConnection;
 	g_iscsi.MaxR2TPerConnection = opts->MaxR2TPerConnection;
+	g_iscsi.pdu_pool_size = opts->pdu_pool_size;
+	g_iscsi.immediate_data_pool_size = opts->immediate_data_pool_size;
+	g_iscsi.data_out_pool_size = opts->data_out_pool_size;
 
 	iscsi_log_globals();
 
@@ -1155,10 +1179,6 @@ spdk_iscsi_fini(spdk_iscsi_fini_cb cb_fn, void *cb_arg)
 static void
 iscsi_fini_done(void *io_device)
 {
-	free(g_iscsi.authfile);
-	free(g_iscsi.nodebase);
-
-	pthread_mutex_destroy(&g_iscsi.mutex);
 	g_fini_cb_fn(g_fini_cb_arg);
 }
 
@@ -1176,7 +1196,20 @@ _iscsi_fini_dev_unreg(struct spdk_io_channel_iter *i, int status)
 	iscsi_portal_grps_destroy();
 	iscsi_auth_groups_destroy();
 
-	spdk_io_device_unregister(&g_iscsi, iscsi_fini_done);
+	free(g_iscsi.authfile);
+	free(g_iscsi.nodebase);
+
+	pthread_mutex_destroy(&g_iscsi.mutex);
+	if (g_init_thread != NULL) {
+		/* g_init_thread is set just after the io_device is
+		 * registered, so we can use it to determine if it
+		 * needs to be unregistered (in cases where iscsi init
+		 * fails).
+		 */
+		spdk_io_device_unregister(&g_iscsi, iscsi_fini_done);
+	} else {
+		iscsi_fini_done(NULL);
+	}
 }
 
 static void
@@ -1243,6 +1276,11 @@ iscsi_opts_info_json(struct spdk_json_write_ctx *w)
 	spdk_json_write_named_uint32(w, "max_r2t_per_connection",
 				     g_iscsi.MaxR2TPerConnection);
 
+	spdk_json_write_named_uint32(w, "pdu_pool_size", g_iscsi.pdu_pool_size);
+	spdk_json_write_named_uint32(w, "immediate_data_pool_size",
+				     g_iscsi.immediate_data_pool_size);
+	spdk_json_write_named_uint32(w, "data_out_pool_size", g_iscsi.data_out_pool_size);
+
 	spdk_json_write_object_end(w);
 }
 
diff --git a/lib/jsonrpc/jsonrpc_client_tcp.c b/lib/jsonrpc/jsonrpc_client_tcp.c
index 512f6261c..699860e14 100644
--- a/lib/jsonrpc/jsonrpc_client_tcp.c
+++ b/lib/jsonrpc/jsonrpc_client_tcp.c
@@ -227,23 +227,15 @@ static int
 jsonrpc_client_connect(struct spdk_jsonrpc_client *client, int domain, int protocol,
 		       struct sockaddr *server_addr, socklen_t addrlen)
 {
-	int rc, flags;
+	int rc;
 
-	client->sockfd = socket(domain, SOCK_STREAM, protocol);
+	client->sockfd = socket(domain, SOCK_STREAM | SOCK_NONBLOCK, protocol);
 	if (client->sockfd < 0) {
 		rc = errno;
 		SPDK_ERRLOG("socket() failed\n");
 		return -rc;
 	}
 
-	flags = fcntl(client->sockfd, F_GETFL);
-	if (flags < 0 || fcntl(client->sockfd, F_SETFL, flags | O_NONBLOCK) < 0) {
-		rc = errno;
-		SPDK_ERRLOG("fcntl(): can't set nonblocking mode for socket (%d): %s\n",
-			    errno, spdk_strerror(errno));
-		goto err;
-	}
-
 	rc = connect(client->sockfd, server_addr, addrlen);
 	if (rc != 0) {
 		rc = errno;
diff --git a/lib/jsonrpc/jsonrpc_server_tcp.c b/lib/jsonrpc/jsonrpc_server_tcp.c
index 71f3b5cc2..ba7c8d2a1 100644
--- a/lib/jsonrpc/jsonrpc_server_tcp.c
+++ b/lib/jsonrpc/jsonrpc_server_tcp.c
@@ -41,7 +41,7 @@ spdk_jsonrpc_server_listen(int domain, int protocol,
 			   spdk_jsonrpc_handle_request_fn handle_request)
 {
 	struct spdk_jsonrpc_server *server;
-	int rc, val, flag, i;
+	int rc, val, i;
 
 	server = calloc(1, sizeof(struct spdk_jsonrpc_server));
 	if (server == NULL) {
@@ -57,7 +57,7 @@ spdk_jsonrpc_server_listen(int domain, int protocol,
 
 	server->handle_request = handle_request;
 
-	server->sockfd = socket(domain, SOCK_STREAM, protocol);
+	server->sockfd = socket(domain, SOCK_STREAM | SOCK_NONBLOCK, protocol);
 	if (server->sockfd < 0) {
 		SPDK_ERRLOG("socket() failed\n");
 		free(server);
@@ -67,15 +67,6 @@ spdk_jsonrpc_server_listen(int domain, int protocol,
 	val = 1;
 	setsockopt(server->sockfd, SOL_SOCKET, SO_REUSEADDR, &val, sizeof(val));
 
-	flag = fcntl(server->sockfd, F_GETFL);
-	if (fcntl(server->sockfd, F_SETFL, flag | O_NONBLOCK) < 0) {
-		SPDK_ERRLOG("fcntl can't set nonblocking mode for socket, fd: %d (%s)\n",
-			    server->sockfd, spdk_strerror(errno));
-		close(server->sockfd);
-		free(server);
-		return NULL;
-	}
-
 	rc = bind(server->sockfd, listen_addr, addrlen);
 	if (rc != 0) {
 		SPDK_ERRLOG("could not bind JSON-RPC server: %s\n", spdk_strerror(errno));
diff --git a/lib/nbd/nbd.c b/lib/nbd/nbd.c
index 58e837622..1651dd0e2 100644
--- a/lib/nbd/nbd.c
+++ b/lib/nbd/nbd.c
@@ -148,37 +148,23 @@ spdk_nbd_init(void)
 }
 
 static void
-_nbd_stop_async(void *arg)
+_nbd_fini(void *arg1)
 {
-	struct spdk_nbd_disk *nbd = arg;
-	int rc;
+	struct spdk_nbd_disk *nbd, *nbd_tmp;
 
-	rc = spdk_nbd_stop(nbd);
-	if (rc) {
-		/* spdk_nbd_stop failed because some IO are still executing. Send a message
-		* to this thread to try again later. */
-		spdk_thread_send_msg(spdk_get_thread(),
-				     _nbd_stop_async, nbd);
-	} else {
-		_nbd_fini(NULL);
+	/* Change all nbds into closing state */
+	TAILQ_FOREACH_SAFE(nbd, &g_spdk_nbd.disk_head, tailq, nbd_tmp) {
+		if (nbd->state != NBD_DISK_STATE_HARDDISC) {
+			spdk_nbd_stop(nbd);
+		}
 	}
-}
-
-static void
-_nbd_fini(void *arg1)
-{
-	struct spdk_nbd_disk *nbd_first;
 
-	nbd_first = TAILQ_FIRST(&g_spdk_nbd.disk_head);
-	if (nbd_first) {
-		/* Stop running spdk_nbd_disk */
-		spdk_thread_send_msg(spdk_io_channel_get_thread(nbd_first->ch),
-				     _nbd_stop_async, nbd_first);
-	} else {
-		/* We can directly call final function here, because
-		 spdk_subsystem_fini_next handles the case: current thread does not equal
-		 to g_final_thread */
+	/* Check if all nbds closed */
+	if (!TAILQ_FIRST(&g_spdk_nbd.disk_head)) {
 		g_fini_cb_fn(g_fini_cb_arg);
+	} else {
+		spdk_thread_send_msg(spdk_get_thread(),
+				     _nbd_fini, NULL);
 	}
 }
 
@@ -324,24 +310,6 @@ nbd_put_io(struct spdk_nbd_disk *nbd, struct nbd_io *io)
 	nbd->io_count--;
 }
 
-/*
- * Check whether received nbd_io are all transmitted.
- *
- * \return 1 there is still some nbd_io not transmitted.
- *         0 all nbd_io received are transmitted.
- */
-static int
-nbd_io_xmit_check(struct spdk_nbd_disk *nbd)
-{
-	if (nbd->io_count == 0) {
-		return 0;
-	} else if (nbd->io_count == 1 && nbd->io_in_recv != NULL) {
-		return 0;
-	}
-
-	return 1;
-}
-
 /*
  * Check whether received nbd_io are all executed,
  * and put back executed nbd_io instead of transmitting them
@@ -517,10 +485,6 @@ nbd_io_done(struct spdk_bdev_io *bdev_io, bool success, void *cb_arg)
 	if (bdev_io != NULL) {
 		spdk_bdev_free_io(bdev_io);
 	}
-
-	if (nbd->state == NBD_DISK_STATE_HARDDISC && !nbd_cleanup_io(nbd)) {
-		_nbd_stop(nbd);
-	}
 }
 
 static void
@@ -847,14 +811,6 @@ nbd_io_xmit(struct spdk_nbd_disk *nbd)
 		spdk_interrupt_set_event_types(nbd->intr, SPDK_INTERRUPT_EVENT_IN);
 	}
 
-	/*
-	 * For soft disconnection, nbd server can close connection after all
-	 * outstanding request are transmitted.
-	 */
-	if (nbd->state == NBD_DISK_STATE_SOFTDISC && !nbd_io_xmit_check(nbd)) {
-		return -1;
-	}
-
 	return ret;
 }
 
@@ -897,7 +853,14 @@ nbd_poll(void *arg)
 	if (rc < 0) {
 		SPDK_INFOLOG(nbd, "nbd_poll() returned %s (%d); closing connection\n",
 			     spdk_strerror(-rc), rc);
-		spdk_nbd_stop(nbd);
+		_nbd_stop(nbd);
+	}
+	if (nbd->state != NBD_DISK_STATE_RUNNING) {
+		if (nbd->state == NBD_DISK_STATE_HARDDISC && !nbd_cleanup_io(nbd)) {
+			_nbd_stop(nbd);
+		} else if (nbd->state == NBD_DISK_STATE_SOFTDISC) {
+			spdk_nbd_stop(nbd);
+		}
 	}
 
 	return rc > 0 ? SPDK_POLLER_BUSY : SPDK_POLLER_IDLE;
@@ -957,7 +920,6 @@ nbd_start_complete(struct spdk_nbd_start_ctx *ctx)
 {
 	int		rc;
 	pthread_t	tid;
-	int		flag;
 	unsigned long	nbd_flags = 0;
 
 	rc = ioctl(ctx->nbd->dev_fd, NBD_SET_BLKSIZE, spdk_bdev_get_block_size(ctx->nbd->bdev));
@@ -1016,14 +978,6 @@ nbd_start_complete(struct spdk_nbd_start_ctx *ctx)
 		goto err;
 	}
 
-	flag = fcntl(ctx->nbd->spdk_sp_fd, F_GETFL);
-	if (fcntl(ctx->nbd->spdk_sp_fd, F_SETFL, flag | O_NONBLOCK) < 0) {
-		SPDK_ERRLOG("fcntl can't set nonblocking mode for socket, fd: %d (%s)\n",
-			    ctx->nbd->spdk_sp_fd, spdk_strerror(errno));
-		rc = -errno;
-		goto err;
-	}
-
 	if (spdk_interrupt_mode_is_enabled()) {
 		ctx->nbd->intr = SPDK_INTERRUPT_REGISTER(ctx->nbd->spdk_sp_fd, nbd_poll, ctx->nbd);
 	}
@@ -1039,7 +993,7 @@ nbd_start_complete(struct spdk_nbd_start_ctx *ctx)
 	return;
 
 err:
-	spdk_nbd_stop(ctx->nbd);
+	_nbd_stop(ctx->nbd);
 	if (ctx->cb_fn) {
 		ctx->cb_fn(ctx->cb_arg, NULL, rc);
 	}
@@ -1051,20 +1005,10 @@ nbd_enable_kernel(void *arg)
 {
 	struct spdk_nbd_start_ctx *ctx = arg;
 	int rc;
-	int flag;
 
 	/* Declare device setup by this process */
 	rc = ioctl(ctx->nbd->dev_fd, NBD_SET_SOCK, ctx->nbd->kernel_sp_fd);
 
-	if (!rc) {
-		flag = fcntl(ctx->nbd->kernel_sp_fd, F_GETFL);
-		rc = fcntl(ctx->nbd->kernel_sp_fd, F_SETFL, flag | O_NONBLOCK);
-		if (rc < 0) {
-			SPDK_ERRLOG("fcntl can't set nonblocking mode for socket, fd: %d (%s)\n",
-				    ctx->nbd->kernel_sp_fd, spdk_strerror(errno));
-		}
-	}
-
 	if (rc) {
 		if (errno == EBUSY) {
 			if (ctx->nbd->retry_poller == NULL) {
@@ -1086,7 +1030,7 @@ nbd_enable_kernel(void *arg)
 			spdk_poller_unregister(&ctx->nbd->retry_poller);
 		}
 
-		spdk_nbd_stop(ctx->nbd);
+		_nbd_stop(ctx->nbd);
 
 		if (ctx->cb_fn) {
 			ctx->cb_fn(ctx->cb_arg, NULL, -errno);
@@ -1147,7 +1091,7 @@ spdk_nbd_start(const char *bdev_name, const char *nbd_path,
 	nbd->ch = spdk_bdev_get_io_channel(nbd->bdev_desc);
 	nbd->buf_align = spdk_max(spdk_bdev_get_buf_align(bdev), 64);
 
-	rc = socketpair(AF_UNIX, SOCK_STREAM, 0, sp);
+	rc = socketpair(AF_UNIX, SOCK_STREAM | SOCK_NONBLOCK, 0, sp);
 	if (rc != 0) {
 		SPDK_ERRLOG("socketpair failed\n");
 		rc = -errno;
@@ -1167,7 +1111,7 @@ spdk_nbd_start(const char *bdev_name, const char *nbd_path,
 	TAILQ_INIT(&nbd->executed_io_list);
 
 	/* Add nbd_disk to the end of disk list */
-	nbd_disk_register(ctx->nbd);
+	rc = nbd_disk_register(ctx->nbd);
 	if (rc != 0) {
 		goto err;
 	}
@@ -1188,7 +1132,7 @@ spdk_nbd_start(const char *bdev_name, const char *nbd_path,
 err:
 	free(ctx);
 	if (nbd) {
-		spdk_nbd_stop(nbd);
+		_nbd_stop(nbd);
 	}
 
 	if (cb_fn) {
diff --git a/lib/nvme/Makefile b/lib/nvme/Makefile
index 65a494b34..f722ea56d 100644
--- a/lib/nvme/Makefile
+++ b/lib/nvme/Makefile
@@ -35,7 +35,7 @@ SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
 SO_VER := 5
-SO_MINOR := 0
+SO_MINOR := 1
 
 C_SRCS = nvme_ctrlr_cmd.c nvme_ctrlr.c nvme_fabric.c nvme_ns_cmd.c nvme_ns.c nvme_pcie_common.c nvme_pcie.c nvme_qpair.c nvme.c nvme_quirks.c nvme_transport.c \
 	nvme_ctrlr_ocssd_cmd.c nvme_ns_ocssd_cmd.c nvme_tcp.c nvme_opal.c nvme_io_msg.c nvme_poll_group.c nvme_zns.c
diff --git a/lib/nvme/nvme.c b/lib/nvme/nvme.c
index 1c92ea9e5..9ef781e2b 100644
--- a/lib/nvme/nvme.c
+++ b/lib/nvme/nvme.c
@@ -678,16 +678,6 @@ nvme_ctrlr_probe(const struct spdk_nvme_transport_id *trid,
 		ctrlr->remove_cb = probe_ctx->remove_cb;
 		ctrlr->cb_ctx = probe_ctx->cb_ctx;
 
-		if (ctrlr->quirks & NVME_QUIRK_MINIMUM_IO_QUEUE_SIZE &&
-		    ctrlr->opts.io_queue_size == DEFAULT_IO_QUEUE_SIZE) {
-			/* If the user specifically set an IO queue size different than the
-			 * default, use that value.  Otherwise overwrite with the quirked value.
-			 * This allows this quirk to be overridden when necessary.
-			 * However, cap.mqes still needs to be respected.
-			 */
-			ctrlr->opts.io_queue_size = spdk_min(DEFAULT_IO_QUEUE_SIZE_FOR_QUIRK, ctrlr->cap.bits.mqes + 1u);
-		}
-
 		nvme_qpair_set_state(ctrlr->adminq, NVME_QPAIR_ENABLED);
 		TAILQ_INSERT_TAIL(&probe_ctx->init_ctrlrs, ctrlr, tailq);
 		return 0;
diff --git a/lib/nvme/nvme_ctrlr.c b/lib/nvme/nvme_ctrlr.c
index d15ebdd73..f9534175c 100644
--- a/lib/nvme/nvme_ctrlr.c
+++ b/lib/nvme/nvme_ctrlr.c
@@ -38,6 +38,7 @@
 
 #include "spdk/env.h"
 #include "spdk/string.h"
+#include "spdk/endian.h"
 
 struct nvme_active_ns_ctx;
 
@@ -48,6 +49,7 @@ static void nvme_ctrlr_identify_active_ns_async(struct nvme_active_ns_ctx *ctx);
 static int nvme_ctrlr_identify_ns_async(struct spdk_nvme_ns *ns);
 static int nvme_ctrlr_identify_ns_iocs_specific_async(struct spdk_nvme_ns *ns);
 static int nvme_ctrlr_identify_id_desc_async(struct spdk_nvme_ns *ns);
+static void nvme_ctrlr_init_cap(struct spdk_nvme_ctrlr *ctrlr);
 
 #define CTRLR_STRING(ctrlr) \
 	((ctrlr->trid.trtype == SPDK_NVME_TRANSPORT_TCP || ctrlr->trid.trtype == SPDK_NVME_TRANSPORT_RDMA) ? \
@@ -156,7 +158,7 @@ spdk_nvme_ctrlr_get_default_ctrlr_opts(struct spdk_nvme_ctrlr_opts *opts, size_t
 	} \
 
 	SET_FIELD(num_io_queues, DEFAULT_MAX_IO_QUEUES);
-	SET_FIELD(use_cmb_sqs, true);
+	SET_FIELD(use_cmb_sqs, false);
 	SET_FIELD(no_shn_notification, false);
 	SET_FIELD(arb_mechanism, SPDK_NVME_CC_AMS_RR);
 	SET_FIELD(arbitration_burst, 0);
@@ -738,7 +740,8 @@ nvme_ctrlr_update_ns_ana_states(const struct spdk_nvme_ana_group_descriptor *des
 			continue;
 		}
 
-		ns = &ctrlr->ns[nsid - 1];
+		ns = spdk_nvme_ctrlr_get_ns(ctrlr, nsid);
+		assert(ns != NULL);
 
 		ns->ana_group_id = desc->ana_group_id;
 		ns->ana_state = desc->ana_state;
@@ -798,6 +801,12 @@ nvme_ctrlr_set_supported_log_pages(struct spdk_nvme_ctrlr *ctrlr)
 			nvme_ctrlr_parse_ana_log_page(ctrlr, nvme_ctrlr_update_ns_ana_states,
 						      ctrlr);
 		}
+	} else {
+		uint32_t i;
+
+		for (i = 0; i < ctrlr->num_ns; i++) {
+			ctrlr->ns[i].ana_state = SPDK_NVME_ANA_OPTIMIZED_STATE;
+		}
 	}
 
 out:
@@ -1133,8 +1142,14 @@ nvme_ctrlr_state_string(enum nvme_ctrlr_state state)
 	switch (state) {
 	case NVME_CTRLR_STATE_INIT_DELAY:
 		return "delay init";
-	case NVME_CTRLR_STATE_INIT:
-		return "init";
+	case NVME_CTRLR_STATE_CONNECT_ADMINQ:
+		return "connect adminq";
+	case NVME_CTRLR_STATE_READ_VS:
+		return "read vs";
+	case NVME_CTRLR_STATE_READ_CAP:
+		return "read cap";
+	case NVME_CTRLR_STATE_CHECK_EN:
+		return "check en";
 	case NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_1:
 		return "disable and wait for CSTS.RDY = 1";
 	case NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0:
@@ -1161,12 +1176,12 @@ nvme_ctrlr_state_string(enum nvme_ctrlr_state state)
 		return "set number of queues";
 	case NVME_CTRLR_STATE_WAIT_FOR_SET_NUM_QUEUES:
 		return "wait for set number of queues";
-	case NVME_CTRLR_STATE_CONSTRUCT_NS:
-		return "construct namespaces";
 	case NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS:
 		return "identify active ns";
 	case NVME_CTRLR_STATE_WAIT_FOR_IDENTIFY_ACTIVE_NS:
 		return "wait for identify active ns";
+	case NVME_CTRLR_STATE_CONSTRUCT_NS:
+		return "construct namespaces";
 	case NVME_CTRLR_STATE_IDENTIFY_NS:
 		return "identify ns";
 	case NVME_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS:
@@ -1403,11 +1418,6 @@ spdk_nvme_ctrlr_reset(struct spdk_nvme_ctrlr *ctrlr)
 
 	ctrlr->adminq->transport_failure_reason = SPDK_NVME_QPAIR_FAILURE_LOCAL;
 	nvme_transport_ctrlr_disconnect_qpair(ctrlr, ctrlr->adminq);
-	rc = nvme_transport_ctrlr_connect_qpair(ctrlr, ctrlr->adminq);
-	if (rc != 0) {
-		NVME_CTRLR_ERRLOG(ctrlr, "Controller reinitialization failed.\n");
-		goto out;
-	}
 
 	/* Doorbell buffer config is invalid during reset */
 	nvme_ctrlr_free_doorbell_buffer(ctrlr);
@@ -1420,7 +1430,6 @@ spdk_nvme_ctrlr_reset(struct spdk_nvme_ctrlr *ctrlr)
 	/* Set the state back to INIT to cause a full hardware reset. */
 	nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_INIT, NVME_TIMEOUT_INFINITE);
 
-	nvme_qpair_set_state(ctrlr->adminq, NVME_QPAIR_ENABLED);
 	while (ctrlr->state != NVME_CTRLR_STATE_READY) {
 		if (nvme_ctrlr_process_init(ctrlr) != 0) {
 			NVME_CTRLR_ERRLOG(ctrlr, "controller reinitialization failed\n");
@@ -1430,13 +1439,13 @@ spdk_nvme_ctrlr_reset(struct spdk_nvme_ctrlr *ctrlr)
 	}
 
 	/*
-	 * For PCIe controllers, the memory locations of the transport qpair
+	 * For non-fabrics controllers, the memory locations of the transport qpair
 	 * don't change when the controller is reset. They simply need to be
 	 * re-enabled with admin commands to the controller. For fabric
 	 * controllers we need to disconnect and reconnect the qpair on its
 	 * own thread outside of the context of the reset.
 	 */
-	if (rc == 0 && ctrlr->trid.trtype == SPDK_NVME_TRANSPORT_PCIE) {
+	if (rc == 0 && !spdk_nvme_ctrlr_is_fabrics(ctrlr)) {
 		/* Reinitialize qpairs */
 		TAILQ_FOREACH(qpair, &ctrlr->active_io_qpairs, tailq) {
 			assert(spdk_bit_array_get(ctrlr->free_io_qids, qpair->id));
@@ -1450,7 +1459,6 @@ spdk_nvme_ctrlr_reset(struct spdk_nvme_ctrlr *ctrlr)
 		}
 	}
 
-out:
 	if (rc) {
 		nvme_ctrlr_fail(ctrlr, false);
 	}
@@ -1784,7 +1792,6 @@ typedef void (*nvme_active_ns_ctx_deleter)(struct nvme_active_ns_ctx *);
 struct nvme_active_ns_ctx {
 	struct spdk_nvme_ctrlr *ctrlr;
 	uint32_t page;
-	uint32_t num_pages;
 	uint32_t next_nsid;
 	uint32_t *new_ns_list;
 	nvme_active_ns_ctx_deleter deleter;
@@ -1796,7 +1803,6 @@ static struct nvme_active_ns_ctx *
 nvme_active_ns_ctx_create(struct spdk_nvme_ctrlr *ctrlr, nvme_active_ns_ctx_deleter deleter)
 {
 	struct nvme_active_ns_ctx *ctx;
-	uint32_t num_pages = 0;
 	uint32_t *new_ns_list = NULL;
 
 	ctx = calloc(1, sizeof(*ctx));
@@ -1805,19 +1811,14 @@ nvme_active_ns_ctx_create(struct spdk_nvme_ctrlr *ctrlr, nvme_active_ns_ctx_dele
 		return NULL;
 	}
 
-	if (ctrlr->num_ns) {
-		/* The allocated size must be a multiple of sizeof(struct spdk_nvme_ns_list) */
-		num_pages = (ctrlr->num_ns * sizeof(new_ns_list[0]) - 1) / sizeof(struct spdk_nvme_ns_list) + 1;
-		new_ns_list = spdk_zmalloc(num_pages * sizeof(struct spdk_nvme_ns_list), ctrlr->page_size,
-					   NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA | SPDK_MALLOC_SHARE);
-		if (!new_ns_list) {
-			NVME_CTRLR_ERRLOG(ctrlr, "Failed to allocate active_ns_list!\n");
-			free(ctx);
-			return NULL;
-		}
+	new_ns_list = spdk_zmalloc(sizeof(struct spdk_nvme_ns_list), ctrlr->page_size,
+				   NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_SHARE);
+	if (!new_ns_list) {
+		NVME_CTRLR_ERRLOG(ctrlr, "Failed to allocate active_ns_list!\n");
+		free(ctx);
+		return NULL;
 	}
 
-	ctx->num_pages = num_pages;
 	ctx->new_ns_list = new_ns_list;
 	ctx->ctrlr = ctrlr;
 	ctx->deleter = deleter;
@@ -1835,8 +1836,12 @@ nvme_active_ns_ctx_destroy(struct nvme_active_ns_ctx *ctx)
 static void
 nvme_ctrlr_identify_active_ns_swap(struct spdk_nvme_ctrlr *ctrlr, uint32_t **new_ns_list)
 {
+	uint32_t max_active_ns_idx = 0;
+
+	while ((*new_ns_list)[max_active_ns_idx++]);
 	spdk_free(ctrlr->active_ns_list);
 	ctrlr->active_ns_list = *new_ns_list;
+	ctrlr->max_active_ns_idx = max_active_ns_idx;
 	*new_ns_list = NULL;
 }
 
@@ -1844,6 +1849,7 @@ static void
 nvme_ctrlr_identify_active_ns_async_done(void *arg, const struct spdk_nvme_cpl *cpl)
 {
 	struct nvme_active_ns_ctx *ctx = arg;
+	uint32_t *new_ns_list = NULL;
 
 	if (spdk_nvme_cpl_is_error(cpl)) {
 		ctx->state = NVME_ACTIVE_NS_STATE_ERROR;
@@ -1851,11 +1857,22 @@ nvme_ctrlr_identify_active_ns_async_done(void *arg, const struct spdk_nvme_cpl *
 	}
 
 	ctx->next_nsid = ctx->new_ns_list[1024 * ctx->page + 1023];
-	if (ctx->next_nsid == 0 || ++ctx->page == ctx->num_pages) {
+	if (ctx->next_nsid == 0) {
 		ctx->state = NVME_ACTIVE_NS_STATE_DONE;
 		goto out;
 	}
 
+	ctx->page++;
+	new_ns_list = spdk_realloc(ctx->new_ns_list,
+				   (ctx->page + 1) * sizeof(struct spdk_nvme_ns_list),
+				   ctx->ctrlr->page_size);
+	if (!new_ns_list) {
+		SPDK_ERRLOG("Failed to reallocate active_ns_list!\n");
+		ctx->state = NVME_ACTIVE_NS_STATE_ERROR;
+		goto out;
+	}
+
+	ctx->new_ns_list = new_ns_list;
 	nvme_ctrlr_identify_active_ns_async(ctx);
 	return;
 
@@ -1872,7 +1889,7 @@ nvme_ctrlr_identify_active_ns_async(struct nvme_active_ns_ctx *ctx)
 	uint32_t i;
 	int rc;
 
-	if (ctrlr->num_ns == 0) {
+	if (ctrlr->cdata.nn == 0) {
 		ctx->state = NVME_ACTIVE_NS_STATE_DONE;
 		goto out;
 	}
@@ -1884,7 +1901,27 @@ nvme_ctrlr_identify_active_ns_async(struct nvme_active_ns_ctx *ctx)
 	 * an active ns list, i.e. all namespaces report as active
 	 */
 	if (ctrlr->vs.raw < SPDK_NVME_VERSION(1, 1, 0) || ctrlr->quirks & NVME_QUIRK_IDENTIFY_CNS) {
-		for (i = 0; i < ctrlr->num_ns; i++) {
+		uint32_t *new_ns_list;
+		uint32_t num_pages;
+
+		/*
+		 * Active NS list must always end with zero element.
+		 * So, we allocate for cdata.nn+1.
+		 */
+		num_pages = spdk_divide_round_up(ctrlr->cdata.nn + 1,
+						 sizeof(struct spdk_nvme_ns_list) / sizeof(new_ns_list[0]));
+		new_ns_list = spdk_realloc(ctx->new_ns_list,
+					   num_pages * sizeof(struct spdk_nvme_ns_list),
+					   ctx->ctrlr->page_size);
+		if (!new_ns_list) {
+			SPDK_ERRLOG("Failed to reallocate active_ns_list!\n");
+			ctx->state = NVME_ACTIVE_NS_STATE_ERROR;
+			goto out;
+		}
+
+		ctx->new_ns_list = new_ns_list;
+		ctx->new_ns_list[ctrlr->cdata.nn] = 0;
+		for (i = 0; i < ctrlr->cdata.nn; i++) {
 			ctx->new_ns_list[i] = i + 1;
 		}
 
@@ -1924,7 +1961,7 @@ _nvme_active_ns_ctx_deleter(struct nvme_active_ns_ctx *ctx)
 	assert(ctx->state == NVME_ACTIVE_NS_STATE_DONE);
 	nvme_ctrlr_identify_active_ns_swap(ctrlr, &ctx->new_ns_list);
 	nvme_active_ns_ctx_destroy(ctx);
-	nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_IDENTIFY_NS, ctrlr->opts.admin_timeout_ms);
+	nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_CONSTRUCT_NS, ctrlr->opts.admin_timeout_ms);
 }
 
 static void
@@ -2303,7 +2340,7 @@ nvme_ctrlr_set_num_queues_done(void *arg, const struct spdk_nvme_cpl *cpl)
 		spdk_nvme_ctrlr_free_qid(ctrlr, i);
 	}
 
-	nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_CONSTRUCT_NS,
+	nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS,
 			     ctrlr->opts.admin_timeout_ms);
 }
 
@@ -2495,12 +2532,9 @@ nvme_ctrlr_destruct_namespaces(struct spdk_nvme_ctrlr *ctrlr)
 		ctrlr->ns = NULL;
 		ctrlr->num_ns = 0;
 	}
-
-	spdk_free(ctrlr->active_ns_list);
-	ctrlr->active_ns_list = NULL;
 }
 
-static void
+void
 nvme_ctrlr_update_namespaces(struct spdk_nvme_ctrlr *ctrlr)
 {
 	uint32_t i, nn = ctrlr->cdata.nn;
@@ -2508,9 +2542,10 @@ nvme_ctrlr_update_namespaces(struct spdk_nvme_ctrlr *ctrlr)
 	bool ns_is_active;
 
 	for (i = 0; i < nn; i++) {
-		struct spdk_nvme_ns	*ns = &ctrlr->ns[i];
 		uint32_t		nsid = i + 1;
+		struct spdk_nvme_ns	*ns = spdk_nvme_ctrlr_get_ns(ctrlr, nsid);
 
+		assert(ns != NULL);
 		nsdata = &ns->nsdata;
 		ns_is_active = spdk_nvme_ctrlr_is_active_ns(ctrlr, nsid);
 
@@ -2575,6 +2610,64 @@ nvme_ctrlr_construct_namespaces(struct spdk_nvme_ctrlr *ctrlr)
 
 fail:
 	nvme_ctrlr_destruct_namespaces(ctrlr);
+	NVME_CTRLR_ERRLOG(ctrlr, "Failed to construct namespaces, err %d\n", rc);
+	nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_ERROR, NVME_TIMEOUT_INFINITE);
+	return rc;
+}
+
+static int
+nvme_ctrlr_clear_changed_ns_log(struct spdk_nvme_ctrlr *ctrlr)
+{
+	struct nvme_completion_poll_status	*status;
+	int		rc = -ENOMEM;
+	char		*buffer = NULL;
+	uint32_t	nsid;
+	size_t		buf_size = (SPDK_NVME_MAX_CHANGED_NAMESPACES * sizeof(uint32_t));
+
+	buffer = spdk_dma_zmalloc(buf_size, 4096, NULL);
+	if (!buffer) {
+		NVME_CTRLR_ERRLOG(ctrlr, "Failed to allocate buffer for getting "
+				  "changed ns log.\n");
+		return rc;
+	}
+
+	status = calloc(1, sizeof(*status));
+	if (!status) {
+		NVME_CTRLR_ERRLOG(ctrlr, "Failed to allocate status tracker\n");
+		goto free_buffer;
+	}
+
+	rc = spdk_nvme_ctrlr_cmd_get_log_page(ctrlr,
+					      SPDK_NVME_LOG_CHANGED_NS_LIST,
+					      SPDK_NVME_GLOBAL_NS_TAG,
+					      buffer, buf_size, 0,
+					      nvme_completion_poll_cb, status);
+
+	if (rc) {
+		NVME_CTRLR_ERRLOG(ctrlr, "spdk_nvme_ctrlr_cmd_get_log_page() failed: rc=%d\n", rc);
+		free(status);
+		goto free_buffer;
+	}
+
+	rc = nvme_wait_for_completion_timeout(ctrlr->adminq, status,
+					      ctrlr->opts.admin_timeout_ms * 1000);
+	if (!status->timed_out) {
+		free(status);
+	}
+
+	if (rc) {
+		NVME_CTRLR_ERRLOG(ctrlr, "wait for spdk_nvme_ctrlr_cmd_get_log_page failed: rc=%d\n", rc);
+		goto free_buffer;
+	}
+
+	/* only check the case of overflow. */
+	nsid = from_le32(buffer);
+	if (nsid == 0xffffffffu) {
+		NVME_CTRLR_WARNLOG(ctrlr, "changed ns log overflowed.\n");
+	}
+
+free_buffer:
+	spdk_dma_free(buffer);
 	return rc;
 }
 
@@ -2590,6 +2683,8 @@ nvme_ctrlr_process_async_event(struct spdk_nvme_ctrlr *ctrlr,
 
 	if ((event.bits.async_event_type == SPDK_NVME_ASYNC_EVENT_TYPE_NOTICE) &&
 	    (event.bits.async_event_info == SPDK_NVME_ASYNC_EVENT_NS_ATTR_CHANGED)) {
+		nvme_ctrlr_clear_changed_ns_log(ctrlr);
+
 		rc = nvme_ctrlr_identify_active_ns(ctrlr);
 		if (rc) {
 			return;
@@ -3059,8 +3154,8 @@ nvme_ctrlr_process_init(struct spdk_nvme_ctrlr *ctrlr)
 	}
 	ctrlr->sleep_timeout_tsc = 0;
 
-	if (nvme_ctrlr_get_cc(ctrlr, &cc) ||
-	    nvme_ctrlr_get_csts(ctrlr, &csts)) {
+	if (ctrlr->state > NVME_CTRLR_STATE_CONNECT_ADMINQ &&
+	    (nvme_ctrlr_get_cc(ctrlr, &cc) || nvme_ctrlr_get_csts(ctrlr, &csts))) {
 		if (!ctrlr->is_failed && ctrlr->state_timeout_tsc != NVME_TIMEOUT_INFINITE) {
 			/* While a device is resetting, it may be unable to service MMIO reads
 			 * temporarily. Allow for this case.
@@ -3094,7 +3189,27 @@ nvme_ctrlr_process_init(struct spdk_nvme_ctrlr *ctrlr)
 		}
 		break;
 
-	case NVME_CTRLR_STATE_INIT:
+	case NVME_CTRLR_STATE_CONNECT_ADMINQ: /* synonymous with NVME_CTRLR_STATE_INIT */
+		rc = nvme_transport_ctrlr_connect_qpair(ctrlr, ctrlr->adminq);
+		if (rc == 0) {
+			nvme_qpair_set_state(ctrlr->adminq, NVME_QPAIR_ENABLED);
+			nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_READ_VS, NVME_TIMEOUT_INFINITE);
+		} else {
+			nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_ERROR, NVME_TIMEOUT_INFINITE);
+		}
+		break;
+
+	case NVME_CTRLR_STATE_READ_VS:
+		nvme_ctrlr_get_vs(ctrlr, &ctrlr->vs);
+		nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_READ_CAP, NVME_TIMEOUT_INFINITE);
+		break;
+
+	case NVME_CTRLR_STATE_READ_CAP:
+		nvme_ctrlr_init_cap(ctrlr);
+		nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_CHECK_EN, NVME_TIMEOUT_INFINITE);
+		break;
+
+	case NVME_CTRLR_STATE_CHECK_EN:
 		/* Begin the hardware initialization by making sure the controller is disabled. */
 		if (cc.bits.en) {
 			NVME_CTRLR_DEBUGLOG(ctrlr, "CC.EN = 1\n");
@@ -3187,8 +3302,11 @@ nvme_ctrlr_process_init(struct spdk_nvme_ctrlr *ctrlr)
 
 	case NVME_CTRLR_STATE_RESET_ADMIN_QUEUE:
 		nvme_transport_qpair_reset(ctrlr->adminq);
-		nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_IDENTIFY,
-				     ctrlr->opts.admin_timeout_ms);
+		if (spdk_nvme_ctrlr_is_discovery(ctrlr)) {
+			nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_READY, NVME_TIMEOUT_INFINITE);
+		} else {
+			nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_IDENTIFY, ctrlr->opts.admin_timeout_ms);
+		}
 		break;
 
 	case NVME_CTRLR_STATE_IDENTIFY:
@@ -3208,16 +3326,18 @@ nvme_ctrlr_process_init(struct spdk_nvme_ctrlr *ctrlr)
 		rc = nvme_ctrlr_set_num_queues(ctrlr);
 		break;
 
-	case NVME_CTRLR_STATE_CONSTRUCT_NS:
-		rc = nvme_ctrlr_construct_namespaces(ctrlr);
-		nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS,
-				     ctrlr->opts.admin_timeout_ms);
-		break;
-
 	case NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS:
 		_nvme_ctrlr_identify_active_ns(ctrlr);
 		break;
 
+	case NVME_CTRLR_STATE_CONSTRUCT_NS:
+		rc = nvme_ctrlr_construct_namespaces(ctrlr);
+		if (!rc) {
+			nvme_ctrlr_set_state(ctrlr, NVME_CTRLR_STATE_IDENTIFY_NS,
+					     ctrlr->opts.admin_timeout_ms);
+		}
+		break;
+
 	case NVME_CTRLR_STATE_IDENTIFY_NS:
 		rc = nvme_ctrlr_identify_namespaces(ctrlr);
 		break;
@@ -3371,13 +3491,10 @@ nvme_ctrlr_construct(struct spdk_nvme_ctrlr *ctrlr)
 	return rc;
 }
 
-/* This function should be called once at ctrlr initialization to set up constant properties. */
-void
-nvme_ctrlr_init_cap(struct spdk_nvme_ctrlr *ctrlr, const union spdk_nvme_cap_register *cap,
-		    const union spdk_nvme_vs_register *vs)
+static void
+nvme_ctrlr_init_cap(struct spdk_nvme_ctrlr *ctrlr)
 {
-	ctrlr->cap = *cap;
-	ctrlr->vs = *vs;
+	nvme_ctrlr_get_cap(ctrlr, &ctrlr->cap);
 
 	if (ctrlr->cap.bits.ams & SPDK_NVME_CAP_AMS_WRR) {
 		ctrlr->flags |= SPDK_NVME_CTRLR_WRR_SUPPORTED;
@@ -3390,6 +3507,15 @@ nvme_ctrlr_init_cap(struct spdk_nvme_ctrlr *ctrlr, const union spdk_nvme_cap_reg
 
 	ctrlr->opts.io_queue_size = spdk_max(ctrlr->opts.io_queue_size, SPDK_NVME_IO_QUEUE_MIN_ENTRIES);
 	ctrlr->opts.io_queue_size = spdk_min(ctrlr->opts.io_queue_size, MAX_IO_QUEUE_ENTRIES);
+	if (ctrlr->quirks & NVME_QUIRK_MINIMUM_IO_QUEUE_SIZE &&
+	    ctrlr->opts.io_queue_size == DEFAULT_IO_QUEUE_SIZE) {
+		/* If the user specifically set an IO queue size different than the
+		 * default, use that value.  Otherwise overwrite with the quirked value.
+		 * This allows this quirk to be overridden when necessary.
+		 * However, cap.mqes still needs to be respected.
+		 */
+		ctrlr->opts.io_queue_size = DEFAULT_IO_QUEUE_SIZE_FOR_QUIRK;
+	}
 	ctrlr->opts.io_queue_size = spdk_min(ctrlr->opts.io_queue_size, ctrlr->cap.bits.mqes + 1u);
 
 	ctrlr->opts.io_queue_requests = spdk_max(ctrlr->opts.io_queue_requests, ctrlr->opts.io_queue_size);
@@ -3451,6 +3577,9 @@ nvme_ctrlr_destruct_poll_async(struct spdk_nvme_ctrlr *ctrlr,
 	}
 
 	nvme_ctrlr_destruct_namespaces(ctrlr);
+	spdk_free(ctrlr->active_ns_list);
+	ctrlr->active_ns_list = NULL;
+	ctrlr->max_active_ns_idx = 0;
 
 	spdk_bit_array_free(&ctrlr->free_io_qids);
 
@@ -3631,12 +3760,12 @@ nvme_ctrlr_active_ns_idx(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid)
 {
 	int32_t result = -1;
 
-	if (ctrlr->active_ns_list == NULL || nsid == 0 || nsid > ctrlr->num_ns) {
+	if (ctrlr->active_ns_list == NULL || nsid == 0 || nsid > ctrlr->cdata.nn) {
 		return result;
 	}
 
 	int32_t lower = 0;
-	int32_t upper = ctrlr->num_ns - 1;
+	int32_t upper = ctrlr->max_active_ns_idx;
 	int32_t mid;
 
 	while (lower <= upper) {
@@ -3673,7 +3802,7 @@ uint32_t
 spdk_nvme_ctrlr_get_next_active_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t prev_nsid)
 {
 	int32_t nsid_idx = nvme_ctrlr_active_ns_idx(ctrlr, prev_nsid);
-	if (ctrlr->active_ns_list && nsid_idx >= 0 && (uint32_t)nsid_idx < ctrlr->num_ns - 1) {
+	if (nsid_idx >= 0 && (uint32_t)nsid_idx < ctrlr->max_active_ns_idx) {
 		return ctrlr->active_ns_list[nsid_idx + 1];
 	}
 	return 0;
@@ -3771,6 +3900,10 @@ spdk_nvme_ctrlr_attach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 	int					res;
 	struct spdk_nvme_ns			*ns;
 
+	if (nsid == 0) {
+		return -EINVAL;
+	}
+
 	status = calloc(1, sizeof(*status));
 	if (!status) {
 		NVME_CTRLR_ERRLOG(ctrlr, "Failed to allocate status tracker\n");
@@ -3797,7 +3930,8 @@ spdk_nvme_ctrlr_attach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 		return res;
 	}
 
-	ns = &ctrlr->ns[nsid - 1];
+	ns = spdk_nvme_ctrlr_get_ns(ctrlr, nsid);
+	assert(ns != NULL);
 	return nvme_ns_construct(ns, nsid, ctrlr);
 }
 
@@ -3809,6 +3943,10 @@ spdk_nvme_ctrlr_detach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 	int					res;
 	struct spdk_nvme_ns			*ns;
 
+	if (nsid == 0) {
+		return -EINVAL;
+	}
+
 	status = calloc(1, sizeof(*status));
 	if (!status) {
 		NVME_CTRLR_ERRLOG(ctrlr, "Failed to allocate status tracker\n");
@@ -3835,7 +3973,8 @@ spdk_nvme_ctrlr_detach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 		return res;
 	}
 
-	ns = &ctrlr->ns[nsid - 1];
+	ns = spdk_nvme_ctrlr_get_ns(ctrlr, nsid);
+	assert(ns != NULL);
 	/* Inactive NS */
 	nvme_ns_destruct(ns);
 
@@ -3870,8 +4009,12 @@ spdk_nvme_ctrlr_create_ns(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns_dat
 	}
 
 	nsid = status->cpl.cdw0;
-	ns = &ctrlr->ns[nsid - 1];
 	free(status);
+
+	assert(nsid > 0);
+
+	ns = spdk_nvme_ctrlr_get_ns(ctrlr, nsid);
+	assert(ns != NULL);
 	/* Inactive NS */
 	res = nvme_ns_construct(ns, nsid, ctrlr);
 	if (res) {
@@ -3889,6 +4032,10 @@ spdk_nvme_ctrlr_delete_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid)
 	int					res;
 	struct spdk_nvme_ns			*ns;
 
+	if (nsid == 0) {
+		return -EINVAL;
+	}
+
 	status = calloc(1, sizeof(*status));
 	if (!status) {
 		NVME_CTRLR_ERRLOG(ctrlr, "Failed to allocate status tracker\n");
@@ -3914,7 +4061,8 @@ spdk_nvme_ctrlr_delete_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid)
 		return res;
 	}
 
-	ns = &ctrlr->ns[nsid - 1];
+	ns = spdk_nvme_ctrlr_get_ns(ctrlr, nsid);
+	assert(ns != NULL);
 	nvme_ns_destruct(ns);
 
 	return 0;
@@ -4156,6 +4304,17 @@ spdk_nvme_ctrlr_is_discovery(struct spdk_nvme_ctrlr *ctrlr)
 			strlen(SPDK_NVMF_DISCOVERY_NQN));
 }
 
+bool
+spdk_nvme_ctrlr_is_fabrics(struct spdk_nvme_ctrlr *ctrlr)
+{
+	assert(ctrlr);
+
+	/* We always define non-fabrics trtypes outside of the 8-bit range
+	 * of NVMe-oF trtype.
+	 */
+	return ctrlr->trid.trtype < UINT8_MAX;
+}
+
 int
 spdk_nvme_ctrlr_security_receive(struct spdk_nvme_ctrlr *ctrlr, uint8_t secp,
 				 uint16_t spsp, uint8_t nssf, void *payload, size_t size)
diff --git a/lib/nvme/nvme_ctrlr_cmd.c b/lib/nvme/nvme_ctrlr_cmd.c
index 29d76f333..3d54ba363 100644
--- a/lib/nvme/nvme_ctrlr_cmd.c
+++ b/lib/nvme/nvme_ctrlr_cmd.c
@@ -1,8 +1,8 @@
 /*-
  *   BSD LICENSE
  *
- *   Copyright (c) Intel Corporation.
- *   All rights reserved.
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2021 Mellanox Technologies LTD. All rights reserved.
  *
  *   Redistribution and use in source and binary forms, with or without
  *   modification, are permitted provided that the following conditions
@@ -93,8 +93,9 @@ spdk_nvme_ctrlr_cmd_io_raw_with_md(struct spdk_nvme_ctrlr *ctrlr,
 
 	/* Caculate metadata length */
 	if (md_buf) {
-		struct spdk_nvme_ns *ns = &ctrlr->ns[cmd->nsid - 1];
+		struct spdk_nvme_ns *ns = spdk_nvme_ctrlr_get_ns(ctrlr, cmd->nsid);
 
+		assert(ns != NULL);
 		assert(ns->sector_size != 0);
 		md_len =  len / ns->sector_size * ns->md_size;
 	}
@@ -988,7 +989,9 @@ nvme_ctrlr_cmd_directive(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 	cmd->opc = opc_type;
 	cmd->nsid = nsid;
 
-	cmd->cdw10 = (payload_size >> 2) - 1;
+	if ((payload_size >> 2) > 0) {
+		cmd->cdw10 = (payload_size >> 2) - 1;
+	}
 	cmd->cdw11_bits.directive.doper = doper;
 	cmd->cdw11_bits.directive.dtype = dtype;
 	cmd->cdw11_bits.directive.dspec = dspec;
diff --git a/lib/nvme/nvme_cuse.c b/lib/nvme/nvme_cuse.c
index 62d1422df..2a38ba4d2 100644
--- a/lib/nvme/nvme_cuse.c
+++ b/lib/nvme/nvme_cuse.c
@@ -96,7 +96,7 @@ cuse_io_ctx_free(struct cuse_io_ctx *ctx)
 	}
 
 static void
-cuse_nvme_admin_cmd_cb(void *arg, const struct spdk_nvme_cpl *cpl)
+cuse_nvme_passthru_cmd_cb(void *arg, const struct spdk_nvme_cpl *cpl)
 {
 	struct cuse_io_ctx *ctx = arg;
 	struct iovec out_iov[2];
@@ -124,13 +124,18 @@ cuse_nvme_admin_cmd_cb(void *arg, const struct spdk_nvme_cpl *cpl)
 }
 
 static void
-cuse_nvme_admin_cmd_execute(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, void *arg)
+cuse_nvme_passthru_cmd_execute(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, void *arg)
 {
 	int rc;
 	struct cuse_io_ctx *ctx = arg;
 
-	rc = spdk_nvme_ctrlr_cmd_admin_raw(ctrlr, &ctx->nvme_cmd, ctx->data, ctx->data_len,
-					   cuse_nvme_admin_cmd_cb, (void *)ctx);
+	if (nsid != 0) {
+		rc = spdk_nvme_ctrlr_cmd_io_raw(ctrlr, ctrlr->external_io_msgs_qpair, &ctx->nvme_cmd, ctx->data,
+						ctx->data_len, cuse_nvme_passthru_cmd_cb, (void *)ctx);
+	} else {
+		rc = spdk_nvme_ctrlr_cmd_admin_raw(ctrlr, &ctx->nvme_cmd, ctx->data, ctx->data_len,
+						   cuse_nvme_passthru_cmd_cb, (void *)ctx);
+	}
 	if (rc < 0) {
 		fuse_reply_err(ctx->req, EINVAL);
 		cuse_io_ctx_free(ctx);
@@ -138,8 +143,8 @@ cuse_nvme_admin_cmd_execute(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, void *
 }
 
 static void
-cuse_nvme_admin_cmd_send(fuse_req_t req, struct nvme_admin_cmd *admin_cmd,
-			 const void *data)
+cuse_nvme_passthru_cmd_send(fuse_req_t req, struct nvme_passthru_cmd *passthru_cmd,
+			    const void *data, int cmd)
 {
 	struct cuse_io_ctx *ctx;
 	struct cuse_device *cuse_device = fuse_req_userdata(req);
@@ -153,22 +158,22 @@ cuse_nvme_admin_cmd_send(fuse_req_t req, struct nvme_admin_cmd *admin_cmd,
 	}
 
 	ctx->req = req;
-	ctx->data_transfer = spdk_nvme_opc_get_data_transfer(admin_cmd->opcode);
+	ctx->data_transfer = spdk_nvme_opc_get_data_transfer(passthru_cmd->opcode);
 
 	memset(&ctx->nvme_cmd, 0, sizeof(ctx->nvme_cmd));
-	ctx->nvme_cmd.opc = admin_cmd->opcode;
-	ctx->nvme_cmd.nsid = admin_cmd->nsid;
-	ctx->nvme_cmd.cdw10 = admin_cmd->cdw10;
-	ctx->nvme_cmd.cdw11 = admin_cmd->cdw11;
-	ctx->nvme_cmd.cdw12 = admin_cmd->cdw12;
-	ctx->nvme_cmd.cdw13 = admin_cmd->cdw13;
-	ctx->nvme_cmd.cdw14 = admin_cmd->cdw14;
-	ctx->nvme_cmd.cdw15 = admin_cmd->cdw15;
+	ctx->nvme_cmd.opc = passthru_cmd->opcode;
+	ctx->nvme_cmd.nsid = passthru_cmd->nsid;
+	ctx->nvme_cmd.cdw10 = passthru_cmd->cdw10;
+	ctx->nvme_cmd.cdw11 = passthru_cmd->cdw11;
+	ctx->nvme_cmd.cdw12 = passthru_cmd->cdw12;
+	ctx->nvme_cmd.cdw13 = passthru_cmd->cdw13;
+	ctx->nvme_cmd.cdw14 = passthru_cmd->cdw14;
+	ctx->nvme_cmd.cdw15 = passthru_cmd->cdw15;
 
-	ctx->data_len = admin_cmd->data_len;
+	ctx->data_len = passthru_cmd->data_len;
 
 	if (ctx->data_len > 0) {
-		ctx->data = spdk_malloc(ctx->data_len, 0, NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA);
+		ctx->data = spdk_malloc(ctx->data_len, 4096, NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA);
 		if (!ctx->data) {
 			SPDK_ERRLOG("Cannot allocate memory for data\n");
 			fuse_reply_err(req, ENOMEM);
@@ -180,7 +185,13 @@ cuse_nvme_admin_cmd_send(fuse_req_t req, struct nvme_admin_cmd *admin_cmd,
 		}
 	}
 
-	rv = nvme_io_msg_send(cuse_device->ctrlr, 0, cuse_nvme_admin_cmd_execute, ctx);
+	if ((unsigned int)cmd != NVME_IOCTL_ADMIN_CMD) {
+		/* Send NS for IO IOCTLs */
+		rv = nvme_io_msg_send(cuse_device->ctrlr, passthru_cmd->nsid, cuse_nvme_passthru_cmd_execute, ctx);
+	} else {
+		/* NS == 0 for Admin IOCTLs */
+		rv = nvme_io_msg_send(cuse_device->ctrlr, 0, cuse_nvme_passthru_cmd_execute, ctx);
+	}
 	if (rv) {
 		SPDK_ERRLOG("Cannot send io msg to the controller\n");
 		fuse_reply_err(req, -rv);
@@ -190,44 +201,44 @@ cuse_nvme_admin_cmd_send(fuse_req_t req, struct nvme_admin_cmd *admin_cmd,
 }
 
 static void
-cuse_nvme_admin_cmd(fuse_req_t req, int cmd, void *arg,
-		    struct fuse_file_info *fi, unsigned flags,
-		    const void *in_buf, size_t in_bufsz, size_t out_bufsz)
+cuse_nvme_passthru_cmd(fuse_req_t req, int cmd, void *arg,
+		       struct fuse_file_info *fi, unsigned flags,
+		       const void *in_buf, size_t in_bufsz, size_t out_bufsz)
 {
-	struct nvme_admin_cmd *admin_cmd;
+	struct nvme_passthru_cmd *passthru_cmd;
 	struct iovec in_iov[2], out_iov[2];
 
 	in_iov[0].iov_base = (void *)arg;
-	in_iov[0].iov_len = sizeof(*admin_cmd);
+	in_iov[0].iov_len = sizeof(*passthru_cmd);
 	if (in_bufsz == 0) {
 		fuse_reply_ioctl_retry(req, in_iov, 1, NULL, 0);
 		return;
 	}
 
-	admin_cmd = (struct nvme_admin_cmd *)in_buf;
+	passthru_cmd = (struct nvme_passthru_cmd *)in_buf;
 
-	switch (spdk_nvme_opc_get_data_transfer(admin_cmd->opcode)) {
+	switch (spdk_nvme_opc_get_data_transfer(passthru_cmd->opcode)) {
 	case SPDK_NVME_DATA_HOST_TO_CONTROLLER:
-		if (admin_cmd->addr != 0) {
-			in_iov[1].iov_base = (void *)admin_cmd->addr;
-			in_iov[1].iov_len = admin_cmd->data_len;
-			if (in_bufsz == sizeof(*admin_cmd)) {
+		if (passthru_cmd->addr != 0) {
+			in_iov[1].iov_base = (void *)passthru_cmd->addr;
+			in_iov[1].iov_len = passthru_cmd->data_len;
+			if (in_bufsz == sizeof(*passthru_cmd)) {
 				fuse_reply_ioctl_retry(req, in_iov, 2, NULL, 0);
 				return;
 			}
-			cuse_nvme_admin_cmd_send(req, admin_cmd, in_buf + sizeof(*admin_cmd));
+			cuse_nvme_passthru_cmd_send(req, passthru_cmd, in_buf + sizeof(*passthru_cmd), cmd);
 		} else {
-			cuse_nvme_admin_cmd_send(req, admin_cmd, NULL);
+			cuse_nvme_passthru_cmd_send(req, passthru_cmd, NULL, cmd);
 		}
 		return;
 	case SPDK_NVME_DATA_NONE:
 	case SPDK_NVME_DATA_CONTROLLER_TO_HOST:
 		if (out_bufsz == 0) {
-			out_iov[0].iov_base = &((struct nvme_admin_cmd *)arg)->result;
+			out_iov[0].iov_base = &((struct nvme_passthru_cmd *)arg)->result;
 			out_iov[0].iov_len = sizeof(uint32_t);
-			if (admin_cmd->data_len > 0) {
-				out_iov[1].iov_base = (void *)admin_cmd->addr;
-				out_iov[1].iov_len = admin_cmd->data_len;
+			if (passthru_cmd->data_len > 0) {
+				out_iov[1].iov_base = (void *)passthru_cmd->addr;
+				out_iov[1].iov_len = passthru_cmd->data_len;
 				fuse_reply_ioctl_retry(req, in_iov, 1, out_iov, 2);
 			} else {
 				fuse_reply_ioctl_retry(req, in_iov, 1, out_iov, 1);
@@ -235,7 +246,7 @@ cuse_nvme_admin_cmd(fuse_req_t req, int cmd, void *arg,
 			return;
 		}
 
-		cuse_nvme_admin_cmd_send(req, admin_cmd, NULL);
+		cuse_nvme_passthru_cmd_send(req, passthru_cmd, NULL, cmd);
 
 		return;
 	case SPDK_NVME_DATA_BIDIRECTIONAL:
@@ -259,6 +270,21 @@ cuse_nvme_reset_execute(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, void *arg)
 	fuse_reply_ioctl_iov(req, 0, NULL, 0);
 }
 
+static void
+cuse_nvme_subsys_reset_execute(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, void *arg)
+{
+	int rc;
+	fuse_req_t req = arg;
+
+	rc = spdk_nvme_ctrlr_reset_subsystem(ctrlr);
+	if (rc) {
+		fuse_reply_err(req, rc);
+		return;
+	}
+
+	fuse_reply_ioctl_iov(req, 0, NULL, 0);
+}
+
 static void
 cuse_nvme_reset(fuse_req_t req, int cmd, void *arg,
 		struct fuse_file_info *fi, unsigned flags,
@@ -273,13 +299,50 @@ cuse_nvme_reset(fuse_req_t req, int cmd, void *arg,
 		return;
 	}
 
-	rv = nvme_io_msg_send(cuse_device->ctrlr, cuse_device->nsid, cuse_nvme_reset_execute, (void *)req);
+	if (cmd == NVME_IOCTL_SUBSYS_RESET) {
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_SUBSYS_RESET\n");
+		rv = nvme_io_msg_send(cuse_device->ctrlr, cuse_device->nsid, cuse_nvme_subsys_reset_execute,
+				      (void *)req);
+	} else {
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_RESET\n");
+		rv = nvme_io_msg_send(cuse_device->ctrlr, cuse_device->nsid, cuse_nvme_reset_execute, (void *)req);
+	}
 	if (rv) {
 		SPDK_ERRLOG("Cannot send reset\n");
 		fuse_reply_err(req, EINVAL);
 	}
 }
 
+static void
+cuse_nvme_rescan_execute(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, void *arg)
+{
+	fuse_req_t req = arg;
+
+	nvme_ctrlr_update_namespaces(ctrlr);
+	fuse_reply_ioctl_iov(req, 0, NULL, 0);
+}
+
+static void
+cuse_nvme_rescan(fuse_req_t req, int cmd, void *arg,
+		 struct fuse_file_info *fi, unsigned flags,
+		 const void *in_buf, size_t in_bufsz, size_t out_bufsz)
+{
+	int rv;
+	struct cuse_device *cuse_device = fuse_req_userdata(req);
+
+	if (cuse_device->nsid) {
+		SPDK_ERRLOG("Namespace rescan not supported\n");
+		fuse_reply_err(req, EINVAL);
+		return;
+	}
+
+	rv = nvme_io_msg_send(cuse_device->ctrlr, cuse_device->nsid, cuse_nvme_rescan_execute, (void *)req);
+	if (rv) {
+		SPDK_ERRLOG("Cannot send rescan\n");
+		fuse_reply_err(req, EINVAL);
+	}
+}
+
 /*****************************************************************************
  * Namespace IO requests
  */
@@ -535,6 +598,22 @@ cuse_blkgetsize(fuse_req_t req, int cmd, void *arg,
 	fuse_reply_ioctl(req, 0, &size, sizeof(size));
 }
 
+static void
+cuse_blkgetsectorsize(fuse_req_t req, int cmd, void *arg,
+		      struct fuse_file_info *fi, unsigned flags,
+		      const void *in_buf, size_t in_bufsz, size_t out_bufsz)
+{
+	int ssize;
+	struct spdk_nvme_ns *ns;
+	struct cuse_device *cuse_device = fuse_req_userdata(req);
+
+	FUSE_REPLY_CHECK_BUFFER(req, arg, out_bufsz, ssize);
+
+	ns = spdk_nvme_ctrlr_get_ns(cuse_device->ctrlr, cuse_device->nsid);
+	ssize = spdk_nvme_ns_get_sector_size(ns);
+	fuse_reply_ioctl(req, 0, &ssize, sizeof(ssize));
+}
+
 static void
 cuse_getid(fuse_req_t req, int cmd, void *arg,
 	   struct fuse_file_info *fi, unsigned flags,
@@ -557,13 +636,20 @@ cuse_ctrlr_ioctl(fuse_req_t req, int cmd, void *arg,
 
 	switch ((unsigned int)cmd) {
 	case NVME_IOCTL_ADMIN_CMD:
-		cuse_nvme_admin_cmd(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_ADMIN_CMD\n");
+		cuse_nvme_passthru_cmd(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
 
 	case NVME_IOCTL_RESET:
+	case NVME_IOCTL_SUBSYS_RESET:
 		cuse_nvme_reset(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
 
+	case NVME_IOCTL_RESCAN:
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_RESCAN\n");
+		cuse_nvme_rescan(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
+		break;
+
 	default:
 		SPDK_ERRLOG("Unsupported IOCTL 0x%X.\n", cmd);
 		fuse_reply_err(req, EINVAL);
@@ -582,27 +668,43 @@ cuse_ns_ioctl(fuse_req_t req, int cmd, void *arg,
 
 	switch ((unsigned int)cmd) {
 	case NVME_IOCTL_ADMIN_CMD:
-		cuse_nvme_admin_cmd(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_ADMIN_CMD\n");
+		cuse_nvme_passthru_cmd(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
 
 	case NVME_IOCTL_SUBMIT_IO:
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_SUBMIT_IO\n");
 		cuse_nvme_submit_io(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
 
+	case NVME_IOCTL_IO_CMD:
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_IO_CMD\n");
+		cuse_nvme_passthru_cmd(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
+		break;
+
 	case NVME_IOCTL_ID:
+		SPDK_DEBUGLOG(nvme_cuse, "NVME_IOCTL_ID\n");
 		cuse_getid(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
 
 	case BLKPBSZGET:
+		SPDK_DEBUGLOG(nvme_cuse, "BLKPBSZGET\n");
 		cuse_blkpbszget(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
 
+	case BLKSSZGET:
+		SPDK_DEBUGLOG(nvme_cuse, "BLKSSZGET\n");
+		cuse_blkgetsectorsize(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
+		break;
+
 	case BLKGETSIZE:
+		SPDK_DEBUGLOG(nvme_cuse, "BLKGETSIZE\n");
 		/* Returns the device size as a number of 512-byte blocks (returns pointer to long) */
 		cuse_blkgetsize(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
 
 	case BLKGETSIZE64:
+		SPDK_DEBUGLOG(nvme_cuse, "BLKGETSIZE64\n");
 		/* Returns the device size in sectors (returns pointer to uint64_t) */
 		cuse_blkgetsize64(req, cmd, arg, fi, flags, in_buf, in_bufsz, out_bufsz);
 		break;
@@ -1103,3 +1205,5 @@ spdk_nvme_cuse_get_ns_name(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, char *n
 
 	return 0;
 }
+
+SPDK_LOG_REGISTER_COMPONENT(nvme_cuse)
diff --git a/lib/nvme/nvme_fabric.c b/lib/nvme/nvme_fabric.c
index 7bf785359..fd4a5cc57 100644
--- a/lib/nvme/nvme_fabric.c
+++ b/lib/nvme/nvme_fabric.c
@@ -265,7 +265,6 @@ nvme_fabric_ctrlr_scan(struct spdk_nvme_probe_ctx *probe_ctx,
 {
 	struct spdk_nvme_ctrlr_opts discovery_opts;
 	struct spdk_nvme_ctrlr *discovery_ctrlr;
-	union spdk_nvme_cc_register cc;
 	int rc;
 	struct nvme_completion_poll_status *status;
 
@@ -283,19 +282,12 @@ nvme_fabric_ctrlr_scan(struct spdk_nvme_probe_ctx *probe_ctx,
 	if (discovery_ctrlr == NULL) {
 		return -1;
 	}
-	nvme_qpair_set_state(discovery_ctrlr->adminq, NVME_QPAIR_ENABLED);
-
-	/* TODO: this should be using the normal NVMe controller initialization process +1 */
-	cc.raw = 0;
-	cc.bits.en = 1;
-	cc.bits.iosqes = 6; /* SQ entry size == 64 == 2^6 */
-	cc.bits.iocqes = 4; /* CQ entry size == 16 == 2^4 */
-	rc = nvme_transport_ctrlr_set_reg_4(discovery_ctrlr, offsetof(struct spdk_nvme_registers, cc.raw),
-					    cc.raw);
-	if (rc < 0) {
-		SPDK_ERRLOG("Failed to set cc\n");
-		nvme_ctrlr_destruct(discovery_ctrlr);
-		return -1;
+
+	while (discovery_ctrlr->state != NVME_CTRLR_STATE_READY) {
+		if (nvme_ctrlr_process_init(discovery_ctrlr) != 0) {
+			nvme_ctrlr_destruct(discovery_ctrlr);
+			return -1;
+		}
 	}
 
 	status = calloc(1, sizeof(*status));
diff --git a/lib/nvme/nvme_internal.h b/lib/nvme/nvme_internal.h
index 115f33ec1..ebedac38f 100644
--- a/lib/nvme/nvme_internal.h
+++ b/lib/nvme/nvme_internal.h
@@ -455,6 +455,7 @@ struct spdk_nvme_qpair {
 	const struct spdk_nvme_transport	*transport;
 
 	uint8_t					transport_failure_reason: 2;
+	uint8_t					last_transport_failure_reason: 2;
 };
 
 struct spdk_nvme_poll_group {
@@ -518,9 +519,29 @@ enum nvme_ctrlr_state {
 	NVME_CTRLR_STATE_INIT_DELAY,
 
 	/**
-	 * Controller has not been initialized yet.
+	 * Connect the admin queue.
 	 */
-	NVME_CTRLR_STATE_INIT,
+	NVME_CTRLR_STATE_CONNECT_ADMINQ,
+
+	/**
+	 * Controller has not started initialized yet.
+	 */
+	NVME_CTRLR_STATE_INIT = NVME_CTRLR_STATE_CONNECT_ADMINQ,
+
+	/**
+	 * Read Version (VS) register.
+	 */
+	NVME_CTRLR_STATE_READ_VS,
+
+	/**
+	 * Read Capabilities (CAP) register.
+	 */
+	NVME_CTRLR_STATE_READ_CAP,
+
+	/**
+	 * Check EN to prepare for controller initialization.
+	 */
+	NVME_CTRLR_STATE_CHECK_EN,
 
 	/**
 	 * Waiting for CSTS.RDY to transition from 0 to 1 so that CC.EN may be set to 0.
@@ -587,11 +608,6 @@ enum nvme_ctrlr_state {
 	 */
 	NVME_CTRLR_STATE_WAIT_FOR_SET_NUM_QUEUES,
 
-	/**
-	 * Construct Namespace data structures of the controller.
-	 */
-	NVME_CTRLR_STATE_CONSTRUCT_NS,
-
 	/**
 	 * Get active Namespace list of the controller.
 	 */
@@ -602,6 +618,11 @@ enum nvme_ctrlr_state {
 	 */
 	NVME_CTRLR_STATE_WAIT_FOR_IDENTIFY_ACTIVE_NS,
 
+	/**
+	 * Construct Namespace data structures of the controller.
+	 */
+	NVME_CTRLR_STATE_CONSTRUCT_NS,
+
 	/**
 	 * Get Identify Namespace Data structure for each NS.
 	 */
@@ -826,6 +847,7 @@ struct spdk_nvme_ctrlr {
 	/**
 	 * Keep track of active namespaces
 	 */
+	uint32_t			max_active_ns_idx;
 	uint32_t			*active_ns_list;
 
 	struct spdk_bit_array		*free_io_qids;
@@ -1031,8 +1053,6 @@ int	nvme_ctrlr_get_vs(struct spdk_nvme_ctrlr *ctrlr, union spdk_nvme_vs_register
 int	nvme_ctrlr_get_cmbsz(struct spdk_nvme_ctrlr *ctrlr, union spdk_nvme_cmbsz_register *cmbsz);
 int	nvme_ctrlr_get_pmrcap(struct spdk_nvme_ctrlr *ctrlr, union spdk_nvme_pmrcap_register *pmrcap);
 bool	nvme_ctrlr_multi_iocs_enabled(struct spdk_nvme_ctrlr *ctrlr);
-void	nvme_ctrlr_init_cap(struct spdk_nvme_ctrlr *ctrlr, const union spdk_nvme_cap_register *cap,
-			    const union spdk_nvme_vs_register *vs);
 void    nvme_ctrlr_process_async_event(struct spdk_nvme_ctrlr *ctrlr,
 				       const struct spdk_nvme_cpl *cpl);
 void nvme_ctrlr_disconnect_qpair(struct spdk_nvme_qpair *qpair);
@@ -1299,6 +1319,7 @@ const struct spdk_nvme_transport *nvme_get_transport(const char *transport_name)
 const struct spdk_nvme_transport *nvme_get_first_transport(void);
 const struct spdk_nvme_transport *nvme_get_next_transport(const struct spdk_nvme_transport
 		*transport);
+void  nvme_ctrlr_update_namespaces(struct spdk_nvme_ctrlr *ctrlr);
 
 /* Transport specific functions */
 struct spdk_nvme_ctrlr *nvme_transport_ctrlr_construct(const struct spdk_nvme_transport_id *trid,
diff --git a/lib/nvme/nvme_ns_cmd.c b/lib/nvme/nvme_ns_cmd.c
index 1f03d761a..7904f6d12 100644
--- a/lib/nvme/nvme_ns_cmd.c
+++ b/lib/nvme/nvme_ns_cmd.c
@@ -1091,6 +1091,40 @@ spdk_nvme_ns_cmd_dataset_management(struct spdk_nvme_ns *ns, struct spdk_nvme_qp
 	return nvme_qpair_submit_request(qpair, req);
 }
 
+int
+spdk_nvme_ns_cmd_copy(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+		      const struct spdk_nvme_scc_source_range *ranges,
+		      uint16_t num_ranges, uint64_t dest_lba,
+		      spdk_nvme_cmd_cb cb_fn, void *cb_arg)
+{
+	struct nvme_request	*req;
+	struct spdk_nvme_cmd	*cmd;
+
+	if (num_ranges == 0) {
+		return -EINVAL;
+	}
+
+	if (ranges == NULL) {
+		return -EINVAL;
+	}
+
+	req = nvme_allocate_request_user_copy(qpair, (void *)ranges,
+					      num_ranges * sizeof(struct spdk_nvme_scc_source_range),
+					      cb_fn, cb_arg, true);
+	if (req == NULL) {
+		return -ENOMEM;
+	}
+
+	cmd = &req->cmd;
+	cmd->opc = SPDK_NVME_OPC_COPY;
+	cmd->nsid = ns->id;
+
+	*(uint64_t *)&cmd->cdw10 = dest_lba;
+	cmd->cdw12 = num_ranges - 1;
+
+	return nvme_qpair_submit_request(qpair, req);
+}
+
 int
 spdk_nvme_ns_cmd_flush(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		       spdk_nvme_cmd_cb cb_fn, void *cb_arg)
diff --git a/lib/nvme/nvme_opal.c b/lib/nvme/nvme_opal.c
index 8bd2ccf02..9f26a9046 100644
--- a/lib/nvme/nvme_opal.c
+++ b/lib/nvme/nvme_opal.c
@@ -841,7 +841,37 @@ static int
 opal_discovery0(struct spdk_opal_dev *dev, void *payload, uint32_t payload_size)
 {
 	int ret;
+	uint16_t i, sp_list_len;
+	uint8_t *sp_list;
+	bool sp_tcg_supported = false;
 
+	/* NVMe 1.4 chapter 5.25.2 Security Protocol 00h */
+	ret = spdk_nvme_ctrlr_security_receive(dev->ctrlr, SPDK_SCSI_SECP_INFO, 0,
+					       0, payload, payload_size);
+	if (ret) {
+		return ret;
+	}
+
+	/* spc4r31 chapter 7.7.1.3 Supported security protocols list description */
+	sp_list_len = from_be16(payload + 6);
+	sp_list = (uint8_t *)payload + 8;
+
+	if (sp_list_len + 8 > (int)payload_size) {
+		return -EINVAL;
+	}
+
+	for (i = 0; i < sp_list_len; i++) {
+		if (sp_list[i] == SPDK_SCSI_SECP_TCG) {
+			sp_tcg_supported = true;
+			break;
+		}
+	}
+
+	if (!sp_tcg_supported) {
+		return -ENOTSUP;
+	}
+
+	memset(payload, 0, payload_size);
 	ret = spdk_nvme_ctrlr_security_receive(dev->ctrlr, SPDK_SCSI_SECP_TCG, LV0_DISCOVERY_COMID,
 					       0, payload, payload_size);
 	if (ret) {
diff --git a/lib/nvme/nvme_pcie.c b/lib/nvme/nvme_pcie.c
index 2a5a7f72d..24d6e889d 100644
--- a/lib/nvme/nvme_pcie.c
+++ b/lib/nvme/nvme_pcie.c
@@ -54,7 +54,7 @@ static bool g_sigset = false;
 static spdk_nvme_pcie_hotplug_filter_cb g_hotplug_filter_cb;
 
 static void
-nvme_sigbus_fault_sighandler(siginfo_t *info, void *ctx)
+nvme_sigbus_fault_sighandler(const void *failure_addr, void *ctx)
 {
 	void *map_address;
 	uint16_t flag = 0;
@@ -908,7 +908,6 @@ static struct spdk_nvme_ctrlr *nvme_pcie_ctrlr_construct(const struct spdk_nvme_
 	struct spdk_pci_device *pci_dev = devhandle;
 	struct nvme_pcie_ctrlr *pctrlr;
 	union spdk_nvme_cap_register cap;
-	union spdk_nvme_vs_register vs;
 	uint16_t cmd_reg;
 	int rc;
 	struct spdk_pci_id pci_id;
@@ -960,15 +959,6 @@ static struct spdk_nvme_ctrlr *nvme_pcie_ctrlr_construct(const struct spdk_nvme_
 		return NULL;
 	}
 
-	if (nvme_ctrlr_get_vs(&pctrlr->ctrlr, &vs)) {
-		SPDK_ERRLOG("get_vs() failed\n");
-		spdk_pci_device_unclaim(pci_dev);
-		spdk_free(pctrlr);
-		return NULL;
-	}
-
-	nvme_ctrlr_init_cap(&pctrlr->ctrlr, &cap, &vs);
-
 	/* Doorbell stride is 2 ^ (dstrd + 2),
 	 * but we want multiples of 4, so drop the + 2 */
 	pctrlr->doorbell_stride_u32 = 1 << cap.bits.dstrd;
@@ -1077,520 +1067,6 @@ nvme_pcie_qpair_iterate_requests(struct spdk_nvme_qpair *qpair,
 	return 0;
 }
 
-static void
-nvme_pcie_fail_request_bad_vtophys(struct spdk_nvme_qpair *qpair, struct nvme_tracker *tr)
-{
-	/*
-	 * Bad vtophys translation, so abort this request and return
-	 *  immediately.
-	 */
-	nvme_pcie_qpair_manual_complete_tracker(qpair, tr, SPDK_NVME_SCT_GENERIC,
-						SPDK_NVME_SC_INVALID_FIELD,
-						1 /* do not retry */, true);
-}
-
-/*
- * Append PRP list entries to describe a virtually contiguous buffer starting at virt_addr of len bytes.
- *
- * *prp_index will be updated to account for the number of PRP entries used.
- */
-static inline int
-nvme_pcie_prp_list_append(struct nvme_tracker *tr, uint32_t *prp_index, void *virt_addr, size_t len,
-			  uint32_t page_size)
-{
-	struct spdk_nvme_cmd *cmd = &tr->req->cmd;
-	uintptr_t page_mask = page_size - 1;
-	uint64_t phys_addr;
-	uint32_t i;
-
-	SPDK_DEBUGLOG(nvme, "prp_index:%u virt_addr:%p len:%u\n",
-		      *prp_index, virt_addr, (uint32_t)len);
-
-	if (spdk_unlikely(((uintptr_t)virt_addr & 3) != 0)) {
-		SPDK_ERRLOG("virt_addr %p not dword aligned\n", virt_addr);
-		return -EFAULT;
-	}
-
-	i = *prp_index;
-	while (len) {
-		uint32_t seg_len;
-
-		/*
-		 * prp_index 0 is stored in prp1, and the rest are stored in the prp[] array,
-		 * so prp_index == count is valid.
-		 */
-		if (spdk_unlikely(i > SPDK_COUNTOF(tr->u.prp))) {
-			SPDK_ERRLOG("out of PRP entries\n");
-			return -EFAULT;
-		}
-
-		phys_addr = spdk_vtophys(virt_addr, NULL);
-		if (spdk_unlikely(phys_addr == SPDK_VTOPHYS_ERROR)) {
-			SPDK_ERRLOG("vtophys(%p) failed\n", virt_addr);
-			return -EFAULT;
-		}
-
-		if (i == 0) {
-			SPDK_DEBUGLOG(nvme, "prp1 = %p\n", (void *)phys_addr);
-			cmd->dptr.prp.prp1 = phys_addr;
-			seg_len = page_size - ((uintptr_t)virt_addr & page_mask);
-		} else {
-			if ((phys_addr & page_mask) != 0) {
-				SPDK_ERRLOG("PRP %u not page aligned (%p)\n", i, virt_addr);
-				return -EFAULT;
-			}
-
-			SPDK_DEBUGLOG(nvme, "prp[%u] = %p\n", i - 1, (void *)phys_addr);
-			tr->u.prp[i - 1] = phys_addr;
-			seg_len = page_size;
-		}
-
-		seg_len = spdk_min(seg_len, len);
-		virt_addr += seg_len;
-		len -= seg_len;
-		i++;
-	}
-
-	cmd->psdt = SPDK_NVME_PSDT_PRP;
-	if (i <= 1) {
-		cmd->dptr.prp.prp2 = 0;
-	} else if (i == 2) {
-		cmd->dptr.prp.prp2 = tr->u.prp[0];
-		SPDK_DEBUGLOG(nvme, "prp2 = %p\n", (void *)cmd->dptr.prp.prp2);
-	} else {
-		cmd->dptr.prp.prp2 = tr->prp_sgl_bus_addr;
-		SPDK_DEBUGLOG(nvme, "prp2 = %p (PRP list)\n", (void *)cmd->dptr.prp.prp2);
-	}
-
-	*prp_index = i;
-	return 0;
-}
-
-static int
-nvme_pcie_qpair_build_request_invalid(struct spdk_nvme_qpair *qpair,
-				      struct nvme_request *req, struct nvme_tracker *tr, bool dword_aligned)
-{
-	assert(0);
-	nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-	return -EINVAL;
-}
-
-/**
- * Build PRP list describing physically contiguous payload buffer.
- */
-static int
-nvme_pcie_qpair_build_contig_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
-				     struct nvme_tracker *tr, bool dword_aligned)
-{
-	uint32_t prp_index = 0;
-	int rc;
-
-	rc = nvme_pcie_prp_list_append(tr, &prp_index, req->payload.contig_or_cb_arg + req->payload_offset,
-				       req->payload_size, qpair->ctrlr->page_size);
-	if (rc) {
-		nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-	}
-
-	return rc;
-}
-
-/**
- * Build an SGL describing a physically contiguous payload buffer.
- *
- * This is more efficient than using PRP because large buffers can be
- * described this way.
- */
-static int
-nvme_pcie_qpair_build_contig_hw_sgl_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
-		struct nvme_tracker *tr, bool dword_aligned)
-{
-	void *virt_addr;
-	uint64_t phys_addr, mapping_length;
-	uint32_t length;
-	struct spdk_nvme_sgl_descriptor *sgl;
-	uint32_t nseg = 0;
-
-	assert(req->payload_size != 0);
-	assert(nvme_payload_type(&req->payload) == NVME_PAYLOAD_TYPE_CONTIG);
-
-	sgl = tr->u.sgl;
-	req->cmd.psdt = SPDK_NVME_PSDT_SGL_MPTR_CONTIG;
-	req->cmd.dptr.sgl1.unkeyed.subtype = 0;
-
-	length = req->payload_size;
-	virt_addr = req->payload.contig_or_cb_arg + req->payload_offset;
-
-	while (length > 0) {
-		if (nseg >= NVME_MAX_SGL_DESCRIPTORS) {
-			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-			return -EFAULT;
-		}
-
-		if (dword_aligned && ((uintptr_t)virt_addr & 3)) {
-			SPDK_ERRLOG("virt_addr %p not dword aligned\n", virt_addr);
-			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-			return -EFAULT;
-		}
-
-		mapping_length = length;
-		phys_addr = spdk_vtophys(virt_addr, &mapping_length);
-		if (phys_addr == SPDK_VTOPHYS_ERROR) {
-			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-			return -EFAULT;
-		}
-
-		mapping_length = spdk_min(length, mapping_length);
-
-		length -= mapping_length;
-		virt_addr += mapping_length;
-
-		sgl->unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
-		sgl->unkeyed.length = mapping_length;
-		sgl->address = phys_addr;
-		sgl->unkeyed.subtype = 0;
-
-		sgl++;
-		nseg++;
-	}
-
-	if (nseg == 1) {
-		/*
-		 * The whole transfer can be described by a single SGL descriptor.
-		 *  Use the special case described by the spec where SGL1's type is Data Block.
-		 *  This means the SGL in the tracker is not used at all, so copy the first (and only)
-		 *  SGL element into SGL1.
-		 */
-		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
-		req->cmd.dptr.sgl1.address = tr->u.sgl[0].address;
-		req->cmd.dptr.sgl1.unkeyed.length = tr->u.sgl[0].unkeyed.length;
-	} else {
-		/* SPDK NVMe driver supports only 1 SGL segment for now, it is enough because
-		 *  NVME_MAX_SGL_DESCRIPTORS * 16 is less than one page.
-		 */
-		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_LAST_SEGMENT;
-		req->cmd.dptr.sgl1.address = tr->prp_sgl_bus_addr;
-		req->cmd.dptr.sgl1.unkeyed.length = nseg * sizeof(struct spdk_nvme_sgl_descriptor);
-	}
-
-	return 0;
-}
-
-/**
- * Build SGL list describing scattered payload buffer.
- */
-static int
-nvme_pcie_qpair_build_hw_sgl_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
-				     struct nvme_tracker *tr, bool dword_aligned)
-{
-	int rc;
-	void *virt_addr;
-	uint64_t phys_addr, mapping_length;
-	uint32_t remaining_transfer_len, remaining_user_sge_len, length;
-	struct spdk_nvme_sgl_descriptor *sgl;
-	uint32_t nseg = 0;
-
-	/*
-	 * Build scattered payloads.
-	 */
-	assert(req->payload_size != 0);
-	assert(nvme_payload_type(&req->payload) == NVME_PAYLOAD_TYPE_SGL);
-	assert(req->payload.reset_sgl_fn != NULL);
-	assert(req->payload.next_sge_fn != NULL);
-	req->payload.reset_sgl_fn(req->payload.contig_or_cb_arg, req->payload_offset);
-
-	sgl = tr->u.sgl;
-	req->cmd.psdt = SPDK_NVME_PSDT_SGL_MPTR_CONTIG;
-	req->cmd.dptr.sgl1.unkeyed.subtype = 0;
-
-	remaining_transfer_len = req->payload_size;
-
-	while (remaining_transfer_len > 0) {
-		rc = req->payload.next_sge_fn(req->payload.contig_or_cb_arg,
-					      &virt_addr, &remaining_user_sge_len);
-		if (rc) {
-			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-			return -EFAULT;
-		}
-
-		/* Bit Bucket SGL descriptor */
-		if ((uint64_t)virt_addr == UINT64_MAX) {
-			/* TODO: enable WRITE and COMPARE when necessary */
-			if (req->cmd.opc != SPDK_NVME_OPC_READ) {
-				SPDK_ERRLOG("Only READ command can be supported\n");
-				goto exit;
-			}
-			if (nseg >= NVME_MAX_SGL_DESCRIPTORS) {
-				SPDK_ERRLOG("Too many SGL entries\n");
-				goto exit;
-			}
-
-			sgl->unkeyed.type = SPDK_NVME_SGL_TYPE_BIT_BUCKET;
-			/* If the SGL describes a destination data buffer, the length of data
-			 * buffer shall be discarded by controller, and the length is included
-			 * in Number of Logical Blocks (NLB) parameter. Otherwise, the length
-			 * is not included in the NLB parameter.
-			 */
-			remaining_user_sge_len = spdk_min(remaining_user_sge_len, remaining_transfer_len);
-			remaining_transfer_len -= remaining_user_sge_len;
-
-			sgl->unkeyed.length = remaining_user_sge_len;
-			sgl->address = 0;
-			sgl->unkeyed.subtype = 0;
-
-			sgl++;
-			nseg++;
-
-			continue;
-		}
-
-		remaining_user_sge_len = spdk_min(remaining_user_sge_len, remaining_transfer_len);
-		remaining_transfer_len -= remaining_user_sge_len;
-		while (remaining_user_sge_len > 0) {
-			if (nseg >= NVME_MAX_SGL_DESCRIPTORS) {
-				SPDK_ERRLOG("Too many SGL entries\n");
-				goto exit;
-			}
-
-			if (dword_aligned && ((uintptr_t)virt_addr & 3)) {
-				SPDK_ERRLOG("virt_addr %p not dword aligned\n", virt_addr);
-				goto exit;
-			}
-
-			mapping_length = remaining_user_sge_len;
-			phys_addr = spdk_vtophys(virt_addr, &mapping_length);
-			if (phys_addr == SPDK_VTOPHYS_ERROR) {
-				goto exit;
-			}
-
-			length = spdk_min(remaining_user_sge_len, mapping_length);
-			remaining_user_sge_len -= length;
-			virt_addr += length;
-
-			if (nseg > 0 && phys_addr ==
-			    (*(sgl - 1)).address + (*(sgl - 1)).unkeyed.length) {
-				/* extend previous entry */
-				(*(sgl - 1)).unkeyed.length += length;
-				continue;
-			}
-
-			sgl->unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
-			sgl->unkeyed.length = length;
-			sgl->address = phys_addr;
-			sgl->unkeyed.subtype = 0;
-
-			sgl++;
-			nseg++;
-		}
-	}
-
-	if (nseg == 1) {
-		/*
-		 * The whole transfer can be described by a single SGL descriptor.
-		 *  Use the special case described by the spec where SGL1's type is Data Block.
-		 *  This means the SGL in the tracker is not used at all, so copy the first (and only)
-		 *  SGL element into SGL1.
-		 */
-		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
-		req->cmd.dptr.sgl1.address = tr->u.sgl[0].address;
-		req->cmd.dptr.sgl1.unkeyed.length = tr->u.sgl[0].unkeyed.length;
-	} else {
-		/* SPDK NVMe driver supports only 1 SGL segment for now, it is enough because
-		 *  NVME_MAX_SGL_DESCRIPTORS * 16 is less than one page.
-		 */
-		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_LAST_SEGMENT;
-		req->cmd.dptr.sgl1.address = tr->prp_sgl_bus_addr;
-		req->cmd.dptr.sgl1.unkeyed.length = nseg * sizeof(struct spdk_nvme_sgl_descriptor);
-	}
-
-	return 0;
-
-exit:
-	nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-	return -EFAULT;
-}
-
-/**
- * Build PRP list describing scattered payload buffer.
- */
-static int
-nvme_pcie_qpair_build_prps_sgl_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
-				       struct nvme_tracker *tr, bool dword_aligned)
-{
-	int rc;
-	void *virt_addr;
-	uint32_t remaining_transfer_len, length;
-	uint32_t prp_index = 0;
-	uint32_t page_size = qpair->ctrlr->page_size;
-
-	/*
-	 * Build scattered payloads.
-	 */
-	assert(nvme_payload_type(&req->payload) == NVME_PAYLOAD_TYPE_SGL);
-	assert(req->payload.reset_sgl_fn != NULL);
-	req->payload.reset_sgl_fn(req->payload.contig_or_cb_arg, req->payload_offset);
-
-	remaining_transfer_len = req->payload_size;
-	while (remaining_transfer_len > 0) {
-		assert(req->payload.next_sge_fn != NULL);
-		rc = req->payload.next_sge_fn(req->payload.contig_or_cb_arg, &virt_addr, &length);
-		if (rc) {
-			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-			return -EFAULT;
-		}
-
-		length = spdk_min(remaining_transfer_len, length);
-
-		/*
-		 * Any incompatible sges should have been handled up in the splitting routine,
-		 *  but assert here as an additional check.
-		 *
-		 * All SGEs except last must end on a page boundary.
-		 */
-		assert((length == remaining_transfer_len) ||
-		       _is_page_aligned((uintptr_t)virt_addr + length, page_size));
-
-		rc = nvme_pcie_prp_list_append(tr, &prp_index, virt_addr, length, page_size);
-		if (rc) {
-			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-			return rc;
-		}
-
-		remaining_transfer_len -= length;
-	}
-
-	return 0;
-}
-
-typedef int(*build_req_fn)(struct spdk_nvme_qpair *, struct nvme_request *, struct nvme_tracker *,
-			   bool);
-
-static build_req_fn const g_nvme_pcie_build_req_table[][2] = {
-	[NVME_PAYLOAD_TYPE_INVALID] = {
-		nvme_pcie_qpair_build_request_invalid,			/* PRP */
-		nvme_pcie_qpair_build_request_invalid			/* SGL */
-	},
-	[NVME_PAYLOAD_TYPE_CONTIG] = {
-		nvme_pcie_qpair_build_contig_request,			/* PRP */
-		nvme_pcie_qpair_build_contig_hw_sgl_request		/* SGL */
-	},
-	[NVME_PAYLOAD_TYPE_SGL] = {
-		nvme_pcie_qpair_build_prps_sgl_request,			/* PRP */
-		nvme_pcie_qpair_build_hw_sgl_request			/* SGL */
-	}
-};
-
-static int
-nvme_pcie_qpair_build_metadata(struct spdk_nvme_qpair *qpair, struct nvme_tracker *tr,
-			       bool sgl_supported, bool dword_aligned)
-{
-	void *md_payload;
-	struct nvme_request *req = tr->req;
-
-	if (req->payload.md) {
-		md_payload = req->payload.md + req->md_offset;
-		if (dword_aligned && ((uintptr_t)md_payload & 3)) {
-			SPDK_ERRLOG("virt_addr %p not dword aligned\n", md_payload);
-			goto exit;
-		}
-
-		if (sgl_supported && dword_aligned) {
-			assert(req->cmd.psdt == SPDK_NVME_PSDT_SGL_MPTR_CONTIG);
-			req->cmd.psdt = SPDK_NVME_PSDT_SGL_MPTR_SGL;
-			tr->meta_sgl.address = spdk_vtophys(md_payload, NULL);
-			if (tr->meta_sgl.address == SPDK_VTOPHYS_ERROR) {
-				goto exit;
-			}
-			tr->meta_sgl.unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
-			tr->meta_sgl.unkeyed.length = req->md_size;
-			tr->meta_sgl.unkeyed.subtype = 0;
-			req->cmd.mptr = tr->prp_sgl_bus_addr - sizeof(struct spdk_nvme_sgl_descriptor);
-		} else {
-			req->cmd.mptr = spdk_vtophys(md_payload, NULL);
-			if (req->cmd.mptr == SPDK_VTOPHYS_ERROR) {
-				goto exit;
-			}
-		}
-	}
-
-	return 0;
-
-exit:
-	nvme_pcie_fail_request_bad_vtophys(qpair, tr);
-	return -EINVAL;
-}
-
-static int
-nvme_pcie_qpair_submit_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req)
-{
-	struct nvme_tracker	*tr;
-	int			rc = 0;
-	struct spdk_nvme_ctrlr	*ctrlr = qpair->ctrlr;
-	struct nvme_pcie_qpair	*pqpair = nvme_pcie_qpair(qpair);
-	enum nvme_payload_type	payload_type;
-	bool			sgl_supported;
-	bool			dword_aligned = true;
-
-	if (spdk_unlikely(nvme_qpair_is_admin_queue(qpair))) {
-		nvme_robust_mutex_lock(&ctrlr->ctrlr_lock);
-	}
-
-	tr = TAILQ_FIRST(&pqpair->free_tr);
-
-	if (tr == NULL) {
-		pqpair->stat->queued_requests++;
-		/* Inform the upper layer to try again later. */
-		rc = -EAGAIN;
-		goto exit;
-	}
-
-	pqpair->stat->submitted_requests++;
-	TAILQ_REMOVE(&pqpair->free_tr, tr, tq_list); /* remove tr from free_tr */
-	TAILQ_INSERT_TAIL(&pqpair->outstanding_tr, tr, tq_list);
-	tr->req = req;
-	tr->cb_fn = req->cb_fn;
-	tr->cb_arg = req->cb_arg;
-	req->cmd.cid = tr->cid;
-
-	if (req->payload_size != 0) {
-		payload_type = nvme_payload_type(&req->payload);
-		/* According to the specification, PRPs shall be used for all
-		 *  Admin commands for NVMe over PCIe implementations.
-		 */
-		sgl_supported = (ctrlr->flags & SPDK_NVME_CTRLR_SGL_SUPPORTED) != 0 &&
-				!nvme_qpair_is_admin_queue(qpair);
-
-		if (sgl_supported) {
-			/* Don't use SGL for DSM command */
-			if (spdk_unlikely((ctrlr->quirks & NVME_QUIRK_NO_SGL_FOR_DSM) &&
-					  (req->cmd.opc == SPDK_NVME_OPC_DATASET_MANAGEMENT))) {
-				sgl_supported = false;
-			}
-		}
-
-		if (sgl_supported && !(ctrlr->flags & SPDK_NVME_CTRLR_SGL_REQUIRES_DWORD_ALIGNMENT)) {
-			dword_aligned = false;
-		}
-		rc = g_nvme_pcie_build_req_table[payload_type][sgl_supported](qpair, req, tr, dword_aligned);
-		if (rc < 0) {
-			goto exit;
-		}
-
-		rc = nvme_pcie_qpair_build_metadata(qpair, tr, sgl_supported, dword_aligned);
-		if (rc < 0) {
-			goto exit;
-		}
-	}
-
-	nvme_pcie_qpair_submit_tracker(qpair, tr);
-
-exit:
-	if (spdk_unlikely(nvme_qpair_is_admin_queue(qpair))) {
-		nvme_robust_mutex_unlock(&ctrlr->ctrlr_lock);
-	}
-
-	return rc;
-}
-
 void
 spdk_nvme_pcie_set_hotplug_filter(spdk_nvme_pcie_hotplug_filter_cb filter_cb)
 {
diff --git a/lib/nvme/nvme_pcie_common.c b/lib/nvme/nvme_pcie_common.c
index 5bf8a841f..6ca93225a 100644
--- a/lib/nvme/nvme_pcie_common.c
+++ b/lib/nvme/nvme_pcie_common.c
@@ -43,11 +43,11 @@
 
 __thread struct nvme_pcie_ctrlr *g_thread_mmio_ctrlr = NULL;
 
-static uint64_t
-nvme_pcie_vtophys(struct spdk_nvme_ctrlr *ctrlr, const void *buf)
+static inline uint64_t
+nvme_pcie_vtophys(struct spdk_nvme_ctrlr *ctrlr, const void *buf, uint64_t *size)
 {
 	if (spdk_likely(ctrlr->trid.trtype == SPDK_NVME_TRANSPORT_PCIE)) {
-		return spdk_vtophys(buf, NULL);
+		return spdk_vtophys(buf, size);
 	} else {
 		/* vfio-user address translation with IOVA=VA mode */
 		return (uint64_t)(uintptr_t)buf;
@@ -187,7 +187,7 @@ nvme_pcie_qpair_construct(struct spdk_nvme_qpair *qpair,
 			assert(pqpair->sq_vaddr != NULL);
 			pqpair->cmd_bus_addr = sq_paddr;
 		} else {
-			pqpair->cmd_bus_addr = nvme_pcie_vtophys(ctrlr, pqpair->cmd);
+			pqpair->cmd_bus_addr = nvme_pcie_vtophys(ctrlr, pqpair->cmd, NULL);
 			if (pqpair->cmd_bus_addr == SPDK_VTOPHYS_ERROR) {
 				SPDK_ERRLOG("spdk_vtophys(pqpair->cmd) failed\n");
 				return -EFAULT;
@@ -210,7 +210,7 @@ nvme_pcie_qpair_construct(struct spdk_nvme_qpair *qpair,
 		assert(pqpair->cq_vaddr != NULL);
 		pqpair->cpl_bus_addr = cq_paddr;
 	} else {
-		pqpair->cpl_bus_addr =  nvme_pcie_vtophys(ctrlr, pqpair->cpl);
+		pqpair->cpl_bus_addr =  nvme_pcie_vtophys(ctrlr, pqpair->cpl, NULL);
 		if (pqpair->cpl_bus_addr == SPDK_VTOPHYS_ERROR) {
 			SPDK_ERRLOG("spdk_vtophys(pqpair->cpl) failed\n");
 			return -EFAULT;
@@ -238,7 +238,7 @@ nvme_pcie_qpair_construct(struct spdk_nvme_qpair *qpair,
 
 	for (i = 0; i < num_trackers; i++) {
 		tr = &pqpair->tr[i];
-		nvme_qpair_construct_tracker(tr, i, nvme_pcie_vtophys(ctrlr, tr));
+		nvme_qpair_construct_tracker(tr, i, nvme_pcie_vtophys(ctrlr, tr, NULL));
 		TAILQ_INSERT_HEAD(&pqpair->free_tr, tr, tq_list);
 	}
 
@@ -633,6 +633,7 @@ nvme_pcie_qpair_complete_tracker(struct spdk_nvme_qpair *qpair, struct nvme_trac
 	struct nvme_request		*req;
 	bool				retry, error;
 	bool				req_from_current_proc = true;
+	bool				print_error;
 
 	req = tr->req;
 
@@ -641,9 +642,13 @@ nvme_pcie_qpair_complete_tracker(struct spdk_nvme_qpair *qpair, struct nvme_trac
 	error = spdk_nvme_cpl_is_error(cpl);
 	retry = error && nvme_completion_is_retry(cpl) &&
 		req->retries < pqpair->retry_count;
+	print_error = error && print_on_error && !qpair->ctrlr->opts.disable_error_logging;
 
-	if (error && print_on_error && !qpair->ctrlr->opts.disable_error_logging) {
+	if (print_error) {
 		spdk_nvme_qpair_print_command(qpair, &req->cmd);
+	}
+
+	if (print_error || SPDK_DEBUGLOG_FLAG_ENABLED("nvme")) {
 		spdk_nvme_qpair_print_completion(qpair, cpl);
 	}
 
@@ -1045,6 +1050,522 @@ free:
 	return 0;
 }
 
+static void
+nvme_pcie_fail_request_bad_vtophys(struct spdk_nvme_qpair *qpair, struct nvme_tracker *tr)
+{
+	/*
+	 * Bad vtophys translation, so abort this request and return
+	 *  immediately.
+	 */
+	nvme_pcie_qpair_manual_complete_tracker(qpair, tr, SPDK_NVME_SCT_GENERIC,
+						SPDK_NVME_SC_INVALID_FIELD,
+						1 /* do not retry */, true);
+}
+
+/*
+ * Append PRP list entries to describe a virtually contiguous buffer starting at virt_addr of len bytes.
+ *
+ * *prp_index will be updated to account for the number of PRP entries used.
+ */
+static inline int
+nvme_pcie_prp_list_append(struct spdk_nvme_ctrlr *ctrlr, struct nvme_tracker *tr,
+			  uint32_t *prp_index, void *virt_addr, size_t len,
+			  uint32_t page_size)
+{
+	struct spdk_nvme_cmd *cmd = &tr->req->cmd;
+	uintptr_t page_mask = page_size - 1;
+	uint64_t phys_addr;
+	uint32_t i;
+
+	SPDK_DEBUGLOG(nvme, "prp_index:%u virt_addr:%p len:%u\n",
+		      *prp_index, virt_addr, (uint32_t)len);
+
+	if (spdk_unlikely(((uintptr_t)virt_addr & 3) != 0)) {
+		SPDK_ERRLOG("virt_addr %p not dword aligned\n", virt_addr);
+		return -EFAULT;
+	}
+
+	i = *prp_index;
+	while (len) {
+		uint32_t seg_len;
+
+		/*
+		 * prp_index 0 is stored in prp1, and the rest are stored in the prp[] array,
+		 * so prp_index == count is valid.
+		 */
+		if (spdk_unlikely(i > SPDK_COUNTOF(tr->u.prp))) {
+			SPDK_ERRLOG("out of PRP entries\n");
+			return -EFAULT;
+		}
+
+		phys_addr = nvme_pcie_vtophys(ctrlr, virt_addr, NULL);
+		if (spdk_unlikely(phys_addr == SPDK_VTOPHYS_ERROR)) {
+			SPDK_ERRLOG("vtophys(%p) failed\n", virt_addr);
+			return -EFAULT;
+		}
+
+		if (i == 0) {
+			SPDK_DEBUGLOG(nvme, "prp1 = %p\n", (void *)phys_addr);
+			cmd->dptr.prp.prp1 = phys_addr;
+			seg_len = page_size - ((uintptr_t)virt_addr & page_mask);
+		} else {
+			if ((phys_addr & page_mask) != 0) {
+				SPDK_ERRLOG("PRP %u not page aligned (%p)\n", i, virt_addr);
+				return -EFAULT;
+			}
+
+			SPDK_DEBUGLOG(nvme, "prp[%u] = %p\n", i - 1, (void *)phys_addr);
+			tr->u.prp[i - 1] = phys_addr;
+			seg_len = page_size;
+		}
+
+		seg_len = spdk_min(seg_len, len);
+		virt_addr += seg_len;
+		len -= seg_len;
+		i++;
+	}
+
+	cmd->psdt = SPDK_NVME_PSDT_PRP;
+	if (i <= 1) {
+		cmd->dptr.prp.prp2 = 0;
+	} else if (i == 2) {
+		cmd->dptr.prp.prp2 = tr->u.prp[0];
+		SPDK_DEBUGLOG(nvme, "prp2 = %p\n", (void *)cmd->dptr.prp.prp2);
+	} else {
+		cmd->dptr.prp.prp2 = tr->prp_sgl_bus_addr;
+		SPDK_DEBUGLOG(nvme, "prp2 = %p (PRP list)\n", (void *)cmd->dptr.prp.prp2);
+	}
+
+	*prp_index = i;
+	return 0;
+}
+
+static int
+nvme_pcie_qpair_build_request_invalid(struct spdk_nvme_qpair *qpair,
+				      struct nvme_request *req, struct nvme_tracker *tr, bool dword_aligned)
+{
+	assert(0);
+	nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+	return -EINVAL;
+}
+
+/**
+ * Build PRP list describing physically contiguous payload buffer.
+ */
+static int
+nvme_pcie_qpair_build_contig_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
+				     struct nvme_tracker *tr, bool dword_aligned)
+{
+	uint32_t prp_index = 0;
+	int rc;
+
+	rc = nvme_pcie_prp_list_append(qpair->ctrlr, tr, &prp_index,
+				       req->payload.contig_or_cb_arg + req->payload_offset,
+				       req->payload_size, qpair->ctrlr->page_size);
+	if (rc) {
+		nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+	}
+
+	return rc;
+}
+
+/**
+ * Build an SGL describing a physically contiguous payload buffer.
+ *
+ * This is more efficient than using PRP because large buffers can be
+ * described this way.
+ */
+static int
+nvme_pcie_qpair_build_contig_hw_sgl_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
+		struct nvme_tracker *tr, bool dword_aligned)
+{
+	void *virt_addr;
+	uint64_t phys_addr, mapping_length;
+	uint32_t length;
+	struct spdk_nvme_sgl_descriptor *sgl;
+	uint32_t nseg = 0;
+
+	assert(req->payload_size != 0);
+	assert(nvme_payload_type(&req->payload) == NVME_PAYLOAD_TYPE_CONTIG);
+
+	sgl = tr->u.sgl;
+	req->cmd.psdt = SPDK_NVME_PSDT_SGL_MPTR_CONTIG;
+	req->cmd.dptr.sgl1.unkeyed.subtype = 0;
+
+	length = req->payload_size;
+	virt_addr = req->payload.contig_or_cb_arg + req->payload_offset;
+
+	while (length > 0) {
+		if (nseg >= NVME_MAX_SGL_DESCRIPTORS) {
+			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+			return -EFAULT;
+		}
+
+		if (dword_aligned && ((uintptr_t)virt_addr & 3)) {
+			SPDK_ERRLOG("virt_addr %p not dword aligned\n", virt_addr);
+			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+			return -EFAULT;
+		}
+
+		mapping_length = length;
+		phys_addr = nvme_pcie_vtophys(qpair->ctrlr, virt_addr, &mapping_length);
+		if (phys_addr == SPDK_VTOPHYS_ERROR) {
+			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+			return -EFAULT;
+		}
+
+		mapping_length = spdk_min(length, mapping_length);
+
+		length -= mapping_length;
+		virt_addr += mapping_length;
+
+		sgl->unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
+		sgl->unkeyed.length = mapping_length;
+		sgl->address = phys_addr;
+		sgl->unkeyed.subtype = 0;
+
+		sgl++;
+		nseg++;
+	}
+
+	if (nseg == 1) {
+		/*
+		 * The whole transfer can be described by a single SGL descriptor.
+		 *  Use the special case described by the spec where SGL1's type is Data Block.
+		 *  This means the SGL in the tracker is not used at all, so copy the first (and only)
+		 *  SGL element into SGL1.
+		 */
+		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
+		req->cmd.dptr.sgl1.address = tr->u.sgl[0].address;
+		req->cmd.dptr.sgl1.unkeyed.length = tr->u.sgl[0].unkeyed.length;
+	} else {
+		/* SPDK NVMe driver supports only 1 SGL segment for now, it is enough because
+		 *  NVME_MAX_SGL_DESCRIPTORS * 16 is less than one page.
+		 */
+		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_LAST_SEGMENT;
+		req->cmd.dptr.sgl1.address = tr->prp_sgl_bus_addr;
+		req->cmd.dptr.sgl1.unkeyed.length = nseg * sizeof(struct spdk_nvme_sgl_descriptor);
+	}
+
+	return 0;
+}
+
+/**
+ * Build SGL list describing scattered payload buffer.
+ */
+static int
+nvme_pcie_qpair_build_hw_sgl_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
+				     struct nvme_tracker *tr, bool dword_aligned)
+{
+	int rc;
+	void *virt_addr;
+	uint64_t phys_addr, mapping_length;
+	uint32_t remaining_transfer_len, remaining_user_sge_len, length;
+	struct spdk_nvme_sgl_descriptor *sgl;
+	uint32_t nseg = 0;
+
+	/*
+	 * Build scattered payloads.
+	 */
+	assert(req->payload_size != 0);
+	assert(nvme_payload_type(&req->payload) == NVME_PAYLOAD_TYPE_SGL);
+	assert(req->payload.reset_sgl_fn != NULL);
+	assert(req->payload.next_sge_fn != NULL);
+	req->payload.reset_sgl_fn(req->payload.contig_or_cb_arg, req->payload_offset);
+
+	sgl = tr->u.sgl;
+	req->cmd.psdt = SPDK_NVME_PSDT_SGL_MPTR_CONTIG;
+	req->cmd.dptr.sgl1.unkeyed.subtype = 0;
+
+	remaining_transfer_len = req->payload_size;
+
+	while (remaining_transfer_len > 0) {
+		rc = req->payload.next_sge_fn(req->payload.contig_or_cb_arg,
+					      &virt_addr, &remaining_user_sge_len);
+		if (rc) {
+			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+			return -EFAULT;
+		}
+
+		/* Bit Bucket SGL descriptor */
+		if ((uint64_t)virt_addr == UINT64_MAX) {
+			/* TODO: enable WRITE and COMPARE when necessary */
+			if (req->cmd.opc != SPDK_NVME_OPC_READ) {
+				SPDK_ERRLOG("Only READ command can be supported\n");
+				goto exit;
+			}
+			if (nseg >= NVME_MAX_SGL_DESCRIPTORS) {
+				SPDK_ERRLOG("Too many SGL entries\n");
+				goto exit;
+			}
+
+			sgl->unkeyed.type = SPDK_NVME_SGL_TYPE_BIT_BUCKET;
+			/* If the SGL describes a destination data buffer, the length of data
+			 * buffer shall be discarded by controller, and the length is included
+			 * in Number of Logical Blocks (NLB) parameter. Otherwise, the length
+			 * is not included in the NLB parameter.
+			 */
+			remaining_user_sge_len = spdk_min(remaining_user_sge_len, remaining_transfer_len);
+			remaining_transfer_len -= remaining_user_sge_len;
+
+			sgl->unkeyed.length = remaining_user_sge_len;
+			sgl->address = 0;
+			sgl->unkeyed.subtype = 0;
+
+			sgl++;
+			nseg++;
+
+			continue;
+		}
+
+		remaining_user_sge_len = spdk_min(remaining_user_sge_len, remaining_transfer_len);
+		remaining_transfer_len -= remaining_user_sge_len;
+		while (remaining_user_sge_len > 0) {
+			if (nseg >= NVME_MAX_SGL_DESCRIPTORS) {
+				SPDK_ERRLOG("Too many SGL entries\n");
+				goto exit;
+			}
+
+			if (dword_aligned && ((uintptr_t)virt_addr & 3)) {
+				SPDK_ERRLOG("virt_addr %p not dword aligned\n", virt_addr);
+				goto exit;
+			}
+
+			mapping_length = remaining_user_sge_len;
+			phys_addr = nvme_pcie_vtophys(qpair->ctrlr, virt_addr, &mapping_length);
+			if (phys_addr == SPDK_VTOPHYS_ERROR) {
+				goto exit;
+			}
+
+			length = spdk_min(remaining_user_sge_len, mapping_length);
+			remaining_user_sge_len -= length;
+			virt_addr += length;
+
+			if (nseg > 0 && phys_addr ==
+			    (*(sgl - 1)).address + (*(sgl - 1)).unkeyed.length) {
+				/* extend previous entry */
+				(*(sgl - 1)).unkeyed.length += length;
+				continue;
+			}
+
+			sgl->unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
+			sgl->unkeyed.length = length;
+			sgl->address = phys_addr;
+			sgl->unkeyed.subtype = 0;
+
+			sgl++;
+			nseg++;
+		}
+	}
+
+	if (nseg == 1) {
+		/*
+		 * The whole transfer can be described by a single SGL descriptor.
+		 *  Use the special case described by the spec where SGL1's type is Data Block.
+		 *  This means the SGL in the tracker is not used at all, so copy the first (and only)
+		 *  SGL element into SGL1.
+		 */
+		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
+		req->cmd.dptr.sgl1.address = tr->u.sgl[0].address;
+		req->cmd.dptr.sgl1.unkeyed.length = tr->u.sgl[0].unkeyed.length;
+	} else {
+		/* SPDK NVMe driver supports only 1 SGL segment for now, it is enough because
+		 *  NVME_MAX_SGL_DESCRIPTORS * 16 is less than one page.
+		 */
+		req->cmd.dptr.sgl1.unkeyed.type = SPDK_NVME_SGL_TYPE_LAST_SEGMENT;
+		req->cmd.dptr.sgl1.address = tr->prp_sgl_bus_addr;
+		req->cmd.dptr.sgl1.unkeyed.length = nseg * sizeof(struct spdk_nvme_sgl_descriptor);
+	}
+
+	return 0;
+
+exit:
+	nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+	return -EFAULT;
+}
+
+/**
+ * Build PRP list describing scattered payload buffer.
+ */
+static int
+nvme_pcie_qpair_build_prps_sgl_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
+				       struct nvme_tracker *tr, bool dword_aligned)
+{
+	int rc;
+	void *virt_addr;
+	uint32_t remaining_transfer_len, length;
+	uint32_t prp_index = 0;
+	uint32_t page_size = qpair->ctrlr->page_size;
+
+	/*
+	 * Build scattered payloads.
+	 */
+	assert(nvme_payload_type(&req->payload) == NVME_PAYLOAD_TYPE_SGL);
+	assert(req->payload.reset_sgl_fn != NULL);
+	req->payload.reset_sgl_fn(req->payload.contig_or_cb_arg, req->payload_offset);
+
+	remaining_transfer_len = req->payload_size;
+	while (remaining_transfer_len > 0) {
+		assert(req->payload.next_sge_fn != NULL);
+		rc = req->payload.next_sge_fn(req->payload.contig_or_cb_arg, &virt_addr, &length);
+		if (rc) {
+			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+			return -EFAULT;
+		}
+
+		length = spdk_min(remaining_transfer_len, length);
+
+		/*
+		 * Any incompatible sges should have been handled up in the splitting routine,
+		 *  but assert here as an additional check.
+		 *
+		 * All SGEs except last must end on a page boundary.
+		 */
+		assert((length == remaining_transfer_len) ||
+		       _is_page_aligned((uintptr_t)virt_addr + length, page_size));
+
+		rc = nvme_pcie_prp_list_append(qpair->ctrlr, tr, &prp_index, virt_addr, length, page_size);
+		if (rc) {
+			nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+			return rc;
+		}
+
+		remaining_transfer_len -= length;
+	}
+
+	return 0;
+}
+
+typedef int(*build_req_fn)(struct spdk_nvme_qpair *, struct nvme_request *, struct nvme_tracker *,
+			   bool);
+
+static build_req_fn const g_nvme_pcie_build_req_table[][2] = {
+	[NVME_PAYLOAD_TYPE_INVALID] = {
+		nvme_pcie_qpair_build_request_invalid,			/* PRP */
+		nvme_pcie_qpair_build_request_invalid			/* SGL */
+	},
+	[NVME_PAYLOAD_TYPE_CONTIG] = {
+		nvme_pcie_qpair_build_contig_request,			/* PRP */
+		nvme_pcie_qpair_build_contig_hw_sgl_request		/* SGL */
+	},
+	[NVME_PAYLOAD_TYPE_SGL] = {
+		nvme_pcie_qpair_build_prps_sgl_request,			/* PRP */
+		nvme_pcie_qpair_build_hw_sgl_request			/* SGL */
+	}
+};
+
+static int
+nvme_pcie_qpair_build_metadata(struct spdk_nvme_qpair *qpair, struct nvme_tracker *tr,
+			       bool sgl_supported, bool dword_aligned)
+{
+	void *md_payload;
+	struct nvme_request *req = tr->req;
+
+	if (req->payload.md) {
+		md_payload = req->payload.md + req->md_offset;
+		if (dword_aligned && ((uintptr_t)md_payload & 3)) {
+			SPDK_ERRLOG("virt_addr %p not dword aligned\n", md_payload);
+			goto exit;
+		}
+
+		if (sgl_supported && dword_aligned) {
+			assert(req->cmd.psdt == SPDK_NVME_PSDT_SGL_MPTR_CONTIG);
+			req->cmd.psdt = SPDK_NVME_PSDT_SGL_MPTR_SGL;
+			tr->meta_sgl.address = nvme_pcie_vtophys(qpair->ctrlr, md_payload, NULL);
+			if (tr->meta_sgl.address == SPDK_VTOPHYS_ERROR) {
+				goto exit;
+			}
+			tr->meta_sgl.unkeyed.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
+			tr->meta_sgl.unkeyed.length = req->md_size;
+			tr->meta_sgl.unkeyed.subtype = 0;
+			req->cmd.mptr = tr->prp_sgl_bus_addr - sizeof(struct spdk_nvme_sgl_descriptor);
+		} else {
+			req->cmd.mptr = nvme_pcie_vtophys(qpair->ctrlr, md_payload, NULL);
+			if (req->cmd.mptr == SPDK_VTOPHYS_ERROR) {
+				goto exit;
+			}
+		}
+	}
+
+	return 0;
+
+exit:
+	nvme_pcie_fail_request_bad_vtophys(qpair, tr);
+	return -EINVAL;
+}
+
+int
+nvme_pcie_qpair_submit_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req)
+{
+	struct nvme_tracker	*tr;
+	int			rc = 0;
+	struct spdk_nvme_ctrlr	*ctrlr = qpair->ctrlr;
+	struct nvme_pcie_qpair	*pqpair = nvme_pcie_qpair(qpair);
+	enum nvme_payload_type	payload_type;
+	bool			sgl_supported;
+	bool			dword_aligned = true;
+
+	if (spdk_unlikely(nvme_qpair_is_admin_queue(qpair))) {
+		nvme_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	}
+
+	tr = TAILQ_FIRST(&pqpair->free_tr);
+
+	if (tr == NULL) {
+		pqpair->stat->queued_requests++;
+		/* Inform the upper layer to try again later. */
+		rc = -EAGAIN;
+		goto exit;
+	}
+
+	pqpair->stat->submitted_requests++;
+	TAILQ_REMOVE(&pqpair->free_tr, tr, tq_list); /* remove tr from free_tr */
+	TAILQ_INSERT_TAIL(&pqpair->outstanding_tr, tr, tq_list);
+	tr->req = req;
+	tr->cb_fn = req->cb_fn;
+	tr->cb_arg = req->cb_arg;
+	req->cmd.cid = tr->cid;
+
+	if (req->payload_size != 0) {
+		payload_type = nvme_payload_type(&req->payload);
+		/* According to the specification, PRPs shall be used for all
+		 *  Admin commands for NVMe over PCIe implementations.
+		 */
+		sgl_supported = (ctrlr->flags & SPDK_NVME_CTRLR_SGL_SUPPORTED) != 0 &&
+				!nvme_qpair_is_admin_queue(qpair);
+
+		if (sgl_supported) {
+			/* Don't use SGL for DSM command */
+			if (spdk_unlikely((ctrlr->quirks & NVME_QUIRK_NO_SGL_FOR_DSM) &&
+					  (req->cmd.opc == SPDK_NVME_OPC_DATASET_MANAGEMENT))) {
+				sgl_supported = false;
+			}
+		}
+
+		if (sgl_supported && !(ctrlr->flags & SPDK_NVME_CTRLR_SGL_REQUIRES_DWORD_ALIGNMENT)) {
+			dword_aligned = false;
+		}
+		rc = g_nvme_pcie_build_req_table[payload_type][sgl_supported](qpair, req, tr, dword_aligned);
+		if (rc < 0) {
+			goto exit;
+		}
+
+		rc = nvme_pcie_qpair_build_metadata(qpair, tr, sgl_supported, dword_aligned);
+		if (rc < 0) {
+			goto exit;
+		}
+	}
+
+	nvme_pcie_qpair_submit_tracker(qpair, tr);
+
+exit:
+	if (spdk_unlikely(nvme_qpair_is_admin_queue(qpair))) {
+		nvme_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	}
+
+	return rc;
+}
+
 struct spdk_nvme_transport_poll_group *
 nvme_pcie_poll_group_create(void)
 {
diff --git a/lib/nvme/nvme_pcie_internal.h b/lib/nvme/nvme_pcie_internal.h
index 5f7b08f9e..d7b554c2c 100644
--- a/lib/nvme/nvme_pcie_internal.h
+++ b/lib/nvme/nvme_pcie_internal.h
@@ -241,6 +241,8 @@ nvme_pcie_qpair_update_mmio_required(struct spdk_nvme_qpair *qpair, uint16_t val
 		return true;
 	}
 
+	spdk_wmb();
+
 	old = *shadow_db;
 	*shadow_db = value;
 
@@ -342,6 +344,7 @@ int nvme_pcie_qpair_destroy(struct spdk_nvme_qpair *qpair);
 struct spdk_nvme_qpair *nvme_pcie_ctrlr_create_io_qpair(struct spdk_nvme_ctrlr *ctrlr, uint16_t qid,
 		const struct spdk_nvme_io_qpair_opts *opts);
 int nvme_pcie_ctrlr_delete_io_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_qpair *qpair);
+int nvme_pcie_qpair_submit_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req);
 
 struct spdk_nvme_transport_poll_group *nvme_pcie_poll_group_create(void);
 int nvme_pcie_poll_group_connect_qpair(struct spdk_nvme_qpair *qpair);
diff --git a/lib/nvme/nvme_qpair.c b/lib/nvme/nvme_qpair.c
index 90a629f18..d4e9b0346 100644
--- a/lib/nvme/nvme_qpair.c
+++ b/lib/nvme/nvme_qpair.c
@@ -658,6 +658,8 @@ nvme_qpair_resubmit_requests(struct spdk_nvme_qpair *qpair, uint32_t num_request
 	int resubmit_rc;
 	struct nvme_request *req;
 
+	assert(num_requests > 0);
+
 	for (i = 0; i < num_requests; i++) {
 		if (qpair->ctrlr->is_resetting) {
 			break;
@@ -733,7 +735,9 @@ spdk_nvme_qpair_process_completions(struct spdk_nvme_qpair *qpair, uint32_t max_
 	 * At this point, ret must represent the number of completions we reaped.
 	 * submit as many queued requests as we completed.
 	 */
-	nvme_qpair_resubmit_requests(qpair, ret);
+	if (ret > 0) {
+		nvme_qpair_resubmit_requests(qpair, ret);
+	}
 
 	return ret;
 }
@@ -932,6 +936,9 @@ _nvme_qpair_submit_request(struct spdk_nvme_qpair *qpair, struct nvme_request *r
 	}
 
 	if (spdk_likely(rc == 0)) {
+		if (SPDK_DEBUGLOG_FLAG_ENABLED("nvme")) {
+			spdk_nvme_print_command(qpair->id, &req->cmd);
+		}
 		req->queued = false;
 		return 0;
 	}
diff --git a/lib/nvme/nvme_quirks.c b/lib/nvme/nvme_quirks.c
index a529933f2..9065467d3 100644
--- a/lib/nvme/nvme_quirks.c
+++ b/lib/nvme/nvme_quirks.c
@@ -92,6 +92,9 @@ static const struct nvme_quirk nvme_quirks[] = {
 		NVME_INTEL_QUIRK_NO_LOG_PAGES |
 		NVME_QUIRK_MAXIMUM_PCI_ACCESS_WIDTH
 	},
+	{	{SPDK_PCI_CLASS_NVME, SPDK_PCI_VID_REDHAT, 0x0010, SPDK_PCI_ANY_ID, SPDK_PCI_ANY_ID},
+		NVME_QUIRK_MAXIMUM_PCI_ACCESS_WIDTH
+	},
 	{	{SPDK_PCI_CLASS_NVME, SPDK_PCI_VID_CNEXLABS, 0x1f1f, SPDK_PCI_ANY_ID, SPDK_PCI_ANY_ID},
 		NVME_QUIRK_IDENTIFY_CNS |
 		NVME_QUIRK_OCSSD
diff --git a/lib/nvme/nvme_rdma.c b/lib/nvme/nvme_rdma.c
index ab0b1d0c5..9d38e9869 100644
--- a/lib/nvme/nvme_rdma.c
+++ b/lib/nvme/nvme_rdma.c
@@ -1738,8 +1738,6 @@ static struct spdk_nvme_ctrlr *nvme_rdma_ctrlr_construct(const struct spdk_nvme_
 		void *devhandle)
 {
 	struct nvme_rdma_ctrlr *rctrlr;
-	union spdk_nvme_cap_register cap;
-	union spdk_nvme_vs_register vs;
 	struct ibv_context **contexts;
 	struct ibv_device_attr dev_attr;
 	int i, flag, rc;
@@ -1827,29 +1825,11 @@ static struct spdk_nvme_ctrlr *nvme_rdma_ctrlr_construct(const struct spdk_nvme_
 		goto destruct_ctrlr;
 	}
 
-	rc = nvme_transport_ctrlr_connect_qpair(&rctrlr->ctrlr, rctrlr->ctrlr.adminq);
-	if (rc < 0) {
-		SPDK_ERRLOG("failed to connect admin qpair\n");
-		goto destruct_ctrlr;
-	}
-
-	if (nvme_ctrlr_get_cap(&rctrlr->ctrlr, &cap)) {
-		SPDK_ERRLOG("get_cap() failed\n");
-		goto destruct_ctrlr;
-	}
-
-	if (nvme_ctrlr_get_vs(&rctrlr->ctrlr, &vs)) {
-		SPDK_ERRLOG("get_vs() failed\n");
-		goto destruct_ctrlr;
-	}
-
 	if (nvme_ctrlr_add_process(&rctrlr->ctrlr, 0) != 0) {
 		SPDK_ERRLOG("nvme_ctrlr_add_process() failed\n");
 		goto destruct_ctrlr;
 	}
 
-	nvme_ctrlr_init_cap(&rctrlr->ctrlr, &cap, &vs);
-
 	SPDK_DEBUGLOG(nvme, "successfully initialized the nvmf ctrlr\n");
 	return &rctrlr->ctrlr;
 
@@ -2599,7 +2579,9 @@ nvme_rdma_poll_group_process_completions(struct spdk_nvme_transport_poll_group *
 
 		nvme_rdma_qpair_submit_sends(rqpair);
 		nvme_rdma_qpair_submit_recvs(rqpair);
-		nvme_qpair_resubmit_requests(&rqpair->qpair, rqpair->num_completions);
+		if (rqpair->num_completions > 0) {
+			nvme_qpair_resubmit_requests(&rqpair->qpair, rqpair->num_completions);
+		}
 	}
 
 	/*
diff --git a/lib/nvme/nvme_tcp.c b/lib/nvme/nvme_tcp.c
index 95b293c19..fa762d0fb 100644
--- a/lib/nvme/nvme_tcp.c
+++ b/lib/nvme/nvme_tcp.c
@@ -81,7 +81,7 @@ struct nvme_tcp_qpair {
 	TAILQ_HEAD(, nvme_tcp_req)		outstanding_reqs;
 
 	TAILQ_HEAD(, nvme_tcp_pdu)		send_queue;
-	struct nvme_tcp_pdu			recv_pdu;
+	struct nvme_tcp_pdu			*recv_pdu;
 	struct nvme_tcp_pdu			*send_pdu; /* only for error pdu and init pdu */
 	struct nvme_tcp_pdu			*send_pdus; /* Used by tcp_reqs */
 	enum nvme_tcp_pdu_recv_state		recv_state;
@@ -130,6 +130,7 @@ struct nvme_tcp_req {
 	 * waiting for H2C complete */
 	uint16_t				ttag_r2t_next;
 	bool					in_capsule_data;
+	bool					pdu_in_use;
 	/* It is used to track whether the req can be safely freed */
 	union {
 		uint8_t raw;
@@ -148,7 +149,7 @@ struct nvme_tcp_req {
 			uint8_t				reserved : 4;
 		} bits;
 	} ordering;
-	struct nvme_tcp_pdu			*send_pdu;
+	struct nvme_tcp_pdu			*pdu;
 	struct iovec				iov[NVME_TCP_MAX_SGL_DESCRIPTORS];
 	uint32_t				iovcnt;
 	/* Used to hold a value received from subsequent R2T while we are still
@@ -200,12 +201,13 @@ nvme_tcp_req_get(struct nvme_tcp_qpair *tqpair)
 	tcp_req->datao = 0;
 	tcp_req->req = NULL;
 	tcp_req->in_capsule_data = false;
+	tcp_req->pdu_in_use = false;
 	tcp_req->r2tl_remain = 0;
 	tcp_req->r2tl_remain_next = 0;
 	tcp_req->active_r2ts = 0;
 	tcp_req->iovcnt = 0;
 	tcp_req->ordering.raw = 0;
-	memset(tcp_req->send_pdu, 0, sizeof(struct nvme_tcp_pdu));
+	memset(tcp_req->pdu, 0, sizeof(struct nvme_tcp_pdu));
 	memset(&tcp_req->rsp, 0, sizeof(struct spdk_nvme_cpl));
 	TAILQ_INSERT_TAIL(&tqpair->outstanding_reqs, tcp_req, link);
 
@@ -271,8 +273,8 @@ nvme_tcp_alloc_reqs(struct nvme_tcp_qpair *tqpair)
 		goto fail;
 	}
 
-	/* Add additional one member for the send_pdu owned by the tqpair */
-	tqpair->send_pdus = spdk_zmalloc((tqpair->num_entries + 1) * sizeof(struct nvme_tcp_pdu),
+	/* Add additional 2 member for the send_pdu, recv_pdu owned by the tqpair */
+	tqpair->send_pdus = spdk_zmalloc((tqpair->num_entries + 2) * sizeof(struct nvme_tcp_pdu),
 					 0x1000, NULL,
 					 SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_DMA);
 
@@ -288,11 +290,12 @@ nvme_tcp_alloc_reqs(struct nvme_tcp_qpair *tqpair)
 		tcp_req = &tqpair->tcp_reqs[i];
 		tcp_req->cid = i;
 		tcp_req->tqpair = tqpair;
-		tcp_req->send_pdu = &tqpair->send_pdus[i];
+		tcp_req->pdu = &tqpair->send_pdus[i];
 		TAILQ_INSERT_TAIL(&tqpair->free_reqs, tcp_req, link);
 	}
 
 	tqpair->send_pdu = &tqpair->send_pdus[i];
+	tqpair->recv_pdu = &tqpair->send_pdus[i + 1];
 
 	return 0;
 fail:
@@ -443,8 +446,9 @@ pdu_data_crc32_compute(struct nvme_tcp_pdu *pdu)
 	/* Data Digest */
 	if (pdu->data_len > 0 && g_nvme_tcp_ddgst[pdu->hdr.common.pdu_type] &&
 	    tqpair->flags.host_ddgst_enable) {
-		/* Only suport this limitated case for the first step */
-		if (tgroup != NULL && tgroup->group.group->accel_fn_table.submit_accel_crc32c &&
+		/* Only suport this limited case for the first step */
+		if ((nvme_qpair_get_state(&tqpair->qpair) >= NVME_QPAIR_CONNECTED) &&
+		    (tgroup != NULL && tgroup->group.group->accel_fn_table.submit_accel_crc32c) &&
 		    spdk_likely(!pdu->dif_ctx && (pdu->data_len % SPDK_NVME_TCP_DIGEST_ALIGNMENT == 0))) {
 			tgroup->group.group->accel_fn_table.submit_accel_crc32c(tgroup->group.group->ctx,
 					&pdu->data_digest_crc32, pdu->data_iov,
@@ -459,22 +463,6 @@ pdu_data_crc32_compute(struct nvme_tcp_pdu *pdu)
 	_tcp_write_pdu(pdu);
 }
 
-static void
-header_crc32_accel_done(void *cb_arg, int status)
-{
-	struct nvme_tcp_pdu *pdu = cb_arg;
-
-	pdu->header_digest_crc32 ^= SPDK_CRC32C_XOR;
-	MAKE_DIGEST_WORD((uint8_t *)pdu->hdr.raw + pdu->hdr.common.hlen, pdu->header_digest_crc32);
-	if (spdk_unlikely(status)) {
-		SPDK_ERRLOG("Failed to compute header digest on pdu=%p\n", pdu);
-		_pdu_write_done(pdu, status);
-		return;
-	}
-
-	pdu_data_crc32_compute(pdu);
-}
-
 static int
 nvme_tcp_qpair_write_pdu(struct nvme_tcp_qpair *tqpair,
 			 struct nvme_tcp_pdu *pdu,
@@ -483,27 +471,16 @@ nvme_tcp_qpair_write_pdu(struct nvme_tcp_qpair *tqpair,
 {
 	int hlen;
 	uint32_t crc32c;
-	struct nvme_tcp_poll_group *tgroup = nvme_tcp_poll_group(tqpair->qpair.poll_group);
 
 	hlen = pdu->hdr.common.hlen;
-
 	pdu->cb_fn = cb_fn;
 	pdu->cb_arg = cb_arg;
 	pdu->qpair = tqpair;
 
 	/* Header Digest */
 	if (g_nvme_tcp_hdgst[pdu->hdr.common.pdu_type] && tqpair->flags.host_hdgst_enable) {
-		if (tgroup != NULL && tgroup->group.group->accel_fn_table.submit_accel_crc32c) {
-			pdu->iov[0].iov_base = &pdu->hdr.raw;
-			pdu->iov[0].iov_len = hlen;
-			tgroup->group.group->accel_fn_table.submit_accel_crc32c(tgroup->group.group->ctx,
-					&pdu->header_digest_crc32,
-					pdu->iov, 1, 0, header_crc32_accel_done, pdu);
-			return 0;
-		} else {
-			crc32c = nvme_tcp_pdu_calc_header_digest(pdu);
-			MAKE_DIGEST_WORD((uint8_t *)pdu->hdr.raw + hlen, crc32c);
-		}
+		crc32c = nvme_tcp_pdu_calc_header_digest(pdu);
+		MAKE_DIGEST_WORD((uint8_t *)pdu->hdr.raw + hlen, crc32c);
 	}
 
 	pdu_data_crc32_compute(pdu);
@@ -694,7 +671,7 @@ nvme_tcp_qpair_capsule_cmd_send(struct nvme_tcp_qpair *tqpair,
 	uint8_t pdo;
 
 	SPDK_DEBUGLOG(nvme, "enter\n");
-	pdu = tcp_req->send_pdu;
+	pdu = tcp_req->pdu;
 
 	capsule_cmd = &pdu->hdr.capsule_cmd;
 	capsule_cmd->common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_CAPSULE_CMD;
@@ -819,7 +796,7 @@ nvme_tcp_qpair_set_recv_state(struct nvme_tcp_qpair *tqpair,
 	switch (state) {
 	case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY:
 	case NVME_TCP_PDU_RECV_STATE_ERROR:
-		memset(&tqpair->recv_pdu, 0, sizeof(struct nvme_tcp_pdu));
+		memset(tqpair->recv_pdu, 0, sizeof(struct nvme_tcp_pdu));
 		break;
 	case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_CH:
 	case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH:
@@ -870,7 +847,6 @@ nvme_tcp_qpair_send_h2c_term_req(struct nvme_tcp_qpair *tqpair, struct nvme_tcp_
 	h2c_term_req->common.plen = h2c_term_req->common.hlen + copy_len;
 	nvme_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_ERROR);
 	nvme_tcp_qpair_write_pdu(tqpair, rsp_pdu, nvme_tcp_qpair_send_h2c_term_req_complete, tqpair);
-
 }
 
 static void
@@ -882,7 +858,7 @@ nvme_tcp_pdu_ch_handle(struct nvme_tcp_qpair *tqpair)
 	uint32_t expected_hlen, hd_len = 0;
 	bool plen_error = false;
 
-	pdu = &tqpair->recv_pdu;
+	pdu = tqpair->recv_pdu;
 
 	SPDK_DEBUGLOG(nvme, "pdu type = %d\n", pdu->hdr.common.pdu_type);
 	if (pdu->hdr.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_IC_RESP) {
@@ -938,7 +914,7 @@ nvme_tcp_pdu_ch_handle(struct nvme_tcp_qpair *tqpair)
 			break;
 
 		default:
-			SPDK_ERRLOG("Unexpected PDU type 0x%02x\n", tqpair->recv_pdu.hdr.common.pdu_type);
+			SPDK_ERRLOG("Unexpected PDU type 0x%02x\n", tqpair->recv_pdu->hdr.common.pdu_type);
 			fes = SPDK_NVME_TCP_TERM_REQ_FES_INVALID_HEADER_FIELD;
 			error_offset = offsetof(struct spdk_nvme_tcp_common_pdu_hdr, pdu_type);
 			goto err;
@@ -958,7 +934,7 @@ nvme_tcp_pdu_ch_handle(struct nvme_tcp_qpair *tqpair)
 		goto err;
 	} else {
 		nvme_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH);
-		nvme_tcp_pdu_calc_psh_len(&tqpair->recv_pdu, tqpair->flags.host_hdgst_enable);
+		nvme_tcp_pdu_calc_psh_len(tqpair->recv_pdu, tqpair->flags.host_hdgst_enable);
 		return;
 	}
 err:
@@ -992,7 +968,6 @@ nvme_tcp_c2h_data_payload_handle(struct nvme_tcp_qpair *tqpair,
 	tcp_req->datao += pdu->data_len;
 	flags = c2h_data->common.flags;
 
-	nvme_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
 	if (flags & SPDK_NVME_TCP_C2H_DATA_FLAGS_SUCCESS) {
 		if (tcp_req->datao == tcp_req->req->payload_size) {
 			tcp_req->rsp.status.p = 0;
@@ -1040,46 +1015,122 @@ nvme_tcp_c2h_term_req_payload_handle(struct nvme_tcp_qpair *tqpair,
 	nvme_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_ERROR);
 }
 
+static void
+_nvme_tcp_pdu_payload_handle(struct nvme_tcp_qpair *tqpair, uint32_t *reaped)
+{
+	struct nvme_tcp_pdu *pdu;
+
+	assert(tqpair != NULL);
+	pdu = tqpair->recv_pdu;
+
+	switch (pdu->hdr.common.pdu_type) {
+	case SPDK_NVME_TCP_PDU_TYPE_C2H_DATA:
+		nvme_tcp_c2h_data_payload_handle(tqpair, pdu, reaped);
+		nvme_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
+		break;
+
+	case SPDK_NVME_TCP_PDU_TYPE_C2H_TERM_REQ:
+		nvme_tcp_c2h_term_req_payload_handle(tqpair, pdu);
+		break;
+
+	default:
+		/* The code should not go to here */
+		SPDK_ERRLOG("The code should not go to here\n");
+		break;
+	}
+}
+
+static void
+tcp_data_recv_crc32_done(void *cb_arg, int status)
+{
+	struct nvme_tcp_req *tcp_req = cb_arg;
+	struct nvme_tcp_pdu *pdu;
+	struct nvme_tcp_qpair *tqpair;
+	int rc;
+	struct nvme_tcp_poll_group *pgroup;
+	int dummy_reaped = 0;
+
+	pdu = tcp_req->pdu;
+	assert(pdu != NULL);
+
+	tqpair = tcp_req->tqpair;
+	assert(tqpair != NULL);
+
+	if (!tqpair->needs_poll) {
+		pgroup = nvme_tcp_poll_group(tqpair->qpair.poll_group);
+		TAILQ_INSERT_TAIL(&pgroup->needs_poll, tqpair, link);
+		tqpair->needs_poll = true;
+	}
+
+	if (spdk_unlikely(status)) {
+		SPDK_ERRLOG("Failed to compute the data digest for pdu =%p\n", pdu);
+		tcp_req->rsp.status.sc = SPDK_NVME_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR;
+		goto end;
+	}
+
+	pdu->data_digest_crc32 ^= SPDK_CRC32C_XOR;
+	rc = MATCH_DIGEST_WORD(pdu->data_digest, pdu->data_digest_crc32);
+	if (rc == 0) {
+		SPDK_ERRLOG("data digest error on tqpair=(%p) with pdu=%p\n", tqpair, pdu);
+		tcp_req->rsp.status.sc = SPDK_NVME_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR;
+	}
+
+end:
+	tcp_req->pdu_in_use = false;
+	nvme_tcp_c2h_data_payload_handle(tqpair, tcp_req->pdu, &dummy_reaped);
+}
+
 static void
 nvme_tcp_pdu_payload_handle(struct nvme_tcp_qpair *tqpair,
 			    uint32_t *reaped)
 {
 	int rc = 0;
 	struct nvme_tcp_pdu *pdu;
-	uint32_t crc32c, error_offset = 0;
-	enum spdk_nvme_tcp_term_req_fes fes;
+	uint32_t crc32c;
+	struct nvme_tcp_poll_group *tgroup;
+	struct nvme_tcp_req *tcp_req;
 
 	assert(tqpair->recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD);
-	pdu = &tqpair->recv_pdu;
+	pdu = tqpair->recv_pdu;
 
 	SPDK_DEBUGLOG(nvme, "enter\n");
 
 	/* check data digest if need */
 	if (pdu->ddgst_enable) {
+		tcp_req = pdu->req;
+		tgroup = nvme_tcp_poll_group(tqpair->qpair.poll_group);
+		/* Only suport this limitated case for the first step */
+		if ((nvme_qpair_get_state(&tqpair->qpair) >= NVME_QPAIR_CONNECTED) &&
+		    (tgroup != NULL && tgroup->group.group->accel_fn_table.submit_accel_crc32c) &&
+		    spdk_likely(!pdu->dif_ctx && (pdu->data_len % SPDK_NVME_TCP_DIGEST_ALIGNMENT == 0)
+				&& !tcp_req->pdu_in_use)) {
+
+			tcp_req->pdu_in_use = true;
+			tcp_req->pdu->hdr = pdu->hdr;
+			tcp_req->pdu->req = tcp_req;
+			memcpy(tcp_req->pdu->data_digest, pdu->data_digest, sizeof(pdu->data_digest));
+			memcpy(tcp_req->pdu->data_iov, pdu->data_iov, sizeof(pdu->data_iov[0]) * pdu->data_iovcnt);
+			tcp_req->pdu->data_iovcnt = pdu->data_iovcnt;
+			tcp_req->pdu->data_len = pdu->data_len;
+
+			nvme_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
+			tgroup->group.group->accel_fn_table.submit_accel_crc32c(tgroup->group.group->ctx,
+					&tcp_req->pdu->data_digest_crc32, tcp_req->pdu->data_iov,
+					tcp_req->pdu->data_iovcnt, 0, tcp_data_recv_crc32_done, tcp_req);
+			return;
+		}
+
 		crc32c = nvme_tcp_pdu_calc_data_digest(pdu);
 		rc = MATCH_DIGEST_WORD(pdu->data_digest, crc32c);
 		if (rc == 0) {
 			SPDK_ERRLOG("data digest error on tqpair=(%p) with pdu=%p\n", tqpair, pdu);
-			fes = SPDK_NVME_TCP_TERM_REQ_FES_HDGST_ERROR;
-			nvme_tcp_qpair_send_h2c_term_req(tqpair, pdu, fes, error_offset);
-			return;
+			tcp_req = pdu->req;
+			assert(tcp_req != NULL);
+			tcp_req->rsp.status.sc = SPDK_NVME_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR;
 		}
 	}
 
-	switch (pdu->hdr.common.pdu_type) {
-	case SPDK_NVME_TCP_PDU_TYPE_C2H_DATA:
-		nvme_tcp_c2h_data_payload_handle(tqpair, pdu, reaped);
-		break;
-
-	case SPDK_NVME_TCP_PDU_TYPE_C2H_TERM_REQ:
-		nvme_tcp_c2h_term_req_payload_handle(tqpair, pdu);
-		break;
-
-	default:
-		/* The code should not go to here */
-		SPDK_ERRLOG("The code should not go to here\n");
-		break;
-	}
+	_nvme_tcp_pdu_payload_handle(tqpair, reaped);
 }
 
 static void
@@ -1332,7 +1383,7 @@ nvme_tcp_send_h2c_data(struct nvme_tcp_req *tcp_req)
 	/* Reinit the send_ack and h2c_send_waiting_ack bits */
 	tcp_req->ordering.bits.send_ack = 0;
 	tcp_req->ordering.bits.h2c_send_waiting_ack = 0;
-	rsp_pdu = tcp_req->send_pdu;
+	rsp_pdu = tcp_req->pdu;
 	memset(rsp_pdu, 0, sizeof(*rsp_pdu));
 	h2c_data = &rsp_pdu->hdr.h2c_data;
 
@@ -1467,7 +1518,7 @@ nvme_tcp_pdu_psh_handle(struct nvme_tcp_qpair *tqpair, uint32_t *reaped)
 	enum spdk_nvme_tcp_term_req_fes fes;
 
 	assert(tqpair->recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH);
-	pdu = &tqpair->recv_pdu;
+	pdu = tqpair->recv_pdu;
 
 	SPDK_DEBUGLOG(nvme, "enter: pdu type =%u\n", pdu->hdr.common.pdu_type);
 	/* check header digest if needed */
@@ -1502,7 +1553,7 @@ nvme_tcp_pdu_psh_handle(struct nvme_tcp_qpair *tqpair, uint32_t *reaped)
 		break;
 
 	default:
-		SPDK_ERRLOG("Unexpected PDU type 0x%02x\n", tqpair->recv_pdu.hdr.common.pdu_type);
+		SPDK_ERRLOG("Unexpected PDU type 0x%02x\n", tqpair->recv_pdu->hdr.common.pdu_type);
 		fes = SPDK_NVME_TCP_TERM_REQ_FES_INVALID_HEADER_FIELD;
 		error_offset = 1;
 		nvme_tcp_qpair_send_h2c_term_req(tqpair, pdu, fes, error_offset);
@@ -1529,7 +1580,7 @@ nvme_tcp_read_pdu(struct nvme_tcp_qpair *tqpair, uint32_t *reaped)
 			break;
 		/* common header */
 		case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_CH:
-			pdu = &tqpair->recv_pdu;
+			pdu = tqpair->recv_pdu;
 			if (pdu->ch_valid_bytes < sizeof(struct spdk_nvme_tcp_common_pdu_hdr)) {
 				rc = nvme_tcp_read_data(tqpair->sock,
 							sizeof(struct spdk_nvme_tcp_common_pdu_hdr) - pdu->ch_valid_bytes,
@@ -1550,7 +1601,7 @@ nvme_tcp_read_pdu(struct nvme_tcp_qpair *tqpair, uint32_t *reaped)
 			break;
 		/* Wait for the pdu specific header  */
 		case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH:
-			pdu = &tqpair->recv_pdu;
+			pdu = tqpair->recv_pdu;
 			rc = nvme_tcp_read_data(tqpair->sock,
 						pdu->psh_len - pdu->psh_valid_bytes,
 						(uint8_t *)&pdu->hdr.raw + sizeof(struct spdk_nvme_tcp_common_pdu_hdr) + pdu->psh_valid_bytes);
@@ -1569,7 +1620,7 @@ nvme_tcp_read_pdu(struct nvme_tcp_qpair *tqpair, uint32_t *reaped)
 			nvme_tcp_pdu_psh_handle(tqpair, reaped);
 			break;
 		case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD:
-			pdu = &tqpair->recv_pdu;
+			pdu = tqpair->recv_pdu;
 			/* check whether the data is valid, if not we just return */
 			if (!pdu->data_len) {
 				return NVME_TCP_PDU_IN_PROGRESS;
@@ -1947,8 +1998,6 @@ static struct spdk_nvme_ctrlr *nvme_tcp_ctrlr_construct(const struct spdk_nvme_t
 		void *devhandle)
 {
 	struct nvme_tcp_ctrlr *tctrlr;
-	union spdk_nvme_cap_register cap;
-	union spdk_nvme_vs_register vs;
 	int rc;
 
 	tctrlr = calloc(1, sizeof(*tctrlr));
@@ -1975,33 +2024,12 @@ static struct spdk_nvme_ctrlr *nvme_tcp_ctrlr_construct(const struct spdk_nvme_t
 		return NULL;
 	}
 
-	rc = nvme_transport_ctrlr_connect_qpair(&tctrlr->ctrlr, tctrlr->ctrlr.adminq);
-	if (rc < 0) {
-		SPDK_ERRLOG("failed to connect admin qpair\n");
-		nvme_tcp_ctrlr_destruct(&tctrlr->ctrlr);
-		return NULL;
-	}
-
-	if (nvme_ctrlr_get_cap(&tctrlr->ctrlr, &cap)) {
-		SPDK_ERRLOG("get_cap() failed\n");
-		nvme_ctrlr_destruct(&tctrlr->ctrlr);
-		return NULL;
-	}
-
-	if (nvme_ctrlr_get_vs(&tctrlr->ctrlr, &vs)) {
-		SPDK_ERRLOG("get_vs() failed\n");
-		nvme_ctrlr_destruct(&tctrlr->ctrlr);
-		return NULL;
-	}
-
 	if (nvme_ctrlr_add_process(&tctrlr->ctrlr, 0) != 0) {
 		SPDK_ERRLOG("nvme_ctrlr_add_process() failed\n");
 		nvme_ctrlr_destruct(&tctrlr->ctrlr);
 		return NULL;
 	}
 
-	nvme_ctrlr_init_cap(&tctrlr->ctrlr, &cap, &vs);
-
 	return &tctrlr->ctrlr;
 }
 
diff --git a/lib/nvme/nvme_transport.c b/lib/nvme/nvme_transport.c
index 7037d4191..f2bb19d9d 100644
--- a/lib/nvme/nvme_transport.c
+++ b/lib/nvme/nvme_transport.c
@@ -347,7 +347,6 @@ int
 nvme_transport_ctrlr_connect_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_qpair *qpair)
 {
 	const struct spdk_nvme_transport *transport = nvme_get_transport(ctrlr->trid.trstring);
-	uint8_t transport_failure_reason;
 	int rc;
 
 	assert(transport != NULL);
@@ -355,7 +354,7 @@ nvme_transport_ctrlr_connect_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nv
 		qpair->transport = transport;
 	}
 
-	transport_failure_reason = qpair->transport_failure_reason;
+	qpair->last_transport_failure_reason = qpair->transport_failure_reason;
 	qpair->transport_failure_reason = SPDK_NVME_QPAIR_FAILURE_NONE;
 
 	nvme_qpair_set_state(qpair, NVME_QPAIR_CONNECTING);
@@ -376,7 +375,7 @@ nvme_transport_ctrlr_connect_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nv
 
 err:
 	/* If the qpair was unable to reconnect, restore the original failure reason. */
-	qpair->transport_failure_reason = transport_failure_reason;
+	qpair->transport_failure_reason = qpair->last_transport_failure_reason;
 	nvme_transport_ctrlr_disconnect_qpair(ctrlr, qpair);
 	nvme_qpair_set_state(qpair, NVME_QPAIR_DISCONNECTED);
 	return rc;
diff --git a/lib/nvme/nvme_vfio_user.c b/lib/nvme/nvme_vfio_user.c
index c674f9f5e..bdf52c534 100644
--- a/lib/nvme/nvme_vfio_user.c
+++ b/lib/nvme/nvme_vfio_user.c
@@ -53,12 +53,6 @@ struct nvme_vfio_ctrlr {
 	struct vfio_device *dev;
 };
 
-static inline uint64_t
-vfio_vtophys(const void *vaddr, uint64_t *size)
-{
-	return (uint64_t)(uintptr_t)vaddr;
-}
-
 static inline struct nvme_vfio_ctrlr *
 nvme_vfio_ctrlr(struct spdk_nvme_ctrlr *ctrlr)
 {
@@ -201,7 +195,6 @@ static struct spdk_nvme_ctrlr *
 	struct nvme_pcie_ctrlr *pctrlr;
 	uint16_t cmd_reg;
 	union spdk_nvme_cap_register cap;
-	union spdk_nvme_vs_register vs;
 	int ret;
 	char ctrlr_path[PATH_MAX];
 	char ctrlr_bar0[PATH_MAX];
@@ -272,12 +265,6 @@ static struct spdk_nvme_ctrlr *
 		goto exit;
 	}
 
-	if (nvme_ctrlr_get_vs(&pctrlr->ctrlr, &vs)) {
-		SPDK_ERRLOG("get_vs() failed\n");
-		goto exit;
-	}
-
-	nvme_ctrlr_init_cap(&pctrlr->ctrlr, &cap, &vs);
 	/* Doorbell stride is 2 ^ (dstrd + 2),
 	 * but we want multiples of 4, so drop the + 2 */
 	pctrlr->doorbell_stride_u32 = 1 << cap.bits.dstrd;
@@ -386,136 +373,6 @@ nvme_vfio_ctrlr_get_max_sges(struct spdk_nvme_ctrlr *ctrlr)
 	return NVME_MAX_SGES;
 }
 
-static inline int
-nvme_vfio_prp_list_append(struct nvme_tracker *tr, uint32_t *prp_index, void *virt_addr, size_t len,
-			  uint32_t page_size)
-{
-	struct spdk_nvme_cmd *cmd = &tr->req->cmd;
-	uintptr_t page_mask = page_size - 1;
-	uint64_t phys_addr;
-	uint32_t i;
-
-	SPDK_DEBUGLOG(nvme_vfio, "prp_index:%u virt_addr:%p len:%u\n",
-		      *prp_index, virt_addr, (uint32_t)len);
-
-	if (spdk_unlikely(((uintptr_t)virt_addr & 3) != 0)) {
-		SPDK_ERRLOG("virt_addr %p not dword aligned\n", virt_addr);
-		return -EFAULT;
-	}
-
-	i = *prp_index;
-	while (len) {
-		uint32_t seg_len;
-
-		/*
-		 * prp_index 0 is stored in prp1, and the rest are stored in the prp[] array,
-		 * so prp_index == count is valid.
-		 */
-		if (spdk_unlikely(i > SPDK_COUNTOF(tr->u.prp))) {
-			SPDK_ERRLOG("out of PRP entries\n");
-			return -EFAULT;
-		}
-
-		phys_addr = vfio_vtophys(virt_addr, NULL);
-
-		if (i == 0) {
-			SPDK_DEBUGLOG(nvme_vfio, "prp1 = %p\n", (void *)phys_addr);
-			cmd->dptr.prp.prp1 = phys_addr;
-			seg_len = page_size - ((uintptr_t)virt_addr & page_mask);
-		} else {
-			if ((phys_addr & page_mask) != 0) {
-				SPDK_ERRLOG("PRP %u not page aligned (%p)\n", i, virt_addr);
-				return -EFAULT;
-			}
-
-			SPDK_DEBUGLOG(nvme_vfio, "prp[%u] = %p\n", i - 1, (void *)phys_addr);
-			tr->u.prp[i - 1] = phys_addr;
-			seg_len = page_size;
-		}
-
-		seg_len = spdk_min(seg_len, len);
-		virt_addr += seg_len;
-		len -= seg_len;
-		i++;
-	}
-
-	cmd->psdt = SPDK_NVME_PSDT_PRP;
-	if (i <= 1) {
-		cmd->dptr.prp.prp2 = 0;
-	} else if (i == 2) {
-		cmd->dptr.prp.prp2 = tr->u.prp[0];
-		SPDK_DEBUGLOG(nvme_vfio, "prp2 = %p\n", (void *)cmd->dptr.prp.prp2);
-	} else {
-		cmd->dptr.prp.prp2 = tr->prp_sgl_bus_addr;
-		SPDK_DEBUGLOG(nvme_vfio, "prp2 = %p (PRP list)\n", (void *)cmd->dptr.prp.prp2);
-	}
-
-	*prp_index = i;
-	return 0;
-}
-
-static int
-nvme_vfio_qpair_build_contig_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req,
-				     struct nvme_tracker *tr, bool dword_aligned)
-{
-	uint32_t prp_index = 0;
-	int rc;
-
-	rc = nvme_vfio_prp_list_append(tr, &prp_index, req->payload.contig_or_cb_arg + req->payload_offset,
-				       req->payload_size, qpair->ctrlr->page_size);
-	if (rc) {
-		nvme_pcie_qpair_manual_complete_tracker(qpair, tr, SPDK_NVME_SCT_GENERIC,
-							SPDK_NVME_SC_INVALID_FIELD,
-							1 /* do not retry */, true);
-	}
-
-	return rc;
-}
-
-static int
-nvme_vfio_qpair_submit_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req)
-{
-	struct nvme_tracker	*tr;
-	int			rc = 0;
-	struct spdk_nvme_ctrlr	*ctrlr = qpair->ctrlr;
-	struct nvme_pcie_qpair	*vqpair = nvme_pcie_qpair(qpair);
-
-	if (spdk_unlikely(nvme_qpair_is_admin_queue(qpair))) {
-		nvme_robust_mutex_lock(&ctrlr->ctrlr_lock);
-	}
-
-	tr = TAILQ_FIRST(&vqpair->free_tr);
-
-	if (tr == NULL) {
-		/* Inform the upper layer to try again later. */
-		rc = -EAGAIN;
-		goto exit;
-	}
-
-	TAILQ_REMOVE(&vqpair->free_tr, tr, tq_list); /* remove tr from free_tr */
-	TAILQ_INSERT_TAIL(&vqpair->outstanding_tr, tr, tq_list);
-	tr->req = req;
-	tr->cb_fn = req->cb_fn;
-	tr->cb_arg = req->cb_arg;
-	req->cmd.cid = tr->cid;
-
-	if (req->payload_size != 0) {
-		rc = nvme_vfio_qpair_build_contig_request(qpair, req, tr, true);
-		if (rc) {
-			goto exit;
-		}
-	}
-
-	nvme_pcie_qpair_submit_tracker(qpair, tr);
-
-exit:
-	if (spdk_unlikely(nvme_qpair_is_admin_queue(qpair))) {
-		nvme_robust_mutex_unlock(&ctrlr->ctrlr_lock);
-	}
-
-	return rc;
-}
-
 const struct spdk_nvme_transport_ops vfio_ops = {
 	.name = "VFIOUSER",
 	.type = SPDK_NVME_TRANSPORT_VFIOUSER,
@@ -540,7 +397,7 @@ const struct spdk_nvme_transport_ops vfio_ops = {
 
 	.qpair_reset = nvme_pcie_qpair_reset,
 	.qpair_abort_reqs = nvme_pcie_qpair_abort_reqs,
-	.qpair_submit_request = nvme_vfio_qpair_submit_request,
+	.qpair_submit_request = nvme_pcie_qpair_submit_request,
 	.qpair_process_completions = nvme_pcie_qpair_process_completions,
 
 	.poll_group_create = nvme_pcie_poll_group_create,
diff --git a/lib/nvme/spdk_nvme.map b/lib/nvme/spdk_nvme.map
index f9d95dba2..7df89b050 100644
--- a/lib/nvme/spdk_nvme.map
+++ b/lib/nvme/spdk_nvme.map
@@ -30,6 +30,7 @@
 	spdk_nvme_pcie_set_hotplug_filter;
 
 	spdk_nvme_ctrlr_is_discovery;
+	spdk_nvme_ctrlr_is_fabrics;
 	spdk_nvme_ctrlr_get_default_ctrlr_opts;
 	spdk_nvme_ctrlr_set_trid;
 	spdk_nvme_ctrlr_reset_subsystem;
@@ -139,6 +140,7 @@
 	spdk_nvme_ns_cmd_readv_with_md;
 	spdk_nvme_ns_cmd_read_with_md;
 	spdk_nvme_ns_cmd_dataset_management;
+	spdk_nvme_ns_cmd_copy;
 	spdk_nvme_ns_cmd_flush;
 	spdk_nvme_ns_cmd_reservation_register;
 	spdk_nvme_ns_cmd_reservation_release;
diff --git a/lib/nvmf/Makefile b/lib/nvmf/Makefile
index e7eab45b9..22cab3985 100644
--- a/lib/nvmf/Makefile
+++ b/lib/nvmf/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 8
+SO_VER := 9
 SO_MINOR := 0
 
 C_SRCS = ctrlr.c ctrlr_discovery.c ctrlr_bdev.c \
diff --git a/lib/nvmf/ctrlr.c b/lib/nvmf/ctrlr.c
index 4fe815d06..678e8ea99 100644
--- a/lib/nvmf/ctrlr.c
+++ b/lib/nvmf/ctrlr.c
@@ -39,14 +39,13 @@
 #include "spdk/bit_array.h"
 #include "spdk/endian.h"
 #include "spdk/thread.h"
-#include "spdk/trace.h"
 #include "spdk/nvme_spec.h"
 #include "spdk/nvmf_cmd.h"
 #include "spdk/string.h"
 #include "spdk/util.h"
 #include "spdk/version.h"
-
 #include "spdk/log.h"
+#include "spdk_internal/usdt.h"
 
 #define MIN_KEEP_ALIVE_TIMEOUT_IN_MS 10000
 #define NVMF_DISC_KATO_IN_MS 120000
@@ -88,6 +87,7 @@ nvmf_invalid_connect_response(struct spdk_nvmf_fabric_connect_rsp *rsp,
 #define SPDK_NVMF_INVALID_CONNECT_DATA(rsp, field)	\
 	nvmf_invalid_connect_response(rsp, 1, offsetof(struct spdk_nvmf_fabric_connect_data, field))
 
+
 static void
 nvmf_ctrlr_stop_keep_alive_timer(struct spdk_nvmf_ctrlr *ctrlr)
 {
@@ -252,6 +252,9 @@ ctrlr_add_qpair_and_update_rsp(struct spdk_nvmf_qpair *qpair,
 	rsp->status_code_specific.success.cntlid = ctrlr->cntlid;
 	SPDK_DEBUGLOG(nvmf, "connect capsule response: cntlid = 0x%04x\n",
 		      rsp->status_code_specific.success.cntlid);
+
+	SPDK_DTRACE_PROBE4(nvmf_ctrlr_add_qpair, qpair, qpair->qid, ctrlr->subsys->subnqn,
+			   ctrlr->hostnqn);
 }
 
 static void
@@ -336,8 +339,7 @@ nvmf_ctrlr_create(struct spdk_nvmf_subsystem *subsystem,
 	ctrlr->qpair_mask = spdk_bit_array_create(transport->opts.max_qpairs_per_ctrlr);
 	if (!ctrlr->qpair_mask) {
 		SPDK_ERRLOG("Failed to allocate controller qpair mask\n");
-		free(ctrlr);
-		return NULL;
+		goto err_qpair_mask;
 	}
 
 	nvmf_ctrlr_cdata_init(transport, subsystem, &ctrlr->cdata);
@@ -431,15 +433,13 @@ nvmf_ctrlr_create(struct spdk_nvmf_subsystem *subsystem,
 	if (ctrlr->subsys->subtype == SPDK_NVMF_SUBTYPE_NVME) {
 		if (spdk_nvmf_qpair_get_listen_trid(req->qpair, &listen_trid) != 0) {
 			SPDK_ERRLOG("Could not get listener transport ID\n");
-			free(ctrlr);
-			return NULL;
+			goto err_listener;
 		}
 
 		ctrlr->listener = nvmf_subsystem_find_listener(ctrlr->subsys, &listen_trid);
 		if (!ctrlr->listener) {
 			SPDK_ERRLOG("Listener was not found\n");
-			free(ctrlr);
-			return NULL;
+			goto err_listener;
 		}
 	}
 
@@ -447,6 +447,11 @@ nvmf_ctrlr_create(struct spdk_nvmf_subsystem *subsystem,
 	spdk_thread_send_msg(subsystem->thread, _nvmf_subsystem_add_ctrlr, req);
 
 	return ctrlr;
+err_listener:
+	spdk_bit_array_free(&ctrlr->qpair_mask);
+err_qpair_mask:
+	free(ctrlr);
+	return NULL;
 }
 
 static void
@@ -709,8 +714,10 @@ _nvmf_ctrlr_connect(struct spdk_nvmf_request *req)
 
 	if (0 == qpair->qid) {
 		qpair->group->stat.admin_qpairs++;
+		qpair->group->stat.current_admin_qpairs++;
 	} else {
 		qpair->group->stat.io_qpairs++;
+		qpair->group->stat.current_io_qpairs++;
 	}
 
 	if (cmd->qid == 0) {
@@ -1587,6 +1594,41 @@ nvmf_ctrlr_set_features_reservation_persistence(struct spdk_nvmf_request *req)
 	return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 }
 
+static int
+nvmf_ctrlr_set_features_host_behavior_support(struct spdk_nvmf_request *req)
+{
+	struct spdk_nvmf_ctrlr *ctrlr = req->qpair->ctrlr;
+	struct spdk_nvme_cpl *response = &req->rsp->nvme_cpl;
+	struct spdk_nvme_host_behavior *host_behavior;
+
+	SPDK_DEBUGLOG(nvmf, "Set Features - Host Behavior Support\n");
+	if (req->iovcnt != 1) {
+		SPDK_ERRLOG("Host Behavior Support invalid iovcnt: %d\n", req->iovcnt);
+		response->status.sct = SPDK_NVME_SCT_GENERIC;
+		response->status.sc = SPDK_NVME_SC_INVALID_FIELD;
+		return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
+	}
+	if (req->iov[0].iov_len != sizeof(struct spdk_nvme_host_behavior)) {
+		SPDK_ERRLOG("Host Behavior Support invalid iov_len: %zd\n", req->iov[0].iov_len);
+		response->status.sct = SPDK_NVME_SCT_GENERIC;
+		response->status.sc = SPDK_NVME_SC_INVALID_FIELD;
+		return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
+	}
+
+	host_behavior = (struct spdk_nvme_host_behavior *)req->iov[0].iov_base;
+	if (host_behavior->acre == 0) {
+		ctrlr->acre_enabled = false;
+	} else if (host_behavior->acre == 1) {
+		ctrlr->acre_enabled = true;
+	} else {
+		SPDK_ERRLOG("Host Behavior Support invalid acre: 0x%02x\n", host_behavior->acre);
+		response->status.sct = SPDK_NVME_SCT_GENERIC;
+		response->status.sc = SPDK_NVME_SC_INVALID_FIELD;
+		return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
+	}
+	return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
+}
+
 static int
 nvmf_ctrlr_set_features_keep_alive_timer(struct spdk_nvmf_request *req)
 {
@@ -1705,11 +1747,73 @@ nvmf_ctrlr_async_event_request(struct spdk_nvmf_request *req)
 	return SPDK_NVMF_REQUEST_EXEC_STATUS_ASYNCHRONOUS;
 }
 
+struct copy_iovs_ctx {
+	struct iovec *iovs;
+	int iovcnt;
+	int cur_iov_idx;
+	size_t cur_iov_offset;
+};
+
 static void
-nvmf_get_firmware_slot_log_page(void *buffer, uint64_t offset, uint32_t length)
+_init_copy_iovs_ctx(struct copy_iovs_ctx *copy_ctx, struct iovec *iovs, int iovcnt)
+{
+	int iov_idx = 0;
+	struct iovec *iov;
+
+	copy_ctx->iovs = iovs;
+	copy_ctx->iovcnt = iovcnt;
+	copy_ctx->cur_iov_idx = 0;
+	copy_ctx->cur_iov_offset = 0;
+
+	while (iov_idx < copy_ctx->iovcnt) {
+		iov = &copy_ctx->iovs[iov_idx];
+		memset(iov->iov_base, 0, iov->iov_len);
+		iov_idx++;
+	}
+}
+
+static size_t
+_copy_buf_to_iovs(struct copy_iovs_ctx *copy_ctx, const void *buf, size_t buf_len)
+{
+	size_t len, iov_remain_len, copied_len = 0;
+	struct iovec *iov;
+
+	if (buf_len == 0) {
+		return 0;
+	}
+
+	while (copy_ctx->cur_iov_idx < copy_ctx->iovcnt) {
+		iov = &copy_ctx->iovs[copy_ctx->cur_iov_idx];
+		iov_remain_len = iov->iov_len - copy_ctx->cur_iov_offset;
+		if (iov_remain_len == 0) {
+			copy_ctx->cur_iov_idx++;
+			copy_ctx->cur_iov_offset = 0;
+			continue;
+		}
+
+		len = spdk_min(iov_remain_len, buf_len - copied_len);
+		memcpy((char *)iov->iov_base + copy_ctx->cur_iov_offset,
+		       (const char *)buf + copied_len,
+		       len);
+		copied_len += len;
+		copy_ctx->cur_iov_offset += len;
+
+		if (buf_len == copied_len) {
+			return copied_len;
+		}
+	}
+
+	return copied_len;
+}
+
+static void
+nvmf_get_firmware_slot_log_page(struct iovec *iovs, int iovcnt, uint64_t offset, uint32_t length)
 {
 	struct spdk_nvme_firmware_page fw_page;
 	size_t copy_len;
+	struct copy_iovs_ctx copy_ctx;
+
+	_init_copy_iovs_ctx(&copy_ctx, iovs, iovcnt);
 
 	memset(&fw_page, 0, sizeof(fw_page));
 	fw_page.afi.active_slot = 1;
@@ -1719,7 +1823,7 @@ nvmf_get_firmware_slot_log_page(void *buffer, uint64_t offset, uint32_t length)
 	if (offset < sizeof(fw_page)) {
 		copy_len = spdk_min(sizeof(fw_page) - offset, length);
 		if (copy_len > 0) {
-			memcpy(buffer, (const char *)&fw_page + offset, copy_len);
+			_copy_buf_to_iovs(&copy_ctx, (const char *)&fw_page + offset, copy_len);
 		}
 	}
 }
@@ -1762,16 +1866,18 @@ nvmf_ctrlr_mask_aen(struct spdk_nvmf_ctrlr *ctrlr,
 #define SPDK_NVMF_ANA_DESC_SIZE	(sizeof(struct spdk_nvme_ana_group_descriptor) +	\
 				 sizeof(uint32_t))
 static void
-nvmf_get_ana_log_page(struct spdk_nvmf_ctrlr *ctrlr, void *data,
+nvmf_get_ana_log_page(struct spdk_nvmf_ctrlr *ctrlr, struct iovec *iovs, int iovcnt,
 		      uint64_t offset, uint32_t length, uint32_t rae)
 {
-	char *buf = data;
 	struct spdk_nvme_ana_page ana_hdr;
 	char _ana_desc[SPDK_NVMF_ANA_DESC_SIZE];
 	struct spdk_nvme_ana_group_descriptor *ana_desc;
-	size_t copy_len;
+	size_t copy_len, copied_len;
 	uint32_t num_ns = 0;
 	struct spdk_nvmf_ns *ns;
+	struct copy_iovs_ctx copy_ctx;
+
+	_init_copy_iovs_ctx(&copy_ctx, iovs, iovcnt);
 
 	if (length == 0) {
 		return;
@@ -1792,9 +1898,9 @@ nvmf_get_ana_log_page(struct spdk_nvmf_ctrlr *ctrlr, void *data,
 		ana_hdr.change_count = 0;
 
 		copy_len = spdk_min(sizeof(ana_hdr) - offset, length);
-		memcpy(buf, (const char *)&ana_hdr + offset, copy_len);
-		length -= copy_len;
-		buf += copy_len;
+		copied_len = _copy_buf_to_iovs(&copy_ctx, (const char *)&ana_hdr + offset, copy_len);
+		assert(copied_len == copy_len);
+		length -= copied_len;
 		offset = 0;
 	}
 
@@ -1821,9 +1927,9 @@ nvmf_get_ana_log_page(struct spdk_nvmf_ctrlr *ctrlr, void *data,
 		ana_desc->change_count = 0;
 
 		copy_len = spdk_min(SPDK_NVMF_ANA_DESC_SIZE - offset, length);
-		memcpy(buf, (const char *)ana_desc + offset, copy_len);
-		length -= copy_len;
-		buf += copy_len;
+		copied_len = _copy_buf_to_iovs(&copy_ctx, (const char *)ana_desc + offset, copy_len);
+		assert(copied_len == copy_len);
+		length -= copied_len;
 		offset = 0;
 
 		if (length == 0) {
@@ -1867,14 +1973,17 @@ nvmf_ctrlr_ns_changed(struct spdk_nvmf_ctrlr *ctrlr, uint32_t nsid)
 
 static void
 nvmf_get_changed_ns_list_log_page(struct spdk_nvmf_ctrlr *ctrlr,
-				  void *buffer, uint64_t offset, uint32_t length, uint32_t rae)
+				  struct iovec *iovs, int iovcnt, uint64_t offset, uint32_t length, uint32_t rae)
 {
 	size_t copy_length;
+	struct copy_iovs_ctx copy_ctx;
+
+	_init_copy_iovs_ctx(&copy_ctx, iovs, iovcnt);
 
 	if (offset < sizeof(ctrlr->changed_ns_list)) {
 		copy_length = spdk_min(length, sizeof(ctrlr->changed_ns_list) - offset);
 		if (copy_length) {
-			memcpy(buffer, (char *)&ctrlr->changed_ns_list + offset, copy_length);
+			_copy_buf_to_iovs(&copy_ctx, (char *)&ctrlr->changed_ns_list + offset, copy_length);
 		}
 	}
 
@@ -1923,36 +2032,34 @@ static const struct spdk_nvme_cmds_and_effect_log_page g_cmds_and_effect_log_pag
 };
 
 static void
-nvmf_get_cmds_and_effects_log_page(void *buffer,
+nvmf_get_cmds_and_effects_log_page(struct iovec *iovs, int iovcnt,
 				   uint64_t offset, uint32_t length)
 {
 	uint32_t page_size = sizeof(struct spdk_nvme_cmds_and_effect_log_page);
 	size_t copy_len = 0;
-	size_t zero_len = length;
+	struct copy_iovs_ctx copy_ctx;
+
+	_init_copy_iovs_ctx(&copy_ctx, iovs, iovcnt);
 
 	if (offset < page_size) {
 		copy_len = spdk_min(page_size - offset, length);
-		zero_len -= copy_len;
-		memcpy(buffer, (char *)(&g_cmds_and_effect_log_page) + offset, copy_len);
-	}
-
-	if (zero_len) {
-		memset((char *)buffer + copy_len, 0, zero_len);
+		_copy_buf_to_iovs(&copy_ctx, (char *)(&g_cmds_and_effect_log_page) + offset, copy_len);
 	}
 }
 
 static void
 nvmf_get_reservation_notification_log_page(struct spdk_nvmf_ctrlr *ctrlr,
-		void *data, uint64_t offset, uint32_t length, uint32_t rae)
+		struct iovec *iovs, int iovcnt, uint64_t offset, uint32_t length, uint32_t rae)
 {
 	uint32_t unit_log_len, avail_log_len, next_pos, copy_len;
 	struct spdk_nvmf_reservation_log *log, *log_tmp;
-	uint8_t *buf = data;
+	struct copy_iovs_ctx copy_ctx;
+
+	_init_copy_iovs_ctx(&copy_ctx, iovs, iovcnt);
 
 	unit_log_len = sizeof(struct spdk_nvme_reservation_notification_log);
-	/* No available log, return 1 zeroed log page */
+	/* No available log, return zeroed log pages */
 	if (!ctrlr->num_avail_log_pages) {
-		memset(buf, 0, spdk_min(length, unit_log_len));
 		return;
 	}
 
@@ -1969,10 +2076,9 @@ nvmf_get_reservation_notification_log_page(struct spdk_nvmf_ctrlr *ctrlr,
 		next_pos += unit_log_len;
 		if (next_pos > offset) {
 			copy_len = spdk_min(next_pos - offset, length);
-			memcpy(buf, &log->log, copy_len);
+			_copy_buf_to_iovs(&copy_ctx, &log->log, copy_len);
 			length -= copy_len;
 			offset += copy_len;
-			buf += copy_len;
 		}
 		free(log);
 
@@ -2048,23 +2154,23 @@ nvmf_ctrlr_get_log_page(struct spdk_nvmf_request *req)
 			/* TODO: actually fill out log page data */
 			return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 		case SPDK_NVME_LOG_FIRMWARE_SLOT:
-			nvmf_get_firmware_slot_log_page(req->data, offset, len);
+			nvmf_get_firmware_slot_log_page(req->iov, req->iovcnt, offset, len);
 			return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 		case SPDK_NVME_LOG_ASYMMETRIC_NAMESPACE_ACCESS:
 			if (subsystem->flags.ana_reporting) {
-				nvmf_get_ana_log_page(ctrlr, req->data, offset, len, rae);
+				nvmf_get_ana_log_page(ctrlr, req->iov, req->iovcnt, offset, len, rae);
 				return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 			} else {
 				goto invalid_log_page;
 			}
 		case SPDK_NVME_LOG_COMMAND_EFFECTS_LOG:
-			nvmf_get_cmds_and_effects_log_page(req->data, offset, len);
+			nvmf_get_cmds_and_effects_log_page(req->iov, req->iovcnt, offset, len);
 			return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 		case SPDK_NVME_LOG_CHANGED_NS_LIST:
-			nvmf_get_changed_ns_list_log_page(ctrlr, req->data, offset, len, rae);
+			nvmf_get_changed_ns_list_log_page(ctrlr, req->iov, req->iovcnt, offset, len, rae);
 			return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 		case SPDK_NVME_LOG_RESERVATION_NOTIFICATION:
-			nvmf_get_reservation_notification_log_page(ctrlr, req->data, offset, len, rae);
+			nvmf_get_reservation_notification_log_page(ctrlr, req->iov, req->iovcnt, offset, len, rae);
 			return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 		default:
 			goto invalid_log_page;
@@ -2072,7 +2178,7 @@ nvmf_ctrlr_get_log_page(struct spdk_nvmf_request *req)
 	}
 
 invalid_log_page:
-	SPDK_ERRLOG("Unsupported Get Log Page 0x%02X\n", lid);
+	SPDK_DEBUGLOG(nvmf, "Unsupported Get Log Page 0x%02X\n", lid);
 	response->status.sct = SPDK_NVME_SCT_GENERIC;
 	response->status.sc = SPDK_NVME_SC_INVALID_FIELD;
 	return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
@@ -2212,7 +2318,6 @@ spdk_nvmf_ctrlr_identify_ctrlr(struct spdk_nvmf_ctrlr *ctrlr, struct spdk_nvme_c
 		cdata->ctratt.host_id_exhid_supported = 1;
 		/* TODO: Concurrent execution of multiple abort commands. */
 		cdata->acl = 0;
-		cdata->aerl = 0;
 		cdata->frmw.slot1_ro = 1;
 		cdata->frmw.num_slots = 1;
 
@@ -2247,6 +2352,11 @@ spdk_nvmf_ctrlr_identify_ctrlr(struct spdk_nvmf_ctrlr *ctrlr, struct spdk_nvme_c
 
 		nvmf_ctrlr_populate_oacs(ctrlr, cdata);
 
+		assert(subsystem->tgt != NULL);
+		cdata->crdt[0] = subsystem->tgt->crdt[0];
+		cdata->crdt[1] = subsystem->tgt->crdt[1];
+		cdata->crdt[2] = subsystem->tgt->crdt[2];
+
 		SPDK_DEBUGLOG(nvmf, "ext ctrlr data: ioccsz 0x%x\n",
 			      cdata->nvmf_specific.ioccsz);
 		SPDK_DEBUGLOG(nvmf, "ext ctrlr data: iorcsz 0x%x\n",
@@ -2585,7 +2695,7 @@ nvmf_ctrlr_get_features(struct spdk_nvmf_request *req)
 		case SPDK_NVME_FEAT_ASYNC_EVENT_CONFIGURATION:
 			return get_features_generic(req, ctrlr->feat.async_event_configuration.raw);
 		default:
-			SPDK_ERRLOG("Get Features command with unsupported feature ID 0x%02x\n", feature);
+			SPDK_DEBUGLOG(nvmf, "Get Features command with unsupported feature ID 0x%02x\n", feature);
 			response->status.sc = SPDK_NVME_SC_INVALID_FIELD;
 			return SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE;
 		}
@@ -2740,6 +2850,8 @@ nvmf_ctrlr_set_features(struct spdk_nvmf_request *req)
 		return nvmf_ctrlr_set_features_reservation_notification_mask(req);
 	case SPDK_NVME_FEAT_HOST_RESERVE_PERSIST:
 		return nvmf_ctrlr_set_features_reservation_persistence(req);
+	case SPDK_NVME_FEAT_HOST_BEHAVIOR_SUPPORT:
+		return nvmf_ctrlr_set_features_host_behavior_support(req);
 	default:
 		SPDK_ERRLOG("Set Features command with unsupported feature ID 0x%02x\n", feature);
 		response->status.sc = SPDK_NVME_SC_INVALID_FIELD;
@@ -3478,16 +3590,30 @@ _nvmf_request_complete(void *ctx)
 	bool is_aer = false;
 	uint32_t nsid;
 	bool paused;
+	uint8_t opcode;
 
 	rsp->sqid = 0;
 	rsp->status.p = 0;
 	rsp->cid = req->cmd->nvme_cmd.cid;
+	nsid = req->cmd->nvme_cmd.nsid;
+	opcode = req->cmd->nvmf_cmd.opcode;
 
 	qpair = req->qpair;
 	if (qpair->ctrlr) {
 		sgroup = &qpair->group->sgroups[qpair->ctrlr->subsys->id];
 		assert(sgroup != NULL);
 		is_aer = req->cmd->nvme_cmd.opc == SPDK_NVME_OPC_ASYNC_EVENT_REQUEST;
+
+		/*
+		 * Set the crd value.
+		 * If the the IO has any error, and dnr (DoNotRetry) is not 1,
+		 * and ACRE is enabled, we will set the crd to 1 to select the first CRDT.
+		 */
+		if (spdk_nvme_cpl_is_error(rsp) &&
+		    rsp->status.dnr == 0 &&
+		    qpair->ctrlr->acre_enabled) {
+			rsp->status.crd = 1;
+		}
 	} else if (spdk_unlikely(nvmf_request_is_fabric_connect(req))) {
 		sgroup = nvmf_subsystem_pg_from_connect_cmd(req);
 	}
@@ -3503,13 +3629,11 @@ _nvmf_request_complete(void *ctx)
 
 	/* AER cmd is an exception */
 	if (sgroup && !is_aer) {
-		if (spdk_unlikely(req->cmd->nvmf_cmd.opcode == SPDK_NVME_OPC_FABRIC ||
+		if (spdk_unlikely(opcode == SPDK_NVME_OPC_FABRIC ||
 				  nvmf_qpair_is_admin_queue(qpair))) {
 			assert(sgroup->mgmt_io_outstanding > 0);
 			sgroup->mgmt_io_outstanding--;
 		} else {
-			nsid = req->cmd->nvme_cmd.nsid;
-
 			/* NOTE: This implicitly also checks for 0, since 0 - 1 wraps around to UINT32_MAX. */
 			if (spdk_likely(nsid - 1 < sgroup->num_ns)) {
 				sgroup->ns_info[nsid - 1].io_outstanding--;
diff --git a/lib/nvmf/ctrlr_bdev.c b/lib/nvmf/ctrlr_bdev.c
index a631b8003..b79254f79 100644
--- a/lib/nvmf/ctrlr_bdev.c
+++ b/lib/nvmf/ctrlr_bdev.c
@@ -140,6 +140,7 @@ nvmf_bdev_ctrlr_identify_ns(struct spdk_nvmf_ns *ns, struct spdk_nvme_ns_data *n
 {
 	struct spdk_bdev *bdev = ns->bdev;
 	uint64_t num_blocks;
+	uint32_t phys_blocklen;
 
 	num_blocks = spdk_bdev_get_num_blocks(bdev);
 
@@ -181,6 +182,15 @@ nvmf_bdev_ctrlr_identify_ns(struct spdk_nvmf_ns *ns, struct spdk_nvme_ns_data *n
 		nsdata->lbaf[0].ms = 0;
 		nsdata->lbaf[0].lbads = spdk_u32log2(spdk_bdev_get_data_block_size(bdev));
 	}
+
+	phys_blocklen = spdk_bdev_get_physical_block_size(bdev);
+	assert(phys_blocklen > 0);
+	/* Linux driver uses min(nawupf, npwg) to set physical_block_size */
+	nsdata->nsfeat.optperf = 1;
+	nsdata->nsfeat.ns_atomic_write_unit = 1;
+	nsdata->npwg = (phys_blocklen >> nsdata->lbaf[0].lbads) - 1;
+	nsdata->nawupf = nsdata->npwg;
+
 	nsdata->noiob = spdk_bdev_get_optimal_io_boundary(bdev);
 	nsdata->nmic.can_share = 1;
 	if (ns->ptpl_file != NULL) {
diff --git a/lib/nvmf/ctrlr_discovery.c b/lib/nvmf/ctrlr_discovery.c
index 7e7be6b63..a24538165 100644
--- a/lib/nvmf/ctrlr_discovery.c
+++ b/lib/nvmf/ctrlr_discovery.c
@@ -44,7 +44,6 @@
 #include "spdk/trace.h"
 #include "spdk/nvmf_spec.h"
 
-#include "spdk/bdev_module.h"
 #include "spdk/log.h"
 
 void
diff --git a/lib/nvmf/fc.c b/lib/nvmf/fc.c
index 88f82e8af..d80cd2d48 100644
--- a/lib/nvmf/fc.c
+++ b/lib/nvmf/fc.c
@@ -116,52 +116,68 @@ SPDK_TRACE_REGISTER_FN(nvmf_fc_trace, "nvmf_fc", TRACE_GROUP_NVMF_FC)
 	spdk_trace_register_object(OBJECT_NVMF_FC_IO, 'r');
 	spdk_trace_register_description("FC_REQ_NEW",
 					TRACE_FC_REQ_INIT,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 1, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 1,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_READ_SUBMIT_TO_BDEV",
 					TRACE_FC_REQ_READ_BDEV,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_READ_XFER_DATA",
 					TRACE_FC_REQ_READ_XFER,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_READ_RSP",
 					TRACE_FC_REQ_READ_RSP,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_WRITE_NEED_BUFFER",
 					TRACE_FC_REQ_WRITE_BUFFS,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_WRITE_XFER_DATA",
 					TRACE_FC_REQ_WRITE_XFER,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_WRITE_SUBMIT_TO_BDEV",
 					TRACE_FC_REQ_WRITE_BDEV,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_WRITE_RSP",
 					TRACE_FC_REQ_WRITE_RSP,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_NONE_SUBMIT_TO_BDEV",
 					TRACE_FC_REQ_NONE_BDEV,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_NONE_RSP",
 					TRACE_FC_REQ_NONE_RSP,
-					OWNER_NONE, OBJECT_NVMF_FC_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_FC_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_SUCCESS",
 					TRACE_FC_REQ_SUCCESS,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_FAILED",
 					TRACE_FC_REQ_FAILED,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_ABORTED",
 					TRACE_FC_REQ_ABORTED,
-					OWNER_NONE, OBJECT_NONE, 0, 1, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_ABORTED_SUBMIT_TO_BDEV",
 					TRACE_FC_REQ_BDEV_ABORTED,
-					OWNER_NONE, OBJECT_NONE, 0, 1, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_PENDING",
 					TRACE_FC_REQ_PENDING,
-					OWNER_NONE, OBJECT_NONE, 0, 1, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("FC_REQ_FUSED_WAITING",
 					TRACE_FC_REQ_FUSED_WAITING,
-					OWNER_NONE, OBJECT_NONE, 0, 1, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 }
 
 /**
@@ -315,7 +331,7 @@ nvmf_fc_record_req_trace_point(struct spdk_nvmf_fc_request *fc_req,
 	}
 	if (tpoint_id != SPDK_TRACE_MAX_TPOINT_ID) {
 		spdk_trace_record(tpoint_id, fc_req->poller_lcore, 0,
-				  (uint64_t)(&fc_req->req), 0);
+				  (uint64_t)(&fc_req->req));
 	}
 }
 
diff --git a/lib/nvmf/nvmf.c b/lib/nvmf/nvmf.c
index 05aaff048..702be795c 100644
--- a/lib/nvmf/nvmf.c
+++ b/lib/nvmf/nvmf.c
@@ -37,11 +37,10 @@
 #include "spdk/bit_array.h"
 #include "spdk/thread.h"
 #include "spdk/nvmf.h"
-#include "spdk/trace.h"
 #include "spdk/endian.h"
 #include "spdk/string.h"
-
 #include "spdk/log.h"
+#include "spdk_internal/usdt.h"
 
 #include "nvmf_internal.h"
 #include "transport.h"
@@ -114,8 +113,11 @@ nvmf_tgt_create_poll_group(void *io_device, void *ctx_buf)
 	struct spdk_nvmf_tgt *tgt = io_device;
 	struct spdk_nvmf_poll_group *group = ctx_buf;
 	struct spdk_nvmf_transport *transport;
+	struct spdk_thread *thread = spdk_get_thread();
 	uint32_t sid;
 
+	SPDK_DTRACE_PROBE1(nvmf_create_poll_group, spdk_thread_get_id(thread));
+
 	TAILQ_INIT(&group->tgroups);
 	TAILQ_INIT(&group->qpairs);
 
@@ -148,7 +150,7 @@ nvmf_tgt_create_poll_group(void *io_device, void *ctx_buf)
 	pthread_mutex_unlock(&tgt->mutex);
 
 	group->poller = SPDK_POLLER_REGISTER(nvmf_poll_group_poll, group, 0);
-	group->thread = spdk_get_thread();
+	group->thread = thread;
 
 	return 0;
 }
@@ -162,6 +164,8 @@ nvmf_tgt_destroy_poll_group(void *io_device, void *ctx_buf)
 	struct spdk_nvmf_subsystem_poll_group *sgroup;
 	uint32_t sid, nsid;
 
+	SPDK_DTRACE_PROBE1(nvmf_destroy_poll_group, spdk_thread_get_id(group->thread));
+
 	pthread_mutex_lock(&tgt->mutex);
 	TAILQ_REMOVE(&tgt->poll_groups, group, link);
 	pthread_mutex_unlock(&tgt->mutex);
@@ -221,8 +225,9 @@ nvmf_tgt_destroy_poll_group_qpairs(struct spdk_nvmf_poll_group *group)
 {
 	struct nvmf_qpair_disconnect_many_ctx *ctx;
 
-	ctx = calloc(1, sizeof(struct nvmf_qpair_disconnect_many_ctx));
+	SPDK_DTRACE_PROBE1(nvmf_destroy_poll_group_qpairs, spdk_thread_get_id(group->thread));
 
+	ctx = calloc(1, sizeof(struct nvmf_qpair_disconnect_many_ctx));
 	if (!ctx) {
 		SPDK_ERRLOG("Failed to allocate memory for destroy poll group ctx\n");
 		return;
@@ -283,6 +288,16 @@ spdk_nvmf_tgt_create(struct spdk_nvmf_target_opts *opts)
 		acceptor_poll_rate = opts->acceptor_poll_rate;
 	}
 
+	if (!opts) {
+		tgt->crdt[0] = 0;
+		tgt->crdt[1] = 0;
+		tgt->crdt[2] = 0;
+	} else {
+		tgt->crdt[0] = opts->crdt[0];
+		tgt->crdt[1] = opts->crdt[1];
+		tgt->crdt[2] = opts->crdt[2];
+	}
+
 	tgt->discovery_genctr = 0;
 	TAILQ_INIT(&tgt->transports);
 	TAILQ_INIT(&tgt->poll_groups);
@@ -450,6 +465,9 @@ nvmf_write_subsystem_config_json(struct spdk_json_write_ctx *w,
 		spdk_json_write_named_uint32(w, "max_namespaces", max_namespaces);
 	}
 
+	spdk_json_write_named_uint32(w, "min_cntlid", spdk_nvmf_subsystem_get_min_cntlid(subsystem));
+	spdk_json_write_named_uint32(w, "max_cntlid", spdk_nvmf_subsystem_get_max_cntlid(subsystem));
+
 	/*     } "params" */
 	spdk_json_write_object_end(w);
 
@@ -569,6 +587,15 @@ spdk_nvmf_tgt_write_config_json(struct spdk_json_write_ctx *w, struct spdk_nvmf_
 
 	spdk_json_write_object_end(w);
 
+	spdk_json_write_object_begin(w);
+	spdk_json_write_named_string(w, "method", "nvmf_set_crdt");
+	spdk_json_write_named_object_begin(w, "params");
+	spdk_json_write_named_uint32(w, "crdt1", tgt->crdt[0]);
+	spdk_json_write_named_uint32(w, "crdt2", tgt->crdt[1]);
+	spdk_json_write_named_uint32(w, "crdt3", tgt->crdt[2]);
+	spdk_json_write_object_end(w);
+	spdk_json_write_object_end(w);
+
 	/* write transports */
 	TAILQ_FOREACH(transport, &tgt->transports, link) {
 		spdk_json_write_object_begin(w);
@@ -895,6 +922,7 @@ spdk_nvmf_poll_group_add(struct spdk_nvmf_poll_group *group,
 
 	/* We add the qpair to the group only it is succesfully added into the tgroup */
 	if (rc == 0) {
+		SPDK_DTRACE_PROBE2(nvmf_poll_group_add_qpair, qpair, spdk_thread_get_id(group->thread));
 		TAILQ_INSERT_TAIL(&group->qpairs, qpair, link);
 		nvmf_qpair_set_state(qpair, SPDK_NVMF_QPAIR_ACTIVE);
 	}
@@ -926,6 +954,7 @@ _nvmf_transport_qpair_fini(void *ctx)
 {
 	struct nvmf_qpair_disconnect_ctx *qpair_ctx = ctx;
 
+	spdk_nvmf_poll_group_remove(qpair_ctx->qpair);
 	nvmf_transport_qpair_fini(qpair_ctx->qpair, _nvmf_transport_qpair_fini_complete, qpair_ctx);
 }
 
@@ -949,12 +978,11 @@ _nvmf_ctrlr_free_from_qpair(void *ctx)
 void
 spdk_nvmf_poll_group_remove(struct spdk_nvmf_qpair *qpair)
 {
-	struct spdk_nvmf_ctrlr *ctrlr = qpair->ctrlr;
 	struct spdk_nvmf_transport_poll_group *tgroup;
-	struct spdk_nvmf_request *req, *tmp;
-	struct spdk_nvmf_subsystem_poll_group *sgroup;
 	int rc;
 
+	SPDK_DTRACE_PROBE2(nvmf_poll_group_remove_qpair, qpair,
+			   spdk_thread_get_id(qpair->group->thread));
 	nvmf_qpair_set_state(qpair, SPDK_NVMF_QPAIR_ERROR);
 
 	/* Find the tgroup and remove the qpair from the tgroup */
@@ -969,18 +997,6 @@ spdk_nvmf_poll_group_remove(struct spdk_nvmf_qpair *qpair)
 		}
 	}
 
-	if (ctrlr) {
-		sgroup = &qpair->group->sgroups[ctrlr->subsys->id];
-		TAILQ_FOREACH_SAFE(req, &sgroup->queued, link, tmp) {
-			if (req->qpair == qpair) {
-				TAILQ_REMOVE(&sgroup->queued, req, link);
-				if (nvmf_transport_req_free(req)) {
-					SPDK_ERRLOG("Transport request free error!\n");
-				}
-			}
-		}
-	}
-
 	TAILQ_REMOVE(&qpair->group->qpairs, qpair, link);
 	qpair->group = NULL;
 }
@@ -991,13 +1007,34 @@ _nvmf_qpair_destroy(void *ctx, int status)
 	struct nvmf_qpair_disconnect_ctx *qpair_ctx = ctx;
 	struct spdk_nvmf_qpair *qpair = qpair_ctx->qpair;
 	struct spdk_nvmf_ctrlr *ctrlr = qpair->ctrlr;
+	struct spdk_nvmf_request *req, *tmp;
+	struct spdk_nvmf_subsystem_poll_group *sgroup;
 
 	assert(qpair->state == SPDK_NVMF_QPAIR_DEACTIVATING);
 	qpair_ctx->qid = qpair->qid;
 
-	spdk_nvmf_poll_group_remove(qpair);
+	if (ctrlr) {
+		if (0 == qpair->qid) {
+			assert(qpair->group->stat.current_admin_qpairs > 0);
+			qpair->group->stat.current_admin_qpairs--;
+		} else {
+			assert(qpair->group->stat.current_io_qpairs > 0);
+			qpair->group->stat.current_io_qpairs--;
+		}
+
+		sgroup = &qpair->group->sgroups[ctrlr->subsys->id];
+		TAILQ_FOREACH_SAFE(req, &sgroup->queued, link, tmp) {
+			if (req->qpair == qpair) {
+				TAILQ_REMOVE(&sgroup->queued, req, link);
+				if (nvmf_transport_req_free(req)) {
+					SPDK_ERRLOG("Transport request free error!/n");
+				}
+			}
+		}
+	}
 
 	if (!ctrlr || !ctrlr->thread) {
+		spdk_nvmf_poll_group_remove(qpair);
 		nvmf_transport_qpair_fini(qpair, _nvmf_transport_qpair_fini_complete, qpair_ctx);
 		return;
 	}
@@ -1018,6 +1055,7 @@ _nvmf_qpair_disconnect_msg(void *ctx)
 int
 spdk_nvmf_qpair_disconnect(struct spdk_nvmf_qpair *qpair, nvmf_qpair_disconnect_cb cb_fn, void *ctx)
 {
+	struct spdk_nvmf_poll_group *group = qpair->group;
 	struct nvmf_qpair_disconnect_ctx *qpair_ctx;
 
 	if (__atomic_test_and_set(&qpair->disconnect_started, __ATOMIC_RELAXED)) {
@@ -1036,8 +1074,8 @@ spdk_nvmf_qpair_disconnect(struct spdk_nvmf_qpair *qpair, nvmf_qpair_disconnect_
 		return 0;
 	}
 
-	assert(qpair->group != NULL);
-	if (spdk_get_thread() != qpair->group->thread) {
+	assert(group != NULL);
+	if (spdk_get_thread() != group->thread) {
 		/* clear the atomic so we can set it on the next call on the proper thread. */
 		__atomic_clear(&qpair->disconnect_started, __ATOMIC_RELAXED);
 		qpair_ctx = calloc(1, sizeof(struct nvmf_qpair_disconnect_ctx));
@@ -1047,12 +1085,13 @@ spdk_nvmf_qpair_disconnect(struct spdk_nvmf_qpair *qpair, nvmf_qpair_disconnect_
 		}
 		qpair_ctx->qpair = qpair;
 		qpair_ctx->cb_fn = cb_fn;
-		qpair_ctx->thread = qpair->group->thread;
+		qpair_ctx->thread = group->thread;
 		qpair_ctx->ctx = ctx;
-		spdk_thread_send_msg(qpair->group->thread, _nvmf_qpair_disconnect_msg, qpair_ctx);
+		spdk_thread_send_msg(group->thread, _nvmf_qpair_disconnect_msg, qpair_ctx);
 		return 0;
 	}
 
+	SPDK_DTRACE_PROBE2(nvmf_qpair_disconnect, qpair, spdk_thread_get_id(group->thread));
 	assert(qpair->state == SPDK_NVMF_QPAIR_ACTIVE);
 	nvmf_qpair_set_state(qpair, SPDK_NVMF_QPAIR_DEACTIVATING);
 
@@ -1064,11 +1103,12 @@ spdk_nvmf_qpair_disconnect(struct spdk_nvmf_qpair *qpair, nvmf_qpair_disconnect_
 
 	qpair_ctx->qpair = qpair;
 	qpair_ctx->cb_fn = cb_fn;
-	qpair_ctx->thread = qpair->group->thread;
+	qpair_ctx->thread = group->thread;
 	qpair_ctx->ctx = ctx;
 
 	/* Check for outstanding I/O */
 	if (!TAILQ_EMPTY(&qpair->outstanding)) {
+		SPDK_DTRACE_PROBE2(nvmf_poll_group_drain_qpair, qpair, spdk_thread_get_id(group->thread));
 		qpair->state_cb = _nvmf_qpair_destroy;
 		qpair->state_cb_arg = qpair_ctx;
 		nvmf_qpair_free_aer(qpair);
@@ -1361,6 +1401,8 @@ fini:
 	}
 }
 
+static void nvmf_poll_group_remove_subsystem_msg(void *ctx);
+
 static void
 remove_subsystem_qpair_cb(void *ctx)
 {
@@ -1369,7 +1411,12 @@ remove_subsystem_qpair_cb(void *ctx)
 	assert(qpair_ctx->count > 0);
 	qpair_ctx->count--;
 	if (qpair_ctx->count == 0) {
-		_nvmf_poll_group_remove_subsystem_cb(ctx, 0);
+		/* All of the asynchronous callbacks for this context have been
+		 * completed.  Call nvmf_poll_group_remove_subsystem_msg() again
+		 * to check if all associated qpairs for this subsystem have
+		 * been removed from the poll group.
+		 */
+		nvmf_poll_group_remove_subsystem_msg(ctx);
 	}
 }
 
@@ -1380,6 +1427,7 @@ nvmf_poll_group_remove_subsystem_msg(void *ctx)
 	struct spdk_nvmf_subsystem *subsystem;
 	struct spdk_nvmf_poll_group *group;
 	struct nvmf_qpair_disconnect_many_ctx *qpair_ctx = ctx;
+	bool qpairs_found = false;
 	int rc = 0;
 
 	group = qpair_ctx->group;
@@ -1387,12 +1435,13 @@ nvmf_poll_group_remove_subsystem_msg(void *ctx)
 
 	/* Initialize count to 1.  This acts like a ref count, to ensure that if spdk_nvmf_qpair_disconnect
 	 * immediately invokes the callback (i.e. the qpairs is already in process of being disconnected)
-	 * that we don't prematurely call _nvmf_poll_group_remove_subsystem_cb() before we've
-	 * iterated the full list of qpairs.
+	 * that we don't recursively call nvmf_poll_group_remove_subsystem_msg before we've iterated the
+	 * full list of qpairs.
 	 */
 	qpair_ctx->count = 1;
 	TAILQ_FOREACH_SAFE(qpair, &group->qpairs, link, qpair_tmp) {
 		if ((qpair->ctrlr != NULL) && (qpair->ctrlr->subsys == subsystem)) {
+			qpairs_found = true;
 			qpair_ctx->count++;
 			rc = spdk_nvmf_qpair_disconnect(qpair, remove_subsystem_qpair_cb, ctx);
 			if (rc) {
@@ -1402,8 +1451,19 @@ nvmf_poll_group_remove_subsystem_msg(void *ctx)
 	}
 	qpair_ctx->count--;
 
+	if (!qpairs_found) {
+		_nvmf_poll_group_remove_subsystem_cb(ctx, 0);
+		return;
+	}
+
 	if (qpair_ctx->count == 0 || rc) {
-		_nvmf_poll_group_remove_subsystem_cb(ctx, rc);
+		/* If count == 0, it means there were some qpairs in the poll group but they
+		 * were already in process of being disconnected.  So we send a message to this
+		 * same thread so that this function executes again later.  We won't actually
+		 * invoke the remove_subsystem_cb until all of the qpairs are actually removed
+		 * from the poll group.
+		 */
+		spdk_thread_send_msg(spdk_get_thread(), nvmf_poll_group_remove_subsystem_msg, ctx);
 	}
 }
 
@@ -1584,6 +1644,8 @@ spdk_nvmf_poll_group_dump_stat(struct spdk_nvmf_poll_group *group, struct spdk_j
 	spdk_json_write_named_string(w, "name", spdk_thread_get_name(spdk_get_thread()));
 	spdk_json_write_named_uint32(w, "admin_qpairs", group->stat.admin_qpairs);
 	spdk_json_write_named_uint32(w, "io_qpairs", group->stat.io_qpairs);
+	spdk_json_write_named_uint32(w, "current_admin_qpairs", group->stat.current_admin_qpairs);
+	spdk_json_write_named_uint32(w, "current_io_qpairs", group->stat.current_io_qpairs);
 	spdk_json_write_named_uint64(w, "pending_bdev_io", group->stat.pending_bdev_io);
 
 	spdk_json_write_named_array_begin(w, "transports");
diff --git a/lib/nvmf/nvmf_internal.h b/lib/nvmf/nvmf_internal.h
index d76b742a3..bfa2dc1c8 100644
--- a/lib/nvmf/nvmf_internal.h
+++ b/lib/nvmf/nvmf_internal.h
@@ -49,6 +49,10 @@
 
 #define NVMF_MAX_ASYNC_EVENTS	(4)
 
+/* The spec reserves cntlid values in the range FFF0h to FFFFh. */
+#define NVMF_MIN_CNTLID 1
+#define NVMF_MAX_CNTLID 0xFFEF
+
 enum spdk_nvmf_subsystem_state {
 	SPDK_NVMF_SUBSYSTEM_INACTIVE = 0,
 	SPDK_NVMF_SUBSYSTEM_ACTIVATING,
@@ -83,6 +87,8 @@ struct spdk_nvmf_tgt {
 	spdk_nvmf_tgt_destroy_done_fn		*destroy_cb_fn;
 	void					*destroy_cb_arg;
 
+	uint16_t				crdt[3];
+
 	TAILQ_ENTRY(spdk_nvmf_tgt)		link;
 };
 
@@ -257,6 +263,7 @@ struct spdk_nvmf_ctrlr {
 	bool				dif_insert_or_strip;
 	bool				in_destruct;
 	bool				disconnect_in_progress;
+	bool				acre_enabled;
 
 	TAILQ_ENTRY(spdk_nvmf_ctrlr)	link;
 };
@@ -286,6 +293,9 @@ struct spdk_nvmf_subsystem {
 	struct spdk_nvmf_ns				**ns;
 	uint32_t					max_nsid;
 
+	uint16_t					min_cntlid;
+	uint16_t					max_cntlid;
+
 	TAILQ_HEAD(, spdk_nvmf_ctrlr)			ctrlrs;
 
 	/* A mutex used to protect the hosts list and allow_any_host flag. Unlike the namespace
@@ -372,6 +382,21 @@ void nvmf_subsystem_set_ana_state(struct spdk_nvmf_subsystem *subsystem,
 				  enum spdk_nvme_ana_state ana_state,
 				  spdk_nvmf_tgt_subsystem_listen_done_fn cb_fn, void *cb_arg);
 
+/**
+ * Sets the controller ID range for a subsystem.
+ * Valid range is [1, 0xFFEF].
+ *
+ * May only be performed on subsystems in the INACTIVE state.
+ *
+ * \param subsystem Subsystem to modify.
+ * \param min_cntlid Minimum controller ID.
+ * \param max_cntlid Maximum controller ID.
+ *
+ * \return 0 on success, or negated errno value on failure.
+ */
+int nvmf_subsystem_set_cntlid_range(struct spdk_nvmf_subsystem *subsystem,
+				    uint16_t min_cntlid, uint16_t max_cntlid);
+
 int nvmf_ctrlr_async_event_ns_notice(struct spdk_nvmf_ctrlr *ctrlr);
 int nvmf_ctrlr_async_event_ana_change_notice(struct spdk_nvmf_ctrlr *ctrlr);
 int nvmf_ctrlr_async_event_discovery_log_change_notice(struct spdk_nvmf_ctrlr *ctrlr);
diff --git a/lib/nvmf/nvmf_rpc.c b/lib/nvmf/nvmf_rpc.c
index 0998b0fa2..8ba9974f0 100644
--- a/lib/nvmf/nvmf_rpc.c
+++ b/lib/nvmf/nvmf_rpc.c
@@ -259,6 +259,9 @@ dump_nvmf_subsystem(struct spdk_json_write_ctx *w, struct spdk_nvmf_subsystem *s
 			spdk_json_write_named_uint32(w, "max_namespaces", max_namespaces);
 		}
 
+		spdk_json_write_named_uint32(w, "min_cntlid", spdk_nvmf_subsystem_get_min_cntlid(subsystem));
+		spdk_json_write_named_uint32(w, "max_cntlid", spdk_nvmf_subsystem_get_max_cntlid(subsystem));
+
 		spdk_json_write_named_array_begin(w, "namespaces");
 		for (ns = spdk_nvmf_subsystem_get_first_ns(subsystem); ns != NULL;
 		     ns = spdk_nvmf_subsystem_get_next_ns(subsystem, ns)) {
@@ -344,6 +347,8 @@ struct rpc_subsystem_create {
 	uint32_t max_namespaces;
 	bool allow_any_host;
 	bool ana_reporting;
+	uint16_t min_cntlid;
+	uint16_t max_cntlid;
 };
 
 static const struct spdk_json_object_decoder rpc_subsystem_create_decoders[] = {
@@ -354,6 +359,8 @@ static const struct spdk_json_object_decoder rpc_subsystem_create_decoders[] = {
 	{"max_namespaces", offsetof(struct rpc_subsystem_create, max_namespaces), spdk_json_decode_uint32, true},
 	{"allow_any_host", offsetof(struct rpc_subsystem_create, allow_any_host), spdk_json_decode_bool, true},
 	{"ana_reporting", offsetof(struct rpc_subsystem_create, ana_reporting), spdk_json_decode_bool, true},
+	{"min_cntlid", offsetof(struct rpc_subsystem_create, min_cntlid), spdk_json_decode_uint16, true},
+	{"max_cntlid", offsetof(struct rpc_subsystem_create, max_cntlid), spdk_json_decode_uint16, true},
 };
 
 static void
@@ -388,6 +395,8 @@ rpc_nvmf_create_subsystem(struct spdk_jsonrpc_request *request,
 						 "Memory allocation failed");
 		return;
 	}
+	req->min_cntlid = NVMF_MIN_CNTLID;
+	req->max_cntlid = NVMF_MAX_CNTLID;
 
 	if (spdk_json_decode_object(params, rpc_subsystem_create_decoders,
 				    SPDK_COUNTOF(rpc_subsystem_create_decoders),
@@ -436,6 +445,14 @@ rpc_nvmf_create_subsystem(struct spdk_jsonrpc_request *request,
 
 	spdk_nvmf_subsystem_set_ana_reporting(subsystem, req->ana_reporting);
 
+	if (nvmf_subsystem_set_cntlid_range(subsystem, req->min_cntlid, req->max_cntlid)) {
+		SPDK_ERRLOG("Subsystem %s: invalid cntlid range [%u-%u]\n", req->nqn, req->min_cntlid,
+			    req->max_cntlid);
+		spdk_jsonrpc_send_error_response_fmt(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS,
+						     "Invalid cntlid range [%u-%u]", req->min_cntlid, req->max_cntlid);
+		goto cleanup;
+	}
+
 	rc = spdk_nvmf_subsystem_start(subsystem,
 				       rpc_nvmf_subsystem_started,
 				       request);
diff --git a/lib/nvmf/rdma.c b/lib/nvmf/rdma.c
index e7f3f8ea6..a4f3def34 100644
--- a/lib/nvmf/rdma.c
+++ b/lib/nvmf/rdma.c
@@ -158,52 +158,70 @@ SPDK_TRACE_REGISTER_FN(nvmf_trace, "nvmf_rdma", TRACE_GROUP_NVMF_RDMA)
 {
 	spdk_trace_register_object(OBJECT_NVMF_RDMA_IO, 'r');
 	spdk_trace_register_description("RDMA_REQ_NEW", TRACE_RDMA_REQUEST_STATE_NEW,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 1, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 1,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_NEED_BUFFER", TRACE_RDMA_REQUEST_STATE_NEED_BUFFER,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_TX_PENDING_C2H",
 					TRACE_RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_TX_PENDING_H2C",
 					TRACE_RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_TX_H2C",
 					TRACE_RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_RDY_TO_EXECUTE",
 					TRACE_RDMA_REQUEST_STATE_READY_TO_EXECUTE,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_EXECUTING",
 					TRACE_RDMA_REQUEST_STATE_EXECUTING,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_EXECUTED",
 					TRACE_RDMA_REQUEST_STATE_EXECUTED,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_RDY_TO_COMPL",
 					TRACE_RDMA_REQUEST_STATE_READY_TO_COMPLETE,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_COMPLETING_C2H",
 					TRACE_RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_COMPLETING",
 					TRACE_RDMA_REQUEST_STATE_COMPLETING,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 	spdk_trace_register_description("RDMA_REQ_COMPLETED",
 					TRACE_RDMA_REQUEST_STATE_COMPLETED,
-					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "qpair");
 
 	spdk_trace_register_description("RDMA_QP_CREATE", TRACE_RDMA_QP_CREATE,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("RDMA_IBV_ASYNC_EVENT", TRACE_RDMA_IBV_ASYNC_EVENT,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "type:   ");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "type");
 	spdk_trace_register_description("RDMA_CM_ASYNC_EVENT", TRACE_RDMA_CM_ASYNC_EVENT,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "type:   ");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "type");
 	spdk_trace_register_description("RDMA_QP_STATE_CHANGE", TRACE_RDMA_QP_STATE_CHANGE,
-					OWNER_NONE, OBJECT_NONE, 0, 1, "state:  ");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_PTR, "state");
 	spdk_trace_register_description("RDMA_QP_DISCONNECT", TRACE_RDMA_QP_DISCONNECT,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("RDMA_QP_DESTROY", TRACE_RDMA_QP_DESTROY,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 }
 
 enum spdk_nvmf_rdma_wr_type {
@@ -249,6 +267,9 @@ struct spdk_nvmf_rdma_request {
 
 	enum spdk_nvmf_rdma_request_state	state;
 
+	/* Data offset in req.iov */
+	uint32_t				offset;
+
 	struct spdk_nvmf_rdma_recv		*recv;
 
 	struct {
@@ -602,8 +623,7 @@ nvmf_rdma_update_ibv_state(struct spdk_nvmf_rdma_qpair *rqpair) {
 
 	if (old_state != new_state)
 	{
-		spdk_trace_record(TRACE_RDMA_QP_STATE_CHANGE, 0, 0,
-				  (uintptr_t)rqpair->cm_id, new_state);
+		spdk_trace_record(TRACE_RDMA_QP_STATE_CHANGE, 0, 0, (uintptr_t)rqpair, new_state);
 	}
 	return new_state;
 }
@@ -864,7 +884,7 @@ nvmf_rdma_qpair_destroy(struct spdk_nvmf_rdma_qpair *rqpair)
 	struct ibv_recv_wr		*bad_recv_wr = NULL;
 	int				rc;
 
-	spdk_trace_record(TRACE_RDMA_QP_DESTROY, 0, 0, (uintptr_t)rqpair->cm_id, 0);
+	spdk_trace_record(TRACE_RDMA_QP_DESTROY, 0, 0, (uintptr_t)rqpair);
 
 	if (rqpair->qd != 0) {
 		struct spdk_nvmf_qpair *qpair = &rqpair->qpair;
@@ -1023,7 +1043,7 @@ nvmf_rdma_qpair_initialize(struct spdk_nvmf_qpair *qpair)
 					  qp_init_attr.cap.max_send_wr);
 	rqpair->max_send_sge = spdk_min(NVMF_DEFAULT_TX_SGE, qp_init_attr.cap.max_send_sge);
 	rqpair->max_recv_sge = spdk_min(NVMF_DEFAULT_RX_SGE, qp_init_attr.cap.max_recv_sge);
-	spdk_trace_record(TRACE_RDMA_QP_CREATE, 0, 0, (uintptr_t)rqpair->cm_id, 0);
+	spdk_trace_record(TRACE_RDMA_QP_CREATE, 0, 0, (uintptr_t)rqpair);
 	SPDK_DEBUGLOG(rdma, "New RDMA Connection: %p\n", qpair);
 
 	if (rqpair->poller->srq == NULL) {
@@ -1414,110 +1434,104 @@ nvmf_rdma_update_remote_addr(struct spdk_nvmf_rdma_request *rdma_req, uint32_t n
 	}
 }
 
-static bool
-nvmf_rdma_fill_wr_sge(struct spdk_nvmf_rdma_device *device,
-		      struct iovec *iov, struct ibv_send_wr **_wr,
-		      uint32_t *_remaining_data_block, uint32_t *_offset,
-		      uint32_t *_num_extra_wrs,
-		      const struct spdk_dif_ctx *dif_ctx)
-{
-	struct ibv_send_wr *wr = *_wr;
-	struct ibv_sge	*sg_ele = &wr->sg_list[wr->num_sge];
-	struct spdk_rdma_memory_translation mem_translation;
-	int		rc;
-	uint32_t	lkey = 0;
-	uint32_t	remaining, data_block_size, md_size, sge_len;
-
-	rc = spdk_rdma_get_translation(device->map, iov->iov_base, iov->iov_len, &mem_translation);
-	if (spdk_unlikely(rc)) {
-		return false;
-	}
-
-	lkey = spdk_rdma_memory_translation_get_lkey(&mem_translation);
-
-	if (spdk_likely(!dif_ctx)) {
-		sg_ele->lkey = lkey;
-		sg_ele->addr = (uintptr_t)(iov->iov_base);
-		sg_ele->length = iov->iov_len;
-		wr->num_sge++;
-	} else {
-		remaining = iov->iov_len - *_offset;
-		data_block_size = dif_ctx->block_size - dif_ctx->md_size;
-		md_size = dif_ctx->md_size;
-
-		while (remaining) {
-			if (wr->num_sge >= SPDK_NVMF_MAX_SGL_ENTRIES) {
-				if (*_num_extra_wrs > 0 && wr->next) {
-					*_wr = wr->next;
-					wr = *_wr;
-					wr->num_sge = 0;
-					sg_ele = &wr->sg_list[wr->num_sge];
-					(*_num_extra_wrs)--;
-				} else {
-					break;
-				}
-			}
-			sg_ele->lkey = lkey;
-			sg_ele->addr = (uintptr_t)((char *)iov->iov_base + *_offset);
-			sge_len = spdk_min(remaining, *_remaining_data_block);
-			sg_ele->length = sge_len;
-			remaining -= sge_len;
-			*_remaining_data_block -= sge_len;
-			*_offset += sge_len;
-
-			sg_ele++;
-			wr->num_sge++;
-
-			if (*_remaining_data_block == 0) {
-				/* skip metadata */
-				*_offset += md_size;
-				/* Metadata that do not fit this IO buffer will be included in the next IO buffer */
-				remaining -= spdk_min(remaining, md_size);
-				*_remaining_data_block = data_block_size;
-			}
-
-			if (remaining == 0) {
-				/* By subtracting the size of the last IOV from the offset, we ensure that we skip
-				   the remaining metadata bits at the beginning of the next buffer */
-				*_offset -= iov->iov_len;
-			}
-		}
-	}
-
-	return true;
-}
-
 static int
 nvmf_rdma_fill_wr_sgl(struct spdk_nvmf_rdma_poll_group *rgroup,
 		      struct spdk_nvmf_rdma_device *device,
 		      struct spdk_nvmf_rdma_request *rdma_req,
 		      struct ibv_send_wr *wr,
-		      uint32_t length,
+		      uint32_t total_length,
 		      uint32_t num_extra_wrs)
 {
-	struct spdk_nvmf_request *req = &rdma_req->req;
+	struct spdk_rdma_memory_translation mem_translation;
 	struct spdk_dif_ctx *dif_ctx = NULL;
+	struct ibv_sge	*sg_ele;
+	struct iovec *iov;
 	uint32_t remaining_data_block = 0;
-	uint32_t offset = 0;
+	uint32_t lkey, remaining;
+	int rc;
 
-	if (spdk_unlikely(rdma_req->req.dif.dif_insert_or_strip)) {
+	if (spdk_unlikely(rdma_req->req.dif_enabled)) {
 		dif_ctx = &rdma_req->req.dif.dif_ctx;
 		remaining_data_block = dif_ctx->block_size - dif_ctx->md_size;
 	}
 
 	wr->num_sge = 0;
 
-	while (length && (num_extra_wrs || wr->num_sge < SPDK_NVMF_MAX_SGL_ENTRIES)) {
-		if (spdk_unlikely(!nvmf_rdma_fill_wr_sge(device, &req->iov[rdma_req->iovpos], &wr,
-				  &remaining_data_block, &offset, &num_extra_wrs, dif_ctx))) {
-			return -EINVAL;
+	while (total_length && (num_extra_wrs || wr->num_sge < SPDK_NVMF_MAX_SGL_ENTRIES)) {
+		iov = &rdma_req->req.iov[rdma_req->iovpos];
+		rc = spdk_rdma_get_translation(device->map, iov->iov_base, iov->iov_len, &mem_translation);
+		if (spdk_unlikely(rc)) {
+			return false;
 		}
 
-		length -= req->iov[rdma_req->iovpos].iov_len;
-		rdma_req->iovpos++;
+		lkey = spdk_rdma_memory_translation_get_lkey(&mem_translation);
+		sg_ele = &wr->sg_list[wr->num_sge];
+		remaining = spdk_min((uint32_t)iov->iov_len - rdma_req->offset, total_length);
+
+		if (spdk_likely(!dif_ctx)) {
+			sg_ele->lkey = lkey;
+			sg_ele->addr = (uintptr_t)iov->iov_base + rdma_req->offset;
+			sg_ele->length = remaining;
+			SPDK_DEBUGLOG(rdma, "sge[%d] %p addr 0x%"PRIx64", len %u\n", wr->num_sge, sg_ele, sg_ele->addr,
+				      sg_ele->length);
+			rdma_req->offset += sg_ele->length;
+			total_length -= sg_ele->length;
+			wr->num_sge++;
+
+			if (rdma_req->offset == iov->iov_len) {
+				rdma_req->offset = 0;
+				rdma_req->iovpos++;
+			}
+		} else {
+			uint32_t data_block_size = dif_ctx->block_size - dif_ctx->md_size;
+			uint32_t md_size = dif_ctx->md_size;
+			uint32_t sge_len;
+
+			while (remaining) {
+				if (wr->num_sge >= SPDK_NVMF_MAX_SGL_ENTRIES) {
+					if (num_extra_wrs > 0 && wr->next) {
+						wr = wr->next;
+						wr->num_sge = 0;
+						sg_ele = &wr->sg_list[wr->num_sge];
+						num_extra_wrs--;
+					} else {
+						break;
+					}
+				}
+				sg_ele->lkey = lkey;
+				sg_ele->addr = (uintptr_t)((char *)iov->iov_base + rdma_req->offset);
+				sge_len = spdk_min(remaining, remaining_data_block);
+				sg_ele->length = sge_len;
+				SPDK_DEBUGLOG(rdma, "sge[%d] %p addr 0x%"PRIx64", len %u\n", wr->num_sge, sg_ele, sg_ele->addr,
+					      sg_ele->length);
+				remaining -= sge_len;
+				remaining_data_block -= sge_len;
+				rdma_req->offset += sge_len;
+				total_length -= sge_len;
+
+				sg_ele++;
+				wr->num_sge++;
+
+				if (remaining_data_block == 0) {
+					/* skip metadata */
+					rdma_req->offset += md_size;
+					total_length -= md_size;
+					/* Metadata that do not fit this IO buffer will be included in the next IO buffer */
+					remaining -= spdk_min(remaining, md_size);
+					remaining_data_block = data_block_size;
+				}
+
+				if (remaining == 0) {
+					/* By subtracting the size of the last IOV from the offset, we ensure that we skip
+					   the remaining metadata bits at the beginning of the next buffer */
+					rdma_req->offset -= spdk_min(iov->iov_len, rdma_req->offset);
+					rdma_req->iovpos++;
+				}
+			}
+		}
 	}
 
-	if (length) {
+	if (total_length) {
 		SPDK_ERRLOG("Not enough SG entries to hold data buffer\n");
 		return -EINVAL;
 	}
@@ -1575,7 +1589,7 @@ nvmf_rdma_request_fill_iovs(struct spdk_nvmf_rdma_transport *rtransport,
 
 	rdma_req->iovpos = 0;
 
-	if (spdk_unlikely(req->dif.dif_insert_or_strip)) {
+	if (spdk_unlikely(req->dif_enabled)) {
 		num_wrs = nvmf_rdma_calc_num_wrs(length, rtransport->transport.opts.io_unit_size,
 						 req->dif.dif_ctx.block_size);
 		if (num_wrs > 1) {
@@ -1618,7 +1632,7 @@ nvmf_rdma_request_fill_iovs_multi_sgl(struct spdk_nvmf_rdma_transport *rtranspor
 	struct spdk_nvmf_request		*req = &rdma_req->req;
 	struct spdk_nvme_sgl_descriptor		*inline_segment, *desc;
 	uint32_t				num_sgl_descriptors;
-	uint32_t				lengths[SPDK_NVMF_MAX_SGL_ENTRIES];
+	uint32_t				lengths[SPDK_NVMF_MAX_SGL_ENTRIES], total_length = 0;
 	uint32_t				i;
 	int					rc;
 
@@ -1632,24 +1646,31 @@ nvmf_rdma_request_fill_iovs_multi_sgl(struct spdk_nvmf_rdma_transport *rtranspor
 	num_sgl_descriptors = inline_segment->unkeyed.length / sizeof(struct spdk_nvme_sgl_descriptor);
 	assert(num_sgl_descriptors <= SPDK_NVMF_MAX_SGL_ENTRIES);
 
-	if (nvmf_request_alloc_wrs(rtransport, rdma_req, num_sgl_descriptors - 1) != 0) {
-		return -ENOMEM;
-	}
-
 	desc = (struct spdk_nvme_sgl_descriptor *)rdma_req->recv->buf + inline_segment->address;
 	for (i = 0; i < num_sgl_descriptors; i++) {
-		if (spdk_likely(!req->dif.dif_insert_or_strip)) {
+		if (spdk_likely(!req->dif_enabled)) {
 			lengths[i] = desc->keyed.length;
 		} else {
 			req->dif.orig_length += desc->keyed.length;
 			lengths[i] = spdk_dif_get_length_with_md(desc->keyed.length, &req->dif.dif_ctx);
 			req->dif.elba_length += lengths[i];
 		}
+		total_length += lengths[i];
 		desc++;
 	}
 
-	rc = spdk_nvmf_request_get_buffers_multi(req, &rgroup->group, &rtransport->transport,
-			lengths, num_sgl_descriptors);
+	if (total_length > rtransport->transport.opts.max_io_size) {
+		SPDK_ERRLOG("Multi SGL length 0x%x exceeds max io size 0x%x\n",
+			    total_length, rtransport->transport.opts.max_io_size);
+		req->rsp->nvme_cpl.status.sc = SPDK_NVME_SC_DATA_SGL_LENGTH_INVALID;
+		return -EINVAL;
+	}
+
+	if (nvmf_request_alloc_wrs(rtransport, rdma_req, num_sgl_descriptors - 1) != 0) {
+		return -ENOMEM;
+	}
+
+	rc = spdk_nvmf_request_get_buffers(req, &rgroup->group, &rtransport->transport, total_length);
 	if (rc != 0) {
 		nvmf_rdma_request_free_data(rdma_req, rtransport);
 		return rc;
@@ -1670,8 +1691,6 @@ nvmf_rdma_request_fill_iovs_multi_sgl(struct spdk_nvmf_rdma_transport *rtranspor
 			goto err_exit;
 		}
 
-		current_wr->num_sge = 0;
-
 		rc = nvmf_rdma_fill_wr_sgl(rgroup, device, rdma_req, current_wr, lengths[i], 0);
 		if (rc != 0) {
 			rc = -ENOMEM;
@@ -1743,7 +1762,7 @@ nvmf_rdma_request_parse_sgl(struct spdk_nvmf_rdma_transport *rtransport,
 		/* fill request length and populate iovs */
 		req->length = length;
 
-		if (spdk_unlikely(req->dif.dif_insert_or_strip)) {
+		if (spdk_unlikely(req->dif_enabled)) {
 			req->dif.orig_length = length;
 			length = spdk_dif_get_length_with_md(length, &req->dif.dif_ctx);
 			req->dif.elba_length = length;
@@ -1848,6 +1867,7 @@ _nvmf_rdma_request_free(struct spdk_nvmf_rdma_request *rdma_req,
 	rdma_req->req.data = NULL;
 	rdma_req->rsp.wr.next = NULL;
 	rdma_req->data.wr.next = NULL;
+	rdma_req->offset = 0;
 	memset(&rdma_req->req.dif, 0, sizeof(rdma_req->req.dif));
 	rqpair->qd--;
 
@@ -1902,7 +1922,7 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_NEW:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_NEW, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 			rdma_recv = rdma_req->recv;
 
 			/* The first element of the SGL is the NVMe command */
@@ -1915,7 +1935,7 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			}
 
 			if (spdk_unlikely(spdk_nvmf_request_get_dif_ctx(&rdma_req->req, &rdma_req->req.dif.dif_ctx))) {
-				rdma_req->req.dif.dif_insert_or_strip = true;
+				rdma_req->req.dif_enabled = true;
 			}
 
 #ifdef SPDK_CONFIG_RDMA_SEND_WITH_INVAL
@@ -1945,7 +1965,7 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_NEED_BUFFER:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_NEED_BUFFER, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 
 			assert(rdma_req->req.xfer != SPDK_NVME_DATA_NONE);
 
@@ -1984,7 +2004,7 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 
 			if (rdma_req != STAILQ_FIRST(&rqpair->pending_rdma_read_queue)) {
 				/* This request needs to wait in line to perform RDMA */
@@ -2010,15 +2030,15 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 			/* Some external code must kick a request into RDMA_REQUEST_STATE_READY_TO_EXECUTE
 			 * to escape this state. */
 			break;
 		case RDMA_REQUEST_STATE_READY_TO_EXECUTE:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_READY_TO_EXECUTE, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 
-			if (spdk_unlikely(rdma_req->req.dif.dif_insert_or_strip)) {
+			if (spdk_unlikely(rdma_req->req.dif_enabled)) {
 				if (rdma_req->req.xfer == SPDK_NVME_DATA_HOST_TO_CONTROLLER) {
 					/* generate DIF for write operation */
 					num_blocks = SPDK_CEIL_DIV(rdma_req->req.dif.elba_length, rdma_req->req.dif.dif_ctx.block_size);
@@ -2044,13 +2064,13 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_EXECUTING:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_EXECUTING, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 			/* Some external code must kick a request into RDMA_REQUEST_STATE_EXECUTED
 			 * to escape this state. */
 			break;
 		case RDMA_REQUEST_STATE_EXECUTED:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_EXECUTED, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 			if (rsp->status.sc == SPDK_NVME_SC_SUCCESS &&
 			    rdma_req->req.xfer == SPDK_NVME_DATA_CONTROLLER_TO_HOST) {
 				STAILQ_INSERT_TAIL(&rqpair->pending_rdma_write_queue, rdma_req, state_link);
@@ -2058,7 +2078,7 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			} else {
 				rdma_req->state = RDMA_REQUEST_STATE_READY_TO_COMPLETE;
 			}
-			if (spdk_unlikely(rdma_req->req.dif.dif_insert_or_strip)) {
+			if (spdk_unlikely(rdma_req->req.dif_enabled)) {
 				/* restore the original length */
 				rdma_req->req.length = rdma_req->req.dif.orig_length;
 
@@ -2084,7 +2104,7 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 
 			if (rdma_req != STAILQ_FIRST(&rqpair->pending_rdma_write_queue)) {
 				/* This request needs to wait in line to perform RDMA */
@@ -2108,7 +2128,7 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_READY_TO_COMPLETE:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_READY_TO_COMPLETE, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 			rc = request_transfer_out(&rdma_req->req, &data_posted);
 			assert(rc == 0); /* No good way to handle this currently */
 			if (rc) {
@@ -2120,19 +2140,19 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		case RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 			/* Some external code must kick a request into RDMA_REQUEST_STATE_COMPLETED
 			 * to escape this state. */
 			break;
 		case RDMA_REQUEST_STATE_COMPLETING:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_COMPLETING, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 			/* Some external code must kick a request into RDMA_REQUEST_STATE_COMPLETED
 			 * to escape this state. */
 			break;
 		case RDMA_REQUEST_STATE_COMPLETED:
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_COMPLETED, 0, 0,
-					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair);
 
 			rqpair->poller->stat.request_latency += spdk_get_ticks() - rdma_req->receive_tsc;
 			_nvmf_rdma_request_free(rdma_req, rtransport);
@@ -2791,7 +2811,7 @@ nvmf_rdma_disconnect(struct rdma_cm_event *evt)
 
 	rqpair = SPDK_CONTAINEROF(qpair, struct spdk_nvmf_rdma_qpair, qpair);
 
-	spdk_trace_record(TRACE_RDMA_QP_DISCONNECT, 0, 0, (uintptr_t)rqpair->cm_id, 0);
+	spdk_trace_record(TRACE_RDMA_QP_DISCONNECT, 0, 0, (uintptr_t)rqpair);
 
 	spdk_nvmf_qpair_disconnect(&rqpair->qpair, NULL, NULL);
 
@@ -3068,7 +3088,7 @@ nvmf_process_ib_event(struct spdk_nvmf_rdma_device *device)
 		rqpair = event.element.qp->qp_context;
 		SPDK_ERRLOG("Fatal event received for rqpair %p\n", rqpair);
 		spdk_trace_record(TRACE_RDMA_IBV_ASYNC_EVENT, 0, 0,
-				  (uintptr_t)rqpair->cm_id, event.event_type);
+				  (uintptr_t)rqpair, event.event_type);
 		nvmf_rdma_update_ibv_state(rqpair);
 		spdk_nvmf_qpair_disconnect(&rqpair->qpair, NULL, NULL);
 		break;
@@ -3088,7 +3108,7 @@ nvmf_process_ib_event(struct spdk_nvmf_rdma_device *device)
 		rqpair = event.element.qp->qp_context;
 		SPDK_DEBUGLOG(rdma, "Last sq drained event received for rqpair %p\n", rqpair);
 		spdk_trace_record(TRACE_RDMA_IBV_ASYNC_EVENT, 0, 0,
-				  (uintptr_t)rqpair->cm_id, event.event_type);
+				  (uintptr_t)rqpair, event.event_type);
 		if (nvmf_rdma_update_ibv_state(rqpair) == IBV_QPS_ERR) {
 			spdk_nvmf_qpair_disconnect(&rqpair->qpair, NULL, NULL);
 		}
@@ -3102,7 +3122,7 @@ nvmf_process_ib_event(struct spdk_nvmf_rdma_device *device)
 			       ibv_event_type_str(event.event_type));
 		rqpair = event.element.qp->qp_context;
 		spdk_trace_record(TRACE_RDMA_IBV_ASYNC_EVENT, 0, 0,
-				  (uintptr_t)rqpair->cm_id, event.event_type);
+				  (uintptr_t)rqpair, event.event_type);
 		nvmf_rdma_update_ibv_state(rqpair);
 		break;
 	case IBV_EVENT_CQ_ERR:
@@ -3192,6 +3212,14 @@ nvmf_rdma_cdata_init(struct spdk_nvmf_transport *transport, struct spdk_nvmf_sub
 	if (transport->opts.dif_insert_or_strip) {
 		cdata->nvmf_specific.ioccsz = sizeof(struct spdk_nvme_cmd) / 16;
 	}
+
+	if (cdata->nvmf_specific.ioccsz > ((sizeof(struct spdk_nvme_cmd) + 0x1000) / 16)) {
+		SPDK_WARNLOG("RDMA is configured to support up to 16 SGL entries while in capsule"
+			     " data is greater than 4KiB.\n");
+		SPDK_WARNLOG("When used in conjunction with the NVMe-oF initiator from the Linux "
+			     "kernel between versions 5.4 and 5.12 data corruption may occur for "
+			     "writes that are not a multiple of 4KiB in size.\n");
+	}
 }
 
 static void
diff --git a/lib/nvmf/spdk_nvmf.map b/lib/nvmf/spdk_nvmf.map
index 0247249bc..fc6ec9cfd 100644
--- a/lib/nvmf/spdk_nvmf.map
+++ b/lib/nvmf/spdk_nvmf.map
@@ -53,6 +53,8 @@
 	spdk_nvmf_subsystem_get_first_ns;
 	spdk_nvmf_subsystem_get_next_ns;
 	spdk_nvmf_subsystem_get_ns;
+	spdk_nvmf_subsystem_get_min_cntlid;
+	spdk_nvmf_subsystem_get_max_cntlid;
 	spdk_nvmf_subsystem_get_max_namespaces;
 	spdk_nvmf_ns_get_id;
 	spdk_nvmf_ns_get_bdev;
@@ -106,7 +108,6 @@
 	spdk_nvmf_ctrlr_get_regs;
 	spdk_nvmf_request_free_buffers;
 	spdk_nvmf_request_get_buffers;
-	spdk_nvmf_request_get_buffers_multi;
 	spdk_nvmf_request_get_dif_ctx;
 	spdk_nvmf_request_exec_fabrics;
 	spdk_nvmf_request_exec;
diff --git a/lib/nvmf/subsystem.c b/lib/nvmf/subsystem.c
index 457fa64d6..1da2a278b 100644
--- a/lib/nvmf/subsystem.c
+++ b/lib/nvmf/subsystem.c
@@ -44,9 +44,11 @@
 #include "spdk/json.h"
 #include "spdk/file.h"
 
+#define __SPDK_BDEV_MODULE_ONLY
 #include "spdk/bdev_module.h"
 #include "spdk/log.h"
 #include "spdk_internal/utf.h"
+#include "spdk_internal/usdt.h"
 
 #define MODEL_NUMBER_DEFAULT "SPDK bdev Controller"
 #define NVMF_SUBSYSTEM_DEFAULT_NAMESPACES 32
@@ -284,6 +286,8 @@ spdk_nvmf_subsystem_create(struct spdk_nvmf_tgt *tgt,
 	subsystem->subtype = type;
 	subsystem->max_nsid = num_ns;
 	subsystem->next_cntlid = 0;
+	subsystem->min_cntlid = NVMF_MIN_CNTLID;
+	subsystem->max_cntlid = NVMF_MAX_CNTLID;
 	snprintf(subsystem->subnqn, sizeof(subsystem->subnqn), "%s", nqn);
 	pthread_mutex_init(&subsystem->mutex, NULL);
 	TAILQ_INIT(&subsystem->listeners);
@@ -505,6 +509,9 @@ subsystem_state_change_done(struct spdk_io_channel_iter *i, int status)
 	struct subsystem_state_change_ctx *ctx = spdk_io_channel_iter_get_ctx(i);
 	enum spdk_nvmf_subsystem_state intermediate_state;
 
+	SPDK_DTRACE_PROBE4(nvmf_subsystem_change_state_done, ctx->subsystem->subnqn,
+			   ctx->requested_state, ctx->original_state, status);
+
 	if (status == 0) {
 		status = nvmf_subsystem_set_state(ctx->subsystem, ctx->requested_state);
 		if (status) {
@@ -540,6 +547,12 @@ static void
 subsystem_state_change_continue(void *ctx, int status)
 {
 	struct spdk_io_channel_iter *i = ctx;
+	struct subsystem_state_change_ctx *_ctx __attribute__((unused));
+
+	_ctx = spdk_io_channel_iter_get_ctx(i);
+	SPDK_DTRACE_PROBE3(nvmf_pg_change_state_done, _ctx->subsystem->subnqn,
+			   _ctx->requested_state, spdk_thread_get_id(spdk_get_thread()));
+
 	spdk_for_each_channel_continue(i, status);
 }
 
@@ -554,6 +567,8 @@ subsystem_state_change_on_pg(struct spdk_io_channel_iter *i)
 	ch = spdk_io_channel_iter_get_channel(i);
 	group = spdk_io_channel_get_ctx(ch);
 
+	SPDK_DTRACE_PROBE3(nvmf_pg_change_state, ctx->subsystem->subnqn,
+			   ctx->requested_state, spdk_thread_get_id(spdk_get_thread()));
 	switch (ctx->requested_state) {
 	case SPDK_NVMF_SUBSYSTEM_INACTIVE:
 		nvmf_poll_group_remove_subsystem(group, ctx->subsystem, subsystem_state_change_continue, i);
@@ -590,6 +605,8 @@ nvmf_subsystem_state_change(struct spdk_nvmf_subsystem *subsystem,
 		return -EBUSY;
 	}
 
+	SPDK_DTRACE_PROBE3(nvmf_subsystem_change_state, subsystem->subnqn,
+			   requested_state, subsystem->state);
 	/* If we are already in the requested state, just call the callback immediately. */
 	if (subsystem->state == requested_state) {
 		subsystem->changing_state = false;
@@ -831,8 +848,13 @@ spdk_nvmf_subsystem_disconnect_host(struct spdk_nvmf_subsystem *subsystem,
 		return -ENOMEM;
 	}
 
-	ctx->subsystem = subsystem;
 	ctx->hostnqn = strdup(hostnqn);
+	if (ctx->hostnqn == NULL) {
+		free(ctx);
+		return -ENOMEM;
+	}
+
+	ctx->subsystem = subsystem;
 	ctx->cb_fn = cb_fn;
 	ctx->cb_arg = cb_arg;
 
@@ -1320,7 +1342,7 @@ nvmf_ns_event(enum spdk_bdev_event_type type,
 {
 	SPDK_DEBUGLOG(nvmf, "Bdev event: type %d, name %s, subsystem_id %d, ns_id %d\n",
 		      type,
-		      bdev->name,
+		      spdk_bdev_get_name(bdev),
 		      ((struct spdk_nvmf_ns *)event_ctx)->subsystem->id,
 		      ((struct spdk_nvmf_ns *)event_ctx)->nsid);
 
@@ -1450,14 +1472,14 @@ spdk_nvmf_subsystem_add_ns_ext(struct spdk_nvmf_subsystem *subsystem, const char
 			rc = nvmf_ns_reservation_restore(ns, &info);
 			if (rc) {
 				SPDK_ERRLOG("Subsystem restore reservation failed\n");
-				subsystem->ns[opts.nsid - 1] = NULL;
-				spdk_bdev_module_release_bdev(ns->bdev);
-				spdk_bdev_close(ns->desc);
-				free(ns);
-				return 0;
+				goto err_ns_reservation_restore;
 			}
 		}
 		ns->ptpl_file = strdup(ptpl_file);
+		if (!ns->ptpl_file) {
+			SPDK_ERRLOG("Namespace ns->ptpl_file allocation failed\n");
+			goto err_strdup;
+		}
 	}
 
 	for (transport = spdk_nvmf_transport_get_first(subsystem->tgt); transport;
@@ -1466,13 +1488,7 @@ spdk_nvmf_subsystem_add_ns_ext(struct spdk_nvmf_subsystem *subsystem, const char
 			rc = transport->ops->subsystem_add_ns(transport, subsystem, ns);
 			if (rc) {
 				SPDK_ERRLOG("Namespace attachment is not allowed by %s transport\n", transport->ops->name);
-				free(ns->ptpl_file);
-				nvmf_ns_reservation_clear_all_registrants(ns);
-				subsystem->ns[opts.nsid - 1] = NULL;
-				spdk_bdev_module_release_bdev(ns->bdev);
-				spdk_bdev_close(ns->desc);
-				free(ns);
-				return 0;
+				goto err_subsystem_add_ns;
 			}
 		}
 	}
@@ -1485,6 +1501,18 @@ spdk_nvmf_subsystem_add_ns_ext(struct spdk_nvmf_subsystem *subsystem, const char
 	nvmf_subsystem_ns_changed(subsystem, opts.nsid);
 
 	return opts.nsid;
+
+err_subsystem_add_ns:
+	free(ns->ptpl_file);
+err_strdup:
+	nvmf_ns_reservation_clear_all_registrants(ns);
+err_ns_reservation_restore:
+	subsystem->ns[opts.nsid - 1] = NULL;
+	spdk_bdev_module_release_bdev(ns->bdev);
+	spdk_bdev_close(ns->desc);
+	free(ns);
+	return 0;
+
 }
 
 static uint32_t
@@ -1631,20 +1659,44 @@ spdk_nvmf_subsystem_get_max_nsid(struct spdk_nvmf_subsystem *subsystem)
 	return subsystem->max_nsid;
 }
 
+int
+nvmf_subsystem_set_cntlid_range(struct spdk_nvmf_subsystem *subsystem,
+				uint16_t min_cntlid, uint16_t max_cntlid)
+{
+	if (subsystem->state != SPDK_NVMF_SUBSYSTEM_INACTIVE) {
+		return -EAGAIN;
+	}
+
+	if (min_cntlid > max_cntlid) {
+		return -EINVAL;
+	}
+	/* The spec reserves cntlid values in the range FFF0h to FFFFh. */
+	if (min_cntlid < NVMF_MIN_CNTLID || min_cntlid > NVMF_MAX_CNTLID ||
+	    max_cntlid < NVMF_MIN_CNTLID || max_cntlid > NVMF_MAX_CNTLID) {
+		return -EINVAL;
+	}
+	subsystem->min_cntlid = min_cntlid;
+	subsystem->max_cntlid = max_cntlid;
+	if (subsystem->next_cntlid < min_cntlid || subsystem->next_cntlid > max_cntlid - 1) {
+		subsystem->next_cntlid = min_cntlid - 1;
+	}
+
+	return 0;
+}
+
 static uint16_t
 nvmf_subsystem_gen_cntlid(struct spdk_nvmf_subsystem *subsystem)
 {
 	int count;
 
 	/*
-	 * In the worst case, we might have to try all CNTLID values between 1 and 0xFFF0 - 1
+	 * In the worst case, we might have to try all CNTLID values between min_cntlid and max_cntlid
 	 * before we find one that is unused (or find that all values are in use).
 	 */
-	for (count = 0; count < 0xFFF0 - 1; count++) {
+	for (count = 0; count < subsystem->max_cntlid - subsystem->min_cntlid + 1; count++) {
 		subsystem->next_cntlid++;
-		if (subsystem->next_cntlid >= 0xFFF0) {
-			/* The spec reserves cntlid values in the range FFF0h to FFFFh. */
-			subsystem->next_cntlid = 1;
+		if (subsystem->next_cntlid > subsystem->max_cntlid) {
+			subsystem->next_cntlid = subsystem->min_cntlid;
 		}
 
 		/* Check if a controller with this cntlid currently exists. */
@@ -1701,6 +1753,18 @@ spdk_nvmf_subsystem_get_max_namespaces(const struct spdk_nvmf_subsystem *subsyst
 	return subsystem->max_nsid;
 }
 
+uint16_t
+spdk_nvmf_subsystem_get_min_cntlid(const struct spdk_nvmf_subsystem *subsystem)
+{
+	return subsystem->min_cntlid;
+}
+
+uint16_t
+spdk_nvmf_subsystem_get_max_cntlid(const struct spdk_nvmf_subsystem *subsystem)
+{
+	return subsystem->max_cntlid;
+}
+
 struct _nvmf_ns_registrant {
 	uint64_t		rkey;
 	char			*host_uuid;
@@ -2638,13 +2702,11 @@ nvmf_ns_reservation_report(struct spdk_nvmf_ns *ns,
 			   struct spdk_nvmf_request *req)
 {
 	struct spdk_nvme_cmd *cmd = &req->cmd->nvme_cmd;
-	struct spdk_nvmf_subsystem *subsystem = ctrlr->subsys;
-	struct spdk_nvmf_ctrlr *ctrlr_tmp;
 	struct spdk_nvmf_registrant *reg, *tmp;
 	struct spdk_nvme_reservation_status_extended_data *status_data;
 	struct spdk_nvme_registered_ctrlr_extended_data *ctrlr_data;
 	uint8_t *payload;
-	uint32_t len, count = 0;
+	uint32_t transfer_len, payload_len = 0;
 	uint32_t regctl = 0;
 	uint8_t status = SPDK_NVME_SC_SUCCESS;
 
@@ -2662,19 +2724,11 @@ nvmf_ns_reservation_report(struct spdk_nvmf_ns *ns,
 		goto exit;
 	}
 
-	/* Get number of registerd controllers, one Host may have more than
-	 * one controller based on different ports.
-	 */
-	TAILQ_FOREACH(ctrlr_tmp, &subsystem->ctrlrs, link) {
-		reg = nvmf_ns_reservation_get_registrant(ns, &ctrlr_tmp->hostid);
-		if (reg) {
-			regctl++;
-		}
-	}
+	/* Number of Dwords of the Reservation Status data structure to transfer */
+	transfer_len = (cmd->cdw10 + 1) * sizeof(uint32_t);
+	payload = req->data;
 
-	len = sizeof(*status_data) + sizeof(*ctrlr_data) * regctl;
-	payload = calloc(1, len);
-	if (!payload) {
+	if (transfer_len < sizeof(struct spdk_nvme_reservation_status_extended_data)) {
 		status = SPDK_NVME_SC_INTERNAL_DEVICE_ERROR;
 		goto exit;
 	}
@@ -2682,23 +2736,25 @@ nvmf_ns_reservation_report(struct spdk_nvmf_ns *ns,
 	status_data = (struct spdk_nvme_reservation_status_extended_data *)payload;
 	status_data->data.gen = ns->gen;
 	status_data->data.rtype = ns->rtype;
-	status_data->data.regctl = regctl;
 	status_data->data.ptpls = ns->ptpl_activated;
+	payload_len += sizeof(struct spdk_nvme_reservation_status_extended_data);
 
 	TAILQ_FOREACH_SAFE(reg, &ns->registrants, link, tmp) {
-		assert(count <= regctl);
+		payload_len += sizeof(struct spdk_nvme_registered_ctrlr_extended_data);
+		if (payload_len > transfer_len) {
+			break;
+		}
+
 		ctrlr_data = (struct spdk_nvme_registered_ctrlr_extended_data *)
-			     (payload + sizeof(*status_data) + sizeof(*ctrlr_data) * count);
+			     (payload + sizeof(*status_data) + sizeof(*ctrlr_data) * regctl);
 		/* Set to 0xffffh for dynamic controller */
 		ctrlr_data->cntlid = 0xffff;
 		ctrlr_data->rcsts.status = (ns->holder == reg) ? true : false;
 		ctrlr_data->rkey = reg->rkey;
 		spdk_uuid_copy((struct spdk_uuid *)ctrlr_data->hostid, &reg->hostid);
-		count++;
+		regctl++;
 	}
-
-	memcpy(req->data, payload, spdk_min(len, (cmd->cdw10 + 1) * sizeof(uint32_t)));
-	free(payload);
+	status_data->data.regctl = regctl;
 
 exit:
 	req->rsp->nvme_cpl.status.sct = SPDK_NVME_SCT_GENERIC;
diff --git a/lib/nvmf/tcp.c b/lib/nvmf/tcp.c
index b16ed22a4..20dfdbb4e 100644
--- a/lib/nvmf/tcp.c
+++ b/lib/nvmf/tcp.c
@@ -128,43 +128,56 @@ SPDK_TRACE_REGISTER_FN(nvmf_tcp_trace, "nvmf_tcp", TRACE_GROUP_NVMF_TCP)
 	spdk_trace_register_object(OBJECT_NVMF_TCP_IO, 'r');
 	spdk_trace_register_description("TCP_REQ_NEW",
 					TRACE_TCP_REQUEST_STATE_NEW,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 1, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 1,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_NEED_BUFFER",
 					TRACE_TCP_REQUEST_STATE_NEED_BUFFER,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_TX_H_TO_C",
 					TRACE_TCP_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_RDY_TO_EXECUTE",
 					TRACE_TCP_REQUEST_STATE_READY_TO_EXECUTE,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_EXECUTING",
 					TRACE_TCP_REQUEST_STATE_EXECUTING,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_EXECUTED",
 					TRACE_TCP_REQUEST_STATE_EXECUTED,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_RDY_TO_COMPLETE",
 					TRACE_TCP_REQUEST_STATE_READY_TO_COMPLETE,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_TRANSFER_C2H",
 					TRACE_TCP_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_COMPLETED",
 					TRACE_TCP_REQUEST_STATE_COMPLETED,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_WRITE_START",
 					TRACE_TCP_FLUSH_WRITEBUF_START,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_WRITE_DONE",
 					TRACE_TCP_FLUSH_WRITEBUF_DONE,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_READ_DONE",
 					TRACE_TCP_READ_FROM_SOCKET_DONE,
-					OWNER_NONE, OBJECT_NONE, 0, 0, "");
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("TCP_REQ_AWAIT_R2T_ACK",
 					TRACE_TCP_REQUEST_STATE_AWAIT_R2T_ACK,
-					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0, 1, "");
+					OWNER_NONE, OBJECT_NVMF_TCP_IO, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 }
 
 struct spdk_nvmf_tcp_req  {
@@ -210,7 +223,7 @@ struct spdk_nvmf_tcp_qpair {
 	enum nvme_tcp_qpair_state		state;
 
 	/* PDU being actively received */
-	struct nvme_tcp_pdu			pdu_in_progress;
+	struct nvme_tcp_pdu			*pdu_in_progress;
 
 	/* Queues to track the requests in all states */
 	TAILQ_HEAD(, spdk_nvmf_tcp_req)		tcp_req_working_queue;
@@ -367,7 +380,7 @@ nvmf_tcp_req_get(struct spdk_nvmf_tcp_qpair *tqpair)
 	memset(&tcp_req->rsp, 0, sizeof(tcp_req->rsp));
 	tcp_req->h2c_offset = 0;
 	tcp_req->has_incapsule_data = false;
-	tcp_req->req.dif.dif_insert_or_strip = false;
+	tcp_req->req.dif_enabled = false;
 
 	TAILQ_REMOVE(&tqpair->tcp_req_free_queue, tcp_req, state_link);
 	TAILQ_INSERT_TAIL(&tqpair->tcp_req_working_queue, tcp_req, state_link);
@@ -872,22 +885,6 @@ pdu_data_crc32_compute(struct nvme_tcp_pdu *pdu)
 	_tcp_write_pdu(pdu);
 }
 
-static void
-header_crc32_accel_done(void *cb_arg, int status)
-{
-	struct nvme_tcp_pdu *pdu = cb_arg;
-
-	pdu->header_digest_crc32 ^= SPDK_CRC32C_XOR;
-	MAKE_DIGEST_WORD((uint8_t *)pdu->hdr.raw + pdu->hdr.common.hlen, pdu->header_digest_crc32);
-	if (spdk_unlikely(status)) {
-		SPDK_ERRLOG("Failed to compute header digest on pdu=%p\n", pdu);
-		_pdu_write_done(pdu, status);
-		return;
-	}
-
-	pdu_data_crc32_compute(pdu);
-}
-
 static void
 nvmf_tcp_qpair_write_pdu(struct spdk_nvmf_tcp_qpair *tqpair,
 			 struct nvme_tcp_pdu *pdu,
@@ -895,24 +892,24 @@ nvmf_tcp_qpair_write_pdu(struct spdk_nvmf_tcp_qpair *tqpair,
 			 void *cb_arg)
 {
 	int hlen;
+	uint32_t crc32c;
 
-	assert(&tqpair->pdu_in_progress != pdu);
+	assert(tqpair->pdu_in_progress != pdu);
 
 	hlen = pdu->hdr.common.hlen;
 	pdu->cb_fn = cb_fn;
 	pdu->cb_arg = cb_arg;
-	pdu->qpair = tqpair;
 
 	pdu->iov[0].iov_base = &pdu->hdr.raw;
 	pdu->iov[0].iov_len = hlen;
 
 	/* Header Digest */
-	if (g_nvme_tcp_hdgst[pdu->hdr.common.pdu_type] && tqpair->host_hdgst_enable && tqpair->group) {
-		spdk_accel_submit_crc32cv(tqpair->group->accel_channel, &pdu->header_digest_crc32,
-					  pdu->iov, 1, 0, header_crc32_accel_done, pdu);
-		return;
+	if (g_nvme_tcp_hdgst[pdu->hdr.common.pdu_type] && tqpair->host_hdgst_enable) {
+		crc32c = nvme_tcp_pdu_calc_header_digest(pdu);
+		MAKE_DIGEST_WORD((uint8_t *)pdu->hdr.raw + hlen, crc32c);
 	}
 
+	/* Data Digest */
 	pdu_data_crc32_compute(pdu);
 }
 
@@ -948,8 +945,8 @@ nvmf_tcp_qpair_init_mem_resource(struct spdk_nvmf_tcp_qpair *tqpair)
 		}
 	}
 
-	/* Add addtional one member, which will be used for mgmt_pdu owned by the tqpair */
-	tqpair->pdus = spdk_dma_malloc((tqpair->resource_count + 1) * sizeof(*tqpair->pdus), 0x1000, NULL);
+	/* Add addtional 2 members, which will be used for mgmt_pdu and pdu_in_progress owned by the tqpair */
+	tqpair->pdus = spdk_dma_zmalloc((tqpair->resource_count + 2) * sizeof(*tqpair->pdus), 0x1000, NULL);
 	if (!tqpair->pdus) {
 		SPDK_ERRLOG("Unable to allocate pdu pool on tqpair =%p.\n", tqpair);
 		return -1;
@@ -981,6 +978,7 @@ nvmf_tcp_qpair_init_mem_resource(struct spdk_nvmf_tcp_qpair *tqpair)
 
 	tqpair->mgmt_pdu = &tqpair->pdus[i];
 	tqpair->mgmt_pdu->qpair = tqpair;
+	tqpair->pdu_in_progress = &tqpair->pdus[i + 1];
 
 	tqpair->recv_buf_size = (in_capsule_data_size + sizeof(struct spdk_nvme_tcp_cmd) + 2 *
 				 SPDK_NVME_TCP_DIGEST_LEN) * SPDK_NVMF_TCP_RECV_BUF_SIZE_FACTOR;
@@ -1259,7 +1257,7 @@ nvmf_tcp_qpair_set_recv_state(struct spdk_nvmf_tcp_qpair *tqpair,
 		break;
 	case NVME_TCP_PDU_RECV_STATE_ERROR:
 	case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY:
-		memset(&tqpair->pdu_in_progress, 0, sizeof(tqpair->pdu_in_progress));
+		memset(tqpair->pdu_in_progress, 0, sizeof(*(tqpair->pdu_in_progress)));
 		break;
 	default:
 		SPDK_ERRLOG("The state(%d) is invalid\n", state);
@@ -1362,10 +1360,12 @@ nvmf_tcp_capsule_cmd_payload_handle(struct spdk_nvmf_tcp_transport *ttransport,
 	struct spdk_nvme_tcp_cmd *capsule_cmd;
 	uint32_t error_offset = 0;
 	enum spdk_nvme_tcp_term_req_fes fes;
+	struct spdk_nvme_cpl *rsp;
 
 	capsule_cmd = &pdu->hdr.capsule_cmd;
 	tcp_req = pdu->req;
 	assert(tcp_req != NULL);
+
 	if (capsule_cmd->common.pdo > SPDK_NVME_TCP_PDU_PDO_MAX_OFFSET) {
 		SPDK_ERRLOG("Expected ICReq capsule_cmd pdu offset <= %d, got %c\n",
 			    SPDK_NVME_TCP_PDU_PDO_MAX_OFFSET, capsule_cmd->common.pdo);
@@ -1375,7 +1375,14 @@ nvmf_tcp_capsule_cmd_payload_handle(struct spdk_nvmf_tcp_transport *ttransport,
 	}
 
 	nvmf_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
-	nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_EXECUTE);
+
+	rsp = &tcp_req->req.rsp->nvme_cpl;
+	if (spdk_unlikely(rsp->status.sc == SPDK_NVME_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR)) {
+		nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_COMPLETE);
+	} else {
+		nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_EXECUTE);
+	}
+
 	nvmf_tcp_req_process(ttransport, tcp_req);
 
 	return;
@@ -1466,7 +1473,7 @@ nvmf_tcp_h2c_data_hdr_handle(struct spdk_nvmf_tcp_transport *ttransport,
 
 	pdu->req = tcp_req;
 
-	if (spdk_unlikely(tcp_req->req.dif.dif_insert_or_strip)) {
+	if (spdk_unlikely(tcp_req->req.dif_enabled)) {
 		pdu->dif_ctx = &tcp_req->req.dif.dif_ctx;
 	}
 
@@ -1584,6 +1591,7 @@ nvmf_tcp_h2c_data_payload_handle(struct spdk_nvmf_tcp_transport *ttransport,
 				 struct nvme_tcp_pdu *pdu)
 {
 	struct spdk_nvmf_tcp_req *tcp_req;
+	struct spdk_nvme_cpl *rsp;
 
 	tcp_req = pdu->req;
 	assert(tcp_req != NULL);
@@ -1598,7 +1606,14 @@ nvmf_tcp_h2c_data_payload_handle(struct spdk_nvmf_tcp_transport *ttransport,
 	 * acknowledged before moving on. */
 	if (tcp_req->h2c_offset == tcp_req->req.length &&
 	    tcp_req->state == TCP_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER) {
-		nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_EXECUTE);
+		/* After receving all the h2c data, we need to check whether there is
+		 * transient transport error */
+		rsp = &tcp_req->req.rsp->nvme_cpl;
+		if (spdk_unlikely(rsp->status.sc == SPDK_NVME_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR)) {
+			nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_COMPLETE);
+		} else {
+			nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_EXECUTE);
+		}
 		nvmf_tcp_req_process(ttransport, tcp_req);
 	}
 }
@@ -1650,30 +1665,10 @@ nvmf_tcp_h2c_term_req_payload_handle(struct spdk_nvmf_tcp_qpair *tqpair,
 }
 
 static void
-nvmf_tcp_pdu_payload_handle(struct spdk_nvmf_tcp_qpair *tqpair,
-			    struct spdk_nvmf_tcp_transport *ttransport)
+_nvmf_tcp_pdu_payload_handle(struct spdk_nvmf_tcp_qpair *tqpair,
+			     struct spdk_nvmf_tcp_transport *ttransport)
 {
-	int rc = 0;
-	struct nvme_tcp_pdu *pdu;
-	uint32_t crc32c, error_offset = 0;
-	enum spdk_nvme_tcp_term_req_fes fes;
-
-	assert(tqpair->recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD);
-	pdu = &tqpair->pdu_in_progress;
-
-	SPDK_DEBUGLOG(nvmf_tcp, "enter\n");
-	/* check data digest if need */
-	if (pdu->ddgst_enable) {
-		crc32c = nvme_tcp_pdu_calc_data_digest(pdu);
-		rc = MATCH_DIGEST_WORD(pdu->data_digest, crc32c);
-		if (rc == 0) {
-			SPDK_ERRLOG("Data digest error on tqpair=(%p) with pdu=%p\n", tqpair, pdu);
-			fes = SPDK_NVME_TCP_TERM_REQ_FES_HDGST_ERROR;
-			nvmf_tcp_send_c2h_term_req(tqpair, pdu, fes, error_offset);
-			return;
-
-		}
-	}
+	struct nvme_tcp_pdu *pdu = tqpair->pdu_in_progress;
 
 	switch (pdu->hdr.common.pdu_type) {
 	case SPDK_NVME_TCP_PDU_TYPE_CAPSULE_CMD:
@@ -1694,6 +1689,36 @@ nvmf_tcp_pdu_payload_handle(struct spdk_nvmf_tcp_qpair *tqpair,
 	}
 }
 
+static void
+nvmf_tcp_pdu_payload_handle(struct spdk_nvmf_tcp_qpair *tqpair,
+			    struct spdk_nvmf_tcp_transport *ttransport)
+{
+	int rc = 0;
+	struct nvme_tcp_pdu *pdu;
+	uint32_t crc32c;
+	struct spdk_nvmf_tcp_req *tcp_req;
+	struct spdk_nvme_cpl *rsp;
+
+	assert(tqpair->recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD);
+	pdu = tqpair->pdu_in_progress;
+
+	SPDK_DEBUGLOG(nvmf_tcp, "enter\n");
+	/* check data digest if need */
+	if (pdu->ddgst_enable) {
+		crc32c = nvme_tcp_pdu_calc_data_digest(pdu);
+		rc = MATCH_DIGEST_WORD(pdu->data_digest, crc32c);
+		if (rc == 0) {
+			SPDK_ERRLOG("Data digest error on tqpair=(%p) with pdu=%p\n", tqpair, pdu);
+			tcp_req = pdu->req;
+			assert(tcp_req != NULL);
+			rsp = &tcp_req->req.rsp->nvme_cpl;
+			rsp->status.sc = SPDK_NVME_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR;
+		}
+	}
+
+	_nvmf_tcp_pdu_payload_handle(tqpair, ttransport);
+}
+
 static void
 nvmf_tcp_send_icresp_complete(void *cb_arg)
 {
@@ -1778,7 +1803,7 @@ nvmf_tcp_pdu_psh_handle(struct spdk_nvmf_tcp_qpair *tqpair,
 	enum spdk_nvme_tcp_term_req_fes fes;
 
 	assert(tqpair->recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH);
-	pdu = &tqpair->pdu_in_progress;
+	pdu = tqpair->pdu_in_progress;
 
 	SPDK_DEBUGLOG(nvmf_tcp, "pdu type of tqpair(%p) is %d\n", tqpair,
 		      pdu->hdr.common.pdu_type);
@@ -1812,7 +1837,7 @@ nvmf_tcp_pdu_psh_handle(struct spdk_nvmf_tcp_qpair *tqpair,
 		break;
 
 	default:
-		SPDK_ERRLOG("Unexpected PDU type 0x%02x\n", tqpair->pdu_in_progress.hdr.common.pdu_type);
+		SPDK_ERRLOG("Unexpected PDU type 0x%02x\n", tqpair->pdu_in_progress->hdr.common.pdu_type);
 		fes = SPDK_NVME_TCP_TERM_REQ_FES_INVALID_HEADER_FIELD;
 		error_offset = 1;
 		nvmf_tcp_send_c2h_term_req(tqpair, pdu, fes, error_offset);
@@ -1830,7 +1855,7 @@ nvmf_tcp_pdu_ch_handle(struct spdk_nvmf_tcp_qpair *tqpair)
 	bool plen_error = false, pdo_error = false;
 
 	assert(tqpair->recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_CH);
-	pdu = &tqpair->pdu_in_progress;
+	pdu = tqpair->pdu_in_progress;
 
 	if (pdu->hdr.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_IC_REQ) {
 		if (tqpair->state != NVME_TCP_QPAIR_STATE_INVALID) {
@@ -1906,7 +1931,7 @@ nvmf_tcp_pdu_ch_handle(struct spdk_nvmf_tcp_qpair *tqpair)
 		goto err;
 	} else {
 		nvmf_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH);
-		nvme_tcp_pdu_calc_psh_len(&tqpair->pdu_in_progress, tqpair->host_hdgst_enable);
+		nvme_tcp_pdu_calc_psh_len(tqpair->pdu_in_progress, tqpair->host_hdgst_enable);
 		return;
 	}
 err:
@@ -1943,7 +1968,7 @@ nvmf_tcp_sock_process(struct spdk_nvmf_tcp_qpair *tqpair)
 		prev_state = tqpair->recv_state;
 		SPDK_DEBUGLOG(nvmf_tcp, "tqpair(%p) recv pdu entering state %d\n", tqpair, prev_state);
 
-		pdu = &tqpair->pdu_in_progress;
+		pdu = tqpair->pdu_in_progress;
 		switch (tqpair->recv_state) {
 		/* Wait for the common header  */
 		case NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY:
@@ -1960,7 +1985,7 @@ nvmf_tcp_sock_process(struct spdk_nvmf_tcp_qpair *tqpair)
 				return NVME_TCP_PDU_FATAL;
 			} else if (rc > 0) {
 				pdu->ch_valid_bytes += rc;
-				spdk_trace_record(TRACE_TCP_READ_FROM_SOCKET_DONE, 0, rc, 0, 0);
+				spdk_trace_record(TRACE_TCP_READ_FROM_SOCKET_DONE, 0, rc, 0);
 				if (spdk_likely(tqpair->recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY)) {
 					nvmf_tcp_qpair_set_recv_state(tqpair, NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_CH);
 				}
@@ -1981,8 +2006,7 @@ nvmf_tcp_sock_process(struct spdk_nvmf_tcp_qpair *tqpair)
 			if (rc < 0) {
 				return NVME_TCP_PDU_FATAL;
 			} else if (rc > 0) {
-				spdk_trace_record(TRACE_TCP_READ_FROM_SOCKET_DONE,
-						  0, rc, 0, 0);
+				spdk_trace_record(TRACE_TCP_READ_FROM_SOCKET_DONE, 0, rc, 0);
 				pdu->psh_valid_bytes += rc;
 			}
 
@@ -2103,7 +2127,7 @@ nvmf_tcp_req_parse_sgl(struct spdk_nvmf_tcp_req *tcp_req,
 
 		SPDK_DEBUGLOG(nvmf_tcp, "Data requested length= 0x%x\n", length);
 
-		if (spdk_unlikely(req->dif.dif_insert_or_strip)) {
+		if (spdk_unlikely(req->dif_enabled)) {
 			req->dif.orig_length = length;
 			length = spdk_dif_get_length_with_md(length, &req->dif.dif_ctx);
 			req->dif.elba_length = length;
@@ -2168,7 +2192,7 @@ nvmf_tcp_req_parse_sgl(struct spdk_nvmf_tcp_req *tcp_req,
 		req->length = length;
 		req->data_from_pool = false;
 
-		if (spdk_unlikely(req->dif.dif_insert_or_strip)) {
+		if (spdk_unlikely(req->dif_enabled)) {
 			length = spdk_dif_get_length_with_md(length, &req->dif.dif_ctx);
 			req->dif.elba_length = length;
 		}
@@ -2261,7 +2285,7 @@ _nvmf_tcp_send_c2h_data(struct spdk_nvmf_tcp_qpair *tqpair,
 
 	c2h_data->common.plen = plen;
 
-	if (spdk_unlikely(tcp_req->req.dif.dif_insert_or_strip)) {
+	if (spdk_unlikely(tcp_req->req.dif_enabled)) {
 		rsp_pdu->dif_ctx = &tcp_req->req.dif.dif_ctx;
 	}
 
@@ -2276,7 +2300,7 @@ _nvmf_tcp_send_c2h_data(struct spdk_nvmf_tcp_qpair *tqpair,
 		c2h_data->common.flags |= SPDK_NVME_TCP_C2H_DATA_FLAGS_SUCCESS;
 	}
 
-	if (spdk_unlikely(tcp_req->req.dif.dif_insert_or_strip)) {
+	if (spdk_unlikely(tcp_req->req.dif_enabled)) {
 		struct spdk_nvme_cpl *rsp = &tcp_req->req.rsp->nvme_cpl;
 		struct spdk_dif_error err_blk = {};
 		uint32_t mapped_length = 0;
@@ -2379,7 +2403,7 @@ nvmf_tcp_set_incapsule_data(struct spdk_nvmf_tcp_qpair *tqpair,
 	struct nvme_tcp_pdu *pdu;
 	uint32_t plen = 0;
 
-	pdu = &tqpair->pdu_in_progress;
+	pdu = tqpair->pdu_in_progress;
 	plen = pdu->hdr.common.hlen;
 
 	if (tqpair->host_hdgst_enable) {
@@ -2428,14 +2452,14 @@ nvmf_tcp_req_process(struct spdk_nvmf_tcp_transport *ttransport,
 			 * to escape this state. */
 			break;
 		case TCP_REQUEST_STATE_NEW:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_NEW, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_NEW, 0, 0, (uintptr_t)tcp_req);
 
 			/* copy the cmd from the receive pdu */
-			tcp_req->cmd = tqpair->pdu_in_progress.hdr.capsule_cmd.ccsqe;
+			tcp_req->cmd = tqpair->pdu_in_progress->hdr.capsule_cmd.ccsqe;
 
 			if (spdk_unlikely(spdk_nvmf_request_get_dif_ctx(&tcp_req->req, &tcp_req->req.dif.dif_ctx))) {
-				tcp_req->req.dif.dif_insert_or_strip = true;
-				tqpair->pdu_in_progress.dif_ctx = &tcp_req->req.dif.dif_ctx;
+				tcp_req->req.dif_enabled = true;
+				tqpair->pdu_in_progress->dif_ctx = &tcp_req->req.dif.dif_ctx;
 			}
 
 			/* The next state transition depends on the data transfer needs of this request. */
@@ -2467,7 +2491,7 @@ nvmf_tcp_req_process(struct spdk_nvmf_tcp_transport *ttransport,
 			STAILQ_INSERT_TAIL(&group->pending_buf_queue, &tcp_req->req, buf_link);
 			break;
 		case TCP_REQUEST_STATE_NEED_BUFFER:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_NEED_BUFFER, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_NEED_BUFFER, 0, 0, (uintptr_t)tcp_req);
 
 			assert(tcp_req->req.xfer != SPDK_NVME_DATA_NONE);
 
@@ -2508,7 +2532,7 @@ nvmf_tcp_req_process(struct spdk_nvmf_tcp_transport *ttransport,
 
 					nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER);
 
-					pdu = &tqpair->pdu_in_progress;
+					pdu = tqpair->pdu_in_progress;
 					SPDK_DEBUGLOG(nvmf_tcp, "Not need to send r2t for tcp_req(%p) on tqpair=%p\n", tcp_req,
 						      tqpair);
 					/* No need to send r2t, contained in the capsuled data */
@@ -2522,20 +2546,20 @@ nvmf_tcp_req_process(struct spdk_nvmf_tcp_transport *ttransport,
 			nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_EXECUTE);
 			break;
 		case TCP_REQUEST_STATE_AWAITING_R2T_ACK:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_AWAIT_R2T_ACK, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_AWAIT_R2T_ACK, 0, 0, (uintptr_t)tcp_req);
 			/* The R2T completion or the h2c data incoming will kick it out of this state. */
 			break;
 		case TCP_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER:
 
 			spdk_trace_record(TRACE_TCP_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER, 0, 0,
-					  (uintptr_t)tcp_req, 0);
+					  (uintptr_t)tcp_req);
 			/* Some external code must kick a request into TCP_REQUEST_STATE_READY_TO_EXECUTE
 			 * to escape this state. */
 			break;
 		case TCP_REQUEST_STATE_READY_TO_EXECUTE:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_READY_TO_EXECUTE, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_READY_TO_EXECUTE, 0, 0, (uintptr_t)tcp_req);
 
-			if (spdk_unlikely(tcp_req->req.dif.dif_insert_or_strip)) {
+			if (spdk_unlikely(tcp_req->req.dif_enabled)) {
 				assert(tcp_req->req.dif.elba_length >= tcp_req->req.length);
 				tcp_req->req.length = tcp_req->req.dif.elba_length;
 			}
@@ -2544,33 +2568,32 @@ nvmf_tcp_req_process(struct spdk_nvmf_tcp_transport *ttransport,
 			spdk_nvmf_request_exec(&tcp_req->req);
 			break;
 		case TCP_REQUEST_STATE_EXECUTING:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_EXECUTING, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_EXECUTING, 0, 0, (uintptr_t)tcp_req);
 			/* Some external code must kick a request into TCP_REQUEST_STATE_EXECUTED
 			 * to escape this state. */
 			break;
 		case TCP_REQUEST_STATE_EXECUTED:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_EXECUTED, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_EXECUTED, 0, 0, (uintptr_t)tcp_req);
 
-			if (spdk_unlikely(tcp_req->req.dif.dif_insert_or_strip)) {
+			if (spdk_unlikely(tcp_req->req.dif_enabled)) {
 				tcp_req->req.length = tcp_req->req.dif.orig_length;
 			}
 
 			nvmf_tcp_req_set_state(tcp_req, TCP_REQUEST_STATE_READY_TO_COMPLETE);
 			break;
 		case TCP_REQUEST_STATE_READY_TO_COMPLETE:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_READY_TO_COMPLETE, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_READY_TO_COMPLETE, 0, 0, (uintptr_t)tcp_req);
 			rc = request_transfer_out(&tcp_req->req);
 			assert(rc == 0); /* No good way to handle this currently */
 			break;
 		case TCP_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST:
 			spdk_trace_record(TRACE_TCP_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST, 0, 0,
-					  (uintptr_t)tcp_req,
-					  0);
+					  (uintptr_t)tcp_req);
 			/* Some external code must kick a request into TCP_REQUEST_STATE_COMPLETED
 			 * to escape this state. */
 			break;
 		case TCP_REQUEST_STATE_COMPLETED:
-			spdk_trace_record(TRACE_TCP_REQUEST_STATE_COMPLETED, 0, 0, (uintptr_t)tcp_req, 0);
+			spdk_trace_record(TRACE_TCP_REQUEST_STATE_COMPLETED, 0, 0, (uintptr_t)tcp_req);
 			if (tcp_req->req.data_from_pool) {
 				spdk_nvmf_request_free_buffers(&tcp_req->req, group, transport);
 			} else if (spdk_unlikely(tcp_req->has_incapsule_data && (tcp_req->cmd.opc == SPDK_NVME_OPC_FABRIC ||
diff --git a/lib/nvmf/transport.c b/lib/nvmf/transport.c
index 563b82ae3..73dcbc240 100644
--- a/lib/nvmf/transport.c
+++ b/lib/nvmf/transport.c
@@ -2,7 +2,7 @@
  *   BSD LICENSE
  *
  *   Copyright (c) Intel Corporation. All rights reserved.
- *   Copyright (c) 2018-2019 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2018-2019, 2021 Mellanox Technologies LTD. All rights reserved.
  *
  *   Redistribution and use in source and binary forms, with or without
  *   modification, are permitted provided that the following conditions
@@ -42,6 +42,7 @@
 #include "spdk/nvmf_transport.h"
 #include "spdk/queue.h"
 #include "spdk/util.h"
+#include "spdk_internal/usdt.h"
 
 #define MAX_MEMPOOL_NAME_LENGTH 40
 #define NVMF_TRANSPORT_DEFAULT_ASSOCIATION_TIMEOUT_IN_MS 120000
@@ -532,6 +533,8 @@ nvmf_transport_qpair_fini(struct spdk_nvmf_qpair *qpair,
 			  spdk_nvmf_transport_qpair_fini_cb cb_fn,
 			  void *cb_arg)
 {
+	SPDK_DTRACE_PROBE1(nvmf_transport_qpair_fini, qpair);
+
 	qpair->transport->ops->qpair_fini(qpair, cb_fn, cb_arg);
 }
 
@@ -721,28 +724,3 @@ spdk_nvmf_request_get_buffers(struct spdk_nvmf_request *req,
 
 	return rc;
 }
-
-int
-spdk_nvmf_request_get_buffers_multi(struct spdk_nvmf_request *req,
-				    struct spdk_nvmf_transport_poll_group *group,
-				    struct spdk_nvmf_transport *transport,
-				    uint32_t *lengths, uint32_t num_lengths)
-{
-	int rc = 0;
-	uint32_t i;
-
-	req->iovcnt = 0;
-
-	for (i = 0; i < num_lengths; i++) {
-		rc = nvmf_request_get_buffers(req, group, transport, lengths[i]);
-		if (rc != 0) {
-			goto err_exit;
-		}
-	}
-
-	return 0;
-
-err_exit:
-	spdk_nvmf_request_free_buffers(req, group, transport);
-	return rc;
-}
diff --git a/lib/nvmf/vfio_user.c b/lib/nvmf/vfio_user.c
index aa20e03f9..fb8f47d23 100644
--- a/lib/nvmf/vfio_user.c
+++ b/lib/nvmf/vfio_user.c
@@ -91,7 +91,7 @@ struct nvmf_vfio_user_req  {
 	void					*cb_arg;
 
 	/* placeholder for gpa_to_vva memory map table, the IO buffer doesn't use it */
-	dma_sg_t				sg[NVMF_VFIO_USER_MAX_IOVECS];
+	dma_sg_t				*sg;
 	struct iovec				iov[NVMF_VFIO_USER_MAX_IOVECS];
 	uint8_t					iovcnt;
 
@@ -106,7 +106,7 @@ struct nvme_q {
 
 	void *addr;
 
-	dma_sg_t sg;
+	dma_sg_t *sg;
 	struct iovec iov;
 
 	uint32_t size;
@@ -157,8 +157,6 @@ struct nvmf_vfio_user_ctrlr {
 	struct nvmf_vfio_user_endpoint		*endpoint;
 	struct nvmf_vfio_user_transport		*transport;
 
-	/* True when the socket connection is active */
-	bool					ready;
 	/* Number of connected queue pairs */
 	uint32_t				num_connected_qps;
 
@@ -193,8 +191,13 @@ struct nvmf_vfio_user_endpoint {
 	TAILQ_ENTRY(nvmf_vfio_user_endpoint)	link;
 };
 
+struct nvmf_vfio_user_transport_opts {
+	bool					disable_mappable_bar0;
+};
+
 struct nvmf_vfio_user_transport {
 	struct spdk_nvmf_transport		transport;
+	struct nvmf_vfio_user_transport_opts    transport_opts;
 	pthread_mutex_t				lock;
 	TAILQ_HEAD(, nvmf_vfio_user_endpoint)	endpoints;
 
@@ -263,7 +266,6 @@ fail_ctrlr(struct nvmf_vfio_user_ctrlr *ctrlr)
 		SPDK_ERRLOG(":%s failing controller\n", ctrlr_id(ctrlr));
 	}
 
-	ctrlr->ready = false;
 	ctrlr->cfs = 1U;
 }
 
@@ -324,6 +326,14 @@ nvmf_vfio_user_destroy(struct spdk_nvmf_transport *transport,
 	return 0;
 }
 
+static const struct spdk_json_object_decoder vfio_user_transport_opts_decoder[] = {
+	{
+		"disable-mappable-bar0",
+		offsetof(struct nvmf_vfio_user_transport, transport_opts.disable_mappable_bar0),
+		spdk_json_decode_bool, true
+	},
+};
+
 static struct spdk_nvmf_transport *
 nvmf_vfio_user_create(struct spdk_nvmf_transport_opts *opts)
 {
@@ -345,6 +355,18 @@ nvmf_vfio_user_create(struct spdk_nvmf_transport_opts *opts)
 	TAILQ_INIT(&vu_transport->endpoints);
 	TAILQ_INIT(&vu_transport->new_qps);
 
+	if (opts->transport_specific != NULL &&
+	    spdk_json_decode_object_relaxed(opts->transport_specific, vfio_user_transport_opts_decoder,
+					    SPDK_COUNTOF(vfio_user_transport_opts_decoder),
+					    vu_transport)) {
+		SPDK_ERRLOG("spdk_json_decode_object_relaxed failed\n");
+		free(vu_transport);
+		return NULL;
+	}
+
+	SPDK_DEBUGLOG(nvmf_vfio, "vfio_user transport: disable_mappable_bar0=%d\n",
+		      vu_transport->transport_opts.disable_mappable_bar0);
+
 	return &vu_transport->transport;
 
 err:
@@ -373,11 +395,11 @@ map_one(vfu_ctx_t *ctx, uint64_t addr, uint64_t len, dma_sg_t *sg, struct iovec
 	assert(iov != NULL);
 
 	ret = vfu_addr_to_sg(ctx, (void *)(uintptr_t)addr, len, sg, 1, PROT_READ | PROT_WRITE);
-	if (ret != 1) {
+	if (ret < 0) {
 		return NULL;
 	}
 
-	ret = vfu_map_sg(ctx, sg, iov, 1);
+	ret = vfu_map_sg(ctx, sg, iov, 1, 0);
 	if (ret != 0) {
 		return NULL;
 	}
@@ -401,34 +423,10 @@ sqhd_advance(struct nvmf_vfio_user_ctrlr *ctrlr, struct nvmf_vfio_user_qpair *qp
 	qpair->sq.head = (qpair->sq.head + 1) % qpair->sq.size;
 }
 
-static void
-insert_queue(struct nvmf_vfio_user_ctrlr *ctrlr, struct nvme_q *q,
-	     const bool is_cq, const uint16_t id)
-{
-	struct nvme_q *_q;
-	struct nvmf_vfio_user_qpair *qpair;
-
-	assert(ctrlr != NULL);
-	assert(q != NULL);
-
-	qpair = ctrlr->qp[id];
-
-	q->is_cq = is_cq;
-	if (is_cq) {
-		_q = &qpair->cq;
-		*_q = *q;
-		*hdbl(ctrlr, _q) = 0;
-	} else {
-		_q = &qpair->sq;
-		*_q = *q;
-		*tdbl(ctrlr, _q) = 0;
-	}
-}
-
 static int
 asq_map(struct nvmf_vfio_user_ctrlr *ctrlr)
 {
-	struct nvme_q q = {};
+	struct nvme_q *sq;
 	const struct spdk_nvmf_registers *regs;
 
 	assert(ctrlr != NULL);
@@ -437,16 +435,18 @@ asq_map(struct nvmf_vfio_user_ctrlr *ctrlr)
 	/* XXX ctrlr->asq == 0 is a valid memory address */
 
 	regs = spdk_nvmf_ctrlr_get_regs(ctrlr->qp[0]->qpair.ctrlr);
-	q.size = regs->aqa.bits.asqs + 1;
-	q.head = ctrlr->doorbells[0] = 0;
-	q.cqid = 0;
-	q.addr = map_one(ctrlr->endpoint->vfu_ctx, regs->asq,
-			 q.size * sizeof(struct spdk_nvme_cmd), &q.sg, &q.iov);
-	if (q.addr == NULL) {
+	sq = &ctrlr->qp[0]->sq;
+	sq->size = regs->aqa.bits.asqs + 1;
+	sq->head = ctrlr->doorbells[0] = 0;
+	sq->cqid = 0;
+	sq->addr = map_one(ctrlr->endpoint->vfu_ctx, regs->asq,
+			   sq->size * sizeof(struct spdk_nvme_cmd), sq->sg, &sq->iov);
+	if (sq->addr == NULL) {
 		return -1;
 	}
-	memset(q.addr, 0, q.size * sizeof(struct spdk_nvme_cmd));
-	insert_queue(ctrlr, &q, false, 0);
+	memset(sq->addr, 0, sq->size * sizeof(struct spdk_nvme_cmd));
+	sq->is_cq = false;
+	*tdbl(ctrlr, sq) = 0;
 
 	return 0;
 }
@@ -503,7 +503,7 @@ cq_tail_advance(struct nvme_q *q)
 static int
 acq_map(struct nvmf_vfio_user_ctrlr *ctrlr)
 {
-	struct nvme_q q = {};
+	struct nvme_q *cq;
 	const struct spdk_nvmf_registers *regs;
 
 	assert(ctrlr != NULL);
@@ -512,22 +512,28 @@ acq_map(struct nvmf_vfio_user_ctrlr *ctrlr)
 
 	regs = spdk_nvmf_ctrlr_get_regs(ctrlr->qp[0]->qpair.ctrlr);
 	assert(regs != NULL);
-
-	q.size = regs->aqa.bits.acqs + 1;
-	q.tail = 0;
-	q.addr = map_one(ctrlr->endpoint->vfu_ctx, regs->acq,
-			 q.size * sizeof(struct spdk_nvme_cpl), &q.sg, &q.iov);
-	if (q.addr == NULL) {
+	cq = &ctrlr->qp[0]->cq;
+	cq->size = regs->aqa.bits.acqs + 1;
+	cq->tail = 0;
+	cq->addr = map_one(ctrlr->endpoint->vfu_ctx, regs->acq,
+			   cq->size * sizeof(struct spdk_nvme_cpl), cq->sg, &cq->iov);
+	if (cq->addr == NULL) {
 		return -1;
 	}
-	memset(q.addr, 0, q.size * sizeof(struct spdk_nvme_cpl));
-	q.is_cq = true;
-	q.ien = true;
-	insert_queue(ctrlr, &q, true, 0);
+	memset(cq->addr, 0, cq->size * sizeof(struct spdk_nvme_cpl));
+	cq->is_cq = true;
+	cq->ien = true;
+	*hdbl(ctrlr, cq) = 0;
 
 	return 0;
 }
 
+static inline dma_sg_t *
+vu_req_to_sg_t(struct nvmf_vfio_user_req *vu_req, uint32_t iovcnt)
+{
+	return (dma_sg_t *)((uintptr_t)vu_req->sg + iovcnt * dma_sg_size());
+}
+
 static void *
 _map_one(void *prv, uint64_t addr, uint64_t len)
 {
@@ -544,7 +550,7 @@ _map_one(void *prv, uint64_t addr, uint64_t len)
 
 	assert(vu_req->iovcnt < NVMF_VFIO_USER_MAX_IOVECS);
 	ret = map_one(vu_qpair->ctrlr->endpoint->vfu_ctx, addr, len,
-		      &vu_req->sg[vu_req->iovcnt],
+		      vu_req_to_sg_t(vu_req, vu_req->iovcnt),
 		      &vu_req->iov[vu_req->iovcnt]);
 	if (spdk_likely(ret != NULL)) {
 		vu_req->iovcnt++;
@@ -695,24 +701,22 @@ unmap_qp(struct nvmf_vfio_user_qpair *qp)
 		      ctrlr_id(ctrlr), qp->qpair.qid);
 
 	if (qp->sq.addr != NULL) {
-		vfu_unmap_sg(ctrlr->endpoint->vfu_ctx, &qp->sq.sg, &qp->sq.iov, 1);
+		vfu_unmap_sg(ctrlr->endpoint->vfu_ctx, qp->sq.sg, &qp->sq.iov, 1);
 		qp->sq.addr = NULL;
 	}
 
 	if (qp->cq.addr != NULL) {
-		vfu_unmap_sg(ctrlr->endpoint->vfu_ctx, &qp->cq.sg, &qp->cq.iov, 1);
+		vfu_unmap_sg(ctrlr->endpoint->vfu_ctx, qp->cq.sg, &qp->cq.iov, 1);
 		qp->cq.addr = NULL;
 	}
 }
 
-/*
- * TODO we can immediately remove the QP from the list because this function
- * is now executed by the SPDK thread.
- */
 static void
-destroy_qp(struct nvmf_vfio_user_ctrlr *ctrlr, uint16_t qid)
+free_qp(struct nvmf_vfio_user_ctrlr *ctrlr, uint16_t qid)
 {
 	struct nvmf_vfio_user_qpair *qpair;
+	struct nvmf_vfio_user_req *vu_req;
+	uint32_t i;
 
 	if (ctrlr == NULL) {
 		return;
@@ -727,8 +731,17 @@ destroy_qp(struct nvmf_vfio_user_ctrlr *ctrlr, uint16_t qid)
 		      qid, qpair);
 
 	unmap_qp(qpair);
+
+	for (i = 0; i < qpair->qsize; i++) {
+		vu_req = &qpair->reqs_internal[i];
+		free(vu_req->sg);
+	}
 	free(qpair->reqs_internal);
+
+	free(qpair->sq.sg);
+	free(qpair->cq.sg);
 	free(qpair);
+
 	ctrlr->qp[qid] = NULL;
 }
 
@@ -737,9 +750,9 @@ static int
 init_qp(struct nvmf_vfio_user_ctrlr *ctrlr, struct spdk_nvmf_transport *transport,
 	const uint16_t qsize, const uint16_t id)
 {
-	int err = 0, i;
+	uint16_t i;
 	struct nvmf_vfio_user_qpair *qpair;
-	struct nvmf_vfio_user_req *vu_req;
+	struct nvmf_vfio_user_req *vu_req, *tmp;
 	struct spdk_nvmf_request *req;
 
 	assert(ctrlr != NULL);
@@ -749,6 +762,17 @@ init_qp(struct nvmf_vfio_user_ctrlr *ctrlr, struct spdk_nvmf_transport *transpor
 	if (qpair == NULL) {
 		return -ENOMEM;
 	}
+	qpair->sq.sg = calloc(1, dma_sg_size());
+	if (qpair->sq.sg == NULL) {
+		free(qpair);
+		return -ENOMEM;
+	}
+	qpair->cq.sg = calloc(1, dma_sg_size());
+	if (qpair->cq.sg == NULL) {
+		free(qpair->sq.sg);
+		free(qpair);
+		return -ENOMEM;
+	}
 
 	qpair->qpair.qid = id;
 	qpair->qpair.transport = transport;
@@ -760,26 +784,38 @@ init_qp(struct nvmf_vfio_user_ctrlr *ctrlr, struct spdk_nvmf_transport *transpor
 	qpair->reqs_internal = calloc(qsize, sizeof(struct nvmf_vfio_user_req));
 	if (qpair->reqs_internal == NULL) {
 		SPDK_ERRLOG("%s: error allocating reqs: %m\n", ctrlr_id(ctrlr));
-		err = -ENOMEM;
-		goto out;
+		goto reqs_err;
 	}
 
 	for (i = 0; i < qsize; i++) {
 		vu_req = &qpair->reqs_internal[i];
-		req = &vu_req->req;
+		vu_req->sg = calloc(NVMF_VFIO_USER_MAX_IOVECS, dma_sg_size());
+		if (vu_req->sg == NULL) {
+			goto sg_err;
+		}
 
+		req = &vu_req->req;
 		req->qpair = &qpair->qpair;
 		req->rsp = (union nvmf_c2h_msg *)&vu_req->rsp;
 		req->cmd = (union nvmf_h2c_msg *)&vu_req->cmd;
 
 		TAILQ_INSERT_TAIL(&qpair->reqs, vu_req, link);
 	}
+
 	ctrlr->qp[id] = qpair;
-out:
-	if (err != 0) {
-		free(qpair);
+	return 0;
+
+sg_err:
+	TAILQ_FOREACH_SAFE(vu_req, &qpair->reqs, link, tmp) {
+		free(vu_req->sg);
 	}
-	return err;
+	free(qpair->reqs_internal);
+
+reqs_err:
+	free(qpair->sq.sg);
+	free(qpair->cq.sg);
+	free(qpair);
+	return -ENOMEM;
 }
 
 /*
@@ -793,10 +829,11 @@ handle_create_io_q(struct nvmf_vfio_user_ctrlr *ctrlr,
 		   struct spdk_nvme_cmd *cmd, const bool is_cq)
 {
 	size_t entry_size;
+	uint16_t qsize;
 	uint16_t sc = SPDK_NVME_SC_SUCCESS;
 	uint16_t sct = SPDK_NVME_SCT_GENERIC;
 	int err = 0;
-	struct nvme_q io_q = {};
+	struct nvme_q *io_q;
 
 	assert(ctrlr != NULL);
 	assert(cmd != NULL);
@@ -823,8 +860,25 @@ handle_create_io_q(struct nvmf_vfio_user_ctrlr *ctrlr,
 		goto out;
 	}
 
+	qsize = cmd->cdw10_bits.create_io_q.qsize + 1;
+	if (qsize > max_queue_size(ctrlr)) {
+		SPDK_ERRLOG("%s: queue too big, want=%d, max=%d\n", ctrlr_id(ctrlr),
+			    qsize, max_queue_size(ctrlr));
+		sct = SPDK_NVME_SCT_COMMAND_SPECIFIC;
+		sc = SPDK_NVME_SC_INVALID_QUEUE_SIZE;
+		goto out;
+	}
+
 	/* TODO break rest of this function into smaller functions */
 	if (is_cq) {
+		err = init_qp(ctrlr, ctrlr->qp[0]->qpair.transport, qsize,
+			      cmd->cdw10_bits.create_io_q.qid);
+		if (err != 0) {
+			sc = SPDK_NVME_SC_INTERNAL_DEVICE_ERROR;
+			goto out;
+		}
+
+		io_q = &ctrlr->qp[cmd->cdw10_bits.create_io_q.qid]->cq;
 		entry_size = sizeof(struct spdk_nvme_cpl);
 		if (cmd->cdw11_bits.create_io_cq.pc != 0x1) {
 			/*
@@ -837,8 +891,8 @@ handle_create_io_q(struct nvmf_vfio_user_ctrlr *ctrlr,
 			sc = SPDK_NVME_SC_INVALID_CONTROLLER_MEM_BUF;
 			goto out;
 		}
-		io_q.ien = cmd->cdw11_bits.create_io_cq.ien;
-		io_q.iv = cmd->cdw11_bits.create_io_cq.iv;
+		io_q->ien = cmd->cdw11_bits.create_io_cq.ien;
+		io_q->iv = cmd->cdw11_bits.create_io_cq.iv;
 	} else {
 		/* CQ must be created before SQ */
 		if (!lookup_io_q(ctrlr, cmd->cdw11_bits.create_io_sq.cqid, true)) {
@@ -849,6 +903,7 @@ handle_create_io_q(struct nvmf_vfio_user_ctrlr *ctrlr,
 			goto out;
 		}
 
+		io_q = &ctrlr->qp[cmd->cdw10_bits.create_io_q.qid]->sq;
 		entry_size = sizeof(struct spdk_nvme_cmd);
 		if (cmd->cdw11_bits.create_io_sq.pc != 0x1) {
 			SPDK_ERRLOG("%s: non-PC SQ not supported\n", ctrlr_id(ctrlr));
@@ -856,42 +911,30 @@ handle_create_io_q(struct nvmf_vfio_user_ctrlr *ctrlr,
 			goto out;
 		}
 
-		io_q.cqid = cmd->cdw11_bits.create_io_sq.cqid;
+		io_q->cqid = cmd->cdw11_bits.create_io_sq.cqid;
 		SPDK_DEBUGLOG(nvmf_vfio, "%s: SQ%d CQID=%d\n", ctrlr_id(ctrlr),
-			      cmd->cdw10_bits.create_io_q.qid, io_q.cqid);
+			      cmd->cdw10_bits.create_io_q.qid, io_q->cqid);
 	}
 
-	io_q.size = cmd->cdw10_bits.create_io_q.qsize + 1;
-	if (io_q.size > max_queue_size(ctrlr)) {
-		SPDK_ERRLOG("%s: queue too big, want=%d, max=%d\n", ctrlr_id(ctrlr),
-			    io_q.size, max_queue_size(ctrlr));
-		sct = SPDK_NVME_SCT_COMMAND_SPECIFIC;
-		sc = SPDK_NVME_SC_INVALID_QUEUE_SIZE;
-		goto out;
-	}
-
-	io_q.addr = map_one(ctrlr->endpoint->vfu_ctx, cmd->dptr.prp.prp1,
-			    io_q.size * entry_size, &io_q.sg, &io_q.iov);
-	if (io_q.addr == NULL) {
+	io_q->is_cq = is_cq;
+	io_q->size = qsize;
+	io_q->addr = map_one(ctrlr->endpoint->vfu_ctx, cmd->dptr.prp.prp1,
+			     io_q->size * entry_size, io_q->sg, &io_q->iov);
+	if (io_q->addr == NULL) {
 		sc = SPDK_NVME_SC_INTERNAL_DEVICE_ERROR;
 		SPDK_ERRLOG("%s: failed to map I/O queue: %m\n", ctrlr_id(ctrlr));
 		goto out;
 	}
-	io_q.prp1 = cmd->dptr.prp.prp1;
-	memset(io_q.addr, 0, io_q.size * entry_size);
+	io_q->prp1 = cmd->dptr.prp.prp1;
+	memset(io_q->addr, 0, io_q->size * entry_size);
 
 	SPDK_DEBUGLOG(nvmf_vfio, "%s: mapped %cQ%d IOVA=%#lx vaddr=%#llx\n",
 		      ctrlr_id(ctrlr), is_cq ? 'C' : 'S',
 		      cmd->cdw10_bits.create_io_q.qid, cmd->dptr.prp.prp1,
-		      (unsigned long long)io_q.addr);
+		      (unsigned long long)io_q->addr);
 
 	if (is_cq) {
-		err = init_qp(ctrlr, ctrlr->qp[0]->qpair.transport, io_q.size,
-			      cmd->cdw10_bits.create_io_q.qid);
-		if (err != 0) {
-			sc = SPDK_NVME_SC_INTERNAL_DEVICE_ERROR;
-			goto out;
-		}
+		*hdbl(ctrlr, io_q) = 0;
 	} else {
 		/*
 		 * After we've returned from the nvmf_vfio_user_poll_group_poll thread, once
@@ -901,9 +944,9 @@ handle_create_io_q(struct nvmf_vfio_user_ctrlr *ctrlr,
 		 * completion callback.
 		 */
 		TAILQ_INSERT_TAIL(&ctrlr->transport->new_qps, ctrlr->qp[cmd->cdw10_bits.create_io_q.qid], link);
+		*tdbl(ctrlr, io_q) = 0;
 
 	}
-	insert_queue(ctrlr, &io_q, is_cq, cmd->cdw10_bits.create_io_q.qid);
 
 out:
 	return post_completion(ctrlr, cmd, &ctrlr->qp[0]->cq, 0, sc, sct);
@@ -1078,7 +1121,15 @@ memory_region_add_cb(vfu_ctx_t *vfu_ctx, vfu_dma_info_t *info)
 	struct nvmf_vfio_user_qpair *qpair;
 	int i, ret;
 
-	if (!info->vaddr || ((uintptr_t)info->mapping.iov_base & MASK_2MB) ||
+	/*
+	 * We're not interested in any DMA regions that aren't mappable (we don't
+	 * support clients that don't share their memory).
+	 */
+	if (!info->vaddr) {
+		return;
+	}
+
+	if (((uintptr_t)info->mapping.iov_base & MASK_2MB) ||
 	    (info->mapping.iov_len & MASK_2MB)) {
 		SPDK_DEBUGLOG(nvmf_vfio, "Invalid memory region vaddr %p, IOVA %#lx-%#lx\n", info->vaddr,
 			      (uintptr_t)info->mapping.iov_base,
@@ -1127,13 +1178,13 @@ memory_region_add_cb(vfu_ctx_t *vfu_ctx, vfu_dma_info_t *info)
 			struct nvme_q *sq = &qpair->sq;
 			struct nvme_q *cq = &qpair->cq;
 
-			sq->addr = map_one(ctrlr->endpoint->vfu_ctx, sq->prp1, sq->size * 64, &sq->sg, &sq->iov);
+			sq->addr = map_one(ctrlr->endpoint->vfu_ctx, sq->prp1, sq->size * 64, sq->sg, &sq->iov);
 			if (!sq->addr) {
 				SPDK_DEBUGLOG(nvmf_vfio, "Memory isn't ready to remap SQID %d %#lx-%#lx\n",
 					      i, sq->prp1, sq->prp1 + sq->size * 64);
 				continue;
 			}
-			cq->addr = map_one(ctrlr->endpoint->vfu_ctx, cq->prp1, cq->size * 16, &cq->sg, &cq->iov);
+			cq->addr = map_one(ctrlr->endpoint->vfu_ctx, cq->prp1, cq->size * 16, cq->sg, &cq->iov);
 			if (!cq->addr) {
 				SPDK_DEBUGLOG(nvmf_vfio, "Memory isn't ready to remap CQID %d %#lx-%#lx\n",
 					      i, cq->prp1, cq->prp1 + cq->size * 16);
@@ -1154,7 +1205,11 @@ memory_region_remove_cb(vfu_ctx_t *vfu_ctx, vfu_dma_info_t *info)
 	void *map_start, *map_end;
 	int i;
 
-	if (!info->vaddr || ((uintptr_t)info->mapping.iov_base & MASK_2MB) ||
+	if (!info->vaddr) {
+		return 0;
+	}
+
+	if (((uintptr_t)info->mapping.iov_base & MASK_2MB) ||
 	    (info->mapping.iov_len & MASK_2MB)) {
 		SPDK_DEBUGLOG(nvmf_vfio, "Invalid memory region vaddr %p, IOVA %#lx-%#lx\n", info->vaddr,
 			      (uintptr_t)info->mapping.iov_base,
@@ -1318,10 +1373,10 @@ access_bar0_fn(vfu_ctx_t *vfu_ctx, char *buf, size_t count, loff_t pos,
 
 	if (pos >= NVMF_VFIO_USER_DOORBELLS_OFFSET) {
 		/*
-		 * XXX The fact that the doorbells can be memory mapped doesn't
-		 * mean thath the client (VFIO in QEMU) is obliged to memory
-		 * map them, it might still elect to access them via regular
-		 * read/write.
+		 * The fact that the doorbells can be memory mapped doesn't mean
+		 * that the client (VFIO in QEMU) is obliged to memory map them,
+		 * it might still elect to access them via regular read/write;
+		 * we might also have had disable_mappable_bar0 set.
 		 */
 		ret = handle_dbl_access(ctrlr, (uint32_t *)buf, count,
 					pos, is_write);
@@ -1430,7 +1485,8 @@ init_pci_config_space(vfu_pci_config_space_t *p)
 }
 
 static int
-vfio_user_dev_info_fill(struct nvmf_vfio_user_endpoint *endpoint)
+vfio_user_dev_info_fill(struct nvmf_vfio_user_transport *vu_transport,
+			struct nvmf_vfio_user_endpoint *endpoint)
 {
 	int ret;
 	ssize_t cap_offset;
@@ -1440,7 +1496,7 @@ vfio_user_dev_info_fill(struct nvmf_vfio_user_endpoint *endpoint)
 	struct pxcap pxcap = {
 		.hdr.id = PCI_CAP_ID_EXP,
 		.pxcaps.ver = 0x2,
-		.pxdcap = {.per = 0x1, .flrc = 0x1},
+		.pxdcap = {.rer = 0x1, .flrc = 0x1},
 		.pxdcap2.ctds = 0x1
 	};
 
@@ -1490,29 +1546,36 @@ vfio_user_dev_info_fill(struct nvmf_vfio_user_endpoint *endpoint)
 	}
 
 	ret = vfu_setup_region(vfu_ctx, VFU_PCI_DEV_CFG_REGION_IDX, NVME_REG_CFG_SIZE,
-			       access_pci_config, VFU_REGION_FLAG_RW, NULL, 0, -1);
+			       access_pci_config, VFU_REGION_FLAG_RW, NULL, 0, -1, 0);
 	if (ret < 0) {
 		SPDK_ERRLOG("vfu_ctx %p failed to setup cfg\n", vfu_ctx);
 		return ret;
 	}
 
-	ret = vfu_setup_region(vfu_ctx, VFU_PCI_DEV_BAR0_REGION_IDX, NVME_REG_BAR0_SIZE,
-			       access_bar0_fn, VFU_REGION_FLAG_RW | VFU_REGION_FLAG_MEM,
-			       sparse_mmap, 1, endpoint->fd);
+	if (vu_transport->transport_opts.disable_mappable_bar0) {
+		ret = vfu_setup_region(vfu_ctx, VFU_PCI_DEV_BAR0_REGION_IDX, NVME_REG_BAR0_SIZE,
+				       access_bar0_fn, VFU_REGION_FLAG_RW | VFU_REGION_FLAG_MEM,
+				       NULL, 0, -1, 0);
+	} else {
+		ret = vfu_setup_region(vfu_ctx, VFU_PCI_DEV_BAR0_REGION_IDX, NVME_REG_BAR0_SIZE,
+				       access_bar0_fn, VFU_REGION_FLAG_RW | VFU_REGION_FLAG_MEM,
+				       sparse_mmap, 1, endpoint->fd, 0);
+	}
+
 	if (ret < 0) {
 		SPDK_ERRLOG("vfu_ctx %p failed to setup bar 0\n", vfu_ctx);
 		return ret;
 	}
 
 	ret = vfu_setup_region(vfu_ctx, VFU_PCI_DEV_BAR4_REGION_IDX, PAGE_SIZE,
-			       NULL, VFU_REGION_FLAG_RW, NULL, 0, -1);
+			       NULL, VFU_REGION_FLAG_RW, NULL, 0, -1, 0);
 	if (ret < 0) {
 		SPDK_ERRLOG("vfu_ctx %p failed to setup bar 4\n", vfu_ctx);
 		return ret;
 	}
 
 	ret = vfu_setup_region(vfu_ctx, VFU_PCI_DEV_BAR5_REGION_IDX, PAGE_SIZE,
-			       NULL, VFU_REGION_FLAG_RW, NULL, 0, -1);
+			       NULL, VFU_REGION_FLAG_RW, NULL, 0, -1, 0);
 	if (ret < 0) {
 		SPDK_ERRLOG("vfu_ctx %p failed to setup bar 5\n", vfu_ctx);
 		return ret;
@@ -1553,13 +1616,13 @@ vfio_user_dev_info_fill(struct nvmf_vfio_user_endpoint *endpoint)
 }
 
 static void
-_destroy_ctrlr(void *ctx)
+_free_ctrlr(void *ctx)
 {
 	struct nvmf_vfio_user_ctrlr *ctrlr = ctx;
 	int i;
 
 	for (i = 0; i < NVMF_VFIO_USER_DEFAULT_MAX_QPAIRS_PER_CTRLR; i++) {
-		destroy_qp(ctrlr, i);
+		free_qp(ctrlr, i);
 	}
 
 	if (ctrlr->endpoint) {
@@ -1571,16 +1634,16 @@ _destroy_ctrlr(void *ctx)
 }
 
 static int
-destroy_ctrlr(struct nvmf_vfio_user_ctrlr *ctrlr)
+free_ctrlr(struct nvmf_vfio_user_ctrlr *ctrlr)
 {
 	assert(ctrlr != NULL);
 
-	SPDK_DEBUGLOG(nvmf_vfio, "destroy %s\n", ctrlr_id(ctrlr));
+	SPDK_DEBUGLOG(nvmf_vfio, "free %s\n", ctrlr_id(ctrlr));
 
 	if (ctrlr->thread == spdk_get_thread()) {
-		_destroy_ctrlr(ctrlr);
+		_free_ctrlr(ctrlr);
 	} else {
-		spdk_thread_send_msg(ctrlr->thread, _destroy_ctrlr, ctrlr);
+		spdk_thread_send_msg(ctrlr->thread, _free_ctrlr, ctrlr);
 	}
 
 	return 0;
@@ -1610,7 +1673,6 @@ nvmf_vfio_user_create_ctrlr(struct nvmf_vfio_user_transport *transport,
 		goto out;
 	}
 	endpoint->ctrlr = ctrlr;
-	ctrlr->ready = true;
 
 	/* Notify the generic layer about the new admin queue pair */
 	TAILQ_INSERT_TAIL(&ctrlr->transport->new_qps, ctrlr->qp[0], link);
@@ -1619,7 +1681,7 @@ out:
 	if (err != 0) {
 		SPDK_ERRLOG("%s: failed to create vfio-user controller: %s\n",
 			    endpoint_id(endpoint), strerror(-err));
-		if (destroy_ctrlr(ctrlr) != 0) {
+		if (free_ctrlr(ctrlr) != 0) {
 			SPDK_ERRLOG("%s: failed to clean up\n",
 				    endpoint_id(endpoint));
 		}
@@ -1671,6 +1733,7 @@ nvmf_vfio_user_listen(struct spdk_nvmf_transport *transport,
 	}
 	free(path);
 
+	endpoint->fd = fd;
 	err = ftruncate(fd, NVMF_VFIO_USER_DOORBELLS_OFFSET + NVMF_VFIO_USER_DOORBELLS_SIZE);
 	if (err != 0) {
 		goto out;
@@ -1684,8 +1747,6 @@ nvmf_vfio_user_listen(struct spdk_nvmf_transport *transport,
 		goto out;
 	}
 
-	endpoint->fd = fd;
-
 	snprintf(uuid, PATH_MAX, "%s/cntrl", endpoint_id(endpoint));
 
 	endpoint->vfu_ctx = vfu_create_ctx(VFU_TRANS_SOCK, uuid, LIBVFIO_USER_FLAG_ATTACH_NB,
@@ -1696,10 +1757,9 @@ nvmf_vfio_user_listen(struct spdk_nvmf_transport *transport,
 		err = -1;
 		goto out;
 	}
-	vfu_setup_log(endpoint->vfu_ctx, vfio_user_log,
-		      SPDK_DEBUGLOG_FLAG_ENABLED("nvmf_vfio") ? LOG_DEBUG : LOG_ERR);
+	vfu_setup_log(endpoint->vfu_ctx, vfio_user_log, LOG_DEBUG);
 
-	err = vfio_user_dev_info_fill(endpoint);
+	err = vfio_user_dev_info_fill(vu_transport, endpoint);
 	if (err < 0) {
 		goto out;
 	}
@@ -1737,7 +1797,7 @@ nvmf_vfio_user_stop_listen(struct spdk_nvmf_transport *transport,
 		if (strcmp(trid->traddr, endpoint->trid.traddr) == 0) {
 			TAILQ_REMOVE(&vu_transport->endpoints, endpoint, link);
 			if (endpoint->ctrlr) {
-				err = destroy_ctrlr(endpoint->ctrlr);
+				err = free_ctrlr(endpoint->ctrlr);
 				if (err != 0) {
 					SPDK_ERRLOG("%s: failed destroy controller: %s\n",
 						    endpoint_id(endpoint), strerror(-err));
@@ -1807,8 +1867,8 @@ nvmf_vfio_user_accept(struct spdk_nvmf_transport *transport)
 	pthread_mutex_lock(&vu_transport->lock);
 
 	TAILQ_FOREACH(endpoint, &vu_transport->endpoints, link) {
-		/* we need try to attach the controller again after reset or shutdown */
-		if (endpoint->ctrlr != NULL && endpoint->ctrlr->ready) {
+		/* try to attach a new controller  */
+		if (endpoint->ctrlr != NULL) {
 			continue;
 		}
 
@@ -1887,7 +1947,7 @@ vfio_user_qpair_disconnect_cb(void *ctx)
 	}
 
 	if (!ctrlr->num_connected_qps) {
-		destroy_ctrlr(ctrlr);
+		free_ctrlr(ctrlr);
 		pthread_mutex_unlock(&endpoint->lock);
 		return;
 	}
@@ -1895,7 +1955,7 @@ vfio_user_qpair_disconnect_cb(void *ctx)
 }
 
 static int
-vfio_user_stop_ctrlr(struct nvmf_vfio_user_ctrlr *ctrlr)
+vfio_user_destroy_ctrlr(struct nvmf_vfio_user_ctrlr *ctrlr)
 {
 	uint32_t i;
 	struct nvmf_vfio_user_qpair *qpair;
@@ -1903,7 +1963,6 @@ vfio_user_stop_ctrlr(struct nvmf_vfio_user_ctrlr *ctrlr)
 
 	SPDK_DEBUGLOG(nvmf_vfio, "%s stop processing\n", ctrlr_id(ctrlr));
 
-	ctrlr->ready = false;
 	endpoint = ctrlr->endpoint;
 	assert(endpoint != NULL);
 
@@ -1934,7 +1993,7 @@ vfio_user_poll_mmio(void *ctx)
 
 		/* initiator shutdown or reset, waiting for another re-connect */
 		if (errno == ENOTCONN) {
-			vfio_user_stop_ctrlr(ctrlr);
+			vfio_user_destroy_ctrlr(ctrlr);
 			return SPDK_POLLER_BUSY;
 		}
 
@@ -1962,8 +2021,7 @@ handle_queue_connect_rsp(struct nvmf_vfio_user_req *req, void *cb_arg)
 
 	if (spdk_nvme_cpl_is_error(&req->req.rsp->nvme_cpl)) {
 		SPDK_ERRLOG("SC %u, SCT %u\n", req->req.rsp->nvme_cpl.status.sc, req->req.rsp->nvme_cpl.status.sct);
-		destroy_qp(ctrlr, qpair->qpair.qid);
-		destroy_ctrlr(ctrlr);
+		free_ctrlr(ctrlr);
 		return -1;
 	}
 
@@ -2132,7 +2190,7 @@ nvmf_vfio_user_close_qpair(struct spdk_nvmf_qpair *qpair,
 
 	assert(qpair != NULL);
 	vu_qpair = SPDK_CONTAINEROF(qpair, struct nvmf_vfio_user_qpair, qpair);
-	destroy_qp(vu_qpair->ctrlr, qpair->qid);
+	free_qp(vu_qpair->ctrlr, qpair->qid);
 
 	if (cb_fn) {
 		cb_fn(cb_arg);
@@ -2440,6 +2498,7 @@ nvmf_vfio_user_opts_init(struct spdk_nvmf_transport_opts *opts)
 	opts->max_aq_depth =		NVMF_VFIO_USER_DEFAULT_AQ_DEPTH;
 	opts->num_shared_buffers =	NVMF_VFIO_USER_DEFAULT_NUM_SHARED_BUFFERS;
 	opts->buf_cache_size =		NVMF_VFIO_USER_DEFAULT_BUFFER_CACHE_SIZE;
+	opts->transport_specific =      NULL;
 }
 
 const struct spdk_nvmf_transport_ops spdk_nvmf_transport_vfio_user = {
diff --git a/lib/scsi/lun.c b/lib/scsi/lun.c
index fef179ef2..469050709 100644
--- a/lib/scsi/lun.c
+++ b/lib/scsi/lun.c
@@ -47,7 +47,7 @@ scsi_lun_complete_task(struct spdk_scsi_lun *lun, struct spdk_scsi_task *task)
 {
 	if (lun) {
 		TAILQ_REMOVE(&lun->tasks, task, scsi_link);
-		spdk_trace_record(TRACE_SCSI_TASK_DONE, lun->dev->id, 0, (uintptr_t)task, 0);
+		spdk_trace_record(TRACE_SCSI_TASK_DONE, lun->dev->id, 0, (uintptr_t)task);
 	}
 	task->cpl_fn(task);
 }
@@ -193,7 +193,7 @@ _scsi_lun_execute_task(struct spdk_scsi_lun *lun, struct spdk_scsi_task *task)
 	int rc;
 
 	task->status = SPDK_SCSI_STATUS_GOOD;
-	spdk_trace_record(TRACE_SCSI_TASK_START, lun->dev->id, task->length, (uintptr_t)task, 0);
+	spdk_trace_record(TRACE_SCSI_TASK_START, lun->dev->id, task->length, (uintptr_t)task);
 	TAILQ_INSERT_TAIL(&lun->tasks, task, scsi_link);
 	if (!lun->removed) {
 		/* Check the command is allowed or not when reservation is exist */
diff --git a/lib/scsi/scsi.c b/lib/scsi/scsi.c
index 947d83d90..90a76e3fd 100644
--- a/lib/scsi/scsi.c
+++ b/lib/scsi/scsi.c
@@ -34,26 +34,15 @@
 
 #include "scsi_internal.h"
 
-struct spdk_scsi_globals g_scsi;
-
 int
 spdk_scsi_init(void)
 {
-	int rc;
-
-	rc = pthread_mutex_init(&g_scsi.mutex, NULL);
-	if (rc != 0) {
-		SPDK_ERRLOG("mutex_init() failed\n");
-		return -1;
-	}
-
 	return 0;
 }
 
 void
 spdk_scsi_fini(void)
 {
-	pthread_mutex_destroy(&g_scsi.mutex);
 }
 
 SPDK_TRACE_REGISTER_FN(scsi_trace, "scsi", TRACE_GROUP_SCSI)
@@ -61,9 +50,11 @@ SPDK_TRACE_REGISTER_FN(scsi_trace, "scsi", TRACE_GROUP_SCSI)
 	spdk_trace_register_owner(OWNER_SCSI_DEV, 'd');
 	spdk_trace_register_object(OBJECT_SCSI_TASK, 't');
 	spdk_trace_register_description("SCSI_TASK_DONE", TRACE_SCSI_TASK_DONE,
-					OWNER_SCSI_DEV, OBJECT_SCSI_TASK, 0, 0, "");
+					OWNER_SCSI_DEV, OBJECT_SCSI_TASK, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 	spdk_trace_register_description("SCSI_TASK_START", TRACE_SCSI_TASK_START,
-					OWNER_SCSI_DEV, OBJECT_SCSI_TASK, 0, 0, "");
+					OWNER_SCSI_DEV, OBJECT_SCSI_TASK, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "");
 }
 
 uint64_t
diff --git a/lib/scsi/scsi_bdev.c b/lib/scsi/scsi_bdev.c
index 06270eb5d..fe6b1d204 100644
--- a/lib/scsi/scsi_bdev.c
+++ b/lib/scsi/scsi_bdev.c
@@ -34,12 +34,6 @@
 
 #include "scsi_internal.h"
 
-/*
- * TODO: move bdev SCSI error code translation tests to bdev unit test
- * and remove this include.
- */
-#include "spdk/bdev_module.h"
-
 #include "spdk/env.h"
 #include "spdk/bdev.h"
 #include "spdk/endian.h"
@@ -1397,13 +1391,13 @@ bdev_scsi_task_complete_unmap_cmd(struct spdk_bdev_io *bdev_io, bool success,
 
 	ctx->count--;
 
-	task->bdev_io = bdev_io;
-
 	if (task->status == SPDK_SCSI_STATUS_GOOD) {
 		spdk_bdev_io_get_scsi_status(bdev_io, &sc, &sk, &asc, &ascq);
 		spdk_scsi_task_set_status(task, sc, sk, asc, ascq);
 	}
 
+	spdk_bdev_free_io(bdev_io);
+
 	if (ctx->count == 0) {
 		scsi_lun_complete_task(task->lun, task);
 		free(ctx);
@@ -1620,9 +1614,17 @@ bdev_scsi_process_block(struct spdk_scsi_task *task)
 		switch (cdb[1] & 0x1f) { /* SERVICE ACTION */
 		case SPDK_SBC_SAI_READ_CAPACITY_16: {
 			uint8_t buffer[32] = {0};
+			uint32_t lbppb, lbppbe;
 
 			to_be64(&buffer[0], spdk_bdev_get_num_blocks(bdev) - 1);
 			to_be32(&buffer[8], spdk_bdev_get_data_block_size(bdev));
+			lbppb = spdk_bdev_get_physical_block_size(bdev) / spdk_bdev_get_data_block_size(bdev);
+			lbppbe = spdk_u32log2(lbppb);
+			if (lbppbe > 0xf) {
+				SPDK_ERRLOG("lbppbe(0x%x) > 0xf\n", lbppbe);
+			} else {
+				buffer[13] = lbppbe;
+			}
 			/*
 			 * Set the TPE bit to 1 to indicate thin provisioning.
 			 * The position of TPE bit is the 7th bit in 14th byte
diff --git a/lib/scsi/scsi_internal.h b/lib/scsi/scsi_internal.h
index 44cc4761e..1648e5983 100644
--- a/lib/scsi/scsi_internal.h
+++ b/lib/scsi/scsi_internal.h
@@ -213,10 +213,4 @@ int scsi2_reserve(struct spdk_scsi_task *task, uint8_t *cdb);
 int scsi2_release(struct spdk_scsi_task *task);
 int scsi2_reserve_check(struct spdk_scsi_task *task);
 
-struct spdk_scsi_globals {
-	pthread_mutex_t mutex;
-};
-
-extern struct spdk_scsi_globals g_scsi;
-
 #endif /* SPDK_SCSI_INTERNAL_H */
diff --git a/lib/thread/Makefile b/lib/thread/Makefile
index de4a09d85..846b32f52 100644
--- a/lib/thread/Makefile
+++ b/lib/thread/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 5
+SO_VER := 6
 SO_MINOR := 0
 
 C_SRCS = thread.c
diff --git a/lib/thread/spdk_thread.map b/lib/thread/spdk_thread.map
index 4f16fa7b8..94b5c5e6b 100644
--- a/lib/thread/spdk_thread.map
+++ b/lib/thread/spdk_thread.map
@@ -57,8 +57,21 @@
 	spdk_interrupt_mode_is_enabled;
 
 	# internal functions in spdk_internal/thread.h
-	spdk_poller_state_str;
+	spdk_poller_get_name;
+	spdk_poller_get_state_str;
+	spdk_poller_get_period_ticks;
+	spdk_poller_get_stats;
+	spdk_io_channel_get_io_device_name;
+	spdk_io_channel_get_ref_count;
 	spdk_io_device_get_name;
+	spdk_thread_get_first_active_poller;
+	spdk_thread_get_next_active_poller;
+	spdk_thread_get_first_timed_poller;
+	spdk_thread_get_next_timed_poller;
+	spdk_thread_get_first_paused_poller;
+	spdk_thread_get_next_paused_poller;
+	spdk_thread_get_first_io_channel;
+	spdk_thread_get_next_io_channel;
 
 	local: *;
 };
diff --git a/lib/thread/thread.c b/lib/thread/thread.c
index 4c4b5da5a..74bb83b42 100644
--- a/lib/thread/thread.c
+++ b/lib/thread/thread.c
@@ -38,11 +38,14 @@
 #include "spdk/queue.h"
 #include "spdk/string.h"
 #include "spdk/thread.h"
+#include "spdk/trace.h"
+#include "spdk/tree.h"
 #include "spdk/util.h"
 #include "spdk/fd_group.h"
 
 #include "spdk/log.h"
 #include "spdk_internal/thread.h"
+#include "thread_internal.h"
 
 #ifdef __linux__
 #include <sys/timerfd.h>
@@ -52,6 +55,108 @@
 #define SPDK_MSG_BATCH_SIZE		8
 #define SPDK_MAX_DEVICE_NAME_LEN	256
 #define SPDK_THREAD_EXIT_TIMEOUT_SEC	5
+#define SPDK_MAX_POLLER_NAME_LEN	256
+#define SPDK_MAX_THREAD_NAME_LEN	256
+
+enum spdk_poller_state {
+	/* The poller is registered with a thread but not currently executing its fn. */
+	SPDK_POLLER_STATE_WAITING,
+
+	/* The poller is currently running its fn. */
+	SPDK_POLLER_STATE_RUNNING,
+
+	/* The poller was unregistered during the execution of its fn. */
+	SPDK_POLLER_STATE_UNREGISTERED,
+
+	/* The poller is in the process of being paused.  It will be paused
+	 * during the next time it's supposed to be executed.
+	 */
+	SPDK_POLLER_STATE_PAUSING,
+
+	/* The poller is registered but currently paused.  It's on the
+	 * paused_pollers list.
+	 */
+	SPDK_POLLER_STATE_PAUSED,
+};
+
+struct spdk_poller {
+	TAILQ_ENTRY(spdk_poller)	tailq;
+	RB_ENTRY(spdk_poller)		node;
+
+	/* Current state of the poller; should only be accessed from the poller's thread. */
+	enum spdk_poller_state		state;
+
+	uint64_t			period_ticks;
+	uint64_t			next_run_tick;
+	uint64_t			run_count;
+	uint64_t			busy_count;
+	spdk_poller_fn			fn;
+	void				*arg;
+	struct spdk_thread		*thread;
+	int				interruptfd;
+	spdk_poller_set_interrupt_mode_cb set_intr_cb_fn;
+	void				*set_intr_cb_arg;
+
+	char				name[SPDK_MAX_POLLER_NAME_LEN + 1];
+};
+
+enum spdk_thread_state {
+	/* The thread is pocessing poller and message by spdk_thread_poll(). */
+	SPDK_THREAD_STATE_RUNNING,
+
+	/* The thread is in the process of termination. It reaps unregistering
+	 * poller are releasing I/O channel.
+	 */
+	SPDK_THREAD_STATE_EXITING,
+
+	/* The thread is exited. It is ready to call spdk_thread_destroy(). */
+	SPDK_THREAD_STATE_EXITED,
+};
+
+struct spdk_thread {
+	uint64_t			tsc_last;
+	struct spdk_thread_stats	stats;
+	/*
+	 * Contains pollers actively running on this thread.  Pollers
+	 *  are run round-robin. The thread takes one poller from the head
+	 *  of the ring, executes it, then puts it back at the tail of
+	 *  the ring.
+	 */
+	TAILQ_HEAD(active_pollers_head, spdk_poller)	active_pollers;
+	/**
+	 * Contains pollers running on this thread with a periodic timer.
+	 */
+	RB_HEAD(timed_pollers_tree, spdk_poller)	timed_pollers;
+	struct spdk_poller				*first_timed_poller;
+	/*
+	 * Contains paused pollers.  Pollers on this queue are waiting until
+	 * they are resumed (in which case they're put onto the active/timer
+	 * queues) or unregistered.
+	 */
+	TAILQ_HEAD(paused_pollers_head, spdk_poller)	paused_pollers;
+	struct spdk_ring		*messages;
+	int				msg_fd;
+	SLIST_HEAD(, spdk_msg)		msg_cache;
+	size_t				msg_cache_count;
+	spdk_msg_fn			critical_msg;
+	uint64_t			id;
+	enum spdk_thread_state		state;
+	int				pending_unregister_count;
+
+	TAILQ_HEAD(, spdk_io_channel)	io_channels;
+	TAILQ_ENTRY(spdk_thread)	tailq;
+
+	char				name[SPDK_MAX_THREAD_NAME_LEN + 1];
+	struct spdk_cpuset		cpumask;
+	uint64_t			exit_timeout_tsc;
+
+	/* Indicates whether this spdk_thread currently runs in interrupt. */
+	bool				in_interrupt;
+	struct spdk_fd_group		*fgrp;
+
+	/* User context allocated at the end */
+	uint8_t				ctx[0];
+};
 
 static pthread_mutex_t g_devlist_mutex = PTHREAD_MUTEX_INITIALIZER;
 
@@ -98,6 +203,45 @@ static uint32_t g_thread_count = 0;
 
 static __thread struct spdk_thread *tls_thread = NULL;
 
+#define TRACE_GROUP_THREAD		0xa
+#define TRACE_THREAD_IOCH_GET   SPDK_TPOINT_ID(TRACE_GROUP_THREAD, 0x0)
+#define TRACE_THREAD_IOCH_PUT   SPDK_TPOINT_ID(TRACE_GROUP_THREAD, 0x1)
+
+SPDK_TRACE_REGISTER_FN(thread_trace, "thread", TRACE_GROUP_THREAD)
+{
+	spdk_trace_register_description("THREAD_IOCH_GET",
+					TRACE_THREAD_IOCH_GET,
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "refcnt");
+	spdk_trace_register_description("THREAD_IOCH_PUT",
+					TRACE_THREAD_IOCH_PUT,
+					OWNER_NONE, OBJECT_NONE, 0,
+					SPDK_TRACE_ARG_TYPE_INT, "refcnt");
+}
+
+/*
+ * If this compare function returns zero when two next_run_ticks are equal,
+ * the macro RB_INSERT() returns a pointer to the element with the same
+ * next_run_tick.
+ *
+ * Fortunately, the macro RB_REMOVE() takes not a key but a pointer to the element
+ * to remove as a parameter.
+ *
+ * Hence we allow RB_INSERT() to insert elements with the same keys on the right
+ * side by returning 1 when two next_run_ticks are equal.
+ */
+static inline int
+timed_poller_compare(struct spdk_poller *poller1, struct spdk_poller *poller2)
+{
+	if (poller1->next_run_tick < poller2->next_run_tick) {
+		return -1;
+	} else {
+		return 1;
+	}
+}
+
+RB_GENERATE_STATIC(timed_pollers_tree, spdk_poller, node, timed_poller_compare);
+
 static inline struct spdk_thread *
 _get_thread(void)
 {
@@ -209,12 +353,12 @@ _free_thread(struct spdk_thread *thread)
 		free(poller);
 	}
 
-	TAILQ_FOREACH_SAFE(poller, &thread->timed_pollers, tailq, ptmp) {
+	RB_FOREACH_SAFE(poller, timed_pollers_tree, &thread->timed_pollers, ptmp) {
 		if (poller->state != SPDK_POLLER_STATE_UNREGISTERED) {
 			SPDK_WARNLOG("timed_poller %s still registered at thread exit\n",
 				     poller->name);
 		}
-		TAILQ_REMOVE(&thread->timed_pollers, poller, tailq);
+		RB_REMOVE(timed_pollers_tree, &thread->timed_pollers, poller);
 		free(poller);
 	}
 
@@ -272,7 +416,7 @@ spdk_thread_create(const char *name, struct spdk_cpuset *cpumask)
 
 	TAILQ_INIT(&thread->io_channels);
 	TAILQ_INIT(&thread->active_pollers);
-	TAILQ_INIT(&thread->timed_pollers);
+	RB_INIT(&thread->timed_pollers);
 	TAILQ_INIT(&thread->paused_pollers);
 	SLIST_INIT(&thread->msg_cache);
 	thread->msg_cache_count = 0;
@@ -370,7 +514,7 @@ thread_exit(struct spdk_thread *thread, uint64_t now)
 		}
 	}
 
-	TAILQ_FOREACH(poller, &thread->timed_pollers, tailq) {
+	RB_FOREACH(poller, timed_pollers_tree, &thread->timed_pollers) {
 		if (poller->state != SPDK_POLLER_STATE_UNREGISTERED) {
 			SPDK_INFOLOG(thread,
 				     "thread %s still has active timed poller %s\n",
@@ -559,24 +703,44 @@ msg_queue_run_batch(struct spdk_thread *thread, uint32_t max_msgs)
 static void
 poller_insert_timer(struct spdk_thread *thread, struct spdk_poller *poller, uint64_t now)
 {
-	struct spdk_poller *iter;
+	struct spdk_poller *tmp __attribute__((unused));
 
 	poller->next_run_tick = now + poller->period_ticks;
 
 	/*
-	 * Insert poller in the thread's timed_pollers list in sorted order by next scheduled
-	 * run time.
+	 * Insert poller in the thread's timed_pollers tree by next scheduled run time
+	 * as its key.
 	 */
-	TAILQ_FOREACH_REVERSE(iter, &thread->timed_pollers, timed_pollers_head, tailq) {
-		if (iter->next_run_tick <= poller->next_run_tick) {
-			TAILQ_INSERT_AFTER(&thread->timed_pollers, iter, poller, tailq);
-			return;
-		}
+	tmp = RB_INSERT(timed_pollers_tree, &thread->timed_pollers, poller);
+	assert(tmp == NULL);
+
+	/* Update the cache only if it is empty or the inserted poller is earlier than it.
+	 * RB_MIN() is not necessary here because all pollers, which has exactly the same
+	 * next_run_tick as the existing poller, are inserted on the right side.
+	 */
+	if (thread->first_timed_poller == NULL ||
+	    poller->next_run_tick < thread->first_timed_poller->next_run_tick) {
+		thread->first_timed_poller = poller;
 	}
+}
+
+#ifdef __linux__
+static inline void
+poller_remove_timer(struct spdk_thread *thread, struct spdk_poller *poller)
+{
+	struct spdk_poller *tmp __attribute__((unused));
 
-	/* No earlier pollers were found, so this poller must be the new head */
-	TAILQ_INSERT_HEAD(&thread->timed_pollers, poller, tailq);
+	tmp = RB_REMOVE(timed_pollers_tree, &thread->timed_pollers, poller);
+	assert(tmp != NULL);
+
+	/* This function is not used in any case that is performance critical.
+	 * Update the cache simply by RB_MIN() if it needs to be changed.
+	 */
+	if (thread->first_timed_poller == poller) {
+		thread->first_timed_poller = RB_MIN(timed_pollers_tree, &thread->timed_pollers);
+	}
 }
+#endif
 
 static void
 thread_insert_poller(struct spdk_thread *thread, struct spdk_poller *poller)
@@ -603,6 +767,125 @@ thread_update_stats(struct spdk_thread *thread, uint64_t end,
 	thread->tsc_last = end;
 }
 
+static inline int
+thread_execute_poller(struct spdk_thread *thread, struct spdk_poller *poller)
+{
+	int rc;
+
+	switch (poller->state) {
+	case SPDK_POLLER_STATE_UNREGISTERED:
+		TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
+		free(poller);
+		return 0;
+	case SPDK_POLLER_STATE_PAUSING:
+		TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
+		TAILQ_INSERT_TAIL(&thread->paused_pollers, poller, tailq);
+		poller->state = SPDK_POLLER_STATE_PAUSED;
+		return 0;
+	case SPDK_POLLER_STATE_WAITING:
+		break;
+	default:
+		assert(false);
+		break;
+	}
+
+	poller->state = SPDK_POLLER_STATE_RUNNING;
+	rc = poller->fn(poller->arg);
+
+	poller->run_count++;
+	if (rc > 0) {
+		poller->busy_count++;
+	}
+
+#ifdef DEBUG
+	if (rc == -1) {
+		SPDK_DEBUGLOG(thread, "Poller %s returned -1\n", poller->name);
+	}
+#endif
+
+	switch (poller->state) {
+	case SPDK_POLLER_STATE_UNREGISTERED:
+		TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
+		free(poller);
+		break;
+	case SPDK_POLLER_STATE_PAUSING:
+		TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
+		TAILQ_INSERT_TAIL(&thread->paused_pollers, poller, tailq);
+		poller->state = SPDK_POLLER_STATE_PAUSED;
+		break;
+	case SPDK_POLLER_STATE_PAUSED:
+	case SPDK_POLLER_STATE_WAITING:
+		break;
+	case SPDK_POLLER_STATE_RUNNING:
+		poller->state = SPDK_POLLER_STATE_WAITING;
+		break;
+	default:
+		assert(false);
+		break;
+	}
+
+	return rc;
+}
+
+static inline int
+thread_execute_timed_poller(struct spdk_thread *thread, struct spdk_poller *poller,
+			    uint64_t now)
+{
+	int rc;
+
+	switch (poller->state) {
+	case SPDK_POLLER_STATE_UNREGISTERED:
+		free(poller);
+		return 0;
+	case SPDK_POLLER_STATE_PAUSING:
+		TAILQ_INSERT_TAIL(&thread->paused_pollers, poller, tailq);
+		poller->state = SPDK_POLLER_STATE_PAUSED;
+		return 0;
+	case SPDK_POLLER_STATE_WAITING:
+		break;
+	default:
+		assert(false);
+		break;
+	}
+
+	poller->state = SPDK_POLLER_STATE_RUNNING;
+	rc = poller->fn(poller->arg);
+
+	poller->run_count++;
+	if (rc > 0) {
+		poller->busy_count++;
+	}
+
+#ifdef DEBUG
+	if (rc == -1) {
+		SPDK_DEBUGLOG(thread, "Timed poller %s returned -1\n", poller->name);
+	}
+#endif
+
+	switch (poller->state) {
+	case SPDK_POLLER_STATE_UNREGISTERED:
+		free(poller);
+		break;
+	case SPDK_POLLER_STATE_PAUSING:
+		TAILQ_INSERT_TAIL(&thread->paused_pollers, poller, tailq);
+		poller->state = SPDK_POLLER_STATE_PAUSED;
+		break;
+	case SPDK_POLLER_STATE_PAUSED:
+		break;
+	case SPDK_POLLER_STATE_RUNNING:
+		poller->state = SPDK_POLLER_STATE_WAITING;
+	/* fallthrough */
+	case SPDK_POLLER_STATE_WAITING:
+		poller_insert_timer(thread, poller, now);
+		break;
+	default:
+		assert(false);
+		break;
+	}
+
+	return rc;
+}
+
 static int
 thread_poll(struct spdk_thread *thread, uint32_t max_msgs, uint64_t now)
 {
@@ -629,87 +912,37 @@ thread_poll(struct spdk_thread *thread, uint32_t max_msgs, uint64_t now)
 				   active_pollers_head, tailq, tmp) {
 		int poller_rc;
 
-		if (poller->state == SPDK_POLLER_STATE_UNREGISTERED) {
-			TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
-			free(poller);
-			continue;
-		} else if (poller->state == SPDK_POLLER_STATE_PAUSING) {
-			TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
-			TAILQ_INSERT_TAIL(&thread->paused_pollers, poller, tailq);
-			poller->state = SPDK_POLLER_STATE_PAUSED;
-			continue;
-		}
-
-		poller->state = SPDK_POLLER_STATE_RUNNING;
-		poller_rc = poller->fn(poller->arg);
-
-		poller->run_count++;
-		if (poller_rc > 0) {
-			poller->busy_count++;
-		}
-
-#ifdef DEBUG
-		if (poller_rc == -1) {
-			SPDK_DEBUGLOG(thread, "Poller %s returned -1\n", poller->name);
-		}
-#endif
-
-		if (poller->state == SPDK_POLLER_STATE_UNREGISTERED) {
-			TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
-			free(poller);
-		} else if (poller->state != SPDK_POLLER_STATE_PAUSED) {
-			poller->state = SPDK_POLLER_STATE_WAITING;
-		}
-
+		poller_rc = thread_execute_poller(thread, poller);
 		if (poller_rc > rc) {
 			rc = poller_rc;
 		}
 	}
 
-	TAILQ_FOREACH_SAFE(poller, &thread->timed_pollers, tailq, tmp) {
+	poller = thread->first_timed_poller;
+	while (poller != NULL) {
 		int timer_rc = 0;
 
-		if (poller->state == SPDK_POLLER_STATE_UNREGISTERED) {
-			TAILQ_REMOVE(&thread->timed_pollers, poller, tailq);
-			free(poller);
-			continue;
-		} else if (poller->state == SPDK_POLLER_STATE_PAUSING) {
-			TAILQ_REMOVE(&thread->timed_pollers, poller, tailq);
-			TAILQ_INSERT_TAIL(&thread->paused_pollers, poller, tailq);
-			poller->state = SPDK_POLLER_STATE_PAUSED;
-			continue;
-		}
-
 		if (now < poller->next_run_tick) {
 			break;
 		}
 
-		poller->state = SPDK_POLLER_STATE_RUNNING;
-		timer_rc = poller->fn(poller->arg);
-
-		poller->run_count++;
-		if (timer_rc > 0) {
-			poller->busy_count++;
-		}
-
-#ifdef DEBUG
-		if (timer_rc == -1) {
-			SPDK_DEBUGLOG(thread, "Timed poller %s returned -1\n", poller->name);
-		}
-#endif
+		tmp = RB_NEXT(timed_pollers_tree, &thread->timed_pollers, poller);
+		RB_REMOVE(timed_pollers_tree, &thread->timed_pollers, poller);
 
-		if (poller->state == SPDK_POLLER_STATE_UNREGISTERED) {
-			TAILQ_REMOVE(&thread->timed_pollers, poller, tailq);
-			free(poller);
-		} else if (poller->state != SPDK_POLLER_STATE_PAUSED) {
-			poller->state = SPDK_POLLER_STATE_WAITING;
-			TAILQ_REMOVE(&thread->timed_pollers, poller, tailq);
-			poller_insert_timer(thread, poller, now);
+		/* Update the cache to the next timed poller in the list
+		 * only if the current poller is still the closest, otherwise,
+		 * do nothing because the cache has been already updated.
+		 */
+		if (thread->first_timed_poller == poller) {
+			thread->first_timed_poller = tmp;
 		}
 
+		timer_rc = thread_execute_timed_poller(thread, poller, now);
 		if (timer_rc > rc) {
 			rc = timer_rc;
 		}
+
+		poller = tmp;
 	}
 
 	return rc;
@@ -769,7 +1002,7 @@ spdk_thread_next_poller_expiration(struct spdk_thread *thread)
 {
 	struct spdk_poller *poller;
 
-	poller = TAILQ_FIRST(&thread->timed_pollers);
+	poller = thread->first_timed_poller;
 	if (poller) {
 		return poller->next_run_tick;
 	}
@@ -787,7 +1020,7 @@ static bool
 thread_has_unpaused_pollers(struct spdk_thread *thread)
 {
 	if (TAILQ_EMPTY(&thread->active_pollers) &&
-	    TAILQ_EMPTY(&thread->timed_pollers)) {
+	    RB_EMPTY(&thread->timed_pollers)) {
 		return false;
 	}
 
@@ -1076,7 +1309,7 @@ period_poller_set_interrupt_mode(struct spdk_poller *poller, void *cb_arg, bool
 		 */
 		now_tick = now_tick - poller->period_ticks + ticks * old_tv.it_value.tv_sec + \
 			   (ticks * old_tv.it_value.tv_nsec) / SPDK_SEC_TO_NSEC;
-		TAILQ_REMOVE(&poller->thread->timed_pollers, poller, tailq);
+		poller_remove_timer(poller->thread, poller);
 		poller_insert_timer(poller->thread, poller, now_tick);
 	}
 }
@@ -1197,6 +1430,22 @@ spdk_poller_register_interrupt(struct spdk_poller *poller,
 	}
 }
 
+static uint64_t
+convert_us_to_ticks(uint64_t us)
+{
+	uint64_t quotient, remainder, ticks;
+
+	if (us) {
+		quotient = us / SPDK_SEC_TO_USEC;
+		remainder = us % SPDK_SEC_TO_USEC;
+		ticks = spdk_get_ticks_hz();
+
+		return ticks * quotient + (ticks * remainder) / SPDK_SEC_TO_USEC;
+	} else {
+		return 0;
+	}
+}
+
 static struct spdk_poller *
 poller_register(spdk_poller_fn fn,
 		void *arg,
@@ -1205,7 +1454,6 @@ poller_register(spdk_poller_fn fn,
 {
 	struct spdk_thread *thread;
 	struct spdk_poller *poller;
-	uint64_t quotient, remainder, ticks;
 
 	thread = spdk_get_thread();
 	if (!thread) {
@@ -1236,15 +1484,7 @@ poller_register(spdk_poller_fn fn,
 	poller->thread = thread;
 	poller->interruptfd = -1;
 
-	if (period_microseconds) {
-		quotient = period_microseconds / SPDK_SEC_TO_USEC;
-		remainder = period_microseconds % SPDK_SEC_TO_USEC;
-		ticks = spdk_get_ticks_hz();
-
-		poller->period_ticks = ticks * quotient + (ticks * remainder) / SPDK_SEC_TO_USEC;
-	} else {
-		poller->period_ticks = 0;
-	}
+	poller->period_ticks = convert_us_to_ticks(period_microseconds);
 
 	if (spdk_interrupt_mode_is_enabled()) {
 		int rc;
@@ -1344,34 +1584,35 @@ spdk_poller_pause(struct spdk_poller *poller)
 {
 	struct spdk_thread *thread;
 
-	if (poller->state == SPDK_POLLER_STATE_PAUSED ||
-	    poller->state == SPDK_POLLER_STATE_PAUSING) {
+	thread = spdk_get_thread();
+	if (!thread) {
+		assert(false);
 		return;
 	}
 
-	thread = spdk_get_thread();
-	if (!thread) {
+	if (poller->thread != thread) {
+		SPDK_ERRLOG("different from the thread that called spdk_poller_pause()\n");
 		assert(false);
 		return;
 	}
 
-	/* If a poller is paused from within itself, we can immediately move it
-	 * on the paused_pollers list.  Otherwise we just set its state to
-	 * SPDK_POLLER_STATE_PAUSING and let spdk_thread_poll() move it.  It
-	 * allows a poller to be paused from another one's context without
-	 * breaking the TAILQ_FOREACH_REVERSE_SAFE iteration.
+	/* We just set its state to SPDK_POLLER_STATE_PAUSING and let
+	 * spdk_thread_poll() move it. It allows a poller to be paused from
+	 * another one's context without breaking the TAILQ_FOREACH_REVERSE_SAFE
+	 * iteration, or from within itself without breaking the logic to always
+	 * remove the closest timed poller in the TAILQ_FOREACH_SAFE iteration.
 	 */
-	if (poller->state != SPDK_POLLER_STATE_RUNNING) {
+	switch (poller->state) {
+	case SPDK_POLLER_STATE_PAUSED:
+	case SPDK_POLLER_STATE_PAUSING:
+		break;
+	case SPDK_POLLER_STATE_RUNNING:
+	case SPDK_POLLER_STATE_WAITING:
 		poller->state = SPDK_POLLER_STATE_PAUSING;
-	} else {
-		if (poller->period_ticks > 0) {
-			TAILQ_REMOVE(&thread->timed_pollers, poller, tailq);
-		} else {
-			TAILQ_REMOVE(&thread->active_pollers, poller, tailq);
-		}
-
-		TAILQ_INSERT_TAIL(&thread->paused_pollers, poller, tailq);
-		poller->state = SPDK_POLLER_STATE_PAUSED;
+		break;
+	default:
+		assert(false);
+		break;
 	}
 }
 
@@ -1380,35 +1621,51 @@ spdk_poller_resume(struct spdk_poller *poller)
 {
 	struct spdk_thread *thread;
 
-	if (poller->state != SPDK_POLLER_STATE_PAUSED &&
-	    poller->state != SPDK_POLLER_STATE_PAUSING) {
+	thread = spdk_get_thread();
+	if (!thread) {
+		assert(false);
 		return;
 	}
 
-	thread = spdk_get_thread();
-	if (!thread) {
+	if (poller->thread != thread) {
+		SPDK_ERRLOG("different from the thread that called spdk_poller_resume()\n");
 		assert(false);
 		return;
 	}
 
 	/* If a poller is paused it has to be removed from the paused pollers
-	 * list and put on the active / timer list depending on its
+	 * list and put on the active list or timer tree depending on its
 	 * period_ticks.  If a poller is still in the process of being paused,
 	 * we just need to flip its state back to waiting, as it's already on
-	 * the appropriate list.
+	 * the appropriate list or tree.
 	 */
-	if (poller->state == SPDK_POLLER_STATE_PAUSED) {
+	switch (poller->state) {
+	case SPDK_POLLER_STATE_PAUSED:
 		TAILQ_REMOVE(&thread->paused_pollers, poller, tailq);
 		thread_insert_poller(thread, poller);
+	/* fallthrough */
+	case SPDK_POLLER_STATE_PAUSING:
+		poller->state = SPDK_POLLER_STATE_WAITING;
+		break;
+	case SPDK_POLLER_STATE_RUNNING:
+	case SPDK_POLLER_STATE_WAITING:
+		break;
+	default:
+		assert(false);
+		break;
 	}
+}
 
-	poller->state = SPDK_POLLER_STATE_WAITING;
+const char *
+spdk_poller_get_name(struct spdk_poller *poller)
+{
+	return poller->name;
 }
 
 const char *
-spdk_poller_state_str(enum spdk_poller_state state)
+spdk_poller_get_state_str(struct spdk_poller *poller)
 {
-	switch (state) {
+	switch (poller->state) {
 	case SPDK_POLLER_STATE_WAITING:
 		return "waiting";
 	case SPDK_POLLER_STATE_RUNNING:
@@ -1424,6 +1681,67 @@ spdk_poller_state_str(enum spdk_poller_state state)
 	}
 }
 
+uint64_t
+spdk_poller_get_period_ticks(struct spdk_poller *poller)
+{
+	return poller->period_ticks;
+}
+
+void
+spdk_poller_get_stats(struct spdk_poller *poller, struct spdk_poller_stats *stats)
+{
+	stats->run_count = poller->run_count;
+	stats->busy_count = poller->busy_count;
+}
+
+struct spdk_poller *
+spdk_thread_get_first_active_poller(struct spdk_thread *thread)
+{
+	return TAILQ_FIRST(&thread->active_pollers);
+}
+
+struct spdk_poller *
+spdk_thread_get_next_active_poller(struct spdk_poller *prev)
+{
+	return TAILQ_NEXT(prev, tailq);
+}
+
+struct spdk_poller *
+spdk_thread_get_first_timed_poller(struct spdk_thread *thread)
+{
+	return RB_MIN(timed_pollers_tree, &thread->timed_pollers);
+}
+
+struct spdk_poller *
+spdk_thread_get_next_timed_poller(struct spdk_poller *prev)
+{
+	return RB_NEXT(timed_pollers_tree, &thread->timed_pollers, prev);
+}
+
+struct spdk_poller *
+spdk_thread_get_first_paused_poller(struct spdk_thread *thread)
+{
+	return TAILQ_FIRST(&thread->paused_pollers);
+}
+
+struct spdk_poller *
+spdk_thread_get_next_paused_poller(struct spdk_poller *prev)
+{
+	return TAILQ_NEXT(prev, tailq);
+}
+
+struct spdk_io_channel *
+spdk_thread_get_first_io_channel(struct spdk_thread *thread)
+{
+	return TAILQ_FIRST(&thread->io_channels);
+}
+
+struct spdk_io_channel *
+spdk_thread_get_next_io_channel(struct spdk_io_channel *prev)
+{
+	return TAILQ_NEXT(prev, tailq);
+}
+
 struct call_thread {
 	struct spdk_thread *cur_thread;
 	spdk_msg_fn fn;
@@ -1527,7 +1845,7 @@ spdk_thread_set_interrupt_mode(bool enable_interrupt)
 	}
 
 	/* Set pollers to expected mode */
-	TAILQ_FOREACH_SAFE(poller, &thread->timed_pollers, tailq, tmp) {
+	RB_FOREACH_SAFE(poller, timed_pollers_tree, &thread->timed_pollers, tmp) {
 		poller_set_interrupt_mode(poller, enable_interrupt);
 	}
 	TAILQ_FOREACH_SAFE(poller, &thread->active_pollers, tailq, tmp) {
@@ -1741,6 +2059,8 @@ spdk_get_io_channel(void *io_device)
 			 *  thread, so return it.
 			 */
 			pthread_mutex_unlock(&g_devlist_mutex);
+			spdk_trace_record(TRACE_THREAD_IOCH_GET, 0, 0,
+					  (uint64_t)spdk_io_channel_get_ctx(ch), ch->ref);
 			return ch;
 		}
 	}
@@ -1776,6 +2096,7 @@ spdk_get_io_channel(void *io_device)
 		return NULL;
 	}
 
+	spdk_trace_record(TRACE_THREAD_IOCH_GET, 0, 0, (uint64_t)spdk_io_channel_get_ctx(ch), 1);
 	return ch;
 }
 
@@ -1842,6 +2163,9 @@ spdk_put_io_channel(struct spdk_io_channel *ch)
 	struct spdk_thread *thread;
 	int rc __attribute__((unused));
 
+	spdk_trace_record(TRACE_THREAD_IOCH_PUT, 0, 0,
+			  (uint64_t)spdk_io_channel_get_ctx(ch), ch->ref);
+
 	thread = spdk_get_thread();
 	if (!thread) {
 		SPDK_ERRLOG("called from non-SPDK thread\n");
@@ -1886,6 +2210,18 @@ spdk_io_channel_get_io_device(struct spdk_io_channel *ch)
 	return ch->dev->io_device;
 }
 
+const char *
+spdk_io_channel_get_io_device_name(struct spdk_io_channel *ch)
+{
+	return spdk_io_device_get_name(ch->dev);
+}
+
+int
+spdk_io_channel_get_ref_count(struct spdk_io_channel *ch)
+{
+	return ch->ref;
+}
+
 struct spdk_io_channel_iter {
 	void *io_device;
 	struct io_device *dev;
diff --git a/lib/thread/thread_internal.h b/lib/thread/thread_internal.h
new file mode 100644
index 000000000..e8de39513
--- /dev/null
+++ b/lib/thread/thread_internal.h
@@ -0,0 +1,68 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_THREAD_INTERNAL_H_
+#define SPDK_THREAD_INTERNAL_H_
+
+#include "spdk/assert.h"
+#include "spdk/queue.h"
+#include "spdk/thread.h"
+
+/**
+ * \brief Represents a per-thread channel for accessing an I/O device.
+ *
+ * An I/O device may be a physical entity (i.e. NVMe controller) or a software
+ *  entity (i.e. a blobstore).
+ *
+ * This structure is not part of the API - all accesses should be done through
+ *  spdk_io_channel function calls.
+ */
+struct spdk_io_channel {
+	struct spdk_thread		*thread;
+	struct io_device		*dev;
+	uint32_t			ref;
+	uint32_t			destroy_ref;
+	TAILQ_ENTRY(spdk_io_channel)	tailq;
+	spdk_io_channel_destroy_cb	destroy_cb;
+
+	uint8_t				_padding[48];
+	/*
+	 * Modules will allocate extra memory off the end of this structure
+	 *  to store references to hardware-specific references (i.e. NVMe queue
+	 *  pairs, or references to child device spdk_io_channels (i.e.
+	 *  virtual bdevs).
+	 */
+};
+
+SPDK_STATIC_ASSERT(sizeof(struct spdk_io_channel) == SPDK_IO_CHANNEL_STRUCT_SIZE, "incorrect size");
+
+#endif /* SPDK_THREAD_INTERNAL_H_ */
diff --git a/lib/trace/Makefile b/lib/trace/Makefile
index 43e221f39..75c4f3460 100644
--- a/lib/trace/Makefile
+++ b/lib/trace/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-SO_VER := 3
+SO_VER := 4
 SO_MINOR := 0
 
 C_SRCS = trace.c trace_flags.c trace_rpc.c
diff --git a/lib/trace/spdk_trace.map b/lib/trace/spdk_trace.map
index 14a03b337..303863f71 100644
--- a/lib/trace/spdk_trace.map
+++ b/lib/trace/spdk_trace.map
@@ -15,6 +15,7 @@
 	spdk_trace_register_owner;
 	spdk_trace_register_object;
 	spdk_trace_register_description;
+	spdk_trace_register_description_ext;
 	spdk_trace_get_first_register_fn;
 	spdk_trace_get_next_register_fn;
 	spdk_trace_enable_tpoint_group;
diff --git a/lib/trace/trace.c b/lib/trace/trace.c
index 621c52aae..bbc8154db 100644
--- a/lib/trace/trace.c
+++ b/lib/trace/trace.c
@@ -47,12 +47,15 @@ struct spdk_trace_histories *g_trace_histories;
 
 void
 _spdk_trace_record(uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id, uint32_t size,
-		   uint64_t object_id, uint64_t arg1)
+		   uint64_t object_id, int num_args, ...)
 {
 	struct spdk_trace_history *lcore_history;
 	struct spdk_trace_entry *next_entry;
-	unsigned lcore;
-	uint64_t next_circular_entry;
+	struct spdk_trace_tpoint *tpoint;
+	const char *strval;
+	unsigned lcore, i, offset;
+	uint64_t intval, next_circular_entry;
+	va_list vl;
 
 	lcore = spdk_env_get_current_core();
 	if (lcore >= SPDK_TRACE_MAX_LCORE) {
@@ -74,7 +77,34 @@ _spdk_trace_record(uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id, uint32_
 	next_entry->poller_id = poller_id;
 	next_entry->size = size;
 	next_entry->object_id = object_id;
-	next_entry->arg1 = arg1;
+
+	tpoint = &g_trace_flags->tpoint[tpoint_id];
+	/* Make sure that the number of arguments passed match tracepoint definition */
+	if (tpoint->num_args != num_args) {
+		assert(0 && "Unexpected number of tracepoint arguments");
+		return;
+	}
+
+	va_start(vl, num_args);
+	for (i = 0, offset = 0; i < tpoint->num_args; ++i) {
+		switch (tpoint->args[i].type) {
+		case SPDK_TRACE_ARG_TYPE_STR:
+			strval = va_arg(vl, const char *);
+			snprintf(&next_entry->args[offset], tpoint->args[i].size, "%s", strval);
+			break;
+		case SPDK_TRACE_ARG_TYPE_INT:
+		case SPDK_TRACE_ARG_TYPE_PTR:
+			intval = va_arg(vl, uint64_t);
+			memcpy(&next_entry->args[offset], &intval, sizeof(intval));
+			break;
+		default:
+			assert(0 && "Invalid trace argument type");
+			break;
+		}
+
+		offset += tpoint->args[i].size;
+	}
+	va_end(vl);
 
 	/* Ensure all elements of the trace entry are visible to outside trace tools */
 	spdk_smp_wmb();
diff --git a/lib/trace/trace_flags.c b/lib/trace/trace_flags.c
index 67bf21037..ec4845a7d 100644
--- a/lib/trace/trace_flags.c
+++ b/lib/trace/trace_flags.c
@@ -277,35 +277,102 @@ spdk_trace_register_object(uint8_t type, char id_prefix)
 	object->id_prefix = id_prefix;
 }
 
-void
-spdk_trace_register_description(const char *name, uint16_t tpoint_id, uint8_t owner_type,
-				uint8_t object_type, uint8_t new_object,
-				uint8_t arg1_type, const char *arg1_name)
+static void
+trace_register_description(const struct spdk_trace_tpoint_opts *opts)
 {
 	struct spdk_trace_tpoint *tpoint;
+	size_t i, remaining_size, max_name_length;
+
+	assert(opts->tpoint_id != 0);
+	assert(opts->tpoint_id < SPDK_TRACE_MAX_TPOINT_ID);
+
+	if (strnlen(opts->name, sizeof(tpoint->name)) == sizeof(tpoint->name)) {
+		SPDK_ERRLOG("name (%s) too long\n", opts->name);
+	}
+
+	tpoint = &g_trace_flags->tpoint[opts->tpoint_id];
+	assert(tpoint->tpoint_id == 0);
 
-	assert(tpoint_id != 0);
-	assert(tpoint_id < SPDK_TRACE_MAX_TPOINT_ID);
+	snprintf(tpoint->name, sizeof(tpoint->name), "%s", opts->name);
+	tpoint->tpoint_id = opts->tpoint_id;
+	tpoint->object_type = opts->object_type;
+	tpoint->owner_type = opts->owner_type;
+	tpoint->new_object = opts->new_object;
+
+	max_name_length = sizeof(tpoint->args[0].name);
+	remaining_size = sizeof(((struct spdk_trace_entry *)0)->args);
+
+	for (i = 0; i < SPDK_TRACE_MAX_ARGS_COUNT; ++i) {
+		if (!opts->args[i].name || opts->args[i].name[0] == '\0') {
+			break;
+		}
+
+		switch (opts->args[i].type) {
+		case SPDK_TRACE_ARG_TYPE_INT:
+		case SPDK_TRACE_ARG_TYPE_PTR:
+			/* The integers and pointers have to be exactly 64b long */
+			assert(opts->args[i].size == sizeof(uint64_t));
+			break;
+		case SPDK_TRACE_ARG_TYPE_STR:
+			/* Strings need to have at least one byte for the NULL terminator */
+			assert(opts->args[i].size > 0);
+			break;
+		default:
+			assert(0 && "invalid trace argument type");
+			break;
+		}
+
+		assert(remaining_size >= opts->args[i].size && "tpoint exceeds max size");
+		remaining_size -= opts->args[i].size;
+
+		if (strnlen(opts->args[i].name, max_name_length) == max_name_length) {
+			SPDK_ERRLOG("argument name (%s) is too long\n", opts->args[i].name);
+		}
+
+		snprintf(tpoint->args[i].name, sizeof(tpoint->args[i].name),
+			 "%s", opts->args[i].name);
+		tpoint->args[i].type = opts->args[i].type;
+		tpoint->args[i].size = opts->args[i].size;
+	}
+
+	tpoint->num_args = i;
+}
+
+void
+spdk_trace_register_description_ext(const struct spdk_trace_tpoint_opts *opts, size_t num_opts)
+{
+	size_t i;
 
 	if (g_trace_flags == NULL) {
 		SPDK_ERRLOG("trace is not initialized\n");
 		return;
 	}
 
-	if (strnlen(name, sizeof(tpoint->name)) == sizeof(tpoint->name)) {
-		SPDK_ERRLOG("name (%s) too long\n", name);
+	for (i = 0; i < num_opts; ++i) {
+		trace_register_description(&opts[i]);
 	}
+}
 
-	tpoint = &g_trace_flags->tpoint[tpoint_id];
-	assert(tpoint->tpoint_id == 0);
+void
+spdk_trace_register_description(const char *name, uint16_t tpoint_id, uint8_t owner_type,
+				uint8_t object_type, uint8_t new_object,
+				uint8_t arg1_type, const char *arg1_name)
+{
+	struct spdk_trace_tpoint_opts opts = {
+		.name = name,
+		.tpoint_id = tpoint_id,
+		.owner_type = owner_type,
+		.object_type = object_type,
+		.new_object = new_object,
+		.args = {{
+				.name = arg1_name,
+				.type = arg1_type,
+				.size = sizeof(uint64_t)
+			}
+		}
+	};
 
-	snprintf(tpoint->name, sizeof(tpoint->name), "%s", name);
-	tpoint->tpoint_id = tpoint_id;
-	tpoint->object_type = object_type;
-	tpoint->owner_type = owner_type;
-	tpoint->new_object = new_object;
-	tpoint->arg1_type = arg1_type;
-	snprintf(tpoint->arg1_name, sizeof(tpoint->arg1_name), "%s", arg1_name);
+	spdk_trace_register_description_ext(&opts, 1);
 }
 
 void
diff --git a/lib/util/Makefile b/lib/util/Makefile
index f4eb147c2..f08543de6 100644
--- a/lib/util/Makefile
+++ b/lib/util/Makefile
@@ -35,11 +35,11 @@ SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
 SO_VER := 3
-SO_MINOR := 0
+SO_MINOR := 1
 
 C_SRCS = base64.c bit_array.c cpuset.c crc16.c crc32.c crc32c.c crc32_ieee.c \
 	 dif.c fd.c file.c iov.c math.c pipe.c strerror_tls.c string.c uuid.c \
-	 fd_group.c
+	 fd_group.c zipf.c
 LIBNAME = util
 LOCAL_SYS_LIBS = -luuid
 
diff --git a/lib/util/crc32c.c b/lib/util/crc32c.c
index 9acd8d80f..685905229 100644
--- a/lib/util/crc32c.c
+++ b/lib/util/crc32c.c
@@ -131,3 +131,21 @@ spdk_crc32c_update(const void *buf, size_t len, uint32_t crc)
 }
 
 #endif
+
+uint32_t
+spdk_crc32c_iov_update(struct iovec *iov, int iovcnt, uint32_t crc32c)
+{
+	int i;
+
+	if (iov == NULL) {
+		return crc32c;
+	}
+
+	for (i = 0; i < iovcnt; i++) {
+		assert(iov[i].iov_base != NULL);
+		assert(iov[i].iov_len != 0);
+		crc32c = spdk_crc32c_update(iov[i].iov_base, iov[i].iov_len, crc32c);
+	}
+
+	return crc32c;
+}
diff --git a/lib/util/spdk_util.map b/lib/util/spdk_util.map
index 31b191af0..88705e416 100644
--- a/lib/util/spdk_util.map
+++ b/lib/util/spdk_util.map
@@ -61,6 +61,7 @@
 	# public functions in crc32.h
 	spdk_crc32_ieee_update;
 	spdk_crc32c_update;
+	spdk_crc32c_iov_update;
 
 	# public functions in dif.h
 	spdk_dif_ctx_init;
@@ -144,5 +145,10 @@
 	spdk_fd_group_event_modify;
 	spdk_fd_group_get_fd;
 
+	# public functions in zipf.h
+	spdk_zipf_create;
+	spdk_zipf_free;
+	spdk_zipf_generate;
+
 	local: *;
 };
diff --git a/lib/util/zipf.c b/lib/util/zipf.c
new file mode 100644
index 000000000..e7a1106bd
--- /dev/null
+++ b/lib/util/zipf.c
@@ -0,0 +1,139 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright(c) Intel Corporation. All rights reserved.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/stdinc.h"
+#include "spdk/util.h"
+#include "spdk/zipf.h"
+
+struct spdk_zipf {
+	uint64_t	range;
+	double		alpha;
+	double		eta;
+	double		theta;
+	double		zetan;
+	double		val1_limit;
+	uint32_t	seed;
+};
+
+static double
+zeta_increment(uint64_t n, double theta)
+{
+	return pow((double) 1.0 / (n + 1), theta);
+}
+
+static double
+zeta(uint64_t range, double theta)
+{
+	double zetan = 0;
+	double inc1, inc2;
+	uint64_t i, calc, count;
+	const uint32_t ZIPF_MAX_ZETA_CALC = 10 * 1000 * 1000;
+	const uint32_t ZIPF_ZETA_ESTIMATE = 1 * 1000 * 1000;
+
+	/* Cumulate zeta discretely for the first ZIPF_MAX_ZETA_CALC
+	 * entries in the range.
+	 */
+	calc = spdk_min(ZIPF_MAX_ZETA_CALC, range);
+	for (i = 0; i < calc; i++) {
+		zetan += zeta_increment(i, theta);
+	}
+
+	/* For the remaining values in the range, increment zetan
+	 * with an approximation for every ZIPF_ZETA_ESTIMATE
+	 * entries.  We will take an average of the increment
+	 * for (i) and (i + ZIPF_ZETA_ESTIMATE), and then multiply
+	 * that by ZIPF_ZETA_ESTIMATE.
+	 *
+	 * Of course, we'll cap ZIPF_ZETA_ESTIMATE to something
+	 * smaller if necessary at the end of the range.
+	 */
+	while (i < range) {
+		count = spdk_min(ZIPF_ZETA_ESTIMATE, range - i);
+		inc1 = zeta_increment(i, theta);
+		inc2 = zeta_increment(i + count, theta);
+		zetan += (inc1 + inc2) * count / 2;
+		i += count;
+	}
+
+	return zetan;
+}
+
+struct spdk_zipf *
+spdk_zipf_create(uint64_t range, double theta, uint32_t seed)
+{
+	struct spdk_zipf *zipf;
+
+	zipf = calloc(1, sizeof(*zipf));
+	if (zipf == NULL) {
+		return NULL;
+	}
+
+	zipf->range = range;
+	zipf->seed = seed;
+
+	zipf->theta = theta;
+	zipf->alpha = 1.0 / (1.0 - zipf->theta);
+	zipf->zetan = zeta(range, theta);
+	zipf->eta = (1.0 - pow(2.0 / zipf->range, 1.0 - zipf->theta)) /
+		    (1.0 - zeta(2, theta) / zipf->zetan);
+	zipf->val1_limit = 1.0 + pow(0.5, zipf->theta);
+
+	return zipf;
+}
+
+void
+spdk_zipf_free(struct spdk_zipf **zipfp)
+{
+	assert(zipfp != NULL);
+	free(*zipfp);
+	*zipfp = NULL;
+}
+
+uint64_t
+spdk_zipf_generate(struct spdk_zipf *zipf)
+{
+	double randu, randz;
+	uint64_t val;
+
+	randu = (double)rand_r(&zipf->seed) / RAND_MAX;
+	randz = randu * zipf->zetan;
+
+	if (randz < 1.0) {
+		return 0;
+	} else if (randz < zipf->val1_limit) {
+		return 1;
+	} else {
+		val = zipf->range * pow(zipf->eta * (randu - 1.0) + 1.0, zipf->alpha);
+		return val % zipf->range;
+	}
+}
diff --git a/lib/vfio_user/vfio_user.c b/lib/vfio_user/vfio_user.c
index 4024dc105..ab1a82efe 100644
--- a/lib/vfio_user/vfio_user.c
+++ b/lib/vfio_user/vfio_user.c
@@ -285,10 +285,10 @@ vfio_user_get_dev_region_info(struct vfio_device *dev, struct vfio_region_info *
 }
 
 int
-vfio_user_get_dev_info(struct vfio_device *dev, struct vfio_device_info *dev_info,
+vfio_user_get_dev_info(struct vfio_device *dev, struct vfio_user_device_info *dev_info,
 		       size_t buf_len)
 {
-	dev_info->argsz = sizeof(struct vfio_device_info);
+	dev_info->argsz = sizeof(struct vfio_user_device_info);
 	return vfio_user_dev_send_request(dev, VFIO_USER_DEVICE_GET_INFO,
 					  dev_info, dev_info->argsz, buf_len, NULL, 0);
 }
@@ -296,18 +296,25 @@ vfio_user_get_dev_info(struct vfio_device *dev, struct vfio_device_info *dev_inf
 int
 vfio_user_dev_dma_map_unmap(struct vfio_device *dev, struct vfio_memory_region *mr, bool map)
 {
-	struct vfio_user_dma_region region;
+	struct vfio_user_dma_map dma_map = { 0 };
+	struct vfio_user_dma_unmap dma_unmap = { 0 };
 
-	region.addr = mr->iova;
-	region.size = mr->size;
-	region.offset = mr->offset;
 	if (map) {
-		region.flags = VFIO_USER_F_DMA_REGION_MAPPABLE;
-		region.prot = PROT_READ | PROT_WRITE;
+		dma_map.argsz = sizeof(struct vfio_user_dma_map);
+		dma_map.addr = mr->iova;
+		dma_map.size = mr->size;
+		dma_map.offset = mr->offset;
+		dma_map.flags = VFIO_USER_F_DMA_REGION_READ | VFIO_USER_F_DMA_REGION_WRITE;
+
+		return vfio_user_dev_send_request(dev, VFIO_USER_DMA_MAP,
+						  &dma_map, sizeof(dma_map), sizeof(dma_map), &mr->fd, 1);
+	} else {
+		dma_unmap.argsz = sizeof(struct vfio_user_dma_unmap);
+		dma_unmap.addr = mr->iova;
+		dma_unmap.size = mr->size;
+		return vfio_user_dev_send_request(dev, VFIO_USER_DMA_UNMAP,
+						  &dma_unmap, sizeof(dma_unmap), sizeof(dma_unmap), &mr->fd, 1);
 	}
-
-	return vfio_user_dev_send_request(dev, map ? VFIO_USER_DMA_MAP : VFIO_USER_DMA_UNMAP,
-					  &region, sizeof(region), sizeof(region), &mr->fd, 1);
 }
 
 int
diff --git a/lib/vfio_user/vfio_user_internal.h b/lib/vfio_user/vfio_user_internal.h
index f14cc87a4..d03b328e5 100644
--- a/lib/vfio_user/vfio_user_internal.h
+++ b/lib/vfio_user/vfio_user_internal.h
@@ -34,6 +34,7 @@
 #define _VFIO_INTERNAL_H
 
 #include <linux/vfio.h>
+#include "spdk/vfio_user_spec.h"
 
 #define VFIO_USER_MAJOR_VER			0
 #define VFIO_USER_MINOR_VER			1
@@ -85,7 +86,7 @@ struct vfio_device {
 };
 
 int vfio_user_dev_setup(struct vfio_device *dev);
-int vfio_user_get_dev_info(struct vfio_device *dev, struct vfio_device_info *dev_info,
+int vfio_user_get_dev_info(struct vfio_device *dev, struct vfio_user_device_info *dev_info,
 			   size_t buf_len);
 int vfio_user_get_dev_region_info(struct vfio_device *dev, struct vfio_region_info *region_info,
 				  size_t buf_len, int *fds, int num_fds);
diff --git a/lib/vfio_user/vfio_user_pci.c b/lib/vfio_user/vfio_user_pci.c
index 6e3b2c34e..37758e293 100644
--- a/lib/vfio_user/vfio_user_pci.c
+++ b/lib/vfio_user/vfio_user_pci.c
@@ -369,7 +369,7 @@ spdk_vfio_user_setup(const char *path)
 {
 	int ret;
 	struct vfio_device *device = NULL;
-	struct vfio_device_info dev_info = {};
+	struct vfio_user_device_info dev_info = {};
 
 	device = calloc(1, sizeof(*device));
 	if (!device) {
diff --git a/lib/vhost/vhost_blk.c b/lib/vhost/vhost_blk.c
index 60eb8e0f3..031ca6b36 100644
--- a/lib/vhost/vhost_blk.c
+++ b/lib/vhost/vhost_blk.c
@@ -609,7 +609,7 @@ process_blk_request(struct spdk_vhost_blk_task *task,
 			return -1;
 		}
 		task->used_len = spdk_min((size_t)VIRTIO_BLK_ID_BYTES, task->iovs[1].iov_len);
-		spdk_strcpy_pad(task->iovs[1].iov_base, spdk_bdev_get_product_name(bvdev->bdev),
+		spdk_strcpy_pad(task->iovs[1].iov_base, spdk_bdev_get_name(bvdev->bdev),
 				task->used_len, ' ');
 		blk_request_finish(true, task);
 		break;
diff --git a/lib/virtio/virtio_pci.c b/lib/virtio/virtio_pci.c
index 8d06172ff..0d341cd5c 100644
--- a/lib/virtio/virtio_pci.c
+++ b/lib/virtio/virtio_pci.c
@@ -89,9 +89,9 @@ static bool g_sigset = false;
 #define PCI_CAP_ID_MSIX		0x11
 
 static void
-virtio_pci_dev_sigbus_handler(siginfo_t *info, void *ctx)
+virtio_pci_dev_sigbus_handler(const void *failure_addr, void *ctx)
 {
-	void *map_address = NULL;;
+	void *map_address = NULL;
 	uint16_t flag = 0;
 	int i;
 
diff --git a/mk/spdk.app.mk b/mk/spdk.app.mk
index 61a23d5e7..5020a20ab 100644
--- a/mk/spdk.app.mk
+++ b/mk/spdk.app.mk
@@ -52,6 +52,10 @@ LIBS += $(SPDK_LIB_LINKER_ARGS)
 
 CLEAN_FILES = $(APP)
 
+ifeq ($(findstring vfio_user,$(SPDK_LIB_FILES)),vfio_user)
+VFIO_USER_LIB_FILE=$(VFIO_USER_LIBRARY_DIR)/libvfio-user.a
+endif
+
 all : $(APP)
 	@:
 
@@ -63,7 +67,7 @@ uninstall: empty_rule
 empty_rule:
 	@:
 
-$(APP) : $(OBJS) $(SPDK_LIB_FILES) $(ENV_LIBS)
+$(APP) : $(OBJS) $(SPDK_LIB_FILES) $(ENV_LIBS) $(VFIO_USER_LIB_FILE)
 	$(LINK_C)
 
 clean :
diff --git a/mk/spdk.common.mk b/mk/spdk.common.mk
index 897be4d21..b76d8d0e4 100644
--- a/mk/spdk.common.mk
+++ b/mk/spdk.common.mk
@@ -198,6 +198,12 @@ endif
 VFIO_USER_INSTALL_DIR=$(VFIO_USER_DIR)/build
 VFIO_USER_INCLUDE_DIR=$(VFIO_USER_INSTALL_DIR)/usr/local/include
 VFIO_USER_LIBRARY_DIR=$(VFIO_USER_INSTALL_DIR)/usr/local/lib64
+ifeq (,$(wildcard $(VFIO_USER_LIBRARY_DIR)/.))
+# Some Linux distros use lib instead of lib64
+# for default installations
+VFIO_USER_LIBRARY_DIR=$(VFIO_USER_INSTALL_DIR)/usr/local/lib
+endif
+
 CFLAGS += -I$(VFIO_USER_INCLUDE_DIR)
 LDFLAGS += -L$(VFIO_USER_LIBRARY_DIR)
 SYS_LIBS += -Wl,-Bstatic -lvfio-user -Wl,-Bdynamic -ljson-c
@@ -280,7 +286,7 @@ endif
 
 ifneq ($(OS),Windows)
 COMMON_CFLAGS += -pthread
-LDFLAGS += -pthread
+SYS_LIBS += -pthread
 endif
 
 CFLAGS   += $(COMMON_CFLAGS) -Wno-pointer-sign -Wstrict-prototypes -Wold-style-definition -std=gnu99
@@ -289,6 +295,7 @@ CXXFLAGS += $(COMMON_CFLAGS)
 SYS_LIBS += -lrt
 SYS_LIBS += -luuid
 SYS_LIBS += -lcrypto
+SYS_LIBS += -lm
 
 ifneq ($(CONFIG_NVME_CUSE)$(CONFIG_FUSE),nn)
 SYS_LIBS += -lfuse3
diff --git a/mk/spdk.lib_deps.mk b/mk/spdk.lib_deps.mk
index ece8233ff..c2203d28b 100644
--- a/mk/spdk.lib_deps.mk
+++ b/mk/spdk.lib_deps.mk
@@ -59,7 +59,7 @@ DEPDIRS-conf := log util
 DEPDIRS-json := log util
 DEPDIRS-rdma := log util
 DEPDIRS-reduce := log util
-DEPDIRS-thread := log util
+DEPDIRS-thread := log util trace
 
 DEPDIRS-nvme := log sock util
 ifeq ($(CONFIG_RDMA),y)
@@ -83,7 +83,8 @@ DEPDIRS-trace := log util $(JSON_LIBS)
 
 DEPDIRS-bdev := log util thread $(JSON_LIBS) notify trace
 DEPDIRS-blobfs := log thread blob trace
-DEPDIRS-event := log util thread $(JSON_LIBS) trace
+DEPDIRS-event := log util thread $(JSON_LIBS) trace init
+DEPDIRS-init := jsonrpc json log rpc thread util
 
 DEPDIRS-ftl := log util thread trace bdev
 DEPDIRS-nbd := log util thread $(JSON_LIBS) bdev
@@ -159,16 +160,16 @@ endif
 # These depdirs include subsystem interdependencies which
 # are not related to symbols, but are defined directly in
 # the SPDK event subsystem code.
-DEPDIRS-event_accel := event accel
-DEPDIRS-event_net := event net
-DEPDIRS-event_vmd := event vmd $(JSON_LIBS) log thread
+DEPDIRS-event_accel := init accel
+DEPDIRS-event_net := init net
+DEPDIRS-event_vmd := init vmd $(JSON_LIBS) log thread
 
-DEPDIRS-event_bdev := event bdev event_accel event_vmd event_sock
+DEPDIRS-event_bdev := init bdev event_accel event_vmd event_sock
 
-DEPDIRS-event_nbd := event nbd event_bdev
-DEPDIRS-event_nvmf := event nvmf event_bdev event_sock $(BDEV_DEPS_THREAD)
-DEPDIRS-event_scsi := event scsi event_bdev
+DEPDIRS-event_nbd := init nbd event_bdev
+DEPDIRS-event_nvmf := init nvmf event_bdev event_sock thread log bdev util $(JSON_LIBS)
+DEPDIRS-event_scsi := init scsi event_bdev
 
-DEPDIRS-event_iscsi := event iscsi event_scsi event_sock
-DEPDIRS-event_vhost := event vhost event_scsi
-DEPDIRS-event_sock := event sock
+DEPDIRS-event_iscsi := init iscsi event_scsi event_sock
+DEPDIRS-event_vhost := init vhost event_scsi
+DEPDIRS-event_sock := init sock
diff --git a/mk/spdk.unittest.mk b/mk/spdk.unittest.mk
index 4dddb9a00..0f9add2c7 100644
--- a/mk/spdk.unittest.mk
+++ b/mk/spdk.unittest.mk
@@ -52,7 +52,7 @@ CFLAGS += -I$(SPDK_ROOT_DIR)/test
 CFLAGS += -ffunction-sections
 LDFLAGS += -Wl,--gc-sections
 
-SPDK_LIB_LIST += thread util log
+SPDK_LIB_LIST += thread util log trace
 
 LIBS += -lcunit $(SPDK_STATIC_LIB_LINKER_ARGS)
 
diff --git a/module/accel/idxd/accel_engine_idxd.c b/module/accel/idxd/accel_engine_idxd.c
index 44a2f742d..c67c0d876 100644
--- a/module/accel/idxd/accel_engine_idxd.c
+++ b/module/accel/idxd/accel_engine_idxd.c
@@ -58,48 +58,75 @@ enum channel_state {
 
 static bool g_idxd_initialized = false;
 
-struct pci_device {
-	struct spdk_pci_device *pci_dev;
-	TAILQ_ENTRY(pci_device) tailq;
-};
-static TAILQ_HEAD(, pci_device) g_pci_devices = TAILQ_HEAD_INITIALIZER(g_pci_devices);
-
 struct idxd_device {
 	struct				spdk_idxd_device *idxd;
 	TAILQ_ENTRY(idxd_device)	tailq;
 };
 static TAILQ_HEAD(, idxd_device) g_idxd_devices = TAILQ_HEAD_INITIALIZER(g_idxd_devices);
 static struct idxd_device *g_next_dev = NULL;
+static uint32_t g_num_devices = 0;
+static pthread_mutex_t g_dev_lock = PTHREAD_MUTEX_INITIALIZER;
 
 struct idxd_io_channel {
 	struct spdk_idxd_io_channel	*chan;
 	struct idxd_device		*dev;
 	enum channel_state		state;
 	struct spdk_poller		*poller;
+	uint32_t			num_outstanding;
+	uint32_t			max_outstanding;
 	TAILQ_HEAD(, spdk_accel_task)	queued_tasks;
 };
 
 static struct spdk_io_channel *idxd_get_io_channel(void);
 
 static struct idxd_device *
-idxd_select_device(void)
+idxd_select_device(struct idxd_io_channel *chan)
 {
+	uint32_t count = 0;
+	struct idxd_device *dev;
+
 	/*
 	 * We allow channels to share underlying devices,
-	 * selection is round-robin based.
+	 * selection is round-robin based with a limitation
+	 * on how many channel can share one device.
 	 */
+	do {
+		/* select next device */
+		pthread_mutex_lock(&g_dev_lock);
+		g_next_dev = TAILQ_NEXT(g_next_dev, tailq);
+		if (g_next_dev == NULL) {
+			g_next_dev = TAILQ_FIRST(&g_idxd_devices);
+		}
+		dev = g_next_dev;
+		pthread_mutex_unlock(&g_dev_lock);
+
+		/*
+		 * Now see if a channel is available on this one. We only
+		 * allow a specific number of channels to share a device
+		 * to limit outstanding IO for flow control purposes.
+		 */
+		chan->chan = spdk_idxd_get_channel(dev->idxd);
+		if (chan->chan != NULL) {
+			chan->max_outstanding = spdk_idxd_chan_get_max_operations(chan->chan);
+			return dev;
+		}
+	} while (count++ < g_num_devices);
 
-	g_next_dev = TAILQ_NEXT(g_next_dev, tailq);
-	if (g_next_dev == NULL) {
-		g_next_dev = TAILQ_FIRST(&g_idxd_devices);
-	}
-	return g_next_dev;
+	/* we are out of available channels and devices. */
+	SPDK_ERRLOG("No more DSA devices available!\n");
+	return NULL;
 }
 
 static void
 idxd_done(void *cb_arg, int status)
 {
 	struct spdk_accel_task *accel_task = cb_arg;
+	struct idxd_io_channel *chan = spdk_io_channel_get_ctx(accel_task->accel_ch->engine_ch);
+
+	assert(chan->num_outstanding > 0);
+	if (chan->num_outstanding-- == chan->max_outstanding) {
+		chan->state = IDXD_CHANNEL_ACTIVE;
+	}
 
 	spdk_accel_task_complete(accel_task, status);
 }
@@ -112,6 +139,12 @@ _process_single_task(struct spdk_io_channel *ch, struct spdk_accel_task *task)
 	uint8_t fill_pattern = (uint8_t)task->fill_pattern;
 	void *src;
 
+	if (chan->num_outstanding == chan->max_outstanding) {
+		chan->state = IDXD_CHANNEL_PAUSED;
+		return -EBUSY;
+	}
+	chan->num_outstanding++;
+
 	switch (task->op_code) {
 	case ACCEL_OPCODE_MEMMOVE:
 		rc = spdk_idxd_submit_copy(chan->chan, task->dst, task->src, task->nbytes, idxd_done, task);
@@ -133,6 +166,10 @@ _process_single_task(struct spdk_io_channel *ch, struct spdk_accel_task *task)
 		rc = spdk_idxd_submit_crc32c(chan->chan, task->dst, src, task->seed, task->nbytes, idxd_done,
 					     task);
 		break;
+	case ACCEL_OPCODE_COPY_CRC32C:
+		rc = spdk_idxd_submit_copy_crc32c(chan->chan, task->dst, task->src, task->crc_dst, task->seed,
+						  task->nbytes, idxd_done, task);
+		break;
 	default:
 		assert(false);
 		rc = -EINVAL;
@@ -225,7 +262,7 @@ static uint64_t
 idxd_get_capabilities(void)
 {
 	return ACCEL_COPY | ACCEL_FILL | ACCEL_CRC32C | ACCEL_COMPARE |
-	       ACCEL_DUALCAST;
+	       ACCEL_DUALCAST | ACCEL_COPY_CRC32C;
 }
 
 static uint32_t
@@ -241,70 +278,6 @@ static struct spdk_accel_engine idxd_accel_engine = {
 	.submit_tasks		= idxd_submit_tasks,
 };
 
-/*
- * Configure the max number of descriptors that a channel is
- * allowed to use based on the total number of current channels.
- * This is to allow for dynamic load balancing for hw flow control.
- */
-static void
-_config_max_desc(struct spdk_io_channel_iter *i)
-{
-	struct idxd_io_channel *chan;
-	struct spdk_io_channel *ch;
-	struct spdk_idxd_device *idxd;
-	int rc;
-
-	ch = spdk_io_channel_iter_get_channel(i);
-	chan = spdk_io_channel_get_ctx(ch);
-	idxd = spdk_io_channel_iter_get_ctx(i);
-
-	/* reconfigure channel only if this channel is on the same idxd
-	 * device that initiated the rebalance.
-	 */
-	if (chan->dev->idxd == idxd) {
-		rc = spdk_idxd_reconfigure_chan(chan->chan);
-		if (rc == 0) {
-			chan->state = IDXD_CHANNEL_ACTIVE;
-		} else {
-			chan->state = IDXD_CHANNEL_ERROR;
-		}
-	}
-
-	spdk_for_each_channel_continue(i, 0);
-}
-
-/* Pauses a channel so that it can be re-configured. */
-static void
-_pause_chan(struct spdk_io_channel_iter *i)
-{
-	struct idxd_io_channel *chan;
-	struct spdk_io_channel *ch;
-	struct spdk_idxd_device *idxd;
-
-	ch = spdk_io_channel_iter_get_channel(i);
-	chan = spdk_io_channel_get_ctx(ch);
-	idxd = spdk_io_channel_iter_get_ctx(i);
-
-	/* start queueing up new requests if this channel is on the same idxd
-	 * device that initiated the rebalance.
-	 */
-	if (chan->dev->idxd == idxd) {
-		chan->state = IDXD_CHANNEL_PAUSED;
-	}
-
-	spdk_for_each_channel_continue(i, 0);
-}
-
-static void
-_pause_chan_done(struct spdk_io_channel_iter *i, int status)
-{
-	struct spdk_idxd_device *idxd;
-
-	idxd = spdk_io_channel_iter_get_ctx(i);
-
-	spdk_for_each_channel(&idxd_accel_engine, _config_max_desc, idxd, NULL);
-}
-
 static int
 idxd_create_cb(void *io_device, void *ctx_buf)
 {
@@ -312,28 +285,16 @@ idxd_create_cb(void *io_device, void *ctx_buf)
 	struct idxd_device *dev;
 	int rc;
 
-	dev = idxd_select_device();
+	dev = idxd_select_device(chan);
 	if (dev == NULL) {
-		SPDK_ERRLOG("Failed to allocate idxd_device\n");
+		SPDK_ERRLOG("Failed to get an idxd channel\n");
 		return -EINVAL;
 	}
 
-	chan->chan = spdk_idxd_get_channel(dev->idxd);
-	if (chan->chan == NULL) {
-		return -ENOMEM;
-	}
-
 	chan->dev = dev;
 	chan->poller = SPDK_POLLER_REGISTER(idxd_poll, chan, 0);
 	TAILQ_INIT(&chan->queued_tasks);
 
-	/*
-	 * Configure the channel but leave paused until all others
-	 * are paused and re-configured based on the new number of
-	 * channels. This enables dynamic load balancing for HW
-	 * flow control. The idxd device will tell us if rebalance is
-	 * needed based on how many channels are using it.
-	 */
 	rc = spdk_idxd_configure_chan(chan->chan);
 	if (rc) {
 		SPDK_ERRLOG("Failed to configure new channel rc = %d\n", rc);
@@ -342,52 +303,19 @@ idxd_create_cb(void *io_device, void *ctx_buf)
 		return rc;
 	}
 
-	if (spdk_idxd_device_needs_rebalance(chan->dev->idxd) == false) {
-		chan->state = IDXD_CHANNEL_ACTIVE;
-		return 0;
-	}
-
-	chan->state = IDXD_CHANNEL_PAUSED;
-
-	/*
-	 * Pause all channels so that we can set proper flow control
-	 * per channel. When all are paused, we'll update the max
-	 * number of descriptors allowed per channel.
-	 */
-	spdk_for_each_channel(&idxd_accel_engine, _pause_chan, chan->dev->idxd,
-			      _pause_chan_done);
+	chan->num_outstanding = 0;
+	chan->state = IDXD_CHANNEL_ACTIVE;
 
 	return 0;
 }
 
-static void
-_pause_chan_destroy_done(struct spdk_io_channel_iter *i, int status)
-{
-	struct spdk_idxd_device *idxd;
-
-	idxd = spdk_io_channel_iter_get_ctx(i);
-
-	/* Rebalance the rings with the smaller number of remaining channels, but
-	 * pass the idxd device along so its only done on shared channels.
-	 */
-	spdk_for_each_channel(&idxd_accel_engine, _config_max_desc, idxd, NULL);
-}
-
 static void
 idxd_destroy_cb(void *io_device, void *ctx_buf)
 {
 	struct idxd_io_channel *chan = ctx_buf;
-	bool rebalance;
 
 	spdk_poller_unregister(&chan->poller);
-	rebalance = spdk_idxd_put_channel(chan->chan);
-
-	/* Only rebalance if there are still other channels on this device */
-	if (rebalance == true) {
-		/* Pause each channel then rebalance the max number of ring slots. */
-		spdk_for_each_channel(&idxd_accel_engine, _pause_chan, chan->dev->idxd,
-				      _pause_chan_destroy_done);
-	}
+	spdk_idxd_put_channel(chan->chan);
 }
 
 static struct spdk_io_channel *
@@ -396,38 +324,8 @@ idxd_get_io_channel(void)
 	return spdk_get_io_channel(&idxd_accel_engine);
 }
 
-static bool
-probe_cb(void *cb_ctx, struct spdk_pci_device *pci_dev)
-{
-	struct spdk_pci_addr pci_addr = spdk_pci_device_get_addr(pci_dev);
-	struct pci_device *pdev;
-
-	SPDK_NOTICELOG(
-		" Found matching device at %04x:%02x:%02x.%x vendor:0x%04x device:0x%04x\n",
-		pci_addr.domain,
-		pci_addr.bus,
-		pci_addr.dev,
-		pci_addr.func,
-		spdk_pci_device_get_vendor_id(pci_dev),
-		spdk_pci_device_get_device_id(pci_dev));
-
-	pdev = calloc(1, sizeof(*pdev));
-	if (pdev == NULL) {
-		return false;
-	}
-	pdev->pci_dev = pci_dev;
-	TAILQ_INSERT_TAIL(&g_pci_devices, pdev, tailq);
-
-	/* Claim the device in case conflict with other process */
-	if (spdk_pci_device_claim(pci_dev) < 0) {
-		return false;
-	}
-
-	return true;
-}
-
 static void
-attach_cb(void *cb_ctx, struct spdk_pci_device *pci_dev, struct spdk_idxd_device *idxd)
+attach_cb(void *cb_ctx, struct spdk_idxd_device *idxd)
 {
 	struct idxd_device *dev;
 
@@ -443,6 +341,7 @@ attach_cb(void *cb_ctx, struct spdk_pci_device *pci_dev, struct spdk_idxd_device
 	}
 
 	TAILQ_INSERT_TAIL(&g_idxd_devices, dev, tailq);
+	g_num_devices++;
 }
 
 void
@@ -465,11 +364,16 @@ accel_engine_idxd_init(void)
 		return -EINVAL;
 	}
 
-	if (spdk_idxd_probe(NULL, probe_cb, attach_cb) != 0) {
+	if (spdk_idxd_probe(NULL, attach_cb) != 0) {
 		SPDK_ERRLOG("spdk_idxd_probe() failed\n");
 		return -EINVAL;
 	}
 
+	if (TAILQ_EMPTY(&g_idxd_devices)) {
+		SPDK_NOTICELOG("no available idxd devices\n");
+		return -EINVAL;
+	}
+
 	g_idxd_initialized = true;
 	g_batch_max = spdk_idxd_batch_get_max();
 	SPDK_NOTICELOG("Accel engine updated to use IDXD DSA engine.\n");
@@ -483,7 +387,6 @@ static void
 accel_engine_idxd_exit(void *ctx)
 {
 	struct idxd_device *dev;
-	struct pci_device *pci_dev;
 
 	if (g_idxd_initialized) {
 		spdk_io_device_unregister(&idxd_accel_engine, NULL);
@@ -496,13 +399,6 @@ accel_engine_idxd_exit(void *ctx)
 		free(dev);
 	}
 
-	while (!TAILQ_EMPTY(&g_pci_devices)) {
-		pci_dev = TAILQ_FIRST(&g_pci_devices);
-		TAILQ_REMOVE(&g_pci_devices, pci_dev, tailq);
-		spdk_pci_device_detach(pci_dev->pci_dev);
-		free(pci_dev);
-	}
-
 	spdk_accel_engine_module_finish();
 }
 
diff --git a/module/accel/ioat/accel_engine_ioat.c b/module/accel/ioat/accel_engine_ioat.c
index 8d46dde27..2e50159c0 100644
--- a/module/accel/ioat/accel_engine_ioat.c
+++ b/module/accel/ioat/accel_engine_ioat.c
@@ -280,6 +280,11 @@ accel_engine_ioat_init(void)
 		return -1;
 	}
 
+	if (TAILQ_EMPTY(&g_devices)) {
+		SPDK_NOTICELOG("No available ioat devices\n");
+		return -1;
+	}
+
 	g_ioat_initialized = true;
 	SPDK_NOTICELOG("Accel engine updated to use IOAT engine.\n");
 	spdk_accel_hw_engine_register(&ioat_accel_engine);
diff --git a/module/bdev/aio/bdev_aio.c b/module/bdev/aio/bdev_aio.c
index b9925ede6..f7c26d885 100644
--- a/module/bdev/aio/bdev_aio.c
+++ b/module/bdev/aio/bdev_aio.c
@@ -246,10 +246,10 @@ bdev_aio_flush(struct file_disk *fdisk, struct bdev_aio_task *aio_task)
 	}
 }
 
-static int
-bdev_aio_destruct(void *ctx)
+static void
+bdev_aio_destruct_cb(void *io_device)
 {
-	struct file_disk *fdisk = ctx;
+	struct file_disk *fdisk = io_device;
 	int rc = 0;
 
 	TAILQ_REMOVE(&g_aio_disk_head, fdisk, link);
@@ -257,9 +257,18 @@ bdev_aio_destruct(void *ctx)
 	if (rc < 0) {
 		SPDK_ERRLOG("bdev_aio_close() failed\n");
 	}
-	spdk_io_device_unregister(fdisk, NULL);
+
 	aio_free_disk(fdisk);
-	return rc;
+}
+
+static int
+bdev_aio_destruct(void *ctx)
+{
+	struct file_disk *fdisk = ctx;
+
+	spdk_io_device_unregister(fdisk, bdev_aio_destruct_cb);
+
+	return 0;
 }
 
 static int
diff --git a/module/bdev/aio/bdev_aio_rpc.c b/module/bdev/aio/bdev_aio_rpc.c
index 035cded1c..da625c8fe 100644
--- a/module/bdev/aio/bdev_aio_rpc.c
+++ b/module/bdev/aio/bdev_aio_rpc.c
@@ -43,11 +43,17 @@ struct rpc_construct_aio {
 	uint32_t block_size;
 };
 
+struct rpc_construct_aio_ctx {
+	struct rpc_construct_aio req;
+	struct spdk_jsonrpc_request *request;
+};
+
 static void
-free_rpc_construct_aio(struct rpc_construct_aio *req)
+free_rpc_construct_aio(struct rpc_construct_aio_ctx *ctx)
 {
-	free(req->name);
-	free(req->filename);
+	free(ctx->req.name);
+	free(ctx->req.filename);
+	free(ctx);
 }
 
 static const struct spdk_json_object_decoder rpc_construct_aio_decoders[] = {
@@ -56,36 +62,51 @@ static const struct spdk_json_object_decoder rpc_construct_aio_decoders[] = {
 	{"block_size", offsetof(struct rpc_construct_aio, block_size), spdk_json_decode_uint32, true},
 };
 
+static void
+rpc_bdev_aio_create_cb(void *cb_arg)
+{
+	struct rpc_construct_aio_ctx *ctx = cb_arg;
+	struct spdk_jsonrpc_request *request = ctx->request;
+	struct spdk_json_write_ctx *w;
+
+	w = spdk_jsonrpc_begin_result(request);
+	spdk_json_write_string(w, ctx->req.name);
+	spdk_jsonrpc_end_result(request, w);
+	free_rpc_construct_aio(ctx);
+}
+
 static void
 rpc_bdev_aio_create(struct spdk_jsonrpc_request *request,
 		    const struct spdk_json_val *params)
 {
-	struct rpc_construct_aio req = {};
-	struct spdk_json_write_ctx *w;
-	int rc = 0;
+	struct rpc_construct_aio_ctx *ctx;
+	int rc;
+
+	ctx = calloc(1, sizeof(*ctx));
+	if (!ctx) {
+		spdk_jsonrpc_send_error_response(request, -ENOMEM, spdk_strerror(ENOMEM));
+		return;
+	}
 
 	if (spdk_json_decode_object(params, rpc_construct_aio_decoders,
 				    SPDK_COUNTOF(rpc_construct_aio_decoders),
-				    &req)) {
+				    &ctx->req)) {
 		SPDK_ERRLOG("spdk_json_decode_object failed\n");
 		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
 						 "spdk_json_decode_object failed");
-		goto cleanup;
+		free_rpc_construct_aio(ctx);
+		return;
 	}
 
-	rc = create_aio_bdev(req.name, req.filename, req.block_size);
+	ctx->request = request;
+	rc = create_aio_bdev(ctx->req.name, ctx->req.filename, ctx->req.block_size);
 	if (rc) {
 		spdk_jsonrpc_send_error_response(request, rc, spdk_strerror(-rc));
-		goto cleanup;
+		free_rpc_construct_aio(ctx);
+		return;
 	}
 
-
-	w = spdk_jsonrpc_begin_result(request);
-	spdk_json_write_string(w, req.name);
-	spdk_jsonrpc_end_result(request, w);
-
-cleanup:
-	free_rpc_construct_aio(&req);
+	spdk_bdev_wait_for_examine(rpc_bdev_aio_create_cb, ctx);
 }
 SPDK_RPC_REGISTER("bdev_aio_create", rpc_bdev_aio_create, SPDK_RPC_RUNTIME)
 SPDK_RPC_REGISTER_ALIAS_DEPRECATED(bdev_aio_create, construct_aio_bdev)
diff --git a/module/bdev/compress/vbdev_compress.c b/module/bdev/compress/vbdev_compress.c
index 708e67b9b..929af824e 100644
--- a/module/bdev/compress/vbdev_compress.c
+++ b/module/bdev/compress/vbdev_compress.c
@@ -1591,7 +1591,7 @@ static int _set_compbdev_name(struct vbdev_compress *comp_bdev)
 
 	if (!TAILQ_EMPTY(spdk_bdev_get_aliases(comp_bdev->base_bdev))) {
 		aliases = TAILQ_FIRST(spdk_bdev_get_aliases(comp_bdev->base_bdev));
-		comp_bdev->comp_bdev.name = spdk_sprintf_alloc("COMP_%s", aliases->alias);
+		comp_bdev->comp_bdev.name = spdk_sprintf_alloc("COMP_%s", aliases->alias.name);
 		if (!comp_bdev->comp_bdev.name) {
 			SPDK_ERRLOG("could not allocate comp_bdev name for alias\n");
 			return -ENOMEM;
diff --git a/module/bdev/iscsi/bdev_iscsi.c b/module/bdev/iscsi/bdev_iscsi.c
index 7516ea95e..c811d65b6 100644
--- a/module/bdev/iscsi/bdev_iscsi.c
+++ b/module/bdev/iscsi/bdev_iscsi.c
@@ -58,6 +58,19 @@ struct bdev_iscsi_lun;
 
 #define DEFAULT_INITIATOR_NAME "iqn.2016-06.io.spdk:init"
 
+/* MAXIMUM UNMAP LBA COUNT:
+ * indicates the maximum  number of LBAs that may be unmapped
+ * by an UNMAP command.
+ */
+#define BDEV_ISCSI_DEFAULT_MAX_UNMAP_LBA_COUNT (32768)
+
+/* MAXIMUM UNMAP BLOCK DESCRIPTOR COUNT:
+ * indicates the maximum number of UNMAP block descriptors that
+ * shall be contained in the parameter data transferred to the
+ * device server for an UNMAP command.
+ */
+#define BDEV_ISCSI_MAX_UNMAP_BLOCK_DESCS_COUNT (1)
+
 static int bdev_iscsi_initialize(void);
 static TAILQ_HEAD(, bdev_iscsi_conn_req) g_iscsi_conn_req = TAILQ_HEAD_INITIALIZER(
 			g_iscsi_conn_req);
@@ -84,6 +97,7 @@ struct bdev_iscsi_lun {
 	struct spdk_poller		*no_main_ch_poller;
 	struct spdk_thread		*no_main_ch_poller_td;
 	bool				unmap_supported;
+	uint32_t			max_unmap;
 	struct spdk_poller		*poller;
 };
 
@@ -99,6 +113,7 @@ struct bdev_iscsi_conn_req {
 	spdk_bdev_iscsi_create_cb		create_cb;
 	void					*create_cb_arg;
 	bool					unmap_supported;
+	uint32_t				max_unmap;
 	int					lun;
 	int					status;
 	TAILQ_ENTRY(bdev_iscsi_conn_req)	link;
@@ -312,17 +327,41 @@ bdev_iscsi_unmap(struct bdev_iscsi_lun *lun, struct bdev_iscsi_io *iscsi_io,
 		 uint64_t lba, uint64_t num_blocks)
 {
 	struct scsi_task *task;
-	struct unmap_list list[1];
-
-	list[0].lba = lba;
-	list[0].num = num_blocks;
-	task = iscsi_unmap_task(lun->context, 0, 0, 0, list, 1,
+	struct unmap_list list[BDEV_ISCSI_MAX_UNMAP_BLOCK_DESCS_COUNT] = {};
+	struct unmap_list *entry;
+	uint32_t num_unmap_list;
+	uint64_t offset, remaining, unmap_blocks;
+
+	num_unmap_list = spdk_divide_round_up(num_blocks, lun->max_unmap);
+	if (num_unmap_list > BDEV_ISCSI_MAX_UNMAP_BLOCK_DESCS_COUNT) {
+		SPDK_ERRLOG("Too many unmap entries\n");
+		goto failed;
+	}
+
+	remaining = num_blocks;
+	offset = lba;
+	num_unmap_list = 0;
+	entry = &list[0];
+
+	do {
+		unmap_blocks = spdk_min(remaining, lun->max_unmap);
+		entry->lba = offset;
+		entry->num = unmap_blocks;
+		num_unmap_list++;
+		remaining -= unmap_blocks;
+		offset += unmap_blocks;
+		entry++;
+	} while (remaining > 0);
+
+	task = iscsi_unmap_task(lun->context, 0, 0, 0, list, num_unmap_list,
 				bdev_iscsi_command_cb, iscsi_io);
-	if (task == NULL) {
-		SPDK_ERRLOG("failed to get unmap_task\n");
-		bdev_iscsi_io_complete(iscsi_io, SPDK_BDEV_IO_STATUS_FAILED);
+	if (task != NULL) {
 		return;
 	}
+	SPDK_ERRLOG("failed to get unmap_task\n");
+
+failed:
+	bdev_iscsi_io_complete(iscsi_io, SPDK_BDEV_IO_STATUS_FAILED);
 }
 
 static void
@@ -621,9 +660,8 @@ static const struct spdk_bdev_fn_table iscsi_fn_table = {
 };
 
 static int
-create_iscsi_lun(struct iscsi_context *context, int lun_id, char *url, char *initiator_iqn,
-		 char *name,
-		 uint64_t num_blocks, uint32_t block_size, struct spdk_bdev **bdev, bool unmap_supported)
+create_iscsi_lun(struct bdev_iscsi_conn_req *req, uint64_t num_blocks,
+		 uint32_t block_size, struct spdk_bdev **bdev, uint8_t lbppbe)
 {
 	struct bdev_iscsi_lun *lun;
 	int rc;
@@ -634,26 +672,32 @@ create_iscsi_lun(struct iscsi_context *context, int lun_id, char *url, char *ini
 		return -ENOMEM;
 	}
 
-	lun->context = context;
-	lun->lun_id = lun_id;
-	lun->url = url;
-	lun->initiator_iqn = initiator_iqn;
+	lun->context = req->context;
+	lun->lun_id = req->lun;
+	lun->url = req->url;
+	lun->initiator_iqn = req->initiator_iqn;
 
 	pthread_mutex_init(&lun->mutex, NULL);
 
-	lun->bdev.name = name;
+	lun->bdev.name = req->bdev_name;
 	lun->bdev.product_name = "iSCSI LUN";
 	lun->bdev.module = &g_iscsi_bdev_module;
 	lun->bdev.blocklen = block_size;
+	lun->bdev.phys_blocklen = block_size * (1 << lbppbe);
 	lun->bdev.blockcnt = num_blocks;
 	lun->bdev.ctxt = lun;
-	lun->unmap_supported = unmap_supported;
+	lun->unmap_supported = req->unmap_supported;
+	if (lun->unmap_supported) {
+		lun->max_unmap = req->max_unmap;
+		lun->bdev.max_unmap = req->max_unmap;
+		lun->bdev.max_unmap_segments = BDEV_ISCSI_MAX_UNMAP_BLOCK_DESCS_COUNT;
+	}
 
 	lun->bdev.fn_table = &iscsi_fn_table;
 
 	spdk_io_device_register(lun, bdev_iscsi_create_cb, bdev_iscsi_destroy_cb,
 				sizeof(struct bdev_iscsi_io_channel),
-				name);
+				req->bdev_name);
 	rc = spdk_bdev_register(&lun->bdev);
 	if (rc) {
 		spdk_io_device_unregister(lun, NULL);
@@ -690,8 +734,8 @@ iscsi_readcapacity16_cb(struct iscsi_context *iscsi, int status,
 		goto ret;
 	}
 
-	status = create_iscsi_lun(req->context, req->lun, req->url, req->initiator_iqn, req->bdev_name,
-				  readcap16->returned_lba + 1, readcap16->block_length, &bdev, req->unmap_supported);
+	status = create_iscsi_lun(req, readcap16->returned_lba + 1, readcap16->block_length, &bdev,
+				  readcap16->lbppbe);
 	if (status) {
 		SPDK_ERRLOG("Unable to create iscsi bdev: %s (%d)\n", spdk_strerror(-status), status);
 	}
@@ -702,7 +746,37 @@ ret:
 }
 
 static void
-bdev_iscsi_inquiry_cb(struct iscsi_context *context, int status, void *_task, void *private_data)
+bdev_iscsi_inquiry_bl_cb(struct iscsi_context *context, int status, void *_task, void *private_data)
+{
+	struct scsi_task *task = _task;
+	struct scsi_inquiry_block_limits *bl_inq = NULL;
+	struct bdev_iscsi_conn_req *req = private_data;
+
+	if (status == SPDK_SCSI_STATUS_GOOD) {
+		bl_inq = scsi_datain_unmarshall(task);
+		if (bl_inq != NULL) {
+			if (!bl_inq->max_unmap) {
+				SPDK_ERRLOG("Invalid max_unmap, use the default\n");
+				req->max_unmap = BDEV_ISCSI_DEFAULT_MAX_UNMAP_LBA_COUNT;
+			} else {
+				req->max_unmap = bl_inq->max_unmap;
+			}
+		}
+	}
+
+	scsi_free_scsi_task(task);
+	task = iscsi_readcapacity16_task(context, req->lun, iscsi_readcapacity16_cb, req);
+	if (task) {
+		return;
+	}
+
+	SPDK_ERRLOG("iSCSI error: %s\n", iscsi_get_error(req->context));
+	complete_conn_req(req, NULL, status);
+}
+
+static void
+bdev_iscsi_inquiry_lbp_cb(struct iscsi_context *context, int status, void *_task,
+			  void *private_data)
 {
 	struct scsi_task *task = _task;
 	struct scsi_inquiry_logical_block_provisioning *lbp_inq = NULL;
@@ -712,7 +786,17 @@ bdev_iscsi_inquiry_cb(struct iscsi_context *context, int status, void *_task, vo
 		lbp_inq = scsi_datain_unmarshall(task);
 		if (lbp_inq != NULL && lbp_inq->lbpu) {
 			req->unmap_supported = true;
+			scsi_free_scsi_task(task);
+
+			task = iscsi_inquiry_task(context, req->lun, 1,
+						  SCSI_INQUIRY_PAGECODE_BLOCK_LIMITS,
+						  255, bdev_iscsi_inquiry_bl_cb, req);
+			if (task) {
+				return;
+			}
 		}
+	} else {
+		scsi_free_scsi_task(task);
 	}
 
 	task = iscsi_readcapacity16_task(context, req->lun, iscsi_readcapacity16_cb, req);
@@ -737,7 +821,7 @@ iscsi_connect_cb(struct iscsi_context *iscsi, int status,
 
 	task = iscsi_inquiry_task(iscsi, req->lun, 1,
 				  SCSI_INQUIRY_PAGECODE_LOGICAL_BLOCK_PROVISIONING,
-				  255, bdev_iscsi_inquiry_cb, req);
+				  255, bdev_iscsi_inquiry_lbp_cb, req);
 	if (task) {
 		return;
 	}
diff --git a/module/bdev/lvol/vbdev_lvol.c b/module/bdev/lvol/vbdev_lvol.c
index 58e035a72..6f2c0c976 100644
--- a/module/bdev/lvol/vbdev_lvol.c
+++ b/module/bdev/lvol/vbdev_lvol.c
@@ -92,13 +92,13 @@ _vbdev_lvol_change_bdev_alias(struct spdk_lvol *lvol, const char *new_lvol_name)
 	 * while we changed lvs name earlier, we have to iterate alias list to get one,
 	 * and check if there is only one alias */
 
-	TAILQ_FOREACH(tmp, &lvol->bdev->aliases, tailq) {
+	TAILQ_FOREACH(tmp, spdk_bdev_get_aliases(lvol->bdev), tailq) {
 		if (++alias_number > 1) {
 			SPDK_ERRLOG("There is more than 1 alias in bdev %s\n", lvol->bdev->name);
 			return -EINVAL;
 		}
 
-		old_alias = tmp->alias;
+		old_alias = tmp->alias.name;
 	}
 
 	if (alias_number == 0) {
diff --git a/module/bdev/nvme/bdev_nvme.c b/module/bdev/nvme/bdev_nvme.c
index 5239f270d..8f87ca0cf 100644
--- a/module/bdev/nvme/bdev_nvme.c
+++ b/module/bdev/nvme/bdev_nvme.c
@@ -176,7 +176,7 @@ static int bdev_nvme_get_zone_info(struct spdk_nvme_ns *ns, struct spdk_nvme_qpa
 static int bdev_nvme_zone_management(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 				     struct nvme_bdev_io *bio, uint64_t zone_id,
 				     enum spdk_bdev_zone_action action);
-static int bdev_nvme_admin_passthru(struct nvme_io_channel *nvme_ch,
+static int bdev_nvme_admin_passthru(struct nvme_io_path *io_path,
 				    struct nvme_bdev_io *bio,
 				    struct spdk_nvme_cmd *cmd, void *buf, size_t nbytes);
 static int bdev_nvme_io_passthru(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
@@ -185,9 +185,9 @@ static int bdev_nvme_io_passthru(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair
 static int bdev_nvme_io_passthru_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 				    struct nvme_bdev_io *bio,
 				    struct spdk_nvme_cmd *cmd, void *buf, size_t nbytes, void *md_buf, size_t md_len);
-static int bdev_nvme_abort(struct nvme_io_channel *nvme_ch,
+static int bdev_nvme_abort(struct nvme_io_path *io_path,
 			   struct nvme_bdev_io *bio, struct nvme_bdev_io *bio_to_abort);
-static int bdev_nvme_reset(struct nvme_io_channel *nvme_ch, struct nvme_bdev_io *bio);
+static int bdev_nvme_reset(struct nvme_io_path *io_path, struct spdk_bdev_io *bdev_io);
 static int bdev_nvme_failover(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr, bool remove);
 static void remove_cb(void *cb_ctx, struct spdk_nvme_ctrlr *ctrlr);
 
@@ -223,15 +223,15 @@ static config_json_namespace_fn g_config_json_namespace_fn[] = {
 };
 
 struct spdk_nvme_qpair *
-bdev_nvme_get_io_qpair(struct spdk_io_channel *ctrlr_io_ch)
+bdev_nvme_get_io_qpair(struct spdk_io_channel *io_path_ch)
 {
-	struct nvme_io_channel *nvme_ch;
+	struct nvme_io_path *io_path;
 
-	assert(ctrlr_io_ch != NULL);
+	assert(io_path_ch != NULL);
 
-	nvme_ch = spdk_io_channel_get_ctx(ctrlr_io_ch);
+	io_path = spdk_io_channel_get_ctx(io_path_ch);
 
-	return nvme_ch->qpair;
+	return io_path->qpair;
 }
 
 static int
@@ -251,6 +251,52 @@ static struct spdk_bdev_module nvme_if = {
 };
 SPDK_BDEV_MODULE_REGISTER(nvme, &nvme_if)
 
+static inline bool
+bdev_nvme_find_io_path(struct nvme_bdev *nbdev, struct nvme_io_path *io_path,
+		       struct spdk_nvme_ns **_ns, struct spdk_nvme_qpair **_qpair)
+{
+	if (spdk_unlikely(io_path->qpair == NULL)) {
+		/* The device is currently resetting. */
+		return false;
+	}
+
+	*_ns = nbdev->nvme_ns->ns;
+	*_qpair = io_path->qpair;
+	return true;
+}
+
+static inline bool
+bdev_nvme_find_admin_path(struct nvme_io_path *io_path,
+			  struct nvme_bdev_ctrlr **_nvme_bdev_ctrlr)
+{
+	*_nvme_bdev_ctrlr = io_path->ctrlr;
+	return true;
+}
+
+static inline void
+bdev_nvme_io_complete_nvme_status(struct nvme_bdev_io *bio,
+				  const struct spdk_nvme_cpl *cpl)
+{
+	spdk_bdev_io_complete_nvme_status(spdk_bdev_io_from_ctx(bio), cpl->cdw0,
+					  cpl->status.sct, cpl->status.sc);
+}
+
+static inline void
+bdev_nvme_io_complete(struct nvme_bdev_io *bio, int rc)
+{
+	enum spdk_bdev_io_status io_status;
+
+	if (rc == 0) {
+		io_status = SPDK_BDEV_IO_STATUS_SUCCESS;
+	} else if (rc == -ENOMEM) {
+		io_status = SPDK_BDEV_IO_STATUS_NOMEM;
+	} else {
+		io_status = SPDK_BDEV_IO_STATUS_FAILED;
+	}
+
+	spdk_bdev_io_complete(spdk_bdev_io_from_ctx(bio), io_status);
+}
+
 static void
 bdev_nvme_disconnected_qpair_cb(struct spdk_nvme_qpair *qpair, void *poll_group_ctx)
 {
@@ -323,7 +369,7 @@ bdev_nvme_destruct(void *ctx)
 	if (!nvme_ns->populated) {
 		pthread_mutex_unlock(&nvme_ns->ctrlr->mutex);
 
-		nvme_bdev_ctrlr_destruct(nvme_ns->ctrlr);
+		nvme_bdev_ctrlr_release(nvme_ns->ctrlr);
 	} else {
 		pthread_mutex_unlock(&nvme_ns->ctrlr->mutex);
 	}
@@ -338,15 +384,15 @@ static int
 bdev_nvme_flush(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		struct nvme_bdev_io *bio, uint64_t offset, uint64_t nbytes)
 {
-	spdk_bdev_io_complete(spdk_bdev_io_from_ctx(bio), SPDK_BDEV_IO_STATUS_SUCCESS);
+	bdev_nvme_io_complete(bio, 0);
 
 	return 0;
 }
 
 static int
-bdev_nvme_create_qpair(struct nvme_io_channel *nvme_ch)
+bdev_nvme_create_qpair(struct nvme_io_path *io_path)
 {
-	struct spdk_nvme_ctrlr *ctrlr = nvme_ch->ctrlr->ctrlr;
+	struct spdk_nvme_ctrlr *ctrlr = io_path->ctrlr->ctrlr;
 	struct spdk_nvme_io_qpair_opts opts;
 	struct spdk_nvme_qpair *qpair;
 	int rc;
@@ -362,9 +408,9 @@ bdev_nvme_create_qpair(struct nvme_io_channel *nvme_ch)
 		return -1;
 	}
 
-	assert(nvme_ch->group != NULL);
+	assert(io_path->group != NULL);
 
-	rc = spdk_nvme_poll_group_add(nvme_ch->group->group, qpair);
+	rc = spdk_nvme_poll_group_add(io_path->group->group, qpair);
 	if (rc != 0) {
 		SPDK_ERRLOG("Unable to begin polling on NVMe Channel.\n");
 		goto err;
@@ -376,7 +422,7 @@ bdev_nvme_create_qpair(struct nvme_io_channel *nvme_ch)
 		goto err;
 	}
 
-	nvme_ch->qpair = qpair;
+	io_path->qpair = qpair;
 
 	return 0;
 
@@ -387,26 +433,24 @@ err:
 }
 
 static int
-bdev_nvme_destroy_qpair(struct nvme_io_channel *nvme_ch)
+bdev_nvme_destroy_qpair(struct nvme_io_path *io_path)
 {
 	int rc;
 
-	if (nvme_ch->qpair == NULL) {
+	if (io_path->qpair == NULL) {
 		return 0;
 	}
 
-	rc = spdk_nvme_ctrlr_free_io_qpair(nvme_ch->qpair);
+	rc = spdk_nvme_ctrlr_free_io_qpair(io_path->qpair);
 	if (!rc) {
-		nvme_ch->qpair = NULL;
+		io_path->qpair = NULL;
 	}
 	return rc;
 }
 
 static void
-_bdev_nvme_check_pending_destruct(struct spdk_io_channel_iter *i, int status)
+_bdev_nvme_check_pending_destruct(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr)
 {
-	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = spdk_io_channel_iter_get_ctx(i);
-
 	pthread_mutex_lock(&nvme_bdev_ctrlr->mutex);
 	if (nvme_bdev_ctrlr->destruct_after_reset) {
 		assert(nvme_bdev_ctrlr->ref == 0 && nvme_bdev_ctrlr->destruct);
@@ -420,14 +464,22 @@ _bdev_nvme_check_pending_destruct(struct spdk_io_channel_iter *i, int status)
 }
 
 static void
-_bdev_nvme_complete_pending_resets(struct nvme_io_channel *nvme_ch,
+bdev_nvme_check_pending_destruct(struct spdk_io_channel_iter *i, int status)
+{
+	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = spdk_io_channel_iter_get_ctx(i);
+
+	_bdev_nvme_check_pending_destruct(nvme_bdev_ctrlr);
+}
+
+static void
+_bdev_nvme_complete_pending_resets(struct nvme_io_path *io_path,
 				   enum spdk_bdev_io_status status)
 {
 	struct spdk_bdev_io *bdev_io;
 
-	while (!TAILQ_EMPTY(&nvme_ch->pending_resets)) {
-		bdev_io = TAILQ_FIRST(&nvme_ch->pending_resets);
-		TAILQ_REMOVE(&nvme_ch->pending_resets, bdev_io, module_link);
+	while (!TAILQ_EMPTY(&io_path->pending_resets)) {
+		bdev_io = TAILQ_FIRST(&io_path->pending_resets);
+		TAILQ_REMOVE(&io_path->pending_resets, bdev_io, module_link);
 		spdk_bdev_io_complete(bdev_io, status);
 	}
 }
@@ -436,9 +488,9 @@ static void
 bdev_nvme_complete_pending_resets(struct spdk_io_channel_iter *i)
 {
 	struct spdk_io_channel *_ch = spdk_io_channel_iter_get_channel(i);
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(_ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(_ch);
 
-	_bdev_nvme_complete_pending_resets(nvme_ch, SPDK_BDEV_IO_STATUS_SUCCESS);
+	_bdev_nvme_complete_pending_resets(io_path, SPDK_BDEV_IO_STATUS_SUCCESS);
 
 	spdk_for_each_channel_continue(i, 0);
 }
@@ -447,25 +499,43 @@ static void
 bdev_nvme_abort_pending_resets(struct spdk_io_channel_iter *i)
 {
 	struct spdk_io_channel *_ch = spdk_io_channel_iter_get_channel(i);
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(_ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(_ch);
 
-	_bdev_nvme_complete_pending_resets(nvme_ch, SPDK_BDEV_IO_STATUS_FAILED);
+	_bdev_nvme_complete_pending_resets(io_path, SPDK_BDEV_IO_STATUS_FAILED);
 
 	spdk_for_each_channel_continue(i, 0);
 }
 
+static void
+bdev_nvme_reset_io_complete(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
+			    struct spdk_bdev_io *bdev_io, int rc)
+{
+	enum spdk_bdev_io_status io_status = SPDK_BDEV_IO_STATUS_SUCCESS;
+
+	if (rc) {
+		io_status = SPDK_BDEV_IO_STATUS_FAILED;
+	}
+
+	spdk_bdev_io_complete(bdev_io, io_status);
+
+	/* Make sure we clear any pending resets before returning. */
+	spdk_for_each_channel(nvme_bdev_ctrlr,
+			      rc == 0 ? bdev_nvme_complete_pending_resets :
+			      bdev_nvme_abort_pending_resets,
+			      nvme_bdev_ctrlr,
+			      bdev_nvme_check_pending_destruct);
+}
+
 static void
 _bdev_nvme_reset_complete(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr, int rc)
 {
 	struct nvme_bdev_ctrlr_trid *curr_trid;
-	struct nvme_bdev_io *bio = nvme_bdev_ctrlr->reset_bio;
-	enum spdk_bdev_io_status io_status = SPDK_BDEV_IO_STATUS_SUCCESS;
+	struct spdk_bdev_io *bdev_io = nvme_bdev_ctrlr->reset_bdev_io;
 
-	nvme_bdev_ctrlr->reset_bio = NULL;
+	nvme_bdev_ctrlr->reset_bdev_io = NULL;
 
 	if (rc) {
 		SPDK_ERRLOG("Resetting controller failed.\n");
-		io_status = SPDK_BDEV_IO_STATUS_FAILED;
 	} else {
 		SPDK_NOTICELOG("Resetting controller successful.\n");
 	}
@@ -487,16 +557,16 @@ _bdev_nvme_reset_complete(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr, int rc)
 
 	pthread_mutex_unlock(&nvme_bdev_ctrlr->mutex);
 
-	if (bio) {
-		spdk_bdev_io_complete(spdk_bdev_io_from_ctx(bio), io_status);
+	if (bdev_io) {
+		bdev_nvme_reset_io_complete(nvme_bdev_ctrlr, bdev_io, rc);
+	} else {
+		/* Make sure we clear any pending resets before returning. */
+		spdk_for_each_channel(nvme_bdev_ctrlr,
+				      rc == 0 ? bdev_nvme_complete_pending_resets :
+				      bdev_nvme_abort_pending_resets,
+				      nvme_bdev_ctrlr,
+				      bdev_nvme_check_pending_destruct);
 	}
-
-	/* Make sure we clear any pending resets before returning. */
-	spdk_for_each_channel(nvme_bdev_ctrlr,
-			      rc == 0 ? bdev_nvme_complete_pending_resets :
-			      bdev_nvme_abort_pending_resets,
-			      nvme_bdev_ctrlr,
-			      _bdev_nvme_check_pending_destruct);
 }
 
 static void
@@ -511,10 +581,10 @@ static void
 _bdev_nvme_reset_create_qpair(struct spdk_io_channel_iter *i)
 {
 	struct spdk_io_channel *_ch = spdk_io_channel_iter_get_channel(i);
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(_ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(_ch);
 	int rc;
 
-	rc = bdev_nvme_create_qpair(nvme_ch);
+	rc = bdev_nvme_create_qpair(io_path);
 
 	spdk_for_each_channel_continue(i, rc);
 }
@@ -550,10 +620,10 @@ static void
 _bdev_nvme_reset_destroy_qpair(struct spdk_io_channel_iter *i)
 {
 	struct spdk_io_channel *ch = spdk_io_channel_iter_get_channel(i);
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
 	int rc;
 
-	rc = bdev_nvme_destroy_qpair(nvme_ch);
+	rc = bdev_nvme_destroy_qpair(io_path);
 
 	spdk_for_each_channel_continue(i, rc);
 }
@@ -586,25 +656,21 @@ _bdev_nvme_reset(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr)
 }
 
 static int
-bdev_nvme_reset(struct nvme_io_channel *nvme_ch, struct nvme_bdev_io *bio)
+bdev_nvme_reset(struct nvme_io_path *io_path, struct spdk_bdev_io *bdev_io)
 {
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 	int rc;
 
-	rc = _bdev_nvme_reset(nvme_ch->ctrlr);
+	rc = _bdev_nvme_reset(io_path->ctrlr);
 	if (rc == 0) {
-		assert(nvme_ch->ctrlr->reset_bio == NULL);
-		nvme_ch->ctrlr->reset_bio = bio;
-	} else if (rc == -EBUSY) {
-		/* Don't bother resetting if the controller is in the process of being destructed. */
-		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
+		assert(io_path->ctrlr->reset_bdev_io == NULL);
+		io_path->ctrlr->reset_bdev_io = bdev_io;
 	} else if (rc == -EAGAIN) {
 		/*
 		 * Reset call is queued only if it is from the app framework. This is on purpose so that
 		 * we don't interfere with the app framework reset strategy. i.e. we are deferring to the
 		 * upper level. If they are in the middle of a reset, we won't try to schedule another one.
 		 */
-		TAILQ_INSERT_TAIL(&nvme_ch->pending_resets, bdev_io, module_link);
+		TAILQ_INSERT_TAIL(&io_path->pending_resets, bdev_io, module_link);
 	} else {
 		return rc;
 	}
@@ -695,30 +761,37 @@ bdev_nvme_unmap(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		uint64_t offset_blocks,
 		uint64_t num_blocks);
 
+static int
+bdev_nvme_write_zeroes(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+		       struct nvme_bdev_io *bio,
+		       uint64_t offset_blocks,
+		       uint64_t num_blocks);
+
 static void
 bdev_nvme_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io,
 		     bool success)
 {
+	struct nvme_bdev_io *bio = (struct nvme_bdev_io *)bdev_io->driver_ctx;
 	struct spdk_bdev *bdev = bdev_io->bdev;
 	struct nvme_bdev *nbdev = (struct nvme_bdev *)bdev->ctxt;
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
-	struct nvme_bdev_ns *nvme_ns;
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
+	struct spdk_nvme_ns *ns;
 	struct spdk_nvme_qpair *qpair;
 	int ret;
 
 	if (!success) {
-		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
-		return;
+		ret = -EINVAL;
+		goto exit;
 	}
 
-	if (spdk_unlikely(!bdev_nvme_find_io_path(nbdev, nvme_ch, &nvme_ns, &qpair))) {
-		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
-		return;
+	if (spdk_unlikely(!bdev_nvme_find_io_path(nbdev, io_path, &ns, &qpair))) {
+		ret = -ENXIO;
+		goto exit;
 	}
 
-	ret = bdev_nvme_readv(nvme_ns->ns,
+	ret = bdev_nvme_readv(ns,
 			      qpair,
-			      (struct nvme_bdev_io *)bdev_io->driver_ctx,
+			      bio,
 			      bdev_io->u.bdev.iovs,
 			      bdev_io->u.bdev.iovcnt,
 			      bdev_io->u.bdev.md_buf,
@@ -726,50 +799,60 @@ bdev_nvme_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io,
 			      bdev_io->u.bdev.offset_blocks,
 			      bdev->dif_check_flags);
 
-	if (spdk_likely(ret == 0)) {
-		return;
-	} else if (ret == -ENOMEM) {
-		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_NOMEM);
-	} else {
-		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
+exit:
+	if (spdk_unlikely(ret != 0)) {
+		bdev_nvme_io_complete(bio, ret);
 	}
 }
 
-static int
-_bdev_nvme_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
+static void
+bdev_nvme_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
 {
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
 	struct spdk_bdev *bdev = bdev_io->bdev;
 	struct nvme_bdev *nbdev = (struct nvme_bdev *)bdev->ctxt;
 	struct nvme_bdev_io *nbdev_io = (struct nvme_bdev_io *)bdev_io->driver_ctx;
 	struct nvme_bdev_io *nbdev_io_to_abort;
-	struct nvme_bdev_ns *nvme_ns;
+	struct spdk_nvme_ns *ns;
 	struct spdk_nvme_qpair *qpair;
+	int rc = 0;
 
-	if (spdk_unlikely(!bdev_nvme_find_io_path(nbdev, nvme_ch, &nvme_ns, &qpair))) {
-		return -1;
+	if (spdk_unlikely(!bdev_nvme_find_io_path(nbdev, io_path, &ns, &qpair))) {
+		rc = -ENXIO;
+		goto exit;
 	}
 
 	switch (bdev_io->type) {
 	case SPDK_BDEV_IO_TYPE_READ:
 		if (bdev_io->u.bdev.iovs && bdev_io->u.bdev.iovs[0].iov_base) {
-			return bdev_nvme_readv(nvme_ns->ns,
-					       qpair,
-					       nbdev_io,
-					       bdev_io->u.bdev.iovs,
-					       bdev_io->u.bdev.iovcnt,
-					       bdev_io->u.bdev.md_buf,
-					       bdev_io->u.bdev.num_blocks,
-					       bdev_io->u.bdev.offset_blocks,
-					       bdev->dif_check_flags);
+			rc = bdev_nvme_readv(ns,
+					     qpair,
+					     nbdev_io,
+					     bdev_io->u.bdev.iovs,
+					     bdev_io->u.bdev.iovcnt,
+					     bdev_io->u.bdev.md_buf,
+					     bdev_io->u.bdev.num_blocks,
+					     bdev_io->u.bdev.offset_blocks,
+					     bdev->dif_check_flags);
 		} else {
 			spdk_bdev_io_get_buf(bdev_io, bdev_nvme_get_buf_cb,
 					     bdev_io->u.bdev.num_blocks * bdev->blocklen);
-			return 0;
+			rc = 0;
 		}
-
+		break;
 	case SPDK_BDEV_IO_TYPE_WRITE:
-		return bdev_nvme_writev(nvme_ns->ns,
+		rc = bdev_nvme_writev(ns,
+				      qpair,
+				      nbdev_io,
+				      bdev_io->u.bdev.iovs,
+				      bdev_io->u.bdev.iovcnt,
+				      bdev_io->u.bdev.md_buf,
+				      bdev_io->u.bdev.num_blocks,
+				      bdev_io->u.bdev.offset_blocks,
+				      bdev->dif_check_flags);
+		break;
+	case SPDK_BDEV_IO_TYPE_COMPARE:
+		rc = bdev_nvme_comparev(ns,
 					qpair,
 					nbdev_io,
 					bdev_io->u.bdev.iovs,
@@ -778,129 +861,108 @@ _bdev_nvme_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_
 					bdev_io->u.bdev.num_blocks,
 					bdev_io->u.bdev.offset_blocks,
 					bdev->dif_check_flags);
-
-	case SPDK_BDEV_IO_TYPE_COMPARE:
-		return bdev_nvme_comparev(nvme_ns->ns,
-					  qpair,
-					  nbdev_io,
-					  bdev_io->u.bdev.iovs,
-					  bdev_io->u.bdev.iovcnt,
-					  bdev_io->u.bdev.md_buf,
-					  bdev_io->u.bdev.num_blocks,
-					  bdev_io->u.bdev.offset_blocks,
-					  bdev->dif_check_flags);
-
+		break;
 	case SPDK_BDEV_IO_TYPE_COMPARE_AND_WRITE:
-		return bdev_nvme_comparev_and_writev(nvme_ns->ns,
-						     qpair,
-						     nbdev_io,
-						     bdev_io->u.bdev.iovs,
-						     bdev_io->u.bdev.iovcnt,
-						     bdev_io->u.bdev.fused_iovs,
-						     bdev_io->u.bdev.fused_iovcnt,
-						     bdev_io->u.bdev.md_buf,
-						     bdev_io->u.bdev.num_blocks,
-						     bdev_io->u.bdev.offset_blocks,
-						     bdev->dif_check_flags);
-
-	case SPDK_BDEV_IO_TYPE_WRITE_ZEROES:
-		return bdev_nvme_unmap(nvme_ns->ns,
-				       qpair,
-				       nbdev_io,
-				       bdev_io->u.bdev.offset_blocks,
-				       bdev_io->u.bdev.num_blocks);
-
+		rc = bdev_nvme_comparev_and_writev(ns,
+						   qpair,
+						   nbdev_io,
+						   bdev_io->u.bdev.iovs,
+						   bdev_io->u.bdev.iovcnt,
+						   bdev_io->u.bdev.fused_iovs,
+						   bdev_io->u.bdev.fused_iovcnt,
+						   bdev_io->u.bdev.md_buf,
+						   bdev_io->u.bdev.num_blocks,
+						   bdev_io->u.bdev.offset_blocks,
+						   bdev->dif_check_flags);
+		break;
 	case SPDK_BDEV_IO_TYPE_UNMAP:
-		return bdev_nvme_unmap(nvme_ns->ns,
-				       qpair,
-				       nbdev_io,
-				       bdev_io->u.bdev.offset_blocks,
-				       bdev_io->u.bdev.num_blocks);
-
+		rc = bdev_nvme_unmap(ns,
+				     qpair,
+				     nbdev_io,
+				     bdev_io->u.bdev.offset_blocks,
+				     bdev_io->u.bdev.num_blocks);
+		break;
+	case SPDK_BDEV_IO_TYPE_WRITE_ZEROES:
+		rc =  bdev_nvme_write_zeroes(ns, qpair,
+					     nbdev_io,
+					     bdev_io->u.bdev.offset_blocks,
+					     bdev_io->u.bdev.num_blocks);
+		break;
 	case SPDK_BDEV_IO_TYPE_RESET:
-		return bdev_nvme_reset(nvme_ch, nbdev_io);
-
+		rc = bdev_nvme_reset(io_path, bdev_io);
+		break;
 	case SPDK_BDEV_IO_TYPE_FLUSH:
-		return bdev_nvme_flush(nvme_ns->ns,
-				       qpair,
-				       nbdev_io,
-				       bdev_io->u.bdev.offset_blocks,
-				       bdev_io->u.bdev.num_blocks);
-
+		rc = bdev_nvme_flush(ns,
+				     qpair,
+				     nbdev_io,
+				     bdev_io->u.bdev.offset_blocks,
+				     bdev_io->u.bdev.num_blocks);
+		break;
 	case SPDK_BDEV_IO_TYPE_ZONE_APPEND:
-		return bdev_nvme_zone_appendv(nvme_ns->ns,
-					      qpair,
-					      nbdev_io,
-					      bdev_io->u.bdev.iovs,
-					      bdev_io->u.bdev.iovcnt,
-					      bdev_io->u.bdev.md_buf,
-					      bdev_io->u.bdev.num_blocks,
-					      bdev_io->u.bdev.offset_blocks,
-					      bdev->dif_check_flags);
-
+		rc = bdev_nvme_zone_appendv(ns,
+					    qpair,
+					    nbdev_io,
+					    bdev_io->u.bdev.iovs,
+					    bdev_io->u.bdev.iovcnt,
+					    bdev_io->u.bdev.md_buf,
+					    bdev_io->u.bdev.num_blocks,
+					    bdev_io->u.bdev.offset_blocks,
+					    bdev->dif_check_flags);
+		break;
 	case SPDK_BDEV_IO_TYPE_GET_ZONE_INFO:
-		return bdev_nvme_get_zone_info(nvme_ns->ns,
+		rc = bdev_nvme_get_zone_info(ns,
+					     qpair,
+					     nbdev_io,
+					     bdev_io->u.zone_mgmt.zone_id,
+					     bdev_io->u.zone_mgmt.num_zones,
+					     bdev_io->u.zone_mgmt.buf);
+		break;
+	case SPDK_BDEV_IO_TYPE_ZONE_MANAGEMENT:
+		rc = bdev_nvme_zone_management(ns,
 					       qpair,
 					       nbdev_io,
 					       bdev_io->u.zone_mgmt.zone_id,
-					       bdev_io->u.zone_mgmt.num_zones,
-					       bdev_io->u.zone_mgmt.buf);
-
-	case SPDK_BDEV_IO_TYPE_ZONE_MANAGEMENT:
-		return bdev_nvme_zone_management(nvme_ns->ns,
-						 qpair,
-						 nbdev_io,
-						 bdev_io->u.zone_mgmt.zone_id,
-						 bdev_io->u.zone_mgmt.zone_action);
-
+					       bdev_io->u.zone_mgmt.zone_action);
+		break;
 	case SPDK_BDEV_IO_TYPE_NVME_ADMIN:
-		return bdev_nvme_admin_passthru(nvme_ch,
-						nbdev_io,
-						&bdev_io->u.nvme_passthru.cmd,
-						bdev_io->u.nvme_passthru.buf,
-						bdev_io->u.nvme_passthru.nbytes);
-
+		rc = bdev_nvme_admin_passthru(io_path,
+					      nbdev_io,
+					      &bdev_io->u.nvme_passthru.cmd,
+					      bdev_io->u.nvme_passthru.buf,
+					      bdev_io->u.nvme_passthru.nbytes);
+		break;
 	case SPDK_BDEV_IO_TYPE_NVME_IO:
-		return bdev_nvme_io_passthru(nvme_ns->ns,
-					     qpair,
-					     nbdev_io,
-					     &bdev_io->u.nvme_passthru.cmd,
-					     bdev_io->u.nvme_passthru.buf,
-					     bdev_io->u.nvme_passthru.nbytes);
-
+		rc = bdev_nvme_io_passthru(ns,
+					   qpair,
+					   nbdev_io,
+					   &bdev_io->u.nvme_passthru.cmd,
+					   bdev_io->u.nvme_passthru.buf,
+					   bdev_io->u.nvme_passthru.nbytes);
+		break;
 	case SPDK_BDEV_IO_TYPE_NVME_IO_MD:
-		return bdev_nvme_io_passthru_md(nvme_ns->ns,
-						qpair,
-						nbdev_io,
-						&bdev_io->u.nvme_passthru.cmd,
-						bdev_io->u.nvme_passthru.buf,
-						bdev_io->u.nvme_passthru.nbytes,
-						bdev_io->u.nvme_passthru.md_buf,
-						bdev_io->u.nvme_passthru.md_len);
-
+		rc = bdev_nvme_io_passthru_md(ns,
+					      qpair,
+					      nbdev_io,
+					      &bdev_io->u.nvme_passthru.cmd,
+					      bdev_io->u.nvme_passthru.buf,
+					      bdev_io->u.nvme_passthru.nbytes,
+					      bdev_io->u.nvme_passthru.md_buf,
+					      bdev_io->u.nvme_passthru.md_len);
+		break;
 	case SPDK_BDEV_IO_TYPE_ABORT:
 		nbdev_io_to_abort = (struct nvme_bdev_io *)bdev_io->u.abort.bio_to_abort->driver_ctx;
-		return bdev_nvme_abort(nvme_ch,
-				       nbdev_io,
-				       nbdev_io_to_abort);
-
+		rc = bdev_nvme_abort(io_path,
+				     nbdev_io,
+				     nbdev_io_to_abort);
+		break;
 	default:
-		return -EINVAL;
+		rc = -EINVAL;
+		break;
 	}
-	return 0;
-}
-
-static void
-bdev_nvme_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
-{
-	int rc = _bdev_nvme_submit_request(ch, bdev_io);
 
+exit:
 	if (spdk_unlikely(rc != 0)) {
-		if (rc == -ENOMEM) {
-			spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_NOMEM);
-		} else {
-			spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
-		}
+		bdev_nvme_io_complete(nbdev_io, rc);
 	}
 }
 
@@ -913,7 +975,7 @@ bdev_nvme_io_type_supported(void *ctx, enum spdk_bdev_io_type io_type)
 	struct spdk_nvme_ctrlr *ctrlr;
 	const struct spdk_nvme_ctrlr_data *cdata;
 
-	nvme_ns = nvme_bdev_to_bdev_ns(nbdev);
+	nvme_ns = nbdev->nvme_ns;
 	assert(nvme_ns != NULL);
 	ns = nvme_ns->ns;
 	ctrlr = spdk_nvme_ns_get_ctrlr(ns);
@@ -940,21 +1002,7 @@ bdev_nvme_io_type_supported(void *ctx, enum spdk_bdev_io_type io_type)
 
 	case SPDK_BDEV_IO_TYPE_WRITE_ZEROES:
 		cdata = spdk_nvme_ctrlr_get_data(ctrlr);
-		/*
-		 * If an NVMe controller guarantees reading unallocated blocks returns zero,
-		 * we can implement WRITE_ZEROES as an NVMe deallocate command.
-		 */
-		if (cdata->oncs.dsm &&
-		    spdk_nvme_ns_get_dealloc_logical_block_read_value(ns) ==
-		    SPDK_NVME_DEALLOC_READ_00) {
-			return true;
-		}
-		/*
-		 * The NVMe controller write_zeroes function is currently not used by our driver.
-		 * If a user submits an arbitrarily large write_zeroes request to the controller, the request will fail.
-		 * Until this is resolved, we only claim support for write_zeroes if deallocated blocks return 0's when read.
-		 */
-		return false;
+		return cdata->oncs.write_zeroes;
 
 	case SPDK_BDEV_IO_TYPE_COMPARE_AND_WRITE:
 		if (spdk_nvme_ctrlr_get_flags(ctrlr) &
@@ -977,39 +1025,38 @@ bdev_nvme_io_type_supported(void *ctx, enum spdk_bdev_io_type io_type)
 }
 
 static int
-bdev_nvme_create_cb(void *io_device, void *ctx_buf)
+bdev_nvme_create_path_cb(void *io_device, void *ctx_buf)
 {
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = io_device;
-	struct nvme_io_channel *nvme_ch = ctx_buf;
-	struct spdk_io_channel *pg_ch = NULL;
+	struct nvme_io_path *io_path = ctx_buf;
+	struct spdk_io_channel *pg_ch;
 	int rc;
 
-	if (spdk_nvme_ctrlr_is_ocssd_supported(nvme_bdev_ctrlr->ctrlr)) {
-		rc = bdev_ocssd_create_io_channel(nvme_ch);
-		if (rc != 0) {
-			return rc;
-		}
-	}
-
 	pg_ch = spdk_get_io_channel(&g_nvme_bdev_ctrlrs);
 	if (!pg_ch) {
-		rc = -1;
-		goto err_pg_ch;
+		return -1;
 	}
 
-	nvme_ch->group = spdk_io_channel_get_ctx(pg_ch);
+	io_path->group = spdk_io_channel_get_ctx(pg_ch);
 
 #ifdef SPDK_CONFIG_VTUNE
-	nvme_ch->group->collect_spin_stat = true;
+	io_path->group->collect_spin_stat = true;
 #else
-	nvme_ch->group->collect_spin_stat = false;
+	io_path->group->collect_spin_stat = false;
 #endif
 
-	TAILQ_INIT(&nvme_ch->pending_resets);
+	TAILQ_INIT(&io_path->pending_resets);
+
+	if (spdk_nvme_ctrlr_is_ocssd_supported(nvme_bdev_ctrlr->ctrlr)) {
+		rc = bdev_ocssd_create_io_channel(io_path);
+		if (rc != 0) {
+			goto err_ocssd_ch;
+		}
+	}
 
-	nvme_ch->ctrlr = nvme_bdev_ctrlr;
+	io_path->ctrlr = nvme_bdev_ctrlr;
 
-	rc = bdev_nvme_create_qpair(nvme_ch);
+	rc = bdev_nvme_create_qpair(io_path);
 	if (rc != 0) {
 		goto err_qpair;
 	}
@@ -1017,29 +1064,29 @@ bdev_nvme_create_cb(void *io_device, void *ctx_buf)
 	return 0;
 
 err_qpair:
-	spdk_put_io_channel(pg_ch);
-err_pg_ch:
-	if (nvme_ch->ocssd_ch) {
-		bdev_ocssd_destroy_io_channel(nvme_ch);
+	if (io_path->ocssd_ch) {
+		bdev_ocssd_destroy_io_channel(io_path);
 	}
+err_ocssd_ch:
+	spdk_put_io_channel(pg_ch);
 
 	return rc;
 }
 
 static void
-bdev_nvme_destroy_cb(void *io_device, void *ctx_buf)
+bdev_nvme_destroy_path_cb(void *io_device, void *ctx_buf)
 {
-	struct nvme_io_channel *nvme_ch = ctx_buf;
+	struct nvme_io_path *io_path = ctx_buf;
 
-	assert(nvme_ch->group != NULL);
+	assert(io_path->group != NULL);
 
-	if (nvme_ch->ocssd_ch != NULL) {
-		bdev_ocssd_destroy_io_channel(nvme_ch);
+	if (io_path->ocssd_ch != NULL) {
+		bdev_ocssd_destroy_io_channel(io_path);
 	}
 
-	bdev_nvme_destroy_qpair(nvme_ch);
+	bdev_nvme_destroy_qpair(io_path);
 
-	spdk_put_io_channel(spdk_io_channel_from_ctx(nvme_ch->group));
+	spdk_put_io_channel(spdk_io_channel_from_ctx(io_path->group));
 }
 
 static void
@@ -1108,7 +1155,7 @@ bdev_nvme_poll_group_destroy_cb(void *io_device, void *ctx_buf)
 
 	spdk_poller_unregister(&group->poller);
 	if (spdk_nvme_poll_group_destroy(group->group)) {
-		SPDK_ERRLOG("Unable to destroy a poll group for the NVMe bdev module.");
+		SPDK_ERRLOG("Unable to destroy a poll group for the NVMe bdev module.\n");
 		assert(false);
 	}
 }
@@ -1129,6 +1176,25 @@ bdev_nvme_get_module_ctx(void *ctx)
 	return bdev_nvme_get_ctrlr(&nvme_bdev->disk);
 }
 
+static const char *
+_nvme_ana_state_str(enum spdk_nvme_ana_state ana_state)
+{
+	switch (ana_state) {
+	case SPDK_NVME_ANA_OPTIMIZED_STATE:
+		return "optimized";
+	case SPDK_NVME_ANA_NON_OPTIMIZED_STATE:
+		return "non_optimized";
+	case SPDK_NVME_ANA_INACCESSIBLE_STATE:
+		return "inaccessible";
+	case SPDK_NVME_ANA_PERSISTENT_LOSS_STATE:
+		return "persistent_loss";
+	case SPDK_NVME_ANA_CHANGE_STATE:
+		return "change";
+	default:
+		return NULL;
+	}
+}
+
 static int
 bdev_nvme_dump_info_json(void *ctx, struct spdk_json_write_ctx *w)
 {
@@ -1142,7 +1208,7 @@ bdev_nvme_dump_info_json(void *ctx, struct spdk_json_write_ctx *w)
 	union spdk_nvme_csts_register csts;
 	char buf[128];
 
-	nvme_ns = nvme_bdev_to_bdev_ns(nvme_bdev);
+	nvme_ns = nvme_bdev->nvme_ns;
 	assert(nvme_ns != NULL);
 	ns = nvme_ns->ns;
 	ctrlr = spdk_nvme_ns_get_ctrlr(ns);
@@ -1228,6 +1294,11 @@ bdev_nvme_dump_info_json(void *ctx, struct spdk_json_write_ctx *w)
 
 	spdk_json_write_named_uint32(w, "id", spdk_nvme_ns_get_id(ns));
 
+	if (cdata->cmic.ana_reporting) {
+		spdk_json_write_named_string(w, "ana_state",
+					     _nvme_ana_state_str(spdk_nvme_ns_get_ana_state(ns)));
+	}
+
 	spdk_json_write_object_end(w);
 
 	if (cdata->oacs.security) {
@@ -1252,8 +1323,8 @@ bdev_nvme_write_config_json(struct spdk_bdev *bdev, struct spdk_json_write_ctx *
 static uint64_t
 bdev_nvme_get_spin_time(struct spdk_io_channel *ch)
 {
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
-	struct nvme_bdev_poll_group *group = nvme_ch->group;
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
+	struct nvme_bdev_poll_group *group = io_path->group;
 	uint64_t spin_time;
 
 	if (!group || !group->collect_spin_stat) {
@@ -1293,6 +1364,7 @@ nvme_disk_create(struct spdk_bdev *disk, const char *base_name,
 	const struct spdk_nvme_ns_data	*nsdata;
 	int				rc;
 	enum spdk_nvme_csi		csi;
+	uint32_t atomic_bs, phys_bs, bs;
 
 	cdata = spdk_nvme_ctrlr_get_data(ctrlr);
 	csi = spdk_nvme_ns_get_csi(ns);
@@ -1325,6 +1397,9 @@ nvme_disk_create(struct spdk_bdev *disk, const char *base_name,
 		/* Enable if the Volatile Write Cache exists */
 		disk->write_cache = 1;
 	}
+	if (cdata->oncs.write_zeroes) {
+		disk->max_write_zeroes = UINT16_MAX + 1;
+	}
 	disk->blocklen = spdk_nvme_ns_get_extended_sector_size(ns);
 	disk->blockcnt = spdk_nvme_ns_get_num_sectors(ns);
 	disk->optimal_io_boundary = spdk_nvme_ns_get_optimal_io_boundary(ns);
@@ -1335,6 +1410,20 @@ nvme_disk_create(struct spdk_bdev *disk, const char *base_name,
 	}
 
 	nsdata = spdk_nvme_ns_get_data(ns);
+	bs = spdk_nvme_ns_get_sector_size(ns);
+	atomic_bs = bs;
+	phys_bs = bs;
+	if (nsdata->nabo == 0) {
+		if (nsdata->nsfeat.ns_atomic_write_unit && nsdata->nawupf) {
+			atomic_bs = bs * (1 + nsdata->nawupf);
+		} else {
+			atomic_bs = bs * (1 + cdata->awupf);
+		}
+	}
+	if (nsdata->nsfeat.optperf) {
+		phys_bs = bs * (1 + nsdata->npwg);
+	}
+	disk->phys_blocklen = spdk_min(phys_bs, atomic_bs);
 
 	disk->md_len = spdk_nvme_ns_get_md_size(ns);
 	if (disk->md_len != 0) {
@@ -1396,6 +1485,22 @@ nvme_bdev_create(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr, struct nvme_bdev_ns *n
 	return 0;
 }
 
+static bool
+bdev_nvme_compare_ns(struct spdk_nvme_ns *ns1, struct spdk_nvme_ns *ns2)
+{
+	const struct spdk_nvme_ns_data *nsdata1, *nsdata2;
+	const struct spdk_uuid *uuid1, *uuid2;
+
+	nsdata1 = spdk_nvme_ns_get_data(ns1);
+	nsdata2 = spdk_nvme_ns_get_data(ns2);
+	uuid1 = spdk_nvme_ns_get_uuid(ns1);
+	uuid2 = spdk_nvme_ns_get_uuid(ns2);
+
+	return memcmp(nsdata1->nguid, nsdata2->nguid, sizeof(nsdata1->nguid)) == 0 &&
+	       nsdata1->eui64 == nsdata2->eui64 &&
+	       uuid1 != NULL && uuid2 != NULL && spdk_uuid_compare(uuid1, uuid2) == 0;
+}
+
 static void
 nvme_ctrlr_populate_standard_namespace(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
 				       struct nvme_bdev_ns *nvme_ns, struct nvme_async_probe_ctx *ctx)
@@ -1447,7 +1552,8 @@ nvme_abort_cpl(void *ctx, const struct spdk_nvme_cpl *cpl)
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = ctx;
 
 	if (spdk_nvme_cpl_is_error(cpl)) {
-		SPDK_WARNLOG("Abort failed. Resetting controller.\n");
+		SPDK_WARNLOG("Abort failed. Resetting controller. sc is %u, sct is %u.\n", cpl->status.sc,
+			     cpl->status.sct);
 		_bdev_nvme_reset(nvme_bdev_ctrlr);
 	}
 }
@@ -1481,13 +1587,22 @@ timeout_cb(void *cb_arg, struct spdk_nvme_ctrlr *ctrlr,
 	switch (g_opts.action_on_timeout) {
 	case SPDK_BDEV_NVME_TIMEOUT_ACTION_ABORT:
 		if (qpair) {
+			/* Don't send abort to ctrlr when reset is running. */
+			pthread_mutex_lock(&nvme_bdev_ctrlr->mutex);
+			if (nvme_bdev_ctrlr->resetting) {
+				pthread_mutex_unlock(&nvme_bdev_ctrlr->mutex);
+				SPDK_NOTICELOG("Quit abort. Ctrlr is in the process of reseting.\n");
+				return;
+			}
+			pthread_mutex_unlock(&nvme_bdev_ctrlr->mutex);
+
 			rc = spdk_nvme_ctrlr_cmd_abort(ctrlr, qpair, cid,
 						       nvme_abort_cpl, nvme_bdev_ctrlr);
 			if (rc == 0) {
 				return;
 			}
 
-			SPDK_ERRLOG("Unable to send abort. Resetting.\n");
+			SPDK_ERRLOG("Unable to send abort. Resetting, rc is %d.\n", rc);
 		}
 
 	/* FALLTHROUGH */
@@ -1691,16 +1806,29 @@ populate_namespaces_cb(struct nvme_async_probe_ctx *ctx, size_t count, int rc)
 	}
 }
 
+static void
+nvme_bdev_ctrlr_create_done(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
+			    struct nvme_async_probe_ctx *ctx)
+{
+	spdk_io_device_register(nvme_bdev_ctrlr,
+				bdev_nvme_create_path_cb,
+				bdev_nvme_destroy_path_cb,
+				sizeof(struct nvme_io_path),
+				nvme_bdev_ctrlr->name);
+
+	nvme_ctrlr_populate_namespaces(nvme_bdev_ctrlr, ctx);
+}
+
 static int
-_nvme_bdev_ctrlr_create(struct spdk_nvme_ctrlr *ctrlr,
-			const char *name,
-			const struct spdk_nvme_transport_id *trid,
-			uint32_t prchk_flags,
-			struct nvme_bdev_ctrlr **_nvme_bdev_ctrlr)
+nvme_bdev_ctrlr_create(struct spdk_nvme_ctrlr *ctrlr,
+		       const char *name,
+		       const struct spdk_nvme_transport_id *trid,
+		       uint32_t prchk_flags,
+		       struct nvme_async_probe_ctx *ctx)
 {
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr;
 	struct nvme_bdev_ctrlr_trid *trid_entry;
-	uint32_t i;
+	uint32_t i, num_ns;
 	int rc;
 
 	nvme_bdev_ctrlr = calloc(1, sizeof(*nvme_bdev_ctrlr));
@@ -1711,63 +1839,64 @@ _nvme_bdev_ctrlr_create(struct spdk_nvme_ctrlr *ctrlr,
 
 	rc = pthread_mutex_init(&nvme_bdev_ctrlr->mutex, NULL);
 	if (rc != 0) {
-		goto err_init_mutex;
+		free(nvme_bdev_ctrlr);
+		return rc;
 	}
 
 	TAILQ_INIT(&nvme_bdev_ctrlr->trids);
-	nvme_bdev_ctrlr->num_ns = spdk_nvme_ctrlr_get_num_ns(ctrlr);
-	if (nvme_bdev_ctrlr->num_ns != 0) {
-		nvme_bdev_ctrlr->namespaces = calloc(nvme_bdev_ctrlr->num_ns, sizeof(struct nvme_bdev_ns *));
+
+	num_ns = spdk_nvme_ctrlr_get_num_ns(ctrlr);
+	if (num_ns != 0) {
+		nvme_bdev_ctrlr->namespaces = calloc(num_ns, sizeof(struct nvme_bdev_ns *));
 		if (!nvme_bdev_ctrlr->namespaces) {
 			SPDK_ERRLOG("Failed to allocate block namespaces pointer\n");
 			rc = -ENOMEM;
-			goto err_alloc_namespaces;
+			goto err;
 		}
+
+		for (i = 0; i < num_ns; i++) {
+			nvme_bdev_ctrlr->namespaces[i] = calloc(1, sizeof(struct nvme_bdev_ns));
+			if (nvme_bdev_ctrlr->namespaces[i] == NULL) {
+				SPDK_ERRLOG("Failed to allocate block namespace struct\n");
+				rc = -ENOMEM;
+				goto err;
+			}
+			nvme_bdev_ctrlr->num_ns++;
+		}
+
+		assert(num_ns == nvme_bdev_ctrlr->num_ns);
 	}
 
 	trid_entry = calloc(1, sizeof(*trid_entry));
 	if (trid_entry == NULL) {
 		SPDK_ERRLOG("Failed to allocate trid entry pointer\n");
 		rc = -ENOMEM;
-		goto err_alloc_trid;
+		goto err;
 	}
 
 	trid_entry->trid = *trid;
-
-	for (i = 0; i < nvme_bdev_ctrlr->num_ns; i++) {
-		nvme_bdev_ctrlr->namespaces[i] = calloc(1, sizeof(struct nvme_bdev_ns));
-		if (nvme_bdev_ctrlr->namespaces[i] == NULL) {
-			SPDK_ERRLOG("Failed to allocate block namespace struct\n");
-			rc = -ENOMEM;
-			goto err_alloc_namespace;
-		}
-	}
+	nvme_bdev_ctrlr->connected_trid = &trid_entry->trid;
+	TAILQ_INSERT_HEAD(&nvme_bdev_ctrlr->trids, trid_entry, link);
 
 	nvme_bdev_ctrlr->thread = spdk_get_thread();
-	nvme_bdev_ctrlr->adminq_timer_poller = NULL;
 	nvme_bdev_ctrlr->ctrlr = ctrlr;
 	nvme_bdev_ctrlr->ref = 1;
-	nvme_bdev_ctrlr->connected_trid = &trid_entry->trid;
 	nvme_bdev_ctrlr->name = strdup(name);
 	if (nvme_bdev_ctrlr->name == NULL) {
 		rc = -ENOMEM;
-		goto err_alloc_name;
+		goto err;
 	}
 
 	if (spdk_nvme_ctrlr_is_ocssd_supported(nvme_bdev_ctrlr->ctrlr)) {
 		rc = bdev_ocssd_init_ctrlr(nvme_bdev_ctrlr);
 		if (spdk_unlikely(rc != 0)) {
 			SPDK_ERRLOG("Unable to initialize OCSSD controller\n");
-			goto err_init_ocssd;
+			goto err;
 		}
 	}
 
 	nvme_bdev_ctrlr->prchk_flags = prchk_flags;
 
-	spdk_io_device_register(nvme_bdev_ctrlr, bdev_nvme_create_cb, bdev_nvme_destroy_cb,
-				sizeof(struct nvme_io_channel),
-				name);
-
 	nvme_bdev_ctrlr->adminq_timer_poller = SPDK_POLLER_REGISTER(bdev_nvme_poll_adminq, nvme_bdev_ctrlr,
 					       g_opts.nvme_adminq_poll_period_us);
 
@@ -1784,58 +1913,14 @@ _nvme_bdev_ctrlr_create(struct spdk_nvme_ctrlr *ctrlr,
 	if (spdk_nvme_ctrlr_get_flags(nvme_bdev_ctrlr->ctrlr) &
 	    SPDK_NVME_CTRLR_SECURITY_SEND_RECV_SUPPORTED) {
 		nvme_bdev_ctrlr->opal_dev = spdk_opal_dev_construct(nvme_bdev_ctrlr->ctrlr);
-		if (nvme_bdev_ctrlr->opal_dev == NULL) {
-			SPDK_ERRLOG("Failed to initialize Opal\n");
-		}
 	}
 
-	TAILQ_INSERT_HEAD(&nvme_bdev_ctrlr->trids, trid_entry, link);
-
-	if (_nvme_bdev_ctrlr != NULL) {
-		*_nvme_bdev_ctrlr = nvme_bdev_ctrlr;
-	}
+	nvme_bdev_ctrlr_create_done(nvme_bdev_ctrlr, ctx);
 	return 0;
 
-err_init_ocssd:
-	free(nvme_bdev_ctrlr->name);
-err_alloc_name:
-err_alloc_namespace:
-	for (; i > 0; i--) {
-		free(nvme_bdev_ctrlr->namespaces[i - 1]);
-	}
-	free(trid_entry);
-err_alloc_trid:
-	free(nvme_bdev_ctrlr->namespaces);
-err_alloc_namespaces:
-	pthread_mutex_destroy(&nvme_bdev_ctrlr->mutex);
-err_init_mutex:
-	free(nvme_bdev_ctrlr);
-	return rc;
-}
-
-static void
-nvme_bdev_ctrlr_create(struct spdk_nvme_ctrlr *ctrlr,
-		       const char *name,
-		       const struct spdk_nvme_transport_id *trid,
-		       uint32_t prchk_flags,
-		       struct nvme_async_probe_ctx *ctx)
-{
-	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = NULL;
-	int rc;
-
-	rc = _nvme_bdev_ctrlr_create(ctrlr, name, trid, prchk_flags, &nvme_bdev_ctrlr);
-	if (rc != 0) {
-		SPDK_ERRLOG("Failed to create new NVMe controller\n");
-		goto err;
-	}
-
-	nvme_ctrlr_populate_namespaces(nvme_bdev_ctrlr, ctx);
-	return;
-
 err:
-	if (ctx != NULL) {
-		populate_namespaces_cb(ctx, 0, rc);
-	}
+	nvme_bdev_ctrlr_delete(nvme_bdev_ctrlr);
+	return rc;
 }
 
 static void
@@ -1876,7 +1961,7 @@ _nvme_bdev_ctrlr_destruct(void *ctx)
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = ctx;
 
 	nvme_ctrlr_depopulate_namespaces(nvme_bdev_ctrlr);
-	nvme_bdev_ctrlr_destruct(nvme_bdev_ctrlr);
+	nvme_bdev_ctrlr_release(nvme_bdev_ctrlr);
 }
 
 static int
@@ -2102,17 +2187,6 @@ bdev_nvme_compare_trids(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
 	return 0;
 }
 
-static bool
-bdev_nvme_compare_ns(struct spdk_nvme_ns *ns1, struct spdk_nvme_ns *ns2)
-{
-	const struct spdk_nvme_ns_data *nsdata1, *nsdata2;
-
-	nsdata1 = spdk_nvme_ns_get_data(ns1);
-	nsdata2 = spdk_nvme_ns_get_data(ns2);
-
-	return memcmp(nsdata1->nguid, nsdata2->nguid, sizeof(nsdata1->nguid));
-}
-
 static int
 bdev_nvme_compare_namespaces(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
 			     struct spdk_nvme_ctrlr *new_ctrlr)
@@ -2136,7 +2210,7 @@ bdev_nvme_compare_namespaces(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
 		new_ns = spdk_nvme_ctrlr_get_ns(new_ctrlr, nsid);
 		assert(new_ns != NULL);
 
-		if (bdev_nvme_compare_ns(nvme_ns->ns, new_ns) != 0) {
+		if (!bdev_nvme_compare_ns(nvme_ns->ns, new_ns)) {
 			return -EINVAL;
 		}
 	}
@@ -2172,11 +2246,10 @@ _bdev_nvme_add_secondary_trid(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
  * nvme_bdev_ctrlr for failover. After checking if it can access the same
  * namespaces as the primary path, it is disconnected until failover occurs.
  */
-static void
+static int
 bdev_nvme_add_secondary_trid(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
 			     struct spdk_nvme_ctrlr *new_ctrlr,
-			     struct spdk_nvme_transport_id *trid,
-			     struct nvme_async_probe_ctx *ctx)
+			     struct spdk_nvme_transport_id *trid)
 {
 	int rc;
 
@@ -2201,9 +2274,7 @@ exit:
 
 	spdk_nvme_detach(new_ctrlr);
 
-	if (ctx != NULL) {
-		populate_namespaces_cb(ctx, 0, rc);
-	}
+	return rc;
 }
 
 static void
@@ -2213,17 +2284,22 @@ connect_attach_cb(void *cb_ctx, const struct spdk_nvme_transport_id *trid,
 	struct spdk_nvme_ctrlr_opts *user_opts = cb_ctx;
 	struct nvme_bdev_ctrlr	*nvme_bdev_ctrlr;
 	struct nvme_async_probe_ctx *ctx;
+	int rc;
 
 	ctx = SPDK_CONTAINEROF(user_opts, struct nvme_async_probe_ctx, opts);
 	ctx->ctrlr_attached = true;
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name(ctx->base_name);
 	if (nvme_bdev_ctrlr) {
-		bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, ctrlr, &ctx->trid, ctx);
-		return;
+		rc = bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, ctrlr, &ctx->trid);
+	} else {
+		rc = nvme_bdev_ctrlr_create(ctrlr, ctx->base_name, &ctx->trid, ctx->prchk_flags, ctx);
+		if (rc == 0) {
+			return;
+		}
 	}
 
-	nvme_bdev_ctrlr_create(ctrlr, ctx->base_name, &ctx->trid, ctx->prchk_flags, ctx);
+	populate_namespaces_cb(ctx, 0, rc);
 }
 
 static int
@@ -2447,8 +2523,9 @@ bdev_nvme_library_fini(void)
 }
 
 static void
-bdev_nvme_verify_pi_error(struct spdk_bdev_io *bdev_io)
+bdev_nvme_verify_pi_error(struct nvme_bdev_io *bio)
 {
+	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 	struct spdk_bdev *bdev = bdev_io->bdev;
 	struct spdk_dif_ctx dif_ctx;
 	struct spdk_dif_error err_blk = {};
@@ -2488,16 +2565,14 @@ static void
 bdev_nvme_no_pi_readv_done(void *ref, const struct spdk_nvme_cpl *cpl)
 {
 	struct nvme_bdev_io *bio = ref;
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 
 	if (spdk_nvme_cpl_is_success(cpl)) {
 		/* Run PI verification for read data buffer. */
-		bdev_nvme_verify_pi_error(bdev_io);
+		bdev_nvme_verify_pi_error(bio);
 	}
 
 	/* Return original completion status */
-	spdk_bdev_io_complete_nvme_status(bdev_io, bio->cpl.cdw0, bio->cpl.status.sct,
-					  bio->cpl.status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, &bio->cpl);
 }
 
 static void
@@ -2506,8 +2581,8 @@ bdev_nvme_readv_done(void *ref, const struct spdk_nvme_cpl *cpl)
 	struct nvme_bdev_io *bio = ref;
 	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 	struct nvme_bdev *nbdev = (struct nvme_bdev *)bdev_io->bdev->ctxt;
-	struct nvme_io_channel *nvme_ch;
-	struct nvme_bdev_ns *nvme_ns;
+	struct nvme_io_path *io_path;
+	struct spdk_nvme_ns *ns;
 	struct spdk_nvme_qpair *qpair;
 	int ret;
 
@@ -2518,11 +2593,11 @@ bdev_nvme_readv_done(void *ref, const struct spdk_nvme_cpl *cpl)
 		/* Save completion status to use after verifying PI error. */
 		bio->cpl = *cpl;
 
-		nvme_ch = spdk_io_channel_get_ctx(spdk_bdev_io_get_io_channel(bdev_io));
+		io_path = spdk_io_channel_get_ctx(spdk_bdev_io_get_io_channel(bdev_io));
 
-		if (spdk_likely(bdev_nvme_find_io_path(nbdev, nvme_ch, &nvme_ns, &qpair))) {
+		if (spdk_likely(bdev_nvme_find_io_path(nbdev, io_path, &ns, &qpair))) {
 			/* Read without PI checking to verify PI error. */
-			ret = bdev_nvme_no_pi_readv(nvme_ns->ns,
+			ret = bdev_nvme_no_pi_readv(ns,
 						    qpair,
 						    bio,
 						    bdev_io->u.bdev.iovs,
@@ -2536,28 +2611,29 @@ bdev_nvme_readv_done(void *ref, const struct spdk_nvme_cpl *cpl)
 		}
 	}
 
-	spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, cpl);
 }
 
 static void
 bdev_nvme_writev_done(void *ref, const struct spdk_nvme_cpl *cpl)
 {
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx((struct nvme_bdev_io *)ref);
+	struct nvme_bdev_io *bio = ref;
 
 	if (spdk_nvme_cpl_is_pi_error(cpl)) {
 		SPDK_ERRLOG("writev completed with PI error (sct=%d, sc=%d)\n",
 			    cpl->status.sct, cpl->status.sc);
 		/* Run PI verification for write data buffer if PI error is detected. */
-		bdev_nvme_verify_pi_error(bdev_io);
+		bdev_nvme_verify_pi_error(bio);
 	}
 
-	spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, cpl);
 }
 
 static void
 bdev_nvme_zone_appendv_done(void *ref, const struct spdk_nvme_cpl *cpl)
 {
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx((struct nvme_bdev_io *)ref);
+	struct nvme_bdev_io *bio = ref;
+	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 
 	/* spdk_bdev_io_get_append_location() requires that the ALBA is stored in offset_blocks.
 	 * Additionally, offset_blocks has to be set before calling bdev_nvme_verify_pi_error().
@@ -2568,32 +2644,31 @@ bdev_nvme_zone_appendv_done(void *ref, const struct spdk_nvme_cpl *cpl)
 		SPDK_ERRLOG("zone append completed with PI error (sct=%d, sc=%d)\n",
 			    cpl->status.sct, cpl->status.sc);
 		/* Run PI verification for zone append data buffer if PI error is detected. */
-		bdev_nvme_verify_pi_error(bdev_io);
+		bdev_nvme_verify_pi_error(bio);
 	}
 
-	spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, cpl);
 }
 
 static void
 bdev_nvme_comparev_done(void *ref, const struct spdk_nvme_cpl *cpl)
 {
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx((struct nvme_bdev_io *)ref);
+	struct nvme_bdev_io *bio = ref;
 
 	if (spdk_nvme_cpl_is_pi_error(cpl)) {
 		SPDK_ERRLOG("comparev completed with PI error (sct=%d, sc=%d)\n",
 			    cpl->status.sct, cpl->status.sc);
 		/* Run PI verification for compare data buffer if PI error is detected. */
-		bdev_nvme_verify_pi_error(bdev_io);
+		bdev_nvme_verify_pi_error(bio);
 	}
 
-	spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, cpl);
 }
 
 static void
 bdev_nvme_comparev_and_writev_done(void *ref, const struct spdk_nvme_cpl *cpl)
 {
 	struct nvme_bdev_io *bio = ref;
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 
 	/* Compare operation completion */
 	if ((cpl->cdw0 & 0xFF) == SPDK_NVME_OPC_COMPARE) {
@@ -2611,18 +2686,18 @@ bdev_nvme_comparev_and_writev_done(void *ref, const struct spdk_nvme_cpl *cpl)
 			SPDK_ERRLOG("Unexpected write success after compare failure.\n");
 		}
 
-		spdk_bdev_io_complete_nvme_status(bdev_io, bio->cpl.cdw0, bio->cpl.status.sct, bio->cpl.status.sc);
+		bdev_nvme_io_complete_nvme_status(bio, &bio->cpl);
 	} else {
-		spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+		bdev_nvme_io_complete_nvme_status(bio, cpl);
 	}
 }
 
 static void
 bdev_nvme_queued_done(void *ref, const struct spdk_nvme_cpl *cpl)
 {
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx((struct nvme_bdev_io *)ref);
+	struct nvme_bdev_io *bio = ref;
 
-	spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, cpl);
 }
 
 static int
@@ -2669,14 +2744,13 @@ bdev_nvme_get_zone_info_done(void *ref, const struct spdk_nvme_cpl *cpl)
 	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 	struct nvme_bdev *nbdev = (struct nvme_bdev *)bdev_io->bdev->ctxt;
 	struct spdk_io_channel *ch = spdk_bdev_io_get_io_channel(bdev_io);
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
 	uint64_t zone_id = bdev_io->u.zone_mgmt.zone_id;
 	uint32_t zones_to_copy = bdev_io->u.zone_mgmt.num_zones;
 	struct spdk_bdev_zone_info *info = bdev_io->u.zone_mgmt.buf;
-	enum spdk_bdev_io_status status;
 	uint64_t max_zones_per_buf, i;
 	uint32_t zone_report_bufsize;
-	struct nvme_bdev_ns *nvme_ns;
+	struct spdk_nvme_ns *ns;
 	struct spdk_nvme_qpair *qpair;
 	int ret;
 
@@ -2684,83 +2758,76 @@ bdev_nvme_get_zone_info_done(void *ref, const struct spdk_nvme_cpl *cpl)
 		goto out_complete_io_nvme_cpl;
 	}
 
-	if (!bdev_nvme_find_io_path(nbdev, nvme_ch, &nvme_ns, &qpair)) {
-		status = SPDK_BDEV_IO_STATUS_FAILED;
-		goto out_complete_io_status;
+	if (!bdev_nvme_find_io_path(nbdev, io_path, &ns, &qpair)) {
+		ret = -ENXIO;
+		goto out_complete_io_ret;
 	}
 
-	zone_report_bufsize = spdk_nvme_ns_get_max_io_xfer_size(nvme_ns->ns);
+	zone_report_bufsize = spdk_nvme_ns_get_max_io_xfer_size(ns);
 	max_zones_per_buf = (zone_report_bufsize - sizeof(*bio->zone_report_buf)) /
 			    sizeof(bio->zone_report_buf->descs[0]);
 
 	if (bio->zone_report_buf->nr_zones > max_zones_per_buf) {
-		status = SPDK_BDEV_IO_STATUS_FAILED;
-		goto out_complete_io_status;
+		ret = -EINVAL;
+		goto out_complete_io_ret;
 	}
 
 	if (!bio->zone_report_buf->nr_zones) {
-		status = SPDK_BDEV_IO_STATUS_FAILED;
-		goto out_complete_io_status;
+		ret = -EINVAL;
+		goto out_complete_io_ret;
 	}
 
 	for (i = 0; i < bio->zone_report_buf->nr_zones && bio->handled_zones < zones_to_copy; i++) {
 		ret = fill_zone_from_report(&info[bio->handled_zones],
 					    &bio->zone_report_buf->descs[i]);
 		if (ret) {
-			status = SPDK_BDEV_IO_STATUS_FAILED;
-			goto out_complete_io_status;
+			goto out_complete_io_ret;
 		}
 		bio->handled_zones++;
 	}
 
 	if (bio->handled_zones < zones_to_copy) {
-		uint64_t zone_size_lba = spdk_nvme_zns_ns_get_zone_size_sectors(nvme_ns->ns);
+		uint64_t zone_size_lba = spdk_nvme_zns_ns_get_zone_size_sectors(ns);
 		uint64_t slba = zone_id + (zone_size_lba * bio->handled_zones);
 
 		memset(bio->zone_report_buf, 0, zone_report_bufsize);
-		ret = spdk_nvme_zns_report_zones(nvme_ns->ns, qpair,
+		ret = spdk_nvme_zns_report_zones(ns, qpair,
 						 bio->zone_report_buf, zone_report_bufsize,
 						 slba, SPDK_NVME_ZRA_LIST_ALL, true,
 						 bdev_nvme_get_zone_info_done, bio);
 		if (!ret) {
 			return;
-		} else if (ret == -ENOMEM) {
-			status = SPDK_BDEV_IO_STATUS_NOMEM;
-			goto out_complete_io_status;
 		} else {
-			status = SPDK_BDEV_IO_STATUS_FAILED;
-			goto out_complete_io_status;
+			goto out_complete_io_ret;
 		}
 	}
 
 out_complete_io_nvme_cpl:
 	free(bio->zone_report_buf);
 	bio->zone_report_buf = NULL;
-	spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, cpl);
 	return;
 
-out_complete_io_status:
+out_complete_io_ret:
 	free(bio->zone_report_buf);
 	bio->zone_report_buf = NULL;
-	spdk_bdev_io_complete(bdev_io, status);
+	bdev_nvme_io_complete(bio, ret);
 }
 
 static void
 bdev_nvme_zone_management_done(void *ref, const struct spdk_nvme_cpl *cpl)
 {
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx((struct nvme_bdev_io *)ref);
+	struct nvme_bdev_io *bio = ref;
 
-	spdk_bdev_io_complete_nvme_status(bdev_io, cpl->cdw0, cpl->status.sct, cpl->status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, cpl);
 }
 
 static void
 bdev_nvme_admin_passthru_completion(void *ctx)
 {
 	struct nvme_bdev_io *bio = ctx;
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
 
-	spdk_bdev_io_complete_nvme_status(bdev_io,
-					  bio->cpl.cdw0, bio->cpl.status.sct, bio->cpl.status.sc);
+	bdev_nvme_io_complete_nvme_status(bio, &bio->cpl);
 }
 
 static void
@@ -3146,6 +3213,23 @@ bdev_nvme_unmap(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 	return rc;
 }
 
+static int
+bdev_nvme_write_zeroes(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+		       struct nvme_bdev_io *bio,
+		       uint64_t offset_blocks,
+		       uint64_t num_blocks)
+{
+	if (num_blocks > UINT16_MAX + 1) {
+		SPDK_ERRLOG("NVMe write zeroes is limited to 16-bit block count\n");
+		return -EINVAL;
+	}
+
+	return spdk_nvme_ns_cmd_write_zeroes(ns, qpair,
+					     offset_blocks, num_blocks,
+					     bdev_nvme_queued_done, bio,
+					     0);
+}
+
 static int
 bdev_nvme_get_zone_info(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 			struct nvme_bdev_io *bio, uint64_t zone_id, uint32_t num_zones,
@@ -3203,13 +3287,13 @@ bdev_nvme_zone_management(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair
 }
 
 static int
-bdev_nvme_admin_passthru(struct nvme_io_channel *nvme_ch, struct nvme_bdev_io *bio,
+bdev_nvme_admin_passthru(struct nvme_io_path *io_path, struct nvme_bdev_io *bio,
 			 struct spdk_nvme_cmd *cmd, void *buf, size_t nbytes)
 {
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr;
 	uint32_t max_xfer_size;
 
-	if (!bdev_nvme_find_admin_path(nvme_ch, &nvme_bdev_ctrlr)) {
+	if (!bdev_nvme_find_admin_path(io_path, &nvme_bdev_ctrlr)) {
 		return -EINVAL;
 	}
 
@@ -3278,53 +3362,36 @@ bdev_nvme_io_passthru_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 			(uint32_t)nbytes, md_buf, bdev_nvme_queued_done, bio);
 }
 
-static void
-bdev_nvme_abort_admin_cmd(void *ctx)
+static int
+bdev_nvme_abort(struct nvme_io_path *io_path, struct nvme_bdev_io *bio,
+		struct nvme_bdev_io *bio_to_abort)
 {
-	struct nvme_bdev_io *bio = ctx;
-	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(bio);
-	struct nvme_io_channel *nvme_ch;
-	struct nvme_bdev_io *bio_to_abort;
 	int rc;
 
-	nvme_ch = spdk_io_channel_get_ctx(spdk_bdev_io_get_io_channel(bdev_io));
-	bio_to_abort = (struct nvme_bdev_io *)bdev_io->u.abort.bio_to_abort->driver_ctx;
+	bio->orig_thread = spdk_get_thread();
 
-	rc = spdk_nvme_ctrlr_cmd_abort_ext(nvme_ch->ctrlr->ctrlr,
-					   NULL,
+	rc = spdk_nvme_ctrlr_cmd_abort_ext(io_path->ctrlr->ctrlr,
+					   io_path->qpair,
 					   bio_to_abort,
 					   bdev_nvme_abort_done, bio);
 	if (rc == -ENOENT) {
-		/* If no admin command was found in admin qpair, complete the abort
-		 * request with failure.
+		/* If no command was found in I/O qpair, the target command may be
+		 * admin command.
 		 */
+		rc = spdk_nvme_ctrlr_cmd_abort_ext(io_path->ctrlr->ctrlr,
+						   NULL,
+						   bio_to_abort,
+						   bdev_nvme_abort_done, bio);
+	}
+
+	if (rc == -ENOENT) {
+		/* If no command was found, complete the abort request with failure. */
 		bio->cpl.cdw0 |= 1U;
 		bio->cpl.status.sc = SPDK_NVME_SC_SUCCESS;
 		bio->cpl.status.sct = SPDK_NVME_SCT_GENERIC;
 
-		spdk_thread_send_msg(bio->orig_thread, bdev_nvme_abort_completion, bio);
-	}
-}
+		bdev_nvme_abort_completion(bio);
 
-static int
-bdev_nvme_abort(struct nvme_io_channel *nvme_ch, struct nvme_bdev_io *bio,
-		struct nvme_bdev_io *bio_to_abort)
-{
-	int rc;
-
-	bio->orig_thread = spdk_get_thread();
-
-	rc = spdk_nvme_ctrlr_cmd_abort_ext(nvme_ch->ctrlr->ctrlr,
-					   nvme_ch->qpair,
-					   bio_to_abort,
-					   bdev_nvme_abort_done, bio);
-	if (rc == -ENOENT) {
-		/* If no command was found in I/O qpair, the target command may be
-		 * admin command. Only a single thread tries aborting admin command
-		 * to clean I/O flow.
-		 */
-		spdk_thread_send_msg(nvme_ch->ctrlr->thread,
-				     bdev_nvme_abort_admin_cmd, bio);
 		rc = 0;
 	}
 
diff --git a/module/bdev/nvme/bdev_nvme.h b/module/bdev/nvme/bdev_nvme.h
index 98a311a0a..5bfe1b545 100644
--- a/module/bdev/nvme/bdev_nvme.h
+++ b/module/bdev/nvme/bdev_nvme.h
@@ -63,7 +63,7 @@ struct spdk_bdev_nvme_opts {
 	bool delay_cmd_submit;
 };
 
-struct spdk_nvme_qpair *bdev_nvme_get_io_qpair(struct spdk_io_channel *ctrlr_io_ch);
+struct spdk_nvme_qpair *bdev_nvme_get_io_qpair(struct spdk_io_channel *io_path_ch);
 void bdev_nvme_get_opts(struct spdk_bdev_nvme_opts *opts);
 int bdev_nvme_set_opts(const struct spdk_bdev_nvme_opts *opts);
 int bdev_nvme_set_hotplug(bool enabled, uint64_t period_us, spdk_msg_fn cb, void *cb_ctx);
diff --git a/module/bdev/nvme/bdev_nvme_rpc.c b/module/bdev/nvme/bdev_nvme_rpc.c
index 27cd39901..6000ab146 100644
--- a/module/bdev/nvme/bdev_nvme_rpc.c
+++ b/module/bdev/nvme/bdev_nvme_rpc.c
@@ -218,36 +218,48 @@ static const struct spdk_json_object_decoder rpc_bdev_nvme_attach_controller_dec
 struct rpc_bdev_nvme_attach_controller_ctx {
 	struct rpc_bdev_nvme_attach_controller req;
 	uint32_t count;
+	size_t bdev_count;
 	const char *names[NVME_MAX_BDEVS_PER_RPC];
 	struct spdk_jsonrpc_request *request;
 };
 
 static void
-rpc_bdev_nvme_attach_controller_done(void *cb_ctx, size_t bdev_count, int rc)
+rpc_bdev_nvme_attach_controller_examined(void *cb_ctx)
 {
 	struct rpc_bdev_nvme_attach_controller_ctx *ctx = cb_ctx;
 	struct spdk_jsonrpc_request *request = ctx->request;
 	struct spdk_json_write_ctx *w;
 	size_t i;
 
-	if (rc < 0) {
-		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS, "Invalid parameters");
-		goto exit;
-	}
-
 	w = spdk_jsonrpc_begin_result(request);
 	spdk_json_write_array_begin(w);
-	for (i = 0; i < bdev_count; i++) {
+	for (i = 0; i < ctx->bdev_count; i++) {
 		spdk_json_write_string(w, ctx->names[i]);
 	}
 	spdk_json_write_array_end(w);
 	spdk_jsonrpc_end_result(request, w);
 
-exit:
 	free_rpc_bdev_nvme_attach_controller(&ctx->req);
 	free(ctx);
 }
 
+static void
+rpc_bdev_nvme_attach_controller_done(void *cb_ctx, size_t bdev_count, int rc)
+{
+	struct rpc_bdev_nvme_attach_controller_ctx *ctx = cb_ctx;
+	struct spdk_jsonrpc_request *request = ctx->request;
+
+	if (rc < 0) {
+		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS, "Invalid parameters");
+		free_rpc_bdev_nvme_attach_controller(&ctx->req);
+		free(ctx);
+		return;
+	}
+
+	ctx->bdev_count = bdev_count;
+	spdk_bdev_wait_for_examine(rpc_bdev_nvme_attach_controller_examined, ctx);
+}
+
 static void
 rpc_bdev_nvme_attach_controller(struct spdk_jsonrpc_request *request,
 				const struct spdk_json_val *params)
diff --git a/module/bdev/nvme/bdev_ocssd.c b/module/bdev/nvme/bdev_ocssd.c
index c2df83ad6..ac6fe3fcb 100644
--- a/module/bdev/nvme/bdev_ocssd.c
+++ b/module/bdev/nvme/bdev_ocssd.c
@@ -105,6 +105,22 @@ bdev_ocssd_get_ns_from_nvme(struct nvme_bdev_ns *nvme_ns)
 	return nvme_ns->type_ctx;
 }
 
+static inline bool
+bdev_ocssd_find_io_path(struct nvme_bdev *nbdev, struct nvme_io_path *io_path,
+			struct bdev_ocssd_ns **_ocssd_ns,
+			struct spdk_nvme_ns **_ns, struct spdk_nvme_qpair **_qpair)
+{
+	if (spdk_unlikely(io_path->qpair == NULL)) {
+		/* The device is currently resetting. */
+		return false;
+	}
+
+	*_ocssd_ns = bdev_ocssd_get_ns_from_nvme(nbdev->nvme_ns);
+	*_ns = nbdev->nvme_ns->ns;
+	*_qpair = io_path->qpair;
+	return true;
+}
+
 static int
 bdev_ocssd_library_init(void)
 {
@@ -222,7 +238,7 @@ bdev_ocssd_destruct(void *ctx)
 	if (!nvme_ns->populated) {
 		pthread_mutex_unlock(&nvme_ns->ctrlr->mutex);
 
-		nvme_bdev_ctrlr_destruct(nvme_ns->ctrlr);
+		nvme_bdev_ctrlr_release(nvme_ns->ctrlr);
 	} else {
 		pthread_mutex_unlock(&nvme_ns->ctrlr->mutex);
 	}
@@ -369,12 +385,11 @@ bdev_ocssd_read_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 }
 
 static int
-bdev_ocssd_read(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
-		struct spdk_nvme_qpair *qpair,
+bdev_ocssd_read(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+		struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		struct bdev_ocssd_io *ocdev_io, struct iovec *iov, int iovcnt,
 		void *md, uint64_t lba_count, uint64_t lba)
 {
-	struct bdev_ocssd_ns *ocssd_ns = bdev_ocssd_get_ns_from_nvme(nvme_ns);
 	const size_t zone_size = bdev_ocssd_get_zone_size(ocssd_bdev);
 
 	if ((lba % zone_size) + lba_count > zone_size) {
@@ -389,7 +404,7 @@ bdev_ocssd_read(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
 
 	lba = bdev_ocssd_to_disk_lba(ocssd_ns, lba);
 
-	return spdk_nvme_ns_cmd_readv_with_md(nvme_ns->ns, qpair, lba,
+	return spdk_nvme_ns_cmd_readv_with_md(ns, qpair, lba,
 					      lba_count, bdev_ocssd_read_cb,
 					      ocdev_io, 0, bdev_ocssd_reset_sgl,
 					      bdev_ocssd_next_sge, md, 0, 0);
@@ -410,12 +425,11 @@ bdev_ocssd_write_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 }
 
 static int
-bdev_ocssd_write(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
-		 struct spdk_nvme_qpair *qpair,
+bdev_ocssd_write(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+		 struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		 struct bdev_ocssd_io *ocdev_io, struct iovec *iov, int iovcnt,
 		 void *md, uint64_t lba_count, uint64_t lba)
 {
-	struct bdev_ocssd_ns *ocssd_ns = bdev_ocssd_get_ns_from_nvme(nvme_ns);
 	const size_t zone_size = bdev_ocssd_get_zone_size(ocssd_bdev);
 	struct bdev_ocssd_zone *zone;
 	int rc;
@@ -438,7 +452,7 @@ bdev_ocssd_write(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
 
 	lba = bdev_ocssd_to_disk_lba(ocssd_ns, lba);
 
-	rc = spdk_nvme_ns_cmd_writev_with_md(nvme_ns->ns, qpair, lba,
+	rc = spdk_nvme_ns_cmd_writev_with_md(ns, qpair, lba,
 					     lba_count, bdev_ocssd_write_cb,
 					     ocdev_io, 0, bdev_ocssd_reset_sgl,
 					     bdev_ocssd_next_sge, md, 0, 0);
@@ -465,12 +479,11 @@ bdev_ocssd_append_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 }
 
 static int
-bdev_ocssd_zone_append(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
-		       struct spdk_nvme_qpair *qpair,
+bdev_ocssd_zone_append(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+		       struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		       struct bdev_ocssd_io *ocdev_io, struct iovec *iov, int iovcnt,
 		       void *md, uint64_t lba_count, uint64_t lba)
 {
-	struct bdev_ocssd_ns *ocssd_ns = bdev_ocssd_get_ns_from_nvme(nvme_ns);
 	struct bdev_ocssd_zone *zone;
 	int rc = 0;
 
@@ -498,7 +511,7 @@ bdev_ocssd_zone_append(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_
 
 	lba = bdev_ocssd_to_disk_lba(ocssd_ns, zone->write_pointer);
 
-	rc = spdk_nvme_ns_cmd_writev_with_md(nvme_ns->ns, qpair, lba,
+	rc = spdk_nvme_ns_cmd_writev_with_md(ns, qpair, lba,
 					     lba_count, bdev_ocssd_append_cb,
 					     ocdev_io, 0, bdev_ocssd_reset_sgl,
 					     bdev_ocssd_next_sge, md, 0, 0);
@@ -514,8 +527,9 @@ static void
 bdev_ocssd_io_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io, bool success)
 {
 	struct ocssd_bdev *ocssd_bdev = (struct ocssd_bdev *)bdev_io->bdev->ctxt;
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
-	struct nvme_bdev_ns *nvme_ns;
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
+	struct bdev_ocssd_ns *ocssd_ns;
+	struct spdk_nvme_ns *ns;
 	struct spdk_nvme_qpair *qpair;
 	int rc;
 
@@ -524,14 +538,15 @@ bdev_ocssd_io_get_buf_cb(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_i
 		return;
 	}
 
-	if (spdk_unlikely(!bdev_nvme_find_io_path(&ocssd_bdev->nvme_bdev, nvme_ch,
-			  &nvme_ns, &qpair))) {
+	if (spdk_unlikely(!bdev_ocssd_find_io_path(&ocssd_bdev->nvme_bdev, io_path,
+			  &ocssd_ns, &ns, &qpair))) {
 		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
 		return;
 	}
 
 	rc = bdev_ocssd_read(ocssd_bdev,
-			     nvme_ns,
+			     ocssd_ns,
+			     ns,
 			     qpair,
 			     (struct bdev_ocssd_io *)bdev_io->driver_ctx,
 			     bdev_io->u.bdev.iovs,
@@ -561,11 +576,10 @@ bdev_ocssd_reset_zone_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 }
 
 static int
-bdev_ocssd_reset_zone(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
-		      struct spdk_nvme_qpair *qpair,
+bdev_ocssd_reset_zone(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+		      struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		      struct bdev_ocssd_io *ocdev_io, uint64_t slba, size_t num_zones)
 {
-	struct bdev_ocssd_ns *ocssd_ns = bdev_ocssd_get_ns_from_nvme(nvme_ns);
 	uint64_t offset, zone_size = bdev_ocssd_get_zone_size(ocssd_bdev);
 	struct bdev_ocssd_zone *zone;
 	int rc;
@@ -587,7 +601,7 @@ bdev_ocssd_reset_zone(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_n
 
 	ocdev_io->io.zone = zone;
 
-	rc = spdk_nvme_ocssd_ns_cmd_vector_reset(nvme_ns->ns, qpair,
+	rc = spdk_nvme_ocssd_ns_cmd_vector_reset(ns, qpair,
 			ocdev_io->io.lba, num_zones, NULL,
 			bdev_ocssd_reset_zone_cb, ocdev_io);
 	if (spdk_unlikely(rc != 0)) {
@@ -597,8 +611,8 @@ bdev_ocssd_reset_zone(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_n
 	return rc;
 }
 
-static int _bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev,
-				     struct nvme_bdev_ns *nvme_ns,
+static int _bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+				     struct spdk_nvme_ns *ns,
 				     struct bdev_ocssd_io *ocdev_io, uint64_t zone_id);
 
 static void
@@ -639,8 +653,8 @@ bdev_ocssd_zone_info_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 	struct spdk_ocssd_chunk_information_entry *chunk_info = &ocdev_io->zone_info.chunk_info;
 	struct spdk_bdev_io *bdev_io = spdk_bdev_io_from_ctx(ctx);
 	struct ocssd_bdev *ocssd_bdev = bdev_io->bdev->ctxt;
-	struct nvme_io_channel *nvme_ch;
-	struct nvme_bdev_ns *nvme_ns;
+	struct nvme_io_path *io_path;
+	struct spdk_nvme_ns *ns;
 	struct spdk_nvme_qpair *qpair;
 	struct bdev_ocssd_ns *ocssd_ns;
 	struct spdk_bdev_zone_info *zone_info;
@@ -651,15 +665,14 @@ bdev_ocssd_zone_info_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 		return;
 	}
 
-	nvme_ch = spdk_io_channel_get_ctx(spdk_bdev_io_get_io_channel(bdev_io));
+	io_path = spdk_io_channel_get_ctx(spdk_bdev_io_get_io_channel(bdev_io));
 
-	if (spdk_unlikely(!bdev_nvme_find_io_path(&ocssd_bdev->nvme_bdev, nvme_ch, &nvme_ns, &qpair))) {
+	if (spdk_unlikely(!bdev_ocssd_find_io_path(&ocssd_bdev->nvme_bdev, io_path, &ocssd_ns, &ns,
+			  &qpair))) {
 		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
 		return;
 	}
 
-	ocssd_ns = bdev_ocssd_get_ns_from_nvme(nvme_ns);
-
 	zone_info = ((struct spdk_bdev_zone_info *)bdev_io->u.zone_mgmt.buf) +
 		    ocdev_io->zone_info.chunk_offset;
 	bdev_ocssd_fill_zone_info(ocssd_bdev, ocssd_ns, zone_info, chunk_info);
@@ -667,7 +680,7 @@ bdev_ocssd_zone_info_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 	if (++ocdev_io->zone_info.chunk_offset == bdev_io->u.zone_mgmt.num_zones) {
 		spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_SUCCESS);
 	} else {
-		rc = _bdev_ocssd_get_zone_info(ocssd_bdev, nvme_ns, ocdev_io,
+		rc = _bdev_ocssd_get_zone_info(ocssd_bdev, ocssd_ns, ns, ocdev_io,
 					       bdev_io->u.zone_mgmt.zone_id);
 		if (spdk_unlikely(rc != 0)) {
 			if (rc == -ENOMEM) {
@@ -680,18 +693,18 @@ bdev_ocssd_zone_info_cb(void *ctx, const struct spdk_nvme_cpl *cpl)
 }
 
 static int
-_bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
+_bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+			  struct spdk_nvme_ns *ns,
 			  struct bdev_ocssd_io *ocdev_io, uint64_t zone_id)
 {
-	struct bdev_ocssd_ns *ocssd_ns = bdev_ocssd_get_ns_from_nvme(nvme_ns);
 	uint64_t lba, offset, zone_size = bdev_ocssd_get_zone_size(ocssd_bdev);
 
 	lba = zone_id + ocdev_io->zone_info.chunk_offset * zone_size;
 	offset = bdev_ocssd_to_chunk_info_offset(ocssd_ns, lba);
 
-	return spdk_nvme_ctrlr_cmd_get_log_page(spdk_nvme_ns_get_ctrlr(nvme_ns->ns),
+	return spdk_nvme_ctrlr_cmd_get_log_page(spdk_nvme_ns_get_ctrlr(ns),
 						SPDK_OCSSD_LOG_CHUNK_INFO,
-						spdk_nvme_ns_get_id(nvme_ns->ns),
+						spdk_nvme_ns_get_id(ns),
 						&ocdev_io->zone_info.chunk_info,
 						sizeof(ocdev_io->zone_info.chunk_info),
 						offset * sizeof(ocdev_io->zone_info.chunk_info),
@@ -699,7 +712,8 @@ _bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nv
 }
 
 static int
-bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
+bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+			 struct spdk_nvme_ns *ns,
 			 struct bdev_ocssd_io *ocdev_io, uint64_t zone_id, uint32_t num_zones)
 {
 	uint64_t zone_size = bdev_ocssd_get_zone_size(ocssd_bdev);
@@ -716,17 +730,19 @@ bdev_ocssd_get_zone_info(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvm
 
 	ocdev_io->zone_info.chunk_offset = 0;
 
-	return _bdev_ocssd_get_zone_info(ocssd_bdev, nvme_ns, ocdev_io, zone_id);
+	return _bdev_ocssd_get_zone_info(ocssd_bdev, ocssd_ns, ns, ocdev_io, zone_id);
 }
 
 static int
-bdev_ocssd_zone_management(struct ocssd_bdev *ocssd_bdev, struct nvme_bdev_ns *nvme_ns,
-			   struct spdk_nvme_qpair *qpair, struct spdk_bdev_io *bdev_io)
+bdev_ocssd_zone_management(struct ocssd_bdev *ocssd_bdev, struct bdev_ocssd_ns *ocssd_ns,
+			   struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+			   struct spdk_bdev_io *bdev_io)
 {
 	switch (bdev_io->u.zone_mgmt.zone_action) {
 	case SPDK_BDEV_ZONE_RESET:
 		return bdev_ocssd_reset_zone(ocssd_bdev,
-					     nvme_ns,
+					     ocssd_ns,
+					     ns,
 					     qpair,
 					     (struct bdev_ocssd_io *)bdev_io->driver_ctx,
 					     bdev_io->u.zone_mgmt.zone_id,
@@ -741,14 +757,14 @@ static void bdev_ocssd_submit_request(struct spdk_io_channel *ch, struct spdk_bd
 static int
 bdev_ocssd_poll_pending(void *ctx)
 {
-	struct nvme_io_channel *nvme_ch = ctx;
+	struct nvme_io_path *io_path = ctx;
 	struct ocssd_io_channel *ocssd_ch;
 	struct spdk_bdev_io *bdev_io;
 	struct spdk_io_channel *ch;
 	TAILQ_HEAD(, spdk_bdev_io) pending_requests;
 	int num_requests = 0;
 
-	ocssd_ch = nvme_ch->ocssd_ch;
+	ocssd_ch = io_path->ocssd_ch;
 
 	TAILQ_INIT(&pending_requests);
 	TAILQ_SWAP(&ocssd_ch->pending_requests, &pending_requests, spdk_bdev_io, module_link);
@@ -770,8 +786,8 @@ bdev_ocssd_poll_pending(void *ctx)
 static void
 bdev_ocssd_delay_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
 {
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
-	struct ocssd_io_channel *ocssd_ch = nvme_ch->ocssd_ch;
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
+	struct ocssd_io_channel *ocssd_ch = io_path->ocssd_ch;
 
 	TAILQ_INSERT_TAIL(&ocssd_ch->pending_requests, bdev_io, module_link);
 	spdk_poller_resume(ocssd_ch->pending_poller);
@@ -780,14 +796,15 @@ bdev_ocssd_delay_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_i
 static int
 _bdev_ocssd_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
 {
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
 	struct ocssd_bdev *ocssd_bdev = (struct ocssd_bdev *)bdev_io->bdev->ctxt;
 	struct bdev_ocssd_io *ocdev_io = (struct bdev_ocssd_io *)bdev_io->driver_ctx;
-	struct nvme_bdev_ns *nvme_ns;
+	struct bdev_ocssd_ns *ocssd_ns;
+	struct spdk_nvme_ns *ns;
 	struct spdk_nvme_qpair *qpair;
 
-	if (spdk_unlikely(!bdev_nvme_find_io_path(&ocssd_bdev->nvme_bdev, nvme_ch,
-			  &nvme_ns, &qpair))) {
+	if (spdk_unlikely(!bdev_ocssd_find_io_path(&ocssd_bdev->nvme_bdev, io_path,
+			  &ocssd_ns, &ns, &qpair))) {
 		return -1;
 	}
 
@@ -795,7 +812,8 @@ _bdev_ocssd_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev
 	case SPDK_BDEV_IO_TYPE_READ:
 		if (bdev_io->u.bdev.iovs && bdev_io->u.bdev.iovs[0].iov_base) {
 			return bdev_ocssd_read(ocssd_bdev,
-					       nvme_ns,
+					       ocssd_ns,
+					       ns,
 					       qpair,
 					       ocdev_io,
 					       bdev_io->u.bdev.iovs,
@@ -811,7 +829,8 @@ _bdev_ocssd_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev
 
 	case SPDK_BDEV_IO_TYPE_WRITE:
 		return bdev_ocssd_write(ocssd_bdev,
-					nvme_ns,
+					ocssd_ns,
+					ns,
 					qpair,
 					ocdev_io,
 					bdev_io->u.bdev.iovs,
@@ -821,18 +840,20 @@ _bdev_ocssd_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev
 					bdev_io->u.bdev.offset_blocks);
 
 	case SPDK_BDEV_IO_TYPE_ZONE_MANAGEMENT:
-		return bdev_ocssd_zone_management(ocssd_bdev, nvme_ns, qpair, bdev_io);
+		return bdev_ocssd_zone_management(ocssd_bdev, ocssd_ns, ns, qpair, bdev_io);
 
 	case SPDK_BDEV_IO_TYPE_GET_ZONE_INFO:
 		return bdev_ocssd_get_zone_info(ocssd_bdev,
-						nvme_ns,
+						ocssd_ns,
+						ns,
 						ocdev_io,
 						bdev_io->u.zone_mgmt.zone_id,
 						bdev_io->u.zone_mgmt.num_zones);
 
 	case SPDK_BDEV_IO_TYPE_ZONE_APPEND:
 		return bdev_ocssd_zone_append(ocssd_bdev,
-					      nvme_ns,
+					      ocssd_ns,
+					      ns,
 					      qpair,
 					      ocdev_io,
 					      bdev_io->u.bdev.iovs,
@@ -1496,7 +1517,7 @@ bdev_ocssd_depopulate_namespace(struct nvme_bdev_ns *nvme_ns)
 }
 
 int
-bdev_ocssd_create_io_channel(struct nvme_io_channel *nvme_ch)
+bdev_ocssd_create_io_channel(struct nvme_io_path *io_path)
 {
 	struct ocssd_io_channel *ocssd_ch;
 
@@ -1505,7 +1526,7 @@ bdev_ocssd_create_io_channel(struct nvme_io_channel *nvme_ch)
 		return -ENOMEM;
 	}
 
-	ocssd_ch->pending_poller = SPDK_POLLER_REGISTER(bdev_ocssd_poll_pending, nvme_ch, 0);
+	ocssd_ch->pending_poller = SPDK_POLLER_REGISTER(bdev_ocssd_poll_pending, io_path, 0);
 	if (ocssd_ch->pending_poller == NULL) {
 		SPDK_ERRLOG("Failed to register pending requests poller\n");
 		free(ocssd_ch);
@@ -1516,16 +1537,16 @@ bdev_ocssd_create_io_channel(struct nvme_io_channel *nvme_ch)
 	spdk_poller_pause(ocssd_ch->pending_poller);
 
 	TAILQ_INIT(&ocssd_ch->pending_requests);
-	nvme_ch->ocssd_ch = ocssd_ch;
+	io_path->ocssd_ch = ocssd_ch;
 
 	return 0;
 }
 
 void
-bdev_ocssd_destroy_io_channel(struct nvme_io_channel *nvme_ch)
+bdev_ocssd_destroy_io_channel(struct nvme_io_path *io_path)
 {
-	spdk_poller_unregister(&nvme_ch->ocssd_ch->pending_poller);
-	free(nvme_ch->ocssd_ch);
+	spdk_poller_unregister(&io_path->ocssd_ch->pending_poller);
+	free(io_path->ocssd_ch);
 }
 
 int
diff --git a/module/bdev/nvme/bdev_ocssd.h b/module/bdev/nvme/bdev_ocssd.h
index 81de06596..33783c34f 100644
--- a/module/bdev/nvme/bdev_ocssd.h
+++ b/module/bdev/nvme/bdev_ocssd.h
@@ -50,8 +50,8 @@ void bdev_ocssd_populate_namespace(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr,
 void bdev_ocssd_depopulate_namespace(struct nvme_bdev_ns *nvme_ns);
 void bdev_ocssd_namespace_config_json(struct spdk_json_write_ctx *w, struct nvme_bdev_ns *nvme_ns);
 
-int bdev_ocssd_create_io_channel(struct nvme_io_channel *ioch);
-void bdev_ocssd_destroy_io_channel(struct nvme_io_channel *ioch);
+int bdev_ocssd_create_io_channel(struct nvme_io_path *ioch);
+void bdev_ocssd_destroy_io_channel(struct nvme_io_path *ioch);
 
 int bdev_ocssd_init_ctrlr(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr);
 void bdev_ocssd_fini_ctrlr(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr);
diff --git a/module/bdev/nvme/common.c b/module/bdev/nvme/common.c
index 1cfe580f7..94bde3b1f 100644
--- a/module/bdev/nvme/common.c
+++ b/module/bdev/nvme/common.c
@@ -112,7 +112,7 @@ nvme_bdev_dump_trid_json(const struct spdk_nvme_transport_id *trid, struct spdk_
 	}
 }
 
-static void
+void
 nvme_bdev_ctrlr_delete(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr)
 {
 	struct nvme_bdev_ctrlr_trid *trid, *tmp_trid;
@@ -175,7 +175,7 @@ nvme_bdev_ctrlr_unregister(void *ctx)
 }
 
 void
-nvme_bdev_ctrlr_destruct(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr)
+nvme_bdev_ctrlr_release(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr)
 {
 	pthread_mutex_lock(&nvme_bdev_ctrlr->mutex);
 
@@ -210,5 +210,5 @@ nvme_ctrlr_depopulate_namespace_done(struct nvme_bdev_ns *nvme_ns)
 	}
 	pthread_mutex_unlock(&nvme_bdev_ctrlr->mutex);
 
-	nvme_bdev_ctrlr_destruct(nvme_bdev_ctrlr);
+	nvme_bdev_ctrlr_release(nvme_bdev_ctrlr);
 }
diff --git a/module/bdev/nvme/common.h b/module/bdev/nvme/common.h
index d5e306c9d..48cea7e3a 100644
--- a/module/bdev/nvme/common.h
+++ b/module/bdev/nvme/common.h
@@ -107,7 +107,7 @@ struct nvme_bdev_ctrlr {
 
 	struct ocssd_bdev_ctrlr			*ocssd_ctrlr;
 
-	struct nvme_bdev_io			*reset_bio;
+	struct spdk_bdev_io			*reset_bdev_io;
 
 	/** linked list pointer for device list */
 	TAILQ_ENTRY(nvme_bdev_ctrlr)		tailq;
@@ -154,7 +154,7 @@ struct nvme_async_probe_ctx {
 
 struct ocssd_io_channel;
 
-struct nvme_io_channel {
+struct nvme_io_path {
 	struct nvme_bdev_ctrlr		*ctrlr;
 	struct spdk_nvme_qpair		*qpair;
 	struct nvme_bdev_poll_group	*group;
@@ -174,35 +174,8 @@ struct nvme_bdev_ctrlr *nvme_bdev_next_ctrlr(struct nvme_bdev_ctrlr *prev);
 void nvme_bdev_dump_trid_json(const struct spdk_nvme_transport_id *trid,
 			      struct spdk_json_write_ctx *w);
 
-void nvme_bdev_ctrlr_destruct(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr);
+void nvme_bdev_ctrlr_release(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr);
 void nvme_bdev_ctrlr_unregister(void *ctx);
-
-static inline bool
-bdev_nvme_find_io_path(struct nvme_bdev *nbdev, struct nvme_io_channel *nvme_ch,
-		       struct nvme_bdev_ns **_nvme_ns, struct spdk_nvme_qpair **_qpair)
-{
-	if (spdk_unlikely(nvme_ch->qpair == NULL)) {
-		/* The device is currently resetting. */
-		return false;
-	}
-
-	*_nvme_ns = nbdev->nvme_ns;
-	*_qpair = nvme_ch->qpair;
-	return true;
-}
-
-static inline bool
-bdev_nvme_find_admin_path(struct nvme_io_channel *nvme_ch,
-			  struct nvme_bdev_ctrlr **_nvme_bdev_ctrlr)
-{
-	*_nvme_bdev_ctrlr = nvme_ch->ctrlr;
-	return true;
-}
-
-static inline struct nvme_bdev_ns *
-nvme_bdev_to_bdev_ns(struct nvme_bdev *nbdev)
-{
-	return nbdev->nvme_ns;
-}
+void nvme_bdev_ctrlr_delete(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr);
 
 #endif /* SPDK_COMMON_BDEV_NVME_H */
diff --git a/module/bdev/ocf/utils.c b/module/bdev/ocf/utils.c
index 3a1df3c9e..dacab10b7 100644
--- a/module/bdev/ocf/utils.c
+++ b/module/bdev/ocf/utils.c
@@ -69,6 +69,12 @@ ocf_get_cache_modename(ocf_cache_mode_t mode)
 	}
 }
 
+int
+ocf_get_cache_line_size(ocf_cache_t cache)
+{
+	return ocf_cache_get_line_size(cache) / KiB;
+}
+
 int
 vbdev_ocf_mngt_start(struct vbdev_ocf *vbdev, vbdev_ocf_mngt_fn *path,
 		     vbdev_ocf_mngt_callback cb, void *cb_arg)
diff --git a/module/bdev/ocf/utils.h b/module/bdev/ocf/utils.h
index 73bf6c93a..eb9f26d31 100644
--- a/module/bdev/ocf/utils.h
+++ b/module/bdev/ocf/utils.h
@@ -40,6 +40,9 @@
 ocf_cache_mode_t ocf_get_cache_mode(const char *cache_mode);
 const char *ocf_get_cache_modename(ocf_cache_mode_t mode);
 
+/* Get cache line size in KiB units */
+int ocf_get_cache_line_size(ocf_cache_t cache);
+
 /* Initiate management operation
  * Receives NULL terminated array of functions (path)
  * and callback (cb)
diff --git a/module/bdev/ocf/vbdev_ocf.c b/module/bdev/ocf/vbdev_ocf.c
index fcc7ef3ee..04491a0c1 100644
--- a/module/bdev/ocf/vbdev_ocf.c
+++ b/module/bdev/ocf/vbdev_ocf.c
@@ -198,7 +198,11 @@ static void
 unregister_finish(struct vbdev_ocf *vbdev)
 {
 	spdk_bdev_destruct_done(&vbdev->exp_bdev, vbdev->state.stop_status);
-	ocf_mngt_cache_put(vbdev->ocf_cache);
+
+	if (vbdev->ocf_cache) {
+		ocf_mngt_cache_put(vbdev->ocf_cache);
+	}
+
 	vbdev_ocf_cache_ctx_put(vbdev->cache_ctx);
 	vbdev_ocf_mngt_continue(vbdev, 0);
 }
@@ -740,7 +744,7 @@ vbdev_ocf_dump_info_json(void *opaque, struct spdk_json_write_ctx *w)
 	spdk_json_write_named_string(w, "mode",
 				     ocf_get_cache_modename(ocf_cache_get_mode(vbdev->ocf_cache)));
 	spdk_json_write_named_uint32(w, "cache_line_size",
-				     ocf_cache_get_line_size(vbdev->ocf_cache));
+				     ocf_get_cache_line_size(vbdev->ocf_cache));
 	spdk_json_write_named_bool(w, "metadata_volatile",
 				   vbdev->cfg.cache.metadata_volatile);
 
@@ -761,7 +765,7 @@ vbdev_ocf_write_json_config(struct spdk_bdev *bdev, struct spdk_json_write_ctx *
 	spdk_json_write_named_string(w, "mode",
 				     ocf_get_cache_modename(ocf_cache_get_mode(vbdev->ocf_cache)));
 	spdk_json_write_named_uint32(w, "cache_line_size",
-				     ocf_cache_get_line_size(vbdev->ocf_cache));
+				     ocf_get_cache_line_size(vbdev->ocf_cache));
 	spdk_json_write_named_string(w, "cache_bdev_name", vbdev->cache.name);
 	spdk_json_write_named_string(w, "core_bdev_name", vbdev->core.name);
 	spdk_json_write_object_end(w);
@@ -1092,6 +1096,7 @@ start_cache(struct vbdev_ocf *vbdev)
 
 	rc = ocf_mngt_cache_start(vbdev_ocf_ctx, &vbdev->ocf_cache, &vbdev->cfg.cache);
 	if (rc) {
+		SPDK_ERRLOG("Could not start cache %s: %d\n", vbdev->name, rc);
 		vbdev_ocf_mngt_exit(vbdev, unregister_path_dirty, rc);
 		return;
 	}
diff --git a/module/bdev/passthru/vbdev_passthru.c b/module/bdev/passthru/vbdev_passthru.c
index 5d72bce46..06d245b63 100644
--- a/module/bdev/passthru/vbdev_passthru.c
+++ b/module/bdev/passthru/vbdev_passthru.c
@@ -347,7 +347,8 @@ vbdev_passthru_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *b
 				     _pt_complete_io, bdev_io);
 		break;
 	case SPDK_BDEV_IO_TYPE_ZCOPY:
-		rc = spdk_bdev_zcopy_start(pt_node->base_desc, pt_ch->base_ch, bdev_io->u.bdev.offset_blocks,
+		rc = spdk_bdev_zcopy_start(pt_node->base_desc, pt_ch->base_ch, NULL, 0,
+					   bdev_io->u.bdev.offset_blocks,
 					   bdev_io->u.bdev.num_blocks, bdev_io->u.bdev.zcopy.populate,
 					   _pt_complete_zcopy_io, bdev_io);
 		break;
diff --git a/module/bdev/rbd/bdev_rbd.c b/module/bdev/rbd/bdev_rbd.c
index 26adc96ef..e8a1a5e4f 100644
--- a/module/bdev/rbd/bdev_rbd.c
+++ b/module/bdev/rbd/bdev_rbd.c
@@ -62,6 +62,9 @@ struct bdev_rbd {
 	char *user_id;
 	char *pool_name;
 	char **config;
+	rados_t cluster;
+	rados_t *cluster_p;
+	char *cluster_name;
 	rbd_image_info_t info;
 	TAILQ_ENTRY(bdev_rbd) tailq;
 	struct spdk_poller *reset_timer;
@@ -75,7 +78,6 @@ struct bdev_rbd_group_channel {
 
 struct bdev_rbd_io_channel {
 	rados_ioctx_t io_ctx;
-	rados_t cluster;
 	int pfd;
 	rbd_image_t image;
 	struct bdev_rbd *disk;
@@ -86,6 +88,61 @@ struct bdev_rbd_io {
 	size_t	total_len;
 };
 
+struct bdev_rbd_cluster {
+	char *name;
+	char *user_id;
+	char **config_param;
+	char *config_file;
+	rados_t cluster;
+	uint32_t ref;
+	STAILQ_ENTRY(bdev_rbd_cluster) link;
+};
+
+static STAILQ_HEAD(, bdev_rbd_cluster) g_map_bdev_rbd_cluster = STAILQ_HEAD_INITIALIZER(
+			g_map_bdev_rbd_cluster);
+static pthread_mutex_t g_map_bdev_rbd_cluster_mutex = PTHREAD_MUTEX_INITIALIZER;
+
+static void
+bdev_rbd_cluster_free(struct bdev_rbd_cluster *entry)
+{
+	assert(entry != NULL);
+
+	bdev_rbd_free_config(entry->config_param);
+	free(entry->config_file);
+	free(entry->user_id);
+	free(entry->name);
+	free(entry);
+}
+
+static void
+bdev_rbd_put_cluster(rados_t **cluster)
+{
+	struct bdev_rbd_cluster *entry;
+
+	assert(cluster != NULL);
+
+	/* No need go through the map if *cluster equals to NULL */
+	if (*cluster == NULL) {
+		return;
+	}
+
+	pthread_mutex_lock(&g_map_bdev_rbd_cluster_mutex);
+	STAILQ_FOREACH(entry, &g_map_bdev_rbd_cluster, link) {
+		if (*cluster != &entry->cluster) {
+			continue;
+		}
+
+		assert(entry->ref > 0);
+		entry->ref--;
+		*cluster = NULL;
+		pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+		return;
+	}
+
+	pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+	SPDK_ERRLOG("Cannot find the entry for cluster=%p\n", cluster);
+}
+
 static void
 bdev_rbd_free(struct bdev_rbd *rbd)
 {
@@ -98,6 +155,14 @@ bdev_rbd_free(struct bdev_rbd *rbd)
 	free(rbd->user_id);
 	free(rbd->pool_name);
 	bdev_rbd_free_config(rbd->config);
+
+	if (rbd->cluster_name) {
+		bdev_rbd_put_cluster(&rbd->cluster_p);
+		free(rbd->cluster_name);
+	} else if (rbd->cluster) {
+		rados_shutdown(rbd->cluster);
+	}
+
 	free(rbd);
 }
 
@@ -138,8 +203,8 @@ bdev_rbd_dup_config(const char *const *config)
 }
 
 static int
-bdev_rados_context_init(const char *user_id, const char *rbd_pool_name, const char *const *config,
-			rados_t *cluster, rados_ioctx_t *io_ctx)
+bdev_rados_cluster_init(const char *user_id, const char *const *config,
+			rados_t *cluster)
 {
 	int ret;
 
@@ -176,30 +241,92 @@ bdev_rados_context_init(const char *user_id, const char *rbd_pool_name, const ch
 		return -1;
 	}
 
-	ret = rados_ioctx_create(*cluster, rbd_pool_name, io_ctx);
+	return 0;
+}
+
+static int
+bdev_rbd_get_cluster(const char *cluster_name, rados_t **cluster)
+{
+	struct bdev_rbd_cluster *entry;
+
+	if (cluster == NULL) {
+		SPDK_ERRLOG("cluster should not be NULL\n");
+		return -1;
+	}
+
+	pthread_mutex_lock(&g_map_bdev_rbd_cluster_mutex);
+	STAILQ_FOREACH(entry, &g_map_bdev_rbd_cluster, link) {
+		if (strcmp(cluster_name, entry->name) == 0) {
+			entry->ref++;
+			*cluster = &entry->cluster;
+			pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+			return 0;
+		}
+	}
+
+	pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+	return -1;
+}
+
+static int
+bdev_rbd_shared_cluster_init(const char *cluster_name, rados_t **cluster)
+{
+	int ret;
 
+	ret = bdev_rbd_get_cluster(cluster_name, cluster);
 	if (ret < 0) {
-		SPDK_ERRLOG("Failed to create ioctx\n");
-		rados_shutdown(*cluster);
+		SPDK_ERRLOG("Failed to create rados_t struct\n");
 		return -1;
 	}
 
-	return 0;
+	return ret;
+}
+
+static void *
+bdev_rbd_cluster_handle(void *arg)
+{
+	void *ret = arg;
+	struct bdev_rbd *rbd = arg;
+	int rc;
+
+	rc = bdev_rados_cluster_init(rbd->user_id, (const char *const *)rbd->config,
+				     &rbd->cluster);
+	if (rc < 0) {
+		SPDK_ERRLOG("Failed to create rados cluster for user_id=%s and rbd_pool=%s\n",
+			    rbd->user_id ? rbd->user_id : "admin (the default)", rbd->pool_name);
+		ret = NULL;
+	}
+
+	return ret;
 }
 
 static int
 bdev_rbd_init(struct bdev_rbd *rbd)
 {
 	int ret = 0;
-	rados_t cluster = NULL;
 	rados_ioctx_t io_ctx = NULL;
 	rbd_image_t image = NULL;
 
-	ret = bdev_rados_context_init(rbd->user_id, rbd->pool_name, (const char *const *)rbd->config,
-				      &cluster, &io_ctx);
+	if (!rbd->cluster_name) {
+		rbd->cluster_p = &rbd->cluster;
+		/* Cluster should be created in non-SPDK thread to avoid conflict between
+		 * Rados and SPDK thread */
+		if (spdk_call_unaffinitized(bdev_rbd_cluster_handle, rbd) == NULL) {
+			SPDK_ERRLOG("Cannot create the rados object on rbd=%p\n", rbd);
+			return -1;
+		}
+	} else {
+		ret = bdev_rbd_shared_cluster_init(rbd->cluster_name, &rbd->cluster_p);
+		if (ret < 0) {
+			SPDK_ERRLOG("Failed to create rados object for rbd =%p on cluster_name=%s\n",
+				    rbd, rbd->cluster_name);
+			return -1;
+		}
+	}
+
+	ret = rados_ioctx_create(*(rbd->cluster_p), rbd->pool_name, &io_ctx);
 	if (ret < 0) {
-		SPDK_ERRLOG("Failed to create rados context for user_id=%s and rbd_pool=%s\n",
-			    rbd->user_id ? rbd->user_id : "admin (the default)", rbd->pool_name);
+		SPDK_ERRLOG("Failed to create ioctx\n");
 		return -1;
 	}
 
@@ -216,7 +343,6 @@ bdev_rbd_init(struct bdev_rbd *rbd)
 
 end:
 	rados_ioctx_destroy(io_ctx);
-	rados_shutdown(cluster);
 	return ret;
 }
 
@@ -325,14 +451,23 @@ bdev_rbd_reset(struct bdev_rbd *disk, struct spdk_bdev_io *bdev_io)
 	disk->reset_timer = SPDK_POLLER_REGISTER(bdev_rbd_reset_timer, disk, 1 * 1000 * 1000);
 }
 
+static void
+bdev_rbd_free_cb(void *io_device)
+{
+	struct bdev_rbd *rbd = io_device;
+
+	assert(rbd != NULL);
+
+	bdev_rbd_free((struct bdev_rbd *)rbd);
+}
+
 static int
 bdev_rbd_destruct(void *ctx)
 {
 	struct bdev_rbd *rbd = ctx;
 
-	spdk_io_device_unregister(rbd, NULL);
+	spdk_io_device_unregister(rbd, bdev_rbd_free_cb);
 
-	bdev_rbd_free(rbd);
 	return 0;
 }
 
@@ -447,10 +582,6 @@ bdev_rbd_free_channel(struct bdev_rbd_io_channel *ch)
 		rados_ioctx_destroy(ch->io_ctx);
 	}
 
-	if (ch->cluster) {
-		rados_shutdown(ch->cluster);
-	}
-
 	if (ch->pfd >= 0) {
 		close(ch->pfd);
 	}
@@ -465,16 +596,13 @@ bdev_rbd_handle(void *arg)
 {
 	struct bdev_rbd_io_channel *ch = arg;
 	void *ret = arg;
-	int rc;
 
-	rc = bdev_rados_context_init(ch->disk->user_id, ch->disk->pool_name,
-				     (const char *const *)ch->disk->config,
-				     &ch->cluster, &ch->io_ctx);
-	if (rc < 0) {
-		SPDK_ERRLOG("Failed to create rados context for user_id %s and rbd_pool=%s\n",
-			    ch->disk->user_id ? ch->disk->user_id : "admin (the default)", ch->disk->pool_name);
+	assert(ch->disk->cluster_p != NULL);
+
+	if (rados_ioctx_create(*(ch->disk->cluster_p), ch->disk->pool_name, &ch->io_ctx) < 0) {
+		SPDK_ERRLOG("Failed to create ioctx\n");
 		ret = NULL;
-		goto end;
+		return ret;
 	}
 
 	if (rbd_open(ch->io_ctx, ch->disk->rbd_name, &ch->image, NULL) < 0) {
@@ -482,7 +610,6 @@ bdev_rbd_handle(void *arg)
 		ret = NULL;
 	}
 
-end:
 	return ret;
 }
 
@@ -558,6 +685,40 @@ bdev_rbd_get_io_channel(void *ctx)
 	return spdk_get_io_channel(rbd_bdev);
 }
 
+static void
+bdev_rbd_cluster_dump_entry(const char *cluster_name, struct spdk_json_write_ctx *w)
+{
+	struct bdev_rbd_cluster *entry;
+
+	pthread_mutex_lock(&g_map_bdev_rbd_cluster_mutex);
+	STAILQ_FOREACH(entry, &g_map_bdev_rbd_cluster, link) {
+		if (strcmp(cluster_name, entry->name)) {
+			continue;
+		}
+		if (entry->user_id) {
+			spdk_json_write_named_string(w, "user_id", entry->user_id);
+		}
+
+		if (entry->config_param) {
+			char **config_entry = entry->config_param;
+
+			spdk_json_write_named_object_begin(w, "config_param");
+			while (*config_entry) {
+				spdk_json_write_named_string(w, config_entry[0], config_entry[1]);
+				config_entry += 2;
+			}
+			spdk_json_write_object_end(w);
+		} else if (entry->config_file) {
+			spdk_json_write_named_string(w, "config_file", entry->config_file);
+		}
+
+		pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+		return;
+	}
+
+	pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+}
+
 static int
 bdev_rbd_dump_info_json(void *ctx, struct spdk_json_write_ctx *w)
 {
@@ -569,6 +730,11 @@ bdev_rbd_dump_info_json(void *ctx, struct spdk_json_write_ctx *w)
 
 	spdk_json_write_named_string(w, "rbd_name", rbd_bdev->rbd_name);
 
+	if (rbd_bdev->cluster_name) {
+		bdev_rbd_cluster_dump_entry(rbd_bdev->cluster_name, w);
+		goto end;
+	}
+
 	if (rbd_bdev->user_id) {
 		spdk_json_write_named_string(w, "user_id", rbd_bdev->user_id);
 	}
@@ -584,6 +750,7 @@ bdev_rbd_dump_info_json(void *ctx, struct spdk_json_write_ctx *w)
 		spdk_json_write_object_end(w);
 	}
 
+end:
 	spdk_json_write_object_end(w);
 
 	return 0;
@@ -623,6 +790,76 @@ bdev_rbd_write_config_json(struct spdk_bdev *bdev, struct spdk_json_write_ctx *w
 	spdk_json_write_object_end(w);
 }
 
+static void
+dump_single_cluster_entry(struct bdev_rbd_cluster *entry, struct spdk_json_write_ctx *w)
+{
+	assert(entry != NULL);
+
+	spdk_json_write_object_begin(w);
+	spdk_json_write_named_string(w, "cluster_name", entry->name);
+
+	if (entry->user_id) {
+		spdk_json_write_named_string(w, "user_id", entry->user_id);
+	}
+
+	if (entry->config_param) {
+		char **config_entry = entry->config_param;
+
+		spdk_json_write_named_object_begin(w, "config_param");
+		while (*config_entry) {
+			spdk_json_write_named_string(w, config_entry[0], config_entry[1]);
+			config_entry += 2;
+		}
+		spdk_json_write_object_end(w);
+	} else if (entry->config_file) {
+		spdk_json_write_named_string(w, "config_file", entry->config_file);
+	}
+
+	spdk_json_write_object_end(w);
+}
+
+int
+bdev_rbd_get_clusters_info(struct spdk_jsonrpc_request *request, const char *name)
+{
+	struct bdev_rbd_cluster *entry;
+	struct spdk_json_write_ctx *w;
+
+	pthread_mutex_lock(&g_map_bdev_rbd_cluster_mutex);
+
+	if (STAILQ_EMPTY(&g_map_bdev_rbd_cluster)) {
+		pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+		return -ENOENT;
+	}
+
+	/* If cluster name is provided */
+	if (name) {
+		STAILQ_FOREACH(entry, &g_map_bdev_rbd_cluster, link) {
+			if (strcmp(name, entry->name) == 0) {
+				w = spdk_jsonrpc_begin_result(request);
+				dump_single_cluster_entry(entry, w);
+				spdk_jsonrpc_end_result(request, w);
+
+				pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+				return 0;
+			}
+		}
+
+		pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+		return -ENOENT;
+	}
+
+	w = spdk_jsonrpc_begin_result(request);
+	spdk_json_write_array_begin(w);
+	STAILQ_FOREACH(entry, &g_map_bdev_rbd_cluster, link) {
+		dump_single_cluster_entry(entry, w);
+	}
+	spdk_json_write_array_end(w);
+	spdk_jsonrpc_end_result(request, w);
+	pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+
+	return 0;
+}
+
 static const struct spdk_bdev_fn_table rbd_fn_table = {
 	.destruct		= bdev_rbd_destruct,
 	.submit_request		= bdev_rbd_submit_request,
@@ -632,12 +869,174 @@ static const struct spdk_bdev_fn_table rbd_fn_table = {
 	.write_config_json	= bdev_rbd_write_config_json,
 };
 
+static int
+rbd_register_cluster(const char *name, const char *user_id, const char *const *config_param,
+		     const char *config_file)
+{
+	struct bdev_rbd_cluster *entry;
+	int rc;
+
+	pthread_mutex_lock(&g_map_bdev_rbd_cluster_mutex);
+	STAILQ_FOREACH(entry, &g_map_bdev_rbd_cluster, link) {
+		if (strcmp(name, entry->name) == 0) {
+			SPDK_ERRLOG("Cluster name=%s already exists\n", name);
+			pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+			return -1;
+		}
+	}
+
+	entry = calloc(1, sizeof(*entry));
+	if (!entry) {
+		SPDK_ERRLOG("Cannot allocate an entry for name=%s\n", name);
+		pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+		return -1;
+	}
+
+	entry->name = strdup(name);
+	if (entry->name == NULL) {
+		SPDK_ERRLOG("Failed to save the name =%s on entry =%p\n", name, entry);
+		goto err_handle;
+	}
+
+	if (user_id) {
+		entry->user_id = strdup(user_id);
+		if (entry->user_id == NULL) {
+			SPDK_ERRLOG("Failed to save the str =%s on entry =%p\n", user_id, entry);
+			goto err_handle;
+		}
+	}
+
+	/* The first priority is the config_param, then we use the config_file */
+	if (config_param) {
+		entry->config_param = bdev_rbd_dup_config(config_param);
+		if (entry->config_param == NULL) {
+			SPDK_ERRLOG("Failed to save the config_param=%p on entry = %p\n", config_param, entry);
+			goto err_handle;
+		}
+	} else if (config_file) {
+		entry->config_file = strdup(config_file);
+		if (entry->config_file == NULL) {
+			SPDK_ERRLOG("Failed to save the config_file=%s on entry = %p\n", config_file, entry);
+			goto err_handle;
+		}
+	}
+
+	rc = rados_create(&entry->cluster, user_id);
+	if (rc < 0) {
+		SPDK_ERRLOG("Failed to create rados_t struct\n");
+		goto err_handle;
+	}
+
+	if (config_param) {
+		const char *const *config_entry = config_param;
+		while (*config_entry) {
+			rc = rados_conf_set(entry->cluster, config_entry[0], config_entry[1]);
+			if (rc < 0) {
+				SPDK_ERRLOG("Failed to set %s = %s\n", config_entry[0], config_entry[1]);
+				rados_shutdown(entry->cluster);
+				goto err_handle;
+			}
+			config_entry += 2;
+		}
+	} else {
+		rc = rados_conf_read_file(entry->cluster, entry->config_file);
+		if (rc < 0) {
+			SPDK_ERRLOG("Failed to read conf file\n");
+			rados_shutdown(entry->cluster);
+			goto err_handle;
+		}
+	}
+
+	rc = rados_connect(entry->cluster);
+	if (rc < 0) {
+		SPDK_ERRLOG("Failed to connect to rbd_pool on cluster=%p\n", entry->cluster);
+		rados_shutdown(entry->cluster);
+		goto err_handle;
+	}
+
+	STAILQ_INSERT_TAIL(&g_map_bdev_rbd_cluster, entry, link);
+	pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+
+	return 0;
+
+err_handle:
+	bdev_rbd_cluster_free(entry);
+	pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+	return -1;
+}
+
+int
+bdev_rbd_unregister_cluster(const char *name)
+{
+	struct bdev_rbd_cluster *entry;
+	int rc = 0;
+
+	if (name == NULL) {
+		return -1;
+	}
+
+	pthread_mutex_lock(&g_map_bdev_rbd_cluster_mutex);
+	STAILQ_FOREACH(entry, &g_map_bdev_rbd_cluster, link) {
+		if (strcmp(name, entry->name) == 0) {
+			if (entry->ref == 0) {
+				STAILQ_REMOVE(&g_map_bdev_rbd_cluster, entry, bdev_rbd_cluster, link);
+				rados_shutdown(entry->cluster);
+				bdev_rbd_cluster_free(entry);
+			} else {
+				SPDK_ERRLOG("Cluster with name=%p is still used and we cannot delete it\n",
+					    entry->name);
+				rc = -1;
+			}
+
+			pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+			return rc;
+		}
+	}
+
+	pthread_mutex_unlock(&g_map_bdev_rbd_cluster_mutex);
+
+	SPDK_ERRLOG("Could not find the cluster name =%p\n", name);
+
+	return -1;
+}
+
+static void *
+_bdev_rbd_register_cluster(void *arg)
+{
+	struct cluster_register_info *info = arg;
+	void *ret = arg;
+	int rc;
+
+	rc = rbd_register_cluster((const char *)info->name, (const char *)info->user_id,
+				  (const char *const *)info->config_param, (const char *)info->config_file);
+	if (rc) {
+		ret = NULL;
+	}
+
+	return ret;
+}
+
+int
+bdev_rbd_register_cluster(struct cluster_register_info *info)
+{
+	assert(info != NULL);
+
+	/* Rados cluster info need to be created in non SPDK-thread to avoid CPU
+	 * resource contention */
+	if (spdk_call_unaffinitized(_bdev_rbd_register_cluster, info) == NULL) {
+		return -1;
+	}
+
+	return 0;
+}
+
 int
 bdev_rbd_create(struct spdk_bdev **bdev, const char *name, const char *user_id,
 		const char *pool_name,
 		const char *const *config,
 		const char *rbd_name,
-		uint32_t block_size)
+		uint32_t block_size,
+		const char *cluster_name)
 {
 	struct bdev_rbd *rbd;
 	int ret;
@@ -666,6 +1065,13 @@ bdev_rbd_create(struct spdk_bdev **bdev, const char *name, const char *user_id,
 		}
 	}
 
+	if (cluster_name) {
+		rbd->cluster_name = strdup(cluster_name);
+		if (!rbd->cluster_name) {
+			bdev_rbd_free(rbd);
+			return -ENOMEM;
+		}
+	}
 	rbd->pool_name = strdup(pool_name);
 	if (!rbd->pool_name) {
 		bdev_rbd_free(rbd);
diff --git a/module/bdev/rbd/bdev_rbd.h b/module/bdev/rbd/bdev_rbd.h
index 1d16a02db..c7399251c 100644
--- a/module/bdev/rbd/bdev_rbd.h
+++ b/module/bdev/rbd/bdev_rbd.h
@@ -37,6 +37,14 @@
 #include "spdk/stdinc.h"
 
 #include "spdk/bdev.h"
+#include "spdk/rpc.h"
+
+struct cluster_register_info {
+	char *name;
+	char *user_id;
+	char **config_param;
+	char *config_file;
+};
 
 void bdev_rbd_free_config(char **config);
 char **bdev_rbd_dup_config(const char *const *config);
@@ -46,7 +54,7 @@ typedef void (*spdk_delete_rbd_complete)(void *cb_arg, int bdeverrno);
 int bdev_rbd_create(struct spdk_bdev **bdev, const char *name, const char *user_id,
 		    const char *pool_name,
 		    const char *const *config,
-		    const char *rbd_name, uint32_t block_size);
+		    const char *rbd_name, uint32_t block_size, const char *cluster_name);
 /**
  * Delete rbd bdev.
  *
@@ -65,4 +73,27 @@ void bdev_rbd_delete(struct spdk_bdev *bdev, spdk_delete_rbd_complete cb_fn,
  */
 int bdev_rbd_resize(struct spdk_bdev *bdev, const uint64_t new_size_in_mb);
 
+/**
+ * Create a Rados cluster.
+ *
+ * \param info the info to register the Rados cluster object
+ */
+int bdev_rbd_register_cluster(struct cluster_register_info *info);
+
+/**
+ * Delete a registered cluster.
+ *
+ * \param name the name of the cluster to be deleted.
+ */
+int bdev_rbd_unregister_cluster(const char *name);
+
+/**
+ * Show the cluster info of a given name. If given name is empty,
+ * the info of every registered cluster name will be showed.
+ *
+ * \param request the json request.
+ * \param name the name of the cluster.
+ */
+int bdev_rbd_get_clusters_info(struct spdk_jsonrpc_request *request, const char *name);
+
 #endif /* SPDK_BDEV_RBD_H */
diff --git a/module/bdev/rbd/bdev_rbd_rpc.c b/module/bdev/rbd/bdev_rbd_rpc.c
index ea2a7f463..b066c1ee9 100644
--- a/module/bdev/rbd/bdev_rbd_rpc.c
+++ b/module/bdev/rbd/bdev_rbd_rpc.c
@@ -32,7 +32,6 @@
  */
 
 #include "bdev_rbd.h"
-#include "spdk/rpc.h"
 #include "spdk/util.h"
 #include "spdk/string.h"
 #include "spdk/log.h"
@@ -44,6 +43,7 @@ struct rpc_create_rbd {
 	char *rbd_name;
 	uint32_t block_size;
 	char **config;
+	char *cluster_name;
 };
 
 static void
@@ -54,6 +54,7 @@ free_rpc_create_rbd(struct rpc_create_rbd *req)
 	free(req->pool_name);
 	free(req->rbd_name);
 	bdev_rbd_free_config(req->config);
+	free(req->cluster_name);
 }
 
 static int
@@ -104,7 +105,8 @@ static const struct spdk_json_object_decoder rpc_create_rbd_decoders[] = {
 	{"pool_name", offsetof(struct rpc_create_rbd, pool_name), spdk_json_decode_string},
 	{"rbd_name", offsetof(struct rpc_create_rbd, rbd_name), spdk_json_decode_string},
 	{"block_size", offsetof(struct rpc_create_rbd, block_size), spdk_json_decode_uint32},
-	{"config", offsetof(struct rpc_create_rbd, config), bdev_rbd_decode_config, true}
+	{"config", offsetof(struct rpc_create_rbd, config), bdev_rbd_decode_config, true},
+	{"cluster_name", offsetof(struct rpc_create_rbd, cluster_name), spdk_json_decode_string, true}
 };
 
 static void
@@ -128,7 +130,7 @@ rpc_bdev_rbd_create(struct spdk_jsonrpc_request *request,
 	rc = bdev_rbd_create(&bdev, req.name, req.user_id, req.pool_name,
 			     (const char *const *)req.config,
 			     req.rbd_name,
-			     req.block_size);
+			     req.block_size, req.cluster_name);
 	if (rc) {
 		spdk_jsonrpc_send_error_response(request, rc, spdk_strerror(-rc));
 		goto cleanup;
@@ -244,3 +246,132 @@ cleanup:
 	free_rpc_bdev_rbd_resize(&req);
 }
 SPDK_RPC_REGISTER("bdev_rbd_resize", rpc_bdev_rbd_resize, SPDK_RPC_RUNTIME)
+
+static void
+free_rpc_register_cluster(struct cluster_register_info *req)
+{
+	free(req->name);
+	free(req->user_id);
+	bdev_rbd_free_config(req->config_param);
+	free(req->config_file);
+}
+
+static const struct spdk_json_object_decoder rpc_register_cluster_decoders[] = {
+	{"name", offsetof(struct cluster_register_info, name), spdk_json_decode_string, true},
+	{"user_id", offsetof(struct cluster_register_info, user_id), spdk_json_decode_string, true},
+	{"config_param", offsetof(struct cluster_register_info, config_param), bdev_rbd_decode_config, true},
+	{"config_file", offsetof(struct cluster_register_info, config_file), bdev_rbd_decode_config, true}
+};
+
+static void
+rpc_bdev_rbd_register_cluster(struct spdk_jsonrpc_request *request,
+			      const struct spdk_json_val *params)
+{
+	struct cluster_register_info req = {};
+	int rc = 0;
+	struct spdk_json_write_ctx *w;
+
+	if (spdk_json_decode_object(params, rpc_register_cluster_decoders,
+				    SPDK_COUNTOF(rpc_register_cluster_decoders),
+				    &req)) {
+		SPDK_DEBUGLOG(bdev_rbd, "spdk_json_decode_object failed\n");
+		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
+						 "spdk_json_decode_object failed");
+		goto cleanup;
+	}
+
+	rc = bdev_rbd_register_cluster(&req);
+	if (rc) {
+		spdk_jsonrpc_send_error_response(request, rc, spdk_strerror(-rc));
+		goto cleanup;
+	}
+
+	w = spdk_jsonrpc_begin_result(request);
+	spdk_json_write_string(w, req.name);
+	spdk_jsonrpc_end_result(request, w);
+cleanup:
+	free_rpc_register_cluster(&req);
+}
+SPDK_RPC_REGISTER("bdev_rbd_register_cluster", rpc_bdev_rbd_register_cluster, SPDK_RPC_RUNTIME)
+
+struct rpc_bdev_rbd_unregister_cluster {
+	char *name;
+};
+
+static void
+free_rpc_bdev_cluster_unregister(struct rpc_bdev_rbd_unregister_cluster *req)
+{
+	free(req->name);
+}
+
+static const struct spdk_json_object_decoder rpc_bdev_rbd_unregister_cluster_decoders[] = {
+	{"name", offsetof(struct rpc_bdev_rbd_unregister_cluster, name), spdk_json_decode_string},
+};
+
+static void
+rpc_bdev_rbd_unregister_cluster(struct spdk_jsonrpc_request *request,
+				const struct spdk_json_val *params)
+{
+	struct rpc_bdev_rbd_unregister_cluster req = {NULL};
+	int rc;
+
+	if (spdk_json_decode_object(params, rpc_bdev_rbd_unregister_cluster_decoders,
+				    SPDK_COUNTOF(rpc_bdev_rbd_unregister_cluster_decoders),
+				    &req)) {
+		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
+						 "spdk_json_decode_object failed");
+		goto cleanup;
+	}
+
+	rc = bdev_rbd_unregister_cluster(req.name);
+	if (rc) {
+		spdk_jsonrpc_send_error_response(request, rc, spdk_strerror(-rc));
+		goto cleanup;
+	}
+
+	spdk_jsonrpc_send_bool_response(request, true);
+
+cleanup:
+	free_rpc_bdev_cluster_unregister(&req);
+}
+SPDK_RPC_REGISTER("bdev_rbd_unregister_cluster", rpc_bdev_rbd_unregister_cluster, SPDK_RPC_RUNTIME)
+
+struct rpc_bdev_rbd_get_cluster_info {
+	char *name;
+};
+
+static void
+free_rpc_bdev_rbd_get_cluster_info(struct rpc_bdev_rbd_get_cluster_info *req)
+{
+	free(req->name);
+}
+
+static const struct spdk_json_object_decoder rpc_bdev_rbd_get_cluster_info_decoders[] = {
+	{"name", offsetof(struct rpc_bdev_rbd_get_cluster_info, name), spdk_json_decode_string, true},
+};
+
+static void
+rpc_bdev_rbd_get_clusters_info(struct spdk_jsonrpc_request *request,
+			       const struct spdk_json_val *params)
+{
+	struct rpc_bdev_rbd_get_cluster_info req = {NULL};
+	int rc;
+
+	if (params && spdk_json_decode_object(params, rpc_bdev_rbd_get_cluster_info_decoders,
+					      SPDK_COUNTOF(rpc_bdev_rbd_get_cluster_info_decoders),
+					      &req)) {
+		spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INTERNAL_ERROR,
+						 "spdk_json_decode_object failed");
+		goto cleanup;
+	}
+
+	rc = bdev_rbd_get_clusters_info(request, req.name);
+	if (rc) {
+		spdk_jsonrpc_send_error_response(request, rc, spdk_strerror(-rc));
+		goto cleanup;
+	}
+
+cleanup:
+	free_rpc_bdev_rbd_get_cluster_info(&req);
+}
+SPDK_RPC_REGISTER("bdev_rbd_get_clusters_info", rpc_bdev_rbd_get_clusters_info, SPDK_RPC_RUNTIME)
diff --git a/module/bdev/zone_block/vbdev_zone_block.c b/module/bdev/zone_block/vbdev_zone_block.c
index d6fd296d0..81a9dc33e 100644
--- a/module/bdev/zone_block/vbdev_zone_block.c
+++ b/module/bdev/zone_block/vbdev_zone_block.c
@@ -776,8 +776,6 @@ zone_block_register(const char *base_bdev_name)
 		bdev_node->zone_shift = spdk_u64log2(zone_size);
 		bdev_node->num_zones = base_bdev->blockcnt / zone_size;
 
-		/* Align num_zones to optimal_open_zones */
-		bdev_node->num_zones -= bdev_node->num_zones % name->optimal_open_zones;
 		bdev_node->zones = calloc(bdev_node->num_zones, sizeof(struct block_zone));
 		if (!bdev_node->zones) {
 			rc = -ENOMEM;
diff --git a/module/blob/bdev/blob_bdev.c b/module/blob/bdev/blob_bdev.c
index 1b788c5bf..32c1a59d8 100644
--- a/module/blob/bdev/blob_bdev.c
+++ b/module/blob/bdev/blob_bdev.c
@@ -38,6 +38,7 @@
 #include "spdk/thread.h"
 #include "spdk/log.h"
 #include "spdk/endian.h"
+#define __SPDK_BDEV_MODULE_ONLY
 #include "spdk/bdev_module.h"
 
 struct blob_bdev {
diff --git a/module/blobfs/bdev/blobfs_fuse.c b/module/blobfs/bdev/blobfs_fuse.c
index 16665498a..176f81e14 100644
--- a/module/blobfs/bdev/blobfs_fuse.c
+++ b/module/blobfs/bdev/blobfs_fuse.c
@@ -301,15 +301,19 @@ blobfs_fuse_start(const char *bdev_name, const char *mountpoint, struct spdk_fil
 		return -ENOMEM;
 	}
 
-	rc = fuse_parse_cmdline(&args, &opts);
-	assert(rc == 0);
-
 	bfuse->bdev_name = strdup(bdev_name);
 	bfuse->mountpoint = strdup(mountpoint);
+	if (!bfuse->bdev_name || !bfuse->mountpoint) {
+		rc = -ENOMEM;
+		goto err;
+	}
 	bfuse->fs = fs;
 	bfuse->cb_fn = cb_fn;
 	bfuse->cb_arg = cb_arg;
 
+	rc = fuse_parse_cmdline(&args, &opts);
+	assert(rc == 0);
+
 	fuse_handle = fuse_new(&args, &spdk_fuse_oper, sizeof(spdk_fuse_oper), NULL);
 	fuse_opt_free_args(&args);
 	if (fuse_handle == NULL) {
diff --git a/module/event/subsystems/accel/accel.c b/module/event/subsystems/accel/accel.c
index dd20c3b22..079cac9a6 100644
--- a/module/event/subsystems/accel/accel.c
+++ b/module/event/subsystems/accel/accel.c
@@ -35,7 +35,7 @@
 
 #include "spdk/accel_engine.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 #include "spdk/env.h"
 
 static void
diff --git a/module/event/subsystems/bdev/bdev.c b/module/event/subsystems/bdev/bdev.c
index 9b4917c7e..055e182c3 100644
--- a/module/event/subsystems/bdev/bdev.c
+++ b/module/event/subsystems/bdev/bdev.c
@@ -37,7 +37,7 @@
 #include "spdk/env.h"
 #include "spdk/thread.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 #include "spdk/env.h"
 
 static void
diff --git a/module/event/subsystems/iscsi/iscsi.c b/module/event/subsystems/iscsi/iscsi.c
index a131a6d30..19944cf05 100644
--- a/module/event/subsystems/iscsi/iscsi.c
+++ b/module/event/subsystems/iscsi/iscsi.c
@@ -35,7 +35,7 @@
 
 #include "iscsi/iscsi.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 
 static void
 iscsi_subsystem_init_complete(void *cb_arg, int rc)
diff --git a/module/event/subsystems/nbd/nbd.c b/module/event/subsystems/nbd/nbd.c
index 50340c5c7..1af23b06c 100644
--- a/module/event/subsystems/nbd/nbd.c
+++ b/module/event/subsystems/nbd/nbd.c
@@ -35,7 +35,7 @@
 
 #include "spdk/nbd.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 
 static void
 nbd_subsystem_init(void)
diff --git a/module/event/subsystems/net/net.c b/module/event/subsystems/net/net.c
index 60aca98dc..2086acf2b 100644
--- a/module/event/subsystems/net/net.c
+++ b/module/event/subsystems/net/net.c
@@ -35,7 +35,7 @@
 
 #include "spdk/net.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 
 static void
 interface_subsystem_init(void)
diff --git a/module/event/subsystems/nvmf/event_nvmf.h b/module/event/subsystems/nvmf/event_nvmf.h
index b22927ffc..9c796f9bb 100644
--- a/module/event/subsystems/nvmf/event_nvmf.h
+++ b/module/event/subsystems/nvmf/event_nvmf.h
@@ -39,7 +39,7 @@
 #include "spdk/nvmf.h"
 #include "spdk/queue.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 #include "spdk/log.h"
 
 #define ACCEPT_TIMEOUT_US	10000 /* 10ms */
@@ -57,7 +57,10 @@ struct spdk_nvmf_tgt_conf {
 extern struct spdk_nvmf_tgt_conf g_spdk_nvmf_tgt_conf;
 
 extern uint32_t g_spdk_nvmf_tgt_max_subsystems;
+extern uint16_t g_spdk_nvmf_tgt_crdt[3];
 
 extern struct spdk_nvmf_tgt *g_spdk_nvmf_tgt;
 
+extern struct spdk_cpuset *g_poll_groups_mask;
+
 #endif
diff --git a/module/event/subsystems/nvmf/nvmf_rpc.c b/module/event/subsystems/nvmf/nvmf_rpc.c
index 87c11e393..84705a63c 100644
--- a/module/event/subsystems/nvmf/nvmf_rpc.c
+++ b/module/event/subsystems/nvmf/nvmf_rpc.c
@@ -128,3 +128,43 @@ rpc_nvmf_set_config(struct spdk_jsonrpc_request *request,
 }
 SPDK_RPC_REGISTER("nvmf_set_config", rpc_nvmf_set_config, SPDK_RPC_STARTUP)
 SPDK_RPC_REGISTER_ALIAS_DEPRECATED(nvmf_set_config, set_nvmf_target_config)
+
+struct nvmf_rpc_set_crdt {
+	uint16_t crdt1;
+	uint16_t crdt2;
+	uint16_t crdt3;
+};
+
+static const struct spdk_json_object_decoder rpc_set_crdt_opts_decoders[] = {
+	{"crdt1", offsetof(struct nvmf_rpc_set_crdt, crdt1), spdk_json_decode_uint16, true},
+	{"crdt2", offsetof(struct nvmf_rpc_set_crdt, crdt2), spdk_json_decode_uint16, true},
+	{"crdt3", offsetof(struct nvmf_rpc_set_crdt, crdt3), spdk_json_decode_uint16, true},
+};
+
+static void
+rpc_nvmf_set_crdt(struct spdk_jsonrpc_request *request,
+		  const struct spdk_json_val *params)
+{
+	struct nvmf_rpc_set_crdt rpc_set_crdt;
+
+	rpc_set_crdt.crdt1 = 0;
+	rpc_set_crdt.crdt2 = 0;
+	rpc_set_crdt.crdt3 = 0;
+
+	if (params != NULL) {
+		if (spdk_json_decode_object(params, rpc_set_crdt_opts_decoders,
+					    SPDK_COUNTOF(rpc_set_crdt_opts_decoders), &rpc_set_crdt)) {
+			SPDK_ERRLOG("spdk_json_decode_object() failed\n");
+			spdk_jsonrpc_send_error_response(request, SPDK_JSONRPC_ERROR_INVALID_PARAMS,
+							 "Invalid parameters");
+			return;
+		}
+	}
+
+	g_spdk_nvmf_tgt_crdt[0] = rpc_set_crdt.crdt1;
+	g_spdk_nvmf_tgt_crdt[1] = rpc_set_crdt.crdt2;
+	g_spdk_nvmf_tgt_crdt[2] = rpc_set_crdt.crdt3;
+
+	spdk_jsonrpc_send_bool_response(request, true);
+}
+SPDK_RPC_REGISTER("nvmf_set_crdt", rpc_nvmf_set_crdt, SPDK_RPC_STARTUP)
diff --git a/module/event/subsystems/nvmf/nvmf_tgt.c b/module/event/subsystems/nvmf/nvmf_tgt.c
index 5f5ea5dbd..65b40b8d6 100644
--- a/module/event/subsystems/nvmf/nvmf_tgt.c
+++ b/module/event/subsystems/nvmf/nvmf_tgt.c
@@ -38,7 +38,7 @@
 #include "spdk/log.h"
 #include "spdk/nvme.h"
 #include "spdk/nvmf_cmd.h"
-#include "spdk/util.h"
+#include "spdk_internal/usdt.h"
 
 enum nvmf_tgt_state {
 	NVMF_TGT_INIT_NONE = 0,
@@ -64,8 +64,10 @@ struct spdk_nvmf_tgt_conf g_spdk_nvmf_tgt_conf = {
 	.admin_passthru.identify_ctrlr = false
 };
 
+struct spdk_cpuset *g_poll_groups_mask = NULL;
 struct spdk_nvmf_tgt *g_spdk_nvmf_tgt = NULL;
 uint32_t g_spdk_nvmf_tgt_max_subsystems = 0;
+uint16_t g_spdk_nvmf_tgt_crdt[3] = {0, 0, 0};
 
 static enum nvmf_tgt_state g_tgt_state;
 
@@ -149,6 +151,16 @@ nvmf_tgt_destroy_poll_groups(void)
 	}
 }
 
+static uint32_t
+nvmf_get_cpuset_count(void)
+{
+	if (g_poll_groups_mask) {
+		return spdk_cpuset_count(g_poll_groups_mask);
+	} else {
+		return spdk_env_get_core_count();
+	}
+}
+
 static void
 nvmf_tgt_create_poll_group_done(void *ctx)
 {
@@ -156,9 +168,9 @@ nvmf_tgt_create_poll_group_done(void *ctx)
 
 	TAILQ_INSERT_TAIL(&g_poll_groups, pg, link);
 
-	assert(g_num_poll_groups < spdk_env_get_core_count());
+	assert(g_num_poll_groups < nvmf_get_cpuset_count());
 
-	if (++g_num_poll_groups == spdk_env_get_core_count()) {
+	if (++g_num_poll_groups == nvmf_get_cpuset_count()) {
 		g_tgt_state = NVMF_TGT_INIT_START_SUBSYSTEMS;
 		nvmf_tgt_advance_state();
 	}
@@ -186,7 +198,6 @@ nvmf_tgt_create_poll_group(void *ctx)
 static void
 nvmf_tgt_create_poll_groups(void)
 {
-	struct spdk_cpuset tmp_cpumask = {};
 	uint32_t i;
 	char thread_name[32];
 	struct spdk_thread *thread;
@@ -195,11 +206,12 @@ nvmf_tgt_create_poll_groups(void)
 	assert(g_tgt_init_thread != NULL);
 
 	SPDK_ENV_FOREACH_CORE(i) {
-		spdk_cpuset_zero(&tmp_cpumask);
-		spdk_cpuset_set_cpu(&tmp_cpumask, i, true);
+		if (g_poll_groups_mask && !spdk_cpuset_get_cpu(g_poll_groups_mask, i)) {
+			continue;
+		}
 		snprintf(thread_name, sizeof(thread_name), "nvmf_tgt_poll_group_%u", i);
 
-		thread = spdk_thread_create(thread_name, &tmp_cpumask);
+		thread = spdk_thread_create(thread_name, g_poll_groups_mask);
 		assert(thread != NULL);
 
 		spdk_thread_send_msg(thread, nvmf_tgt_create_poll_group, NULL);
@@ -281,6 +293,9 @@ nvmf_tgt_create_target(void)
 
 	opts.max_subsystems = g_spdk_nvmf_tgt_max_subsystems;
 	opts.acceptor_poll_rate = g_spdk_nvmf_tgt_conf.acceptor_poll_rate;
+	opts.crdt[0] = g_spdk_nvmf_tgt_crdt[0];
+	opts.crdt[1] = g_spdk_nvmf_tgt_crdt[1];
+	opts.crdt[2] = g_spdk_nvmf_tgt_crdt[2];
 	g_spdk_nvmf_tgt = spdk_nvmf_tgt_create(&opts);
 	if (!g_spdk_nvmf_tgt) {
 		SPDK_ERRLOG("spdk_nvmf_tgt_create() failed\n");
@@ -378,6 +393,7 @@ nvmf_tgt_advance_state(void)
 	int ret;
 
 	do {
+		SPDK_DTRACE_PROBE1(nvmf_tgt_state, g_tgt_state);
 		prev_state = g_tgt_state;
 
 		switch (g_tgt_state) {
diff --git a/module/event/subsystems/scsi/scsi.c b/module/event/subsystems/scsi/scsi.c
index f068b0da6..daf818acf 100644
--- a/module/event/subsystems/scsi/scsi.c
+++ b/module/event/subsystems/scsi/scsi.c
@@ -35,7 +35,7 @@
 
 #include "spdk/scsi.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 
 static void
 scsi_subsystem_init(void)
diff --git a/module/event/subsystems/sock/sock.c b/module/event/subsystems/sock/sock.c
index fdcb2160a..5777555c8 100644
--- a/module/event/subsystems/sock/sock.c
+++ b/module/event/subsystems/sock/sock.c
@@ -32,7 +32,7 @@
 
 #include "spdk/stdinc.h"
 #include "spdk/sock.h"
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 
 static void
 sock_subsystem_init(void)
diff --git a/module/event/subsystems/vhost/vhost.c b/module/event/subsystems/vhost/vhost.c
index b348cca5d..1fa9b9f13 100644
--- a/module/event/subsystems/vhost/vhost.c
+++ b/module/event/subsystems/vhost/vhost.c
@@ -35,7 +35,7 @@
 
 #include "spdk/vhost.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 
 static void
 vhost_subsystem_init_done(int rc)
diff --git a/module/event/subsystems/vmd/vmd.c b/module/event/subsystems/vmd/vmd.c
index 2d496994c..07717453f 100644
--- a/module/event/subsystems/vmd/vmd.c
+++ b/module/event/subsystems/vmd/vmd.c
@@ -32,12 +32,15 @@
  */
 
 #include "spdk/stdinc.h"
+
+#include "spdk/json.h"
 #include "spdk/thread.h"
 #include "spdk/likely.h"
+#include "spdk/log.h"
 
 #include "spdk/vmd.h"
 
-#include "spdk_internal/event.h"
+#include "spdk_internal/init.h"
 #include "event_vmd.h"
 
 static struct spdk_poller *g_hotplug_poller;
diff --git a/module/sock/posix/posix.c b/module/sock/posix/posix.c
index 59125f823..bc060fcde 100644
--- a/module/sock/posix/posix.c
+++ b/module/sock/posix/posix.c
@@ -68,7 +68,8 @@ struct spdk_posix_sock {
 	struct spdk_pipe	*recv_pipe;
 	void			*recv_buf;
 	int			recv_buf_sz;
-	bool			pending_events;
+	bool			pipe_has_data;
+	bool			socket_has_data;
 	bool			zcopy;
 
 	int			placement_id;
@@ -76,12 +77,12 @@ struct spdk_posix_sock {
 	TAILQ_ENTRY(spdk_posix_sock)	link;
 };
 
-TAILQ_HEAD(spdk_pending_events_list, spdk_posix_sock);
+TAILQ_HEAD(spdk_has_data_list, spdk_posix_sock);
 
 struct spdk_posix_sock_group_impl {
 	struct spdk_sock_group_impl	base;
 	int				fd;
-	struct spdk_pending_events_list	pending_events;
+	struct spdk_has_data_list	socks_with_data;
 	int				placement_id;
 };
 
@@ -498,12 +499,14 @@ retry:
 		rc = setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &val, sizeof val);
 		if (rc != 0) {
 			close(fd);
+			fd = -1;
 			/* error */
 			continue;
 		}
 		rc = setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, &val, sizeof val);
 		if (rc != 0) {
 			close(fd);
+			fd = -1;
 			/* error */
 			continue;
 		}
@@ -513,6 +516,7 @@ retry:
 			rc = setsockopt(fd, SOL_SOCKET, SO_PRIORITY, &opts->priority, sizeof val);
 			if (rc != 0) {
 				close(fd);
+				fd = -1;
 				/* error */
 				continue;
 			}
@@ -523,6 +527,7 @@ retry:
 			rc = setsockopt(fd, IPPROTO_IPV6, IPV6_V6ONLY, &val, sizeof val);
 			if (rc != 0) {
 				close(fd);
+				fd = -1;
 				/* error */
 				continue;
 			}
@@ -904,13 +909,16 @@ posix_sock_recv_from_pipe(struct spdk_posix_sock *sock, struct iovec *diov, int
 
 	spdk_pipe_reader_advance(sock->recv_pipe, bytes);
 
-	/* If we drained the pipe, take it off the pending_events list. The socket may still have data buffered
-	 * in the kernel to receive, but this will be handled on the next poll call when we get the same EPOLLIN
-	 * event again. */
-	if (sock->base.group_impl && spdk_pipe_reader_bytes_available(sock->recv_pipe) == 0) {
+	/* If we drained the pipe, mark it appropriately */
+	if (spdk_pipe_reader_bytes_available(sock->recv_pipe) == 0) {
+		assert(sock->pipe_has_data == true);
+
 		group = __posix_group_impl(sock->base.group_impl);
-		TAILQ_REMOVE(&group->pending_events, sock, link);
-		sock->pending_events = false;
+		if (group && !sock->socket_has_data) {
+			TAILQ_REMOVE(&group->socks_with_data, sock, link);
+		}
+
+		sock->pipe_has_data = false;
 	}
 
 	return bytes;
@@ -920,34 +928,43 @@ static inline ssize_t
 posix_sock_read(struct spdk_posix_sock *sock)
 {
 	struct iovec iov[2];
-	int bytes;
+	int bytes_avail, bytes_recvd;
 	struct spdk_posix_sock_group_impl *group;
 
-	bytes = spdk_pipe_writer_get_buffer(sock->recv_pipe, sock->recv_buf_sz, iov);
-
-	if (bytes > 0) {
-		bytes = readv(sock->fd, iov, 2);
-		if (bytes > 0) {
-			spdk_pipe_writer_advance(sock->recv_pipe, bytes);
-
-			/* For normal operation, this function is called in response to an EPOLLIN
-			 * event, which already placed the socket onto the pending_events list.
-			 * But between polls the user may repeatedly call posix_sock_read
-			 * and if they clear the pipe on one of those earlier calls, the
-			 * socket will be removed from the pending_events list. In that case,
-			 * if we now found more data, put it back on.
-			 * This essentially never happens in practice because the application
-			 * will stop trying to receive and wait for the next EPOLLIN event, but
-			 * for correctness let's handle it. */
-			if (!sock->pending_events && sock->base.group_impl) {
-				group = __posix_group_impl(sock->base.group_impl);
-				TAILQ_INSERT_TAIL(&group->pending_events, sock, link);
-				sock->pending_events = true;
-			}
+	bytes_avail = spdk_pipe_writer_get_buffer(sock->recv_pipe, sock->recv_buf_sz, iov);
+
+	if (bytes_avail <= 0) {
+		return bytes_avail;
+	}
+
+	bytes_recvd = readv(sock->fd, iov, 2);
+
+	assert(sock->pipe_has_data == false);
+
+	if (bytes_recvd <= 0) {
+		/* Errors count as draining the socket data */
+		if (sock->base.group_impl && sock->socket_has_data) {
+			group = __posix_group_impl(sock->base.group_impl);
+			TAILQ_REMOVE(&group->socks_with_data, sock, link);
 		}
+
+		sock->socket_has_data = false;
+
+		return bytes_recvd;
 	}
 
-	return bytes;
+	spdk_pipe_writer_advance(sock->recv_pipe, bytes_recvd);
+
+#if DEBUG
+	if (sock->base.group_impl) {
+		assert(sock->socket_has_data == true);
+	}
+#endif
+
+	sock->pipe_has_data = true;
+	sock->socket_has_data = false;
+
+	return bytes_recvd;
 }
 
 static ssize_t
@@ -959,26 +976,26 @@ posix_sock_readv(struct spdk_sock *_sock, struct iovec *iov, int iovcnt)
 	size_t len;
 
 	if (sock->recv_pipe == NULL) {
-		if (group && sock->pending_events) {
-			sock->pending_events = false;
-			TAILQ_REMOVE(&group->pending_events, sock, link);
+		assert(sock->pipe_has_data == false);
+		if (group && sock->socket_has_data) {
+			sock->socket_has_data = false;
+			TAILQ_REMOVE(&group->socks_with_data, sock, link);
 		}
 		return readv(sock->fd, iov, iovcnt);
 	}
 
-	len = 0;
-	for (i = 0; i < iovcnt; i++) {
-		len += iov[i].iov_len;
-	}
-
-	if (spdk_pipe_reader_bytes_available(sock->recv_pipe) == 0) {
+	/* If the socket is not in a group, we must assume it always has
+	 * data waiting for us because it is not epolled */
+	if (!sock->pipe_has_data && (group == NULL || sock->socket_has_data)) {
 		/* If the user is receiving a sufficiently large amount of data,
 		 * receive directly to their buffers. */
+		len = 0;
+		for (i = 0; i < iovcnt; i++) {
+			len += iov[i].iov_len;
+		}
+
 		if (len >= MIN_SOCK_PIPE_SIZE) {
-			if (group && sock->pending_events) {
-				sock->pending_events = false;
-				TAILQ_REMOVE(&group->pending_events, sock, link);
-			}
+			/* TODO: Should this detect if kernel socket is drained? */
 			return readv(sock->fd, iov, iovcnt);
 		}
 
@@ -1160,7 +1177,7 @@ posix_sock_group_impl_create(void)
 	}
 
 	group_impl->fd = fd;
-	TAILQ_INIT(&group_impl->pending_events);
+	TAILQ_INIT(&group_impl->socks_with_data);
 	group_impl->placement_id = -1;
 
 	if (g_spdk_posix_sock_impl_opts.enable_placement_id == PLACEMENT_CPU) {
@@ -1256,9 +1273,9 @@ posix_sock_group_impl_add_sock(struct spdk_sock_group_impl *_group, struct spdk_
 	/* switched from another polling group due to scheduling */
 	if (spdk_unlikely(sock->recv_pipe != NULL  &&
 			  (spdk_pipe_reader_bytes_available(sock->recv_pipe) > 0))) {
-		assert(sock->pending_events == false);
-		sock->pending_events = true;
-		TAILQ_INSERT_TAIL(&group->pending_events, sock, link);
+		sock->pipe_has_data = true;
+		sock->socket_has_data = false;
+		TAILQ_INSERT_TAIL(&group->socks_with_data, sock, link);
 	}
 
 	if (g_spdk_posix_sock_impl_opts.enable_placement_id == PLACEMENT_MARK) {
@@ -1281,9 +1298,10 @@ posix_sock_group_impl_remove_sock(struct spdk_sock_group_impl *_group, struct sp
 	struct spdk_posix_sock *sock = __posix_sock(_sock);
 	int rc;
 
-	if (sock->pending_events) {
-		TAILQ_REMOVE(&group->pending_events, sock, link);
-		sock->pending_events = false;
+	if (sock->pipe_has_data || sock->socket_has_data) {
+		TAILQ_REMOVE(&group->socks_with_data, sock, link);
+		sock->pipe_has_data = false;
+		sock->socket_has_data = false;
 	}
 
 	if (sock->placement_id != -1) {
@@ -1362,7 +1380,7 @@ posix_sock_group_impl_poll(struct spdk_sock_group_impl *_group, int max_events,
 	 */
 	int last_placement_id = -1;
 
-	TAILQ_FOREACH(psock, &group->pending_events, link) {
+	TAILQ_FOREACH(psock, &group->socks_with_data, link) {
 		if (psock->zcopy && psock->placement_id >= 0 &&
 		    psock->placement_id != last_placement_id) {
 			struct pollfd pfd = {psock->fd, POLLIN | POLLERR, 0};
@@ -1433,16 +1451,17 @@ posix_sock_group_impl_poll(struct spdk_sock_group_impl *_group, int max_events,
 		psock = __posix_sock(sock);
 #endif
 
-		/* If the socket does not already have recv pending, add it now */
-		if (!psock->pending_events) {
-			psock->pending_events = true;
-			TAILQ_INSERT_TAIL(&group->pending_events, psock, link);
+		/* If the socket is not already in the list, add it now */
+		if (!psock->socket_has_data && !psock->pipe_has_data) {
+			TAILQ_INSERT_TAIL(&group->socks_with_data, psock, link);
 		}
+
+		psock->socket_has_data = true;
 	}
 
 	num_events = 0;
 
-	TAILQ_FOREACH_SAFE(psock, &group->pending_events, link, ptmp) {
+	TAILQ_FOREACH_SAFE(psock, &group->socks_with_data, link, ptmp) {
 		if (num_events == max_events) {
 			break;
 		}
@@ -1450,15 +1469,16 @@ posix_sock_group_impl_poll(struct spdk_sock_group_impl *_group, int max_events,
 		/* If the socket's cb_fn is NULL, just remove it from the
 		 * list and do not add it to socks array */
 		if (spdk_unlikely(psock->base.cb_fn == NULL)) {
-			psock->pending_events = false;
-			TAILQ_REMOVE(&group->pending_events, psock, link);
+			psock->socket_has_data = false;
+			psock->pipe_has_data = false;
+			TAILQ_REMOVE(&group->socks_with_data, psock, link);
 			continue;
 		}
 
 		socks[num_events++] = &psock->base;
 	}
 
-	/* Cycle the pending_events list so that each time we poll things aren't
+	/* Cycle the has_data list so that each time we poll things aren't
 	 * in the same order. Say we have 6 sockets in the list, named as follows:
 	 * A B C D E F
 	 * And all 6 sockets had epoll events, but max_events is only 3. That means
@@ -1473,9 +1493,9 @@ posix_sock_group_impl_poll(struct spdk_sock_group_impl *_group, int max_events,
 
 		/* Capture pointers to the elements we need */
 		pd = psock;
-		pc = TAILQ_PREV(pd, spdk_pending_events_list, link);
-		pa = TAILQ_FIRST(&group->pending_events);
-		pf = TAILQ_LAST(&group->pending_events, spdk_pending_events_list);
+		pc = TAILQ_PREV(pd, spdk_has_data_list, link);
+		pa = TAILQ_FIRST(&group->socks_with_data);
+		pf = TAILQ_LAST(&group->socks_with_data, spdk_has_data_list);
 
 		/* Break the link between C and D */
 		pc->link.tqe_next = NULL;
@@ -1486,8 +1506,8 @@ posix_sock_group_impl_poll(struct spdk_sock_group_impl *_group, int max_events,
 		pa->link.tqe_prev = &pf->link.tqe_next;
 
 		/* Fix up the list first/last pointers */
-		group->pending_events.tqh_first = pd;
-		group->pending_events.tqh_last = &pc->link.tqe_next;
+		group->socks_with_data.tqh_first = pd;
+		group->socks_with_data.tqh_last = &pc->link.tqe_next;
 	}
 
 	return num_events;
diff --git a/module/sock/uring/uring.c b/module/sock/uring/uring.c
index 3f4b54762..ea19c228d 100644
--- a/module/sock/uring/uring.c
+++ b/module/sock/uring/uring.c
@@ -441,12 +441,14 @@ retry:
 		rc = setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &val, sizeof val);
 		if (rc != 0) {
 			close(fd);
+			fd = -1;
 			/* error */
 			continue;
 		}
 		rc = setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, &val, sizeof val);
 		if (rc != 0) {
 			close(fd);
+			fd = -1;
 			/* error */
 			continue;
 		}
@@ -456,6 +458,7 @@ retry:
 			rc = setsockopt(fd, SOL_SOCKET, SO_PRIORITY, &opts->priority, sizeof val);
 			if (rc != 0) {
 				close(fd);
+				fd = -1;
 				/* error */
 				continue;
 			}
@@ -465,6 +468,7 @@ retry:
 			rc = setsockopt(fd, IPPROTO_IPV6, IPV6_V6ONLY, &val, sizeof val);
 			if (rc != 0) {
 				close(fd);
+				fd = -1;
 				/* error */
 				continue;
 			}
diff --git a/pkg/spdk.spec b/pkg/spdk.spec
index 2cbc75ec0..c6861081d 100644
--- a/pkg/spdk.spec
+++ b/pkg/spdk.spec
@@ -2,12 +2,12 @@
 %bcond_with doc
 
 Name: spdk
-Version: 21.04
+Version: master
 Release: 0%{?dist}
 Epoch: 0
 URL: http://spdk.io
 
-Source: https://github.com/spdk/spdk/archive/v21.04.tar.gz
+Source: https://github.com/spdk/spdk/archive/master.tar.gz
 Summary: Set of libraries and utilities for high performance user-mode storage
 
 %define package_version %{epoch}:%{version}-%{release}
diff --git a/rpmbuild/rpm.sh b/rpmbuild/rpm.sh
index 76b04ed9c..d358d6472 100755
--- a/rpmbuild/rpm.sh
+++ b/rpmbuild/rpm.sh
@@ -46,15 +46,18 @@ get_version() {
 }
 
 build_rpm() (
-	local macros=()
+	local macros=() dir
 
 	macros+=(-D "configure $configure")
 	macros+=(-D "make $make")
 	macros+=(-D "release $release")
 	macros+=(-D "version $version")
 
-	# Prepare default dir structure
-	mkdir -p "$HOME"/rpmbuild/{BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS}
+	# Adjust dir macros to update the final location of the RPMS
+	for dir in build buildroot rpm source spec srcrpm; do
+		mkdir -p "$rpmbuild_dir/$dir"
+		macros+=(-D "_${dir}dir $rpmbuild_dir/$dir")
+	done
 
 	if [[ $configure == *"with-shared"* || $configure == *"with-dpdk"* ]]; then
 		macros+=(-D "dpdk 1")
@@ -82,10 +85,10 @@ build_rpm() (
 
 	fedora_python_sys_path_workaround
 
-	# Despite building in-place, rpmbuild still looks under SOURCES as defined
+	# Despite building in-place, rpmbuild still looks under source dir as defined
 	# in Source:. Create a dummy file to fulfil its needs and to keep Source in
 	# the .spec.
-	: > "$rpmbuild_dir/SOURCES/spdk-$version.tar.gz"
+	: > "$rpmbuild_dir/source/spdk-$version.tar.gz"
 
 	printf '* Starting rpmbuild...\n'
 	rpmbuild --clean --nodebuginfo "${macros[@]}" --build-in-place -ba "$spec"
@@ -99,7 +102,7 @@ release=${RPM_RELEASE:-1}
 requirements=${REQUIREMENTS:-}
 version=${SPDK_VERSION:-$(get_version)}
 
-rpmbuild_dir=$HOME/rpmbuild
+rpmbuild_dir=${BUILDDIR:-"$HOME/rpmbuild"}
 spec=$specdir/spdk.spec
 
 build_rpm
diff --git a/rpmbuild/spdk.spec b/rpmbuild/spdk.spec
index a8f1639e8..b48a9e75e 100644
--- a/rpmbuild/spdk.spec
+++ b/rpmbuild/spdk.spec
@@ -1,20 +1,4 @@
-Name:           spdk
-Version:        %{version}
-Release:        %{release}
-Summary:        Storage Performance Development Kit
-
-License:       BSD
-URL:           https://spdk.io
-Source:        spdk-%{version}.tar.gz
-
-%description
-
-The Storage Performance Development Kit (SPDK) provides a set of tools and libraries for
-writing high performance, scalable, user-mode storage applications. It achieves high
-performance by moving all of the necessary drivers into userspace and operating in a
-polled mode instead of relying on interrupts, which avoids kernel context switches and
-eliminates interrupt handling overhead.
-
+# Global macros
 %define debug_package %{nil}
 
 %{!?deps:%define deps 1}
@@ -24,6 +8,12 @@ eliminates interrupt handling overhead.
 %{!?requirements:%define requirements 0}
 %{!?shared:%define shared 0}
 
+# Spec metadata
+Name:           spdk
+Version:        %{version}
+Release:        %{release}
+Summary:        Storage Performance Development Kit
+
 # This is a minimal set of requirements needed for SPDK apps to run when built with
 # default configuration. These are also predetermined by rpmbuild. Extra requirements
 # can be defined through a comma-separated list passed via $requirements when building
@@ -39,9 +29,21 @@ Requires: openssl-libs
 Requires: zlib
 
 %if %{requirements}
-Requires: %{requirements_list}
+Requires: %(echo "%{requirements_list}")
 %endif
 
+License:       BSD
+URL:           https://spdk.io
+Source:        spdk-%{version}.tar.gz
+
+%description
+
+The Storage Performance Development Kit (SPDK) provides a set of tools and libraries for
+writing high performance, scalable, user-mode storage applications. It achieves high
+performance by moving all of the necessary drivers into userspace and operating in a
+polled mode instead of relying on interrupts, which avoids kernel context switches and
+eliminates interrupt handling overhead.
+
 %prep
 make clean &>/dev/null || :
 %setup
diff --git a/scripts/bpf/gen_enums.sh b/scripts/bpf/gen_enums.sh
new file mode 100755
index 000000000..33fdaf5e7
--- /dev/null
+++ b/scripts/bpf/gen_enums.sh
@@ -0,0 +1,52 @@
+#!/usr/bin/env bash
+set -e
+
+rootdir=$(git rev-parse --show-toplevel)
+
+_print_enums() {
+	local enum_type=$1 enum_string=$2 enum_prefix=$3 enum output
+
+	output=$(< "$rootdir/$(git -C "$rootdir" grep -l "$enum_string" -- lib module)")
+
+	# Isolate the enum block
+	output=${output#*$enum_string$'\n'} output=${output%%$'\n'\};*}
+	# Fold it onto an array
+	IFS="," read -ra output <<< "${output//[[:space:]]/}"
+	# Drop the assignments
+	output=("${output[@]/=*/}")
+
+	for enum in "${!output[@]}"; do
+		if [[ ${output[enum]} != "$enum_prefix"* ]]; then
+			printf 'enum name %s does not start with expected prefix %s\n' "${output[enum]}" "$enum_prefix"
+			return 1
+		fi >&2
+		printf '  @%s[%d] = "%s";\n' "$enum_type" "$enum" "${output[enum]#$enum_prefix}"
+	done
+}
+
+print_enums() {
+	for state in "${!state_enums[@]}"; do
+		_print_enums "$state" "${state_enums["$state"]}" "${state_prefix["$state"]}"
+	done
+}
+
+print_clear() { printf '  clear(@%s);\n' "${!state_enums[@]}"; }
+
+declare -A state_enums=() state_prefix=()
+
+state_enums["target"]="enum nvmf_tgt_state {"
+state_enums["subsystem"]="enum spdk_nvmf_subsystem_state {"
+state_prefix["target"]=NVMF_TGT_
+state_prefix["subsystem"]=SPDK_NVMF_SUBSYSTEM_
+
+enums=$(print_enums)
+clear=$(print_clear)
+
+cat <<- ENUM
+	BEGIN {
+		$enums
+	}
+	END {
+		$clear
+	}
+ENUM
diff --git a/scripts/bpf/nvmf.bt b/scripts/bpf/nvmf.bt
new file mode 100644
index 000000000..e73bb676d
--- /dev/null
+++ b/scripts/bpf/nvmf.bt
@@ -0,0 +1,83 @@
+usdt:__EXE__:nvmf_tgt_state {
+	printf("%d.%06d: nvmf_tgt reached state %s\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       @target[arg1]);
+}
+
+usdt:__EXE__:nvmf_subsystem_change_state {
+	printf("%d.%06d: %s change state from %s to %s start\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       str(arg1), @subsystem[arg3], @subsystem[arg2]);
+}
+
+usdt:__EXE__:nvmf_subsystem_change_state_done {
+	printf("%d.%06d: %s change state from %s to %s %s\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       str(arg1), @subsystem[arg3], @subsystem[arg2], arg4 ? "failed" : "done");
+}
+
+usdt:__EXE__:nvmf_pg_change_state {
+	printf("%d.%06d: %s on thread %d state to %s start\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       str(arg1), arg3, @subsystem[arg2]);
+}
+
+usdt:__EXE__:nvmf_pg_change_state_done {
+	printf("%d.%06d: %s on thread %d state to %s done\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       str(arg1), arg3, @subsystem[arg2]);
+}
+
+usdt:__EXE__:nvmf_create_poll_group {
+	printf("%d.%06d: create poll group on thread: %d\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1);
+}
+
+usdt:__EXE__:nvmf_destroy_poll_group {
+	printf("%d.%06d: destroy poll group on thread: %d\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1);
+}
+
+usdt:__EXE__:nvmf_poll_group_add_qpair {
+	printf("%d.%06d: add qpair: %p to poll group on thread %d\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1, arg2);
+}
+
+usdt:__EXE__:nvmf_destroy_poll_group_qpairs {
+	printf("%d.%06d: destroy qpairs on poll group on thread %d\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1);
+}
+
+usdt:__EXE__:nvmf_poll_group_remove_qpair {
+	printf("%d.%06d: remove qpair: %p from poll group on thread %d\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1, arg2);
+}
+
+usdt:__EXE__:nvmf_qpair_disconnect {
+	printf("%d.%06d: disconnect qpair: %p from poll group on thread %d\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1, arg2);
+}
+
+usdt:__EXE__:nvmf_transport_qpair_fini {
+	printf("%d.%06d: destroy qpair: %p on transport layer\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1);
+}
+
+usdt:__EXE__:nvmf_poll_group_drain_qpair {
+	printf("%d.%06d: drain qpair: %p from poll group on thread %d\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       arg1, arg2);
+}
+
+usdt:__EXE__:nvmf_ctrlr_add_qpair {
+	printf("%d.%06d: %s add qpair: %p, qid: %d for host: %s\n",
+	       elapsed / (uint64)(1000 * 1000), elapsed % (uint64)(1000 * 1000),
+	       str(arg3), arg1, arg2, str(arg4));
+}
diff --git a/scripts/bpf/readv.bt b/scripts/bpf/readv.bt
new file mode 100644
index 000000000..36abebf56
--- /dev/null
+++ b/scripts/bpf/readv.bt
@@ -0,0 +1,3 @@
+tracepoint:syscalls:sys_exit_readv /pid == __PID__/ {
+	@bytes = hist(args->ret);
+}
diff --git a/scripts/bpf/send_msg.bt b/scripts/bpf/send_msg.bt
new file mode 100644
index 000000000..2e1bedf5f
--- /dev/null
+++ b/scripts/bpf/send_msg.bt
@@ -0,0 +1,7 @@
+uprobe:__EXE__:spdk_thread_send_msg {
+	@send_msg[usym(arg1)] = count();
+}
+
+uprobe:__EXE__:spdk_for_each_channel {
+	@for_each_channel[usym(arg1)] = count();
+}
diff --git a/scripts/bpf/syscalls.bt b/scripts/bpf/syscalls.bt
new file mode 100644
index 000000000..3c8cd70bc
--- /dev/null
+++ b/scripts/bpf/syscalls.bt
@@ -0,0 +1,3 @@
+tracepoint:syscalls:sys_enter_* /pid == __PID__/ {
+	@syscall[probe] = count();
+}
diff --git a/scripts/bpf/trace.py b/scripts/bpf/trace.py
new file mode 100755
index 000000000..f5ad7e164
--- /dev/null
+++ b/scripts/bpf/trace.py
@@ -0,0 +1,115 @@
+#!/usr/bin/env python3
+
+from argparse import ArgumentParser
+from dataclasses import dataclass
+from typing import Dict, List, TypeVar
+import json
+import sys
+
+
+@dataclass
+class TracepointArgument:
+    """Describes an SPDK tracepoint argument"""
+    TYPE_INT = 0
+    TYPE_PTR = 1
+    TYPE_STR = 2
+    name: str
+    argtype: int
+
+
+@dataclass
+class Tracepoint:
+    """Describes an SPDK tracepoint, equivalent to struct spdk_trace_tpoint"""
+    name: str
+    id: int
+    new_object: bool
+    args: List[TracepointArgument]
+
+
+@dataclass
+class TraceEntry:
+    """Describes an SPDK tracepoint entry, equivalent to struct spdk_trace_entry"""
+    lcore: int
+    tpoint: Tracepoint
+    tsc: int
+    poller: str
+    size: int
+    object_id: str
+    object_ptr: int
+    time: int
+    args: Dict[str, TypeVar('ArgumentType', str, int)]
+
+
+class Trace:
+    """Stores, parses, and prints out SPDK traces"""
+    def __init__(self, file):
+        self._json = json.load(file)
+        self._argfmt = {TracepointArgument.TYPE_PTR: lambda a: f'0x{a:x}'}
+        self.tpoints = {t.id: t for t in self._parse_tpoints()}
+        self.tsc_rate = self._json['tsc_rate']
+
+    def _parse_tpoints(self):
+        for tpoint in self._json.get('tpoints', []):
+            yield Tracepoint(
+                name=tpoint['name'], id=tpoint['id'],
+                new_object=tpoint['new_object'],
+                args=[TracepointArgument(name=a['name'],
+                                         argtype=a['type'])
+                      for a in tpoint.get('args', [])])
+
+    def _parse_entry(self, entry):
+        tpoint = self.tpoints[entry['tpoint']]
+        obj = entry.get('object', {})
+        return TraceEntry(tpoint=tpoint, lcore=entry['lcore'], tsc=entry['tsc'],
+                          size=entry.get('size'), object_id=obj.get('id'),
+                          object_ptr=obj.get('value'), time=obj.get('time'),
+                          poller=entry.get('poller'),
+                          args={n.name: v for n, v in zip(tpoint.args, entry.get('args', []))})
+
+    def _entries(self):
+        for entry in self._json.get('entries', []):
+            yield self._parse_entry(entry)
+
+    def _format_args(self, entry):
+        args = []
+        for arg, (name, value) in zip(entry.tpoint.args, entry.args.items()):
+            args.append('{}: {}'.format(name, self._argfmt.get(arg.argtype,
+                                                               lambda a: a)(value)))
+        return args
+
+    def print(self):
+        def get_us(tsc, off):
+            return ((tsc - off) * 10 ** 6) / self.tsc_rate
+
+        offset = None
+        for e in self._entries():
+            offset = e.tsc if offset is None else offset
+            timestamp = get_us(e.tsc, offset)
+            diff = get_us(e.time, 0) if e.time is not None else None
+            args = ', '.join(self._format_args(e))
+            fields = [
+                f'{e.lcore:3}',
+                f'{timestamp:16.3f}',
+                f'{e.poller:3}' if e.poller is not None else ' ' * 3,
+                f'{e.tpoint.name:24}',
+                f'size: {e.size:6}' if e.size is not None else ' ' * (len('size: ') + 6),
+                f'id: {e.object_id:8}' if e.object_id is not None else None,
+                f'time: {diff:<8.3f}' if diff is not None else None,
+                args
+            ]
+
+            print(' '.join([*filter(lambda f: f is not None, fields)]).rstrip())
+
+
+def main(argv):
+    parser = ArgumentParser(description='SPDK trace annotation script')
+    parser.add_argument('-i', '--input',
+                        help='JSON-formatted trace file produced by spdk_trace app')
+    args = parser.parse_args(argv)
+
+    file = open(args.input, 'r') if args.input is not None else sys.stdin
+    Trace(file).print()
+
+
+if __name__ == '__main__':
+    main(sys.argv[1:])
diff --git a/scripts/bpftrace.sh b/scripts/bpftrace.sh
new file mode 100755
index 000000000..6a2afbb85
--- /dev/null
+++ b/scripts/bpftrace.sh
@@ -0,0 +1,15 @@
+#!/usr/bin/env bash
+set -e
+
+if [ $# -lt 2 ]; then
+	echo "usage: $0 <pid> <script>"
+	exit 1
+fi
+SCRIPTS_DIR=$(readlink -f $(dirname $0))
+BIN_PATH=$(readlink -f /proc/$1/exe)
+BPF_SCRIPT=$($SCRIPTS_DIR/bpf/gen_enums.sh)
+BPF_SCRIPT+=$(sed "s#__EXE__#${BIN_PATH}#g" "${@:2}" | sed "s#__PID__#${1}#g")
+if [ -n "$ECHO_SCRIPT" ]; then
+	echo "$BPF_SCRIPT"
+fi
+bpftrace -p $1 -e "$BPF_SCRIPT"
diff --git a/scripts/common.sh b/scripts/common.sh
index d560107bf..edd5fa5f5 100644
--- a/scripts/common.sh
+++ b/scripts/common.sh
@@ -28,20 +28,36 @@ function pci_can_use() {
 	return 1
 }
 
+resolve_mod() {
+	local mod=$1 aliases=()
+
+	if aliases=($(modprobe -R "$mod")); then
+		echo "${aliases[0]}"
+	else
+		echo "unknown"
+	fi 2> /dev/null
+}
+
 cache_pci_init() {
 	local -gA pci_bus_cache
 	local -gA pci_ids_vendor
 	local -gA pci_ids_device
+	local -gA pci_bus_driver
+	local -gA pci_mod_driver
+	local -gA pci_mod_resolved
 
 	[[ -z ${pci_bus_cache[*]} || $CMD == reset ]] || return 1
 
 	pci_bus_cache=()
 	pci_bus_ids_vendor=()
 	pci_bus_ids_device=()
+	pci_bus_driver=()
+	pci_mod_driver=()
+	pci_mod_resolved=()
 }
 
 cache_pci() {
-	local pci=$1 class=$2 vendor=$3 device=$4
+	local pci=$1 class=$2 vendor=$3 device=$4 driver=$5 mod=$6
 
 	if [[ -n $class ]]; then
 		class=0x${class/0x/}
@@ -54,6 +70,13 @@ cache_pci() {
 		pci_ids_vendor["$pci"]=$vendor
 		pci_ids_device["$pci"]=$device
 	fi
+	if [[ -n $driver ]]; then
+		pci_bus_driver["$pci"]=$driver
+	fi
+	if [[ -n $mod ]]; then
+		pci_mod_driver["$pci"]=$mod
+		pci_mod_resolved["$pci"]=$(resolve_mod "$mod")
+	fi
 }
 
 cache_pci_bus_sysfs() {
@@ -62,11 +85,20 @@ cache_pci_bus_sysfs() {
 	cache_pci_init || return 0
 
 	local pci
-	local class vendor device
+	local class vendor device driver mod
 
 	for pci in /sys/bus/pci/devices/*; do
-		class=$(< "$pci/class") vendor=$(< "$pci/vendor") device=$(< "$pci/device")
-		cache_pci "${pci##*/}" "$class" "$vendor" "$device"
+		class=$(< "$pci/class") vendor=$(< "$pci/vendor") device=$(< "$pci/device") driver="" mod=""
+		if [[ -e $pci/driver ]]; then
+			driver=$(readlink -f "$pci/driver")
+			driver=${driver##*/}
+		else
+			driver=unbound
+		fi
+		if [[ -e $pci/modalias ]]; then
+			mod=$(< "$pci/modalias")
+		fi
+		cache_pci "${pci##*/}" "$class" "$vendor" "$device" "$driver" "$mod"
 	done
 }
 
@@ -283,3 +315,10 @@ le() { cmp_versions "$1" "<=" "$2"; }
 ge() { cmp_versions "$1" ">=" "$2"; }
 eq() { cmp_versions "$1" "==" "$2"; }
 neq() { ! eq "$1" "$2"; }
+
+if [[ -e "$CONFIG_WPDK_DIR/bin/wpdk_common.sh" ]]; then
+	# Adjust uname to report the operating system as WSL, Msys or Cygwin
+	# and the kernel name as Windows. Define kill() to invoke the SIGTERM
+	# handler before causing a hard stop with TerminateProcess.
+	source "$CONFIG_WPDK_DIR/bin/wpdk_common.sh"
+fi
diff --git a/scripts/fio-wrapper b/scripts/fio-wrapper
new file mode 100755
index 000000000..8c93f31ba
--- /dev/null
+++ b/scripts/fio-wrapper
@@ -0,0 +1,144 @@
+#!/usr/bin/env bash
+rootdir=$(readlink -f "$(dirname "$0")/../")
+
+shopt -s nullglob extglob
+
+fio_config() {
+	local devs=("$@") dev
+
+	cat <<- FIO
+		[global]
+		thread=1
+		invalidate=1
+		rw=$testtype
+		time_based=1
+		runtime=$runtime
+		ioengine=libaio
+		direct=1
+		bs=$blocksize
+		iodepth=$iodepth
+		norandommap=$((verify == 1 ? 0 : 1))
+		numjobs=$numjobs
+		verify_dump=1
+	FIO
+
+	if ((verify == 1)); then
+		cat <<- FIO
+			do_verify=$verify
+			verify=crc32c-intel
+		FIO
+	fi
+
+	for dev in "${!devs[@]}"; do
+		cat <<- FIO
+			[job$dev]
+			filename=/dev/${devs[dev]}
+		FIO
+	done
+}
+
+run_fio() {
+	fio_config "$@" | fio -
+}
+
+get_iscsi() {
+	while read -r; do
+		[[ $REPLY =~ "Attached scsi disk "(sd[a-z]+) ]] && echo "${BASH_REMATCH[1]}"
+	done < <(iscsiadm -m session -P 3)
+}
+
+get_nvme() {
+	local blocks nvme nvme_sub
+	for nvme in /sys/class/nvme/nvme+([0-9]); do
+		# Make sure we touch only the block devices which belong to bdev subsystem and
+		# use supported protocols.
+		[[ $(< "$nvme/transport") == tcp || $(< "$nvme/transport") == rdma ]] || continue
+		for nvme_sub in /sys/class/nvme-subsystem/nvme-subsys+([0-9]); do
+			[[ -e $nvme_sub/${nvme##*/} ]] || continue
+			[[ $(< "$nvme_sub/model") == "SPDK bdev Controller"* ]] || continue
+			blocks+=("$nvme_sub/${nvme##*/}"n*)
+		done
+	done
+	blocks=("${blocks[@]##*/}")
+	printf '%s\n' "${blocks[@]}"
+}
+
+get_devices() {
+	local devs=("$@")
+
+	if ((${#devs[@]} == 0)); then
+		case "$protocol" in
+			iscsi) devs=($(get_iscsi)) ;;
+			nvmf) devs=($(get_nvme)) ;;
+			*) ;;
+		esac
+	fi
+	printf '%s\n' "${devs[@]}"
+}
+
+configure_devices() {
+	local devs=("$@") dev qd
+
+	if [[ -e $rootdir/scripts/sync_dev_uevents.sh ]]; then
+		"$rootdir/scripts/sync_dev_uevents.sh" block/disk "${devs[@]}"
+	fi > /dev/null
+
+	for dev in "${devs[@]}"; do
+		qd=128
+		# Disable all merge tries"
+		echo 2 > "/sys/block/$dev/queue/nomerges"
+		# FIXME: nr_requests already has its default value at 128. Also, when no
+		# scheduler is associated with the device this value cannot be changed
+		# and is automatically adjusted as well.
+		# echo 128 > "/sys/block/$dev/queue/nr_requests"
+		if [[ -e /sys/block/$dev/device/queue_depth ]]; then
+			# FIXME: Is this really needed though? Can't we use the default? This is not
+			# very deterministic as depending on the device we may end up with different
+			# qd in the range of 1-128.
+			while ((qd > 0)) && ! echo "$qd" > "/sys/block/$dev/device/queue_depth"; do
+				((--qd))
+			done 2> /dev/null
+			if ((qd == 0)); then
+				printf 'Failed to set queue_depth (%s)\n' "$dev"
+				return 1
+			fi
+			printf 'queue_depth set to %u (%s)\n' "$qd" "$dev"
+		else
+			printf 'Could not set queue depth (%s)\n' "$dev" >&2
+		fi
+		echo none > "/sys/block/$dev/queue/scheduler"
+	done
+}
+
+# Defaults
+blocksize=4096
+iodepth=1
+numjobs=1
+protocol="nvmf"
+runtime=1
+testtype="read"
+verify=0
+
+# Keep short args compatible with fio.py
+while getopts :i:d:n:p:r:t:v arg; do
+	case "$arg" in
+		i) blocksize=$OPTARG ;;
+		d) iodepth=$OPTARG ;;
+		n) numjobs=$OPTARG ;;
+		p) protocol=$OPTARG ;;
+		r) runtime=$OPTARG ;;
+		t) testtype=$OPTARG ;;
+		v) verify=1 ;;
+		*) ;;
+	esac
+done
+shift $((OPTIND - 1))
+
+devices=($(get_devices "$@"))
+if ((${#devices[@]} == 0)); then
+	printf '* No devices were found for the test, aborting\n' >&2
+	exit 1
+fi
+
+fio_config "${devices[@]}"
+configure_devices "${devices[@]}" && run_fio "${devices[@]}"
diff --git a/scripts/fio.py b/scripts/fio.py
deleted file mode 100755
index 56816436a..000000000
--- a/scripts/fio.py
+++ /dev/null
@@ -1,164 +0,0 @@
-#!/usr/bin/env python3
-
-from subprocess import check_call, call, check_output, Popen, PIPE, CalledProcessError
-import re
-import sys
-import signal
-import os.path
-import time
-import argparse
-
-fio_template = """
-[global]
-thread=1
-invalidate=1
-rw=%(testtype)s
-time_based=1
-runtime=%(runtime)s
-ioengine=libaio
-direct=1
-bs=%(blocksize)d
-iodepth=%(iodepth)d
-norandommap=%(norandommap)d
-numjobs=%(numjobs)s
-%(verify)s
-verify_dump=1
-
-"""
-
-verify_template = """
-do_verify=1
-verify=crc32c-intel
-"""
-
-
-fio_job_template = """
-[job%(jobnumber)d]
-filename=%(device)s
-
-"""
-
-
-def interrupt_handler(signum, frame):
-    fio.terminate()
-    print("FIO terminated")
-    sys.exit(0)
-
-
-def main(io_size, protocol, queue_depth, test_type, runtime, num_jobs, verify):
-    global fio
-
-    if protocol == "nvmf":
-        devices = get_nvmf_target_devices()
-    elif protocol == "iscsi":
-        devices = get_iscsi_target_devices()
-
-    configure_devices(devices)
-    try:
-        fio_executable = check_output("which fio", shell=True).split()[0]
-    except CalledProcessError as e:
-        sys.stderr.write(str(e))
-        sys.stderr.write("\nCan't find the fio binary, please install it.\n")
-        sys.exit(1)
-
-    device_paths = ['/dev/' + dev for dev in devices]
-    print("Device paths:")
-    print(device_paths)
-    sys.stdout.flush()
-    signal.signal(signal.SIGTERM, interrupt_handler)
-    signal.signal(signal.SIGINT, interrupt_handler)
-    fio = Popen([fio_executable, '-'], stdin=PIPE)
-    fio.communicate(create_fio_config(io_size, queue_depth, device_paths, test_type, runtime, num_jobs, verify).encode())
-    fio.stdin.close()
-    rc = fio.wait()
-    print("FIO completed with code %d\n" % rc)
-    sys.stdout.flush()
-    sys.exit(rc)
-
-
-def get_iscsi_target_devices():
-    output = check_output('iscsiadm -m session -P 3', shell=True)
-    return re.findall("Attached scsi disk (sd[a-z]+)", output.decode("ascii"))
-
-
-def get_nvmf_target_devices():
-    output = str(check_output('lsblk -l -o NAME', shell=True).decode())
-    return re.findall("(nvme[0-9]+n[0-9]+)\n", output)
-
-
-def create_fio_config(size, q_depth, devices, test, run_time, num_jobs, verify):
-    norandommap = 0
-    if not verify:
-        verifyfio = ""
-        norandommap = 1
-    else:
-        verifyfio = verify_template
-    fiofile = fio_template % {"blocksize": size, "iodepth": q_depth,
-                              "testtype": test, "runtime": run_time,
-                              "norandommap": norandommap, "verify": verifyfio,
-                              "numjobs": num_jobs}
-    for (i, dev) in enumerate(devices):
-        fiofile += fio_job_template % {"jobnumber": i, "device": dev}
-    return fiofile
-
-
-def set_device_parameter(devices, filename_template, value):
-    valid_value = True
-
-    for dev in devices:
-        filename = filename_template % dev
-        f = open(filename, 'r+b')
-        try:
-            f.write(value.encode())
-            f.close()
-        except OSError:
-            valid_value = False
-            continue
-
-    return valid_value
-
-
-def configure_devices(devices):
-
-    for dev in devices:
-        retry = 30
-        while retry > 0:
-            if os.path.exists("/sys/block/%s/queue/nomerges" % dev):
-                break
-            else:
-                retry = retry - 1
-                time.sleep(0.1)
-
-    set_device_parameter(devices, "/sys/block/%s/queue/nomerges", "2")
-    set_device_parameter(devices, "/sys/block/%s/queue/nr_requests", "128")
-    requested_qd = 128
-    qd = requested_qd
-    while qd > 0:
-        try:
-            set_device_parameter(devices, "/sys/block/%s/device/queue_depth", str(qd))
-            break
-        except IOError:
-            qd = qd - 1
-    if qd == 0:
-        print("Could not set block device queue depths.")
-    elif qd < requested_qd:
-        print("Requested queue_depth {} but only {} is supported.".format(str(requested_qd), str(qd)))
-    if not set_device_parameter(devices, "/sys/block/%s/queue/scheduler", "noop"):
-        set_device_parameter(devices, "/sys/block/%s/queue/scheduler", "none")
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description="fio.py")
-    parser.add_argument("-i", "--io-size", type=int, help="The desired I/O size in bytes.", required=True)
-    parser.add_argument("-p", "--protocol", type=str, help="The protocol we are testing against. One of iscsi or nvmf.", required=True)
-    parser.add_argument("-d", "--queue-depth", type=int, help="The desired queue depth for each job.", required=True)
-    parser.add_argument("-t", "--test-type", type=str, help="The fio I/O pattern to run. e.g. read, randwrite, randrw.", required=True)
-    parser.add_argument("-r", "--runtime", type=int, help="Time in seconds to run the workload.", required=True)
-    parser.add_argument("-n", "--num-jobs", type=int, help="The number of fio jobs to run in your workload. default 1.", default=1)
-    parser.add_argument("-v", "--verify", action="store_true", help="Supply this argument to verify the I/O.", default=False)
-    args = parser.parse_args()
-
-    if args.protocol.lower() != "nvmf" and args.protocol.lower() != "iscsi":
-        parser.error("Protocol must be one of the following: nvmf, iscsi.")
-
-    main(args.io_size, args.protocol, args.queue_depth, args.test_type, args.runtime, args.num_jobs, args.verify)
diff --git a/scripts/get-pmr b/scripts/get-pmr
index 76fd18662..a4c2e8cec 100755
--- a/scripts/get-pmr
+++ b/scripts/get-pmr
@@ -1,13 +1,14 @@
 #!/usr/bin/env bash
 # We simply check if BAR2 is present as that's where PMR or CMB is
 # meant to be located under qemu. If found, print some stats then exit.
+shopt -s nullglob
 
 [[ $(uname -s) == Linux ]] || exit 0
 # Use MSR instead?
 [[ $(< /sys/class/dmi/id/chassis_vendor) == QEMU ]] || exit 0
 
-get_bar2() {
-	echo "0x$(setpci -s "$1" 0x18.L)"
+get_bar() {
+	echo "0x$(setpci -s "$1" "$2.L")"
 }
 
 get_size() {
@@ -22,29 +23,57 @@ get_size() {
 			return 0
 		fi
 	done < /proc/iomem
-	echo "unknown"
+	echo "unknown/unassigned"
 }
 
 info() {
-	local bar=$1
+	local dev=$1
 
 	local pref loc
 
+	local base_addr2
+	local base_addr4
+
+	local bar local bar2 bar3 bar4 bar5
+	local bar_type2
+
 	pref[0]=non-prefetchable
 	pref[1]=prefetchable
 
-	loc[0]=32-bit
-	loc[1]="<1MiB"
-	loc[2]=64-bit
+	print_info() {
+		local bar=$1 base_addr=$2 bar_type=$3
+
+		printf '%s:%s:%s:%s:%s:%s\n' \
+			"${nvme##*/}" \
+			"$dev" \
+			"64-bit" \
+			"${pref[bar & 1 << 3 ? 1 : 0]}" \
+			"$(get_size "$base_addr")" \
+			"$bar_type"
+	}
 
-	echo "${loc[(bar >> 1) & 0x3]}:${pref[bar & 1 << 3 ? 1 : 0]}:$(get_size $((bar & ~0xf)))"
+	bar2=$(get_bar "$dev" 0x18)
+	bar3=$(get_bar "$dev" 0x1c)
+	bar4=$(get_bar "$dev" 0x20)
+	bar5=$(get_bar "$dev" 0x24)
+
+	# QEMU uses 64-bit BARs
+	if ((bar2 & 1 << 2)); then
+		bar_type2=pmr
+		if [[ -e $nvme/cmb ]]; then
+			bar_type2=cmb
+		fi
+		base_addr2=$(((bar2 & ~0xf) + (bar3 << 32)))
+		print_info "$bar2" "$base_addr2" "$bar_type2"
+	fi
+	# QEMU uses 64-bit BARs
+	if ((bar4 & 1 << 2)); then
+		base_addr4=$(((bar4 & ~0xf) + (bar5 << 32)))
+		print_info "$bar4" "$base_addr4" pmr
+	fi
 }
 
 for nvme in /sys/class/nvme/nvme*; do
 	pci=$(readlink -f "$nvme/device") pci=${pci##*/}
-	bar2=$(get_bar2 "$pci") || continue
-	((bar2 != 0x0)) || continue
-	bar=pmr
-	[[ -e $nvme/cmb ]] && bar=cmb
-	echo "${nvme##*/}:$pci:$(info "$bar2"):$bar"
+	info "$pci"
 done
diff --git a/scripts/iostat.py b/scripts/iostat.py
index f683ce865..cb9ebe596 100755
--- a/scripts/iostat.py
+++ b/scripts/iostat.py
@@ -16,6 +16,8 @@ SPDK_BDEV_KB_STAT_HEAD = ['Device', 'tps', 'KB_read/s',
                           'KB_wrtn/s', 'KB_dscd/s', 'KB_read', 'KB_wrtn', 'KB_dscd']
 SPDK_BDEV_MB_STAT_HEAD = ['Device', 'tps', 'MB_read/s',
                           'MB_wrtn/s', 'MB_dscd/s', 'MB_read', 'MB_wrtn', 'MB_dscd']
+SPDK_BDEV_EXT_STAT_HEAD = ['qu-sz', 'aqu-sz', 'wareq-sz', 'rareq-sz', 'w_await(us)', 'r_await(us)', 'util']
+
 
 SPDK_MAX_SECTORS = 0xffffffff
 
@@ -25,6 +27,7 @@ class BdevStat:
     def __init__(self, dictionary):
         if dictionary is None:
             return
+        self.qd_period = 0
         for k, value in dictionary.items():
             if k == 'name':
                 self.bdev_name = value
@@ -46,16 +49,14 @@ class BdevStat:
                 self.wr_ticks = value
             elif k == 'unmap_latency_ticks':
                 self.dc_ticks = value
+            elif k == 'queue_depth_polling_period':
+                self.qd_period = value
             elif k == 'queue_depth':
-                self.ios_pgr = value
+                self.queue_depth = value
             elif k == 'io_time':
-                self.tot_ticks = value
+                self.io_time = value
             elif k == 'weighted_io_time':
-                self.rq_ticks = value
-
-        self.rd_merges = 0
-        self.wr_merges = 0
-        self.dc_merges = 0
+                self.weighted_io_time = value
         self.upt = 0.0
 
     def __getattr__(self, name):
@@ -159,12 +160,13 @@ def get_cpu_stat():
     return cpu_dump_info
 
 
-def read_bdev_stat(last_stat, stat, mb, use_upt):
+def read_bdev_stat(last_stat, stat, mb, use_upt, ext_info):
     if use_upt:
         upt_cur = uptime()
     else:
         upt_cur = stat['ticks']
-        upt_rate = stat['tick_rate']
+
+    upt_rate = stat['tick_rate']
 
     info_stats = []
     unit = 2048 if mb else 2
@@ -215,6 +217,48 @@ def read_bdev_stat(last_stat, stat, mb, use_upt):
                 "{:.2f}".format(wr_sec / unit),
                 "{:.2f}".format(dc_sec / unit),
             ]
+            if ext_info:
+                if _stat.qd_period > 0:
+                    tot_sampling_time = upt * 1000000 / _stat.qd_period
+                    busy_times = (_stat.io_time - _last_stat.io_time) / _stat.qd_period
+
+                    wr_ios = _stat.wr_ios - _last_stat.wr_ios
+                    rd_ios = _stat.rd_ios - _last_stat.rd_ios
+                    if busy_times != 0:
+                        aqu_sz = (_stat.weighted_io_time - _last_stat.weighted_io_time) / _stat.qd_period / busy_times
+                    else:
+                        aqu_sz = 0
+
+                    if wr_ios != 0:
+                        wareq_sz = wr_sec / wr_ios
+                        w_await = (_stat.wr_ticks * 1000000 / upt_rate -
+                                   _last_stat.wr_ticks * 1000000 / upt_rate) / wr_ios
+                    else:
+                        wareq_sz = 0
+                        w_await = 0
+
+                    if rd_ios != 0:
+                        rareq_sz = rd_sec / rd_ios
+                        r_await = (_stat.rd_ticks * 1000000 / upt_rate -
+                                   _last_stat.rd_ticks * 1000000 / upt_rate) / rd_ios
+                    else:
+                        rareq_sz = 0
+                        r_await = 0
+
+                    util = busy_times / tot_sampling_time
+
+                    info_stat += [
+                        "{:.2f}".format(_stat.queue_depth),
+                        "{:.2f}".format(aqu_sz),
+                        "{:.2f}".format(wareq_sz),
+                        "{:.2f}".format(rareq_sz),
+                        "{:.2f}".format(w_await),
+                        "{:.2f}".format(r_await),
+                        "{:.2f}".format(util),
+                    ]
+                else:
+                    info_stat += ["N/A"] * len(SPDK_BDEV_EXT_STAT_HEAD)
+
             info_stats.append(info_stat)
     else:
         for bdev in stat['bdevs']:
@@ -238,10 +282,53 @@ def read_bdev_stat(last_stat, stat, mb, use_upt):
                 "{:.2f}".format(_stat.wr_sectors / unit),
                 "{:.2f}".format(_stat.dc_sectors / unit),
             ]
+
+            # add extended statistics
+            if ext_info:
+                if _stat.qd_period > 0:
+                    tot_sampling_time = upt * 1000000 / _stat.qd_period
+                    busy_times = _stat.io_time / _stat.qd_period
+                    if busy_times != 0:
+                        aqu_sz = _stat.weighted_io_time / _stat.qd_period / busy_times
+                    else:
+                        aqu_sz = 0
+
+                    if _stat.wr_ios != 0:
+                        wareq_sz = _stat.wr_sectors / _stat.wr_ios
+                        w_await = _stat.wr_ticks * 1000000 / upt_rate / _stat.wr_ios
+                    else:
+                        wareq_sz = 0
+                        w_await = 0
+
+                    if _stat.rd_ios != 0:
+                        rareq_sz = _stat.rd_sectors / _stat.rd_ios
+                        r_await = _stat.rd_ticks * 1000000 / upt_rate / _stat.rd_ios
+                    else:
+                        rareq_sz = 0
+                        r_await = 0
+
+                    util = busy_times / tot_sampling_time
+
+                    info_stat += [
+                        "{:.2f}".format(_stat.queue_depth),
+                        "{:.2f}".format(aqu_sz),
+                        "{:.2f}".format(wareq_sz),
+                        "{:.2f}".format(rareq_sz),
+                        "{:.2f}".format(w_await),
+                        "{:.2f}".format(r_await),
+                        "{:.2f}".format(util),
+                    ]
+                else:
+                    info_stat += ["N/A"] * len(SPDK_BDEV_EXT_STAT_HEAD)
+
             info_stats.append(info_stat)
 
-    _stat_format(
-        info_stats, SPDK_BDEV_MB_STAT_HEAD if mb else SPDK_BDEV_KB_STAT_HEAD)
+    head = []
+    head += SPDK_BDEV_MB_STAT_HEAD if mb else SPDK_BDEV_KB_STAT_HEAD
+    if ext_info:
+        head += SPDK_BDEV_EXT_STAT_HEAD
+
+    _stat_format(info_stats, head)
     return bdev_stats
 
 
@@ -258,14 +345,14 @@ def io_stat_display(args, cpu_info, stat):
     if args.bdev_stat and not args.cpu_stat:
         _stat = get_bdev_stat(args.client, args.name)
         bdev_stats = read_bdev_stat(
-            stat, _stat, args.mb_display, args.use_uptime)
+            stat, _stat, args.mb_display, args.use_uptime, args.extended_display)
         return None, bdev_stats
 
     _cpu_info = get_cpu_stat()
     read_cpu_stat(cpu_info, _cpu_info)
 
     _stat = get_bdev_stat(args.client, args.name)
-    bdev_stats = read_bdev_stat(stat, _stat, args.mb_display, args.use_uptime)
+    bdev_stats = read_bdev_stat(stat, _stat, args.mb_display, args.use_uptime, args.extended_display)
     return _cpu_info, bdev_stats
 
 
@@ -343,6 +430,10 @@ if __name__ == "__main__":
     parser.add_argument('-v', dest='verbose', action='store_const', const="INFO",
                         help='Set verbose mode to INFO', default="ERROR")
 
+    parser.add_argument('-x', '--extended', dest='extended_display',
+                        action='store_true', help="Display extended statistics.",
+                        required=False, default=False)
+
     args = parser.parse_args()
     if ((args.interval == 0 and args.time_in_second != 0) or
             (args.interval != 0 and args.time_in_second == 0)):
diff --git a/scripts/perf/nvmf/README.md b/scripts/perf/nvmf/README.md
index 3d0ff9c3e..a4f62a0cc 100644
--- a/scripts/perf/nvmf/README.md
+++ b/scripts/perf/nvmf/README.md
@@ -227,6 +227,7 @@ Optional, SPDK Initiator only:
   "qd": [32, 128],
   "rw": ["randwrite", "write"],
   "rwmixread": 100,
+  "rate_iops": 10000,
   "num_jobs": 2,
   "run_time": 30,
   "ramp_time": 30,
@@ -247,6 +248,10 @@ Required:
 - run_num - number of times each workload combination is run.
   If more than 1 then final result is the average of all runs.
 
+Optional:
+
+- rate_iops - limit IOPS to this number
+
 #### Test Combinations
 
 It is possible to specify more than one value for bs, qd and rw parameters.
diff --git a/scripts/perf/nvmf/config.json b/scripts/perf/nvmf/config.json
index f846691a0..62f931a49 100644
--- a/scripts/perf/nvmf/config.json
+++ b/scripts/perf/nvmf/config.json
@@ -9,13 +9,13 @@
     "mode": "spdk",
     "null_block_devices": 0,
     "nvmet_dir": "/path/to/nvmetcli",
-    "num_cores": "[1]",
+    "core_mask": "[1]",
     "num_shared_buffers": 4096
   },
   "initiator1": {
     "ip": "10.0.0.1",
     "nic_ips": ["192.0.1.2"],
-    "remote_nic_ips": ["192.0.1.1"],
+    "target_nic_ips": ["192.0.1.1"],
     "mode": "spdk",
     "fio_bin": "/path/to/fio/bin",
     "nvmecli_bin": "/path/to/nvmecli/bin",
@@ -24,7 +24,7 @@
   "initiator2": {
     "ip": "10.0.0.2",
     "nic_ips": ["192.0.2.2"],
-    "remote_nic_ips": ["192.0.2.1"],
+    "target_nic_ips": ["192.0.2.1"],
     "mode": "spdk"
   },
   "fio": {
diff --git a/scripts/perf/nvmf/run_nvmf.py b/scripts/perf/nvmf/run_nvmf.py
index df445e6d0..fc14b35ee 100755
--- a/scripts/perf/nvmf/run_nvmf.py
+++ b/scripts/perf/nvmf/run_nvmf.py
@@ -777,7 +777,7 @@ class Initiator(Server):
         # Logic implemented in SPDKInitiator and KernelInitiator classes
         pass
 
-    def gen_fio_config(self, rw, rwmixread, block_size, io_depth, subsys_no, num_jobs=None, ramp_time=0, run_time=10):
+    def gen_fio_config(self, rw, rwmixread, block_size, io_depth, subsys_no, num_jobs=None, ramp_time=0, run_time=10, rate_iops=0):
         fio_conf_template = """
 [global]
 ioengine={ioengine}
@@ -794,6 +794,7 @@ bs={block_size}
 time_based=1
 ramp_time={ramp_time}
 runtime={run_time}
+rate_iops={rate_iops}
 """
         if "spdk" in self.mode:
             bdev_conf = self.gen_spdk_bdev_conf(self.subsystem_info_list)
@@ -835,7 +836,7 @@ runtime={run_time}
 
         fio_config = fio_conf_template.format(ioengine=ioengine, spdk_conf=spdk_conf,
                                               rw=rw, rwmixread=rwmixread, block_size=block_size,
-                                              ramp_time=ramp_time, run_time=run_time)
+                                              ramp_time=ramp_time, run_time=run_time, rate_iops=rate_iops)
         if num_jobs:
             fio_config = fio_config + "numjobs=%s \n" % num_jobs
         if self.cpus_allowed is not None:
@@ -1106,7 +1107,8 @@ class SPDKTarget(Target):
                                                allow_any_host=True, max_namespaces=8)
                 rpc.nvmf.nvmf_subsystem_add_ns(self.client, nqn, bdev_name)
 
-                rpc.nvmf.nvmf_subsystem_add_listener(self.client, nqn,
+                rpc.nvmf.nvmf_subsystem_add_listener(self.client,
+                                                     nqn=nqn,
                                                      trtype=self.transport,
                                                      traddr=ip,
                                                      trsvcid=port,
@@ -1344,6 +1346,10 @@ if __name__ == "__main__":
             fio_rw_mix_read = data[k]["rwmixread"]
             fio_run_num = data[k]["run_num"] if "run_num" in data[k].keys() else None
             fio_num_jobs = data[k]["num_jobs"] if "num_jobs" in data[k].keys() else None
+
+            fio_rate_iops = 0
+            if "rate_iops" in data[k]:
+                fio_rate_iops = data[k]["rate_iops"]
         else:
             continue
 
@@ -1369,7 +1375,7 @@ if __name__ == "__main__":
                 i.kernel_init_connect(i.target_nic_ips, target_obj.subsys_no)
 
             cfg = i.gen_fio_config(rw, fio_rw_mix_read, block_size, io_depth, target_obj.subsys_no,
-                                   fio_num_jobs, fio_ramp_time, fio_run_time)
+                                   fio_num_jobs, fio_ramp_time, fio_run_time, fio_rate_iops)
             configs.append(cfg)
 
         for i, cfg in zip(initiators, configs):
diff --git a/scripts/pkgdep/common.sh b/scripts/pkgdep/common.sh
index 2a3d304e4..041f37daa 100755
--- a/scripts/pkgdep/common.sh
+++ b/scripts/pkgdep/common.sh
@@ -14,8 +14,7 @@ install_liburing() {
 			git clone "${GIT_REPO_LIBURING}" "$liburing_dir"
 		fi
 		# Use commit we know we can compile against. See #1673 as a reference.
-		# FIXME: Switch to liburing-2.0 when it's finally released
-		git -C "$liburing_dir" checkout 5d027b315d78415a31dcc9111f6bd8924ba5b4e6
+		git -C "$liburing_dir" checkout liburing-2.0
 		(cd "$liburing_dir" && ./configure --libdir=/usr/lib64 && make install)
 		echo /usr/lib64 > /etc/ld.so.conf.d/spdk-liburing.conf
 		ldconfig
diff --git a/scripts/pkgdep/debian.sh b/scripts/pkgdep/debian.sh
index 88dd5bdea..40149af15 100755
--- a/scripts/pkgdep/debian.sh
+++ b/scripts/pkgdep/debian.sh
@@ -27,6 +27,8 @@ fi
 apt-get install -y libnuma-dev
 # Additional dependencies for ISA-L used in compression
 apt-get install -y autoconf automake libtool help2man
+# Additional dependencies for USDT
+apt-get install -y systemtap-sdt-dev
 if [[ $INSTALL_DEV_TOOLS == "true" ]]; then
 	# Tools for developers
 	apt-get install -y git astyle pep8 lcov clang sg3-utils pciutils shellcheck abigail-tools bash-completion
diff --git a/scripts/pkgdep/rhel.sh b/scripts/pkgdep/rhel.sh
index 653c57f81..00c82c989 100755
--- a/scripts/pkgdep/rhel.sh
+++ b/scripts/pkgdep/rhel.sh
@@ -104,6 +104,8 @@ fi
 yum install -y autoconf automake libtool help2man
 # Additional dependencies for DPDK
 yum install -y numactl-devel nasm
+# Additional dependencies for USDT
+yum install -y systemtap-sdt-devel
 if [[ $INSTALL_DEV_TOOLS == "true" ]]; then
 	# Tools for developers
 	if echo "$ID $VERSION_ID" | grep -E -q 'centos 8'; then
diff --git a/scripts/rpc.py b/scripts/rpc.py
index f60863cb0..438de7c2a 100755
--- a/scripts/rpc.py
+++ b/scripts/rpc.py
@@ -629,6 +629,46 @@ if __name__ == "__main__":
     p.add_argument('name', help='Virtual zone bdev name')
     p.set_defaults(func=bdev_zone_block_delete)
 
+    def bdev_rbd_register_cluster(args):
+        config_param = None
+        if args.config_param:
+            config_param = {}
+            for entry in args.config:
+                parts = entry.split('=', 1)
+                if len(parts) != 2:
+                    raise Exception('--config %s not in key=value form' % entry)
+                config_param[parts[0]] = parts[1]
+        print_json(rpc.bdev.bdev_rbd_register_cluster(args.client,
+                                                      name=args.name,
+                                                      user=args.user,
+                                                      config_param=config_param,
+                                                      config_file=args.config_file))
+
+    p = subparsers.add_parser('bdev_rbd_register_cluster',
+                              help='Add a Rados cluster with ceph rbd backend')
+    p.add_argument('name', help="Name of the Rados cluster only known to rbd bdev")
+    p.add_argument('--user', help="Ceph user name (i.e. admin, not client.admin)", required=False)
+    p.add_argument('--config_param', action='append', metavar='key=value',
+                   help="adds a key=value configuration option for rados_conf_set (default: rely on config file)")
+    p.add_argument('--config_file', help="The file path of the Rados configuration file", required=False)
+    p.set_defaults(func=bdev_rbd_register_cluster)
+
+    def bdev_rbd_unregister_cluster(args):
+        rpc.bdev.bdev_rbd_unregister_cluster(args.client, name=args.name)
+
+    p = subparsers.add_parser('bdev_rbd_unregister_cluster',
+                              help='Unregister a Rados cluster object')
+    p.add_argument('name', help='Name of the Rados Cluster only known to rbd bdev')
+    p.set_defaults(func=bdev_rbd_unregister_cluster)
+
+    def bdev_rbd_get_clusters_info(args):
+        print_json(rpc.bdev.bdev_rbd_get_clusters_info(args.client, name=args.name))
+
+    p = subparsers.add_parser('bdev_rbd_get_clusters_info',
+                              help='Display registered Rados Cluster names and related info')
+    p.add_argument('-b', '--name', help="Name of the registered Rados Cluster Name. Example: Cluster1", required=False)
+    p.set_defaults(func=bdev_rbd_get_clusters_info)
+
     def bdev_rbd_create(args):
         config = None
         if args.config:
@@ -644,7 +684,8 @@ if __name__ == "__main__":
                                             config=config,
                                             pool_name=args.pool_name,
                                             rbd_name=args.rbd_name,
-                                            block_size=args.block_size))
+                                            block_size=args.block_size,
+                                            cluster_name=args.cluster_name))
 
     p = subparsers.add_parser('bdev_rbd_create', aliases=['construct_rbd_bdev'],
                               help='Add a bdev with ceph rbd backend')
@@ -655,6 +696,7 @@ if __name__ == "__main__":
     p.add_argument('pool_name', help='rbd pool name')
     p.add_argument('rbd_name', help='rbd image name')
     p.add_argument('block_size', help='rbd block size', type=int)
+    p.add_argument('-c', '--cluster_name', help="cluster name to identify the Rados cluster", required=False)
     p.set_defaults(func=bdev_rbd_create)
 
     def bdev_rbd_delete(args):
@@ -933,7 +975,10 @@ if __name__ == "__main__":
             error_recovery_level=args.error_recovery_level,
             allow_duplicated_isid=args.allow_duplicated_isid,
             max_large_datain_per_connection=args.max_large_datain_per_connection,
-            max_r2t_per_connection=args.max_r2t_per_connection)
+            max_r2t_per_connection=args.max_r2t_per_connection,
+            pdu_pool_size=args.pdu_pool_size,
+            immediate_data_pool_size=args.immediate_data_pool_size,
+            data_out_pool_size=args.data_out_pool_size)
 
     p = subparsers.add_parser('iscsi_set_options', aliases=['set_iscsi_options'],
                               help="""Set options of iSCSI subsystem""")
@@ -959,6 +1004,9 @@ if __name__ == "__main__":
     p.add_argument('-p', '--allow-duplicated-isid', help='Allow duplicated initiator session ID.', action='store_true')
     p.add_argument('-x', '--max-large-datain-per-connection', help='Max number of outstanding split read I/Os per connection', type=int)
     p.add_argument('-k', '--max-r2t-per-connection', help='Max number of outstanding R2Ts per connection', type=int)
+    p.add_argument('-u', '--pdu-pool-size', help='Number of PDUs in the pool', type=int)
+    p.add_argument('-j', '--immediate_data-pool-size', help='Number of immediate data buffers in the pool', type=int)
+    p.add_argument('-z', '--data-out-pool-size', help='Number of data out buffers in the pool', type=int)
     p.set_defaults(func=iscsi_set_options)
 
     def iscsi_set_discovery_auth(args):
@@ -1676,7 +1724,7 @@ Format: 'user:u1 secret:s1 muser:mu1 msecret:ms1,user:u2 secret:s2 muser:mu2 mse
     p = subparsers.add_parser('bdev_raid_create', aliases=['construct_raid_bdev'],
                               help='Create new raid bdev')
     p.add_argument('-n', '--name', help='raid bdev name', required=True)
-    p.add_argument('-z', '--strip-size_kb', help='strip size in KB', type=int)
+    p.add_argument('-z', '--strip-size-kb', help='strip size in KB', type=int)
     p.add_argument('-r', '--raid-level', help='raid level, only raid level 0 is supported', required=True)
     p.add_argument('-b', '--base-bdevs', help='base bdevs name, whitespace separated list in quotes', required=True)
     p.set_defaults(func=bdev_raid_create)
@@ -1864,28 +1912,7 @@ Format: 'user:u1 secret:s1 muser:mu1 msecret:ms1,user:u2 secret:s2 muser:mu2 mse
     p.set_defaults(func=nvmf_set_config)
 
     def nvmf_create_transport(args):
-        rpc.nvmf.nvmf_create_transport(args.client,
-                                       trtype=args.trtype,
-                                       tgt_name=args.tgt_name,
-                                       max_queue_depth=args.max_queue_depth,
-                                       max_qpairs_per_ctrlr=args.max_qpairs_per_ctrlr,
-                                       max_io_qpairs_per_ctrlr=args.max_io_qpairs_per_ctrlr,
-                                       in_capsule_data_size=args.in_capsule_data_size,
-                                       max_io_size=args.max_io_size,
-                                       io_unit_size=args.io_unit_size,
-                                       max_aq_depth=args.max_aq_depth,
-                                       num_shared_buffers=args.num_shared_buffers,
-                                       buf_cache_size=args.buf_cache_size,
-                                       num_cqe=args.num_cqe,
-                                       max_srq_depth=args.max_srq_depth,
-                                       no_srq=args.no_srq,
-                                       c2h_success=args.c2h_success,
-                                       dif_insert_or_strip=args.dif_insert_or_strip,
-                                       sock_priority=args.sock_priority,
-                                       acceptor_backlog=args.acceptor_backlog,
-                                       abort_timeout_sec=args.abort_timeout_sec,
-                                       no_wr_batching=args.no_wr_batching,
-                                       control_msg_num=args.control_msg_num)
+        rpc.nvmf.nvmf_create_transport(**vars(args))
 
     p = subparsers.add_parser('nvmf_create_transport', help='Create NVMf transport')
     p.add_argument('-t', '--trtype', help='Transport type (ex. RDMA)', type=str, required=True)
@@ -1938,7 +1965,9 @@ Format: 'user:u1 secret:s1 muser:mu1 msecret:ms1,user:u2 secret:s2 muser:mu2 mse
                                        model_number=args.model_number,
                                        allow_any_host=args.allow_any_host,
                                        max_namespaces=args.max_namespaces,
-                                       ana_reporting=args.ana_reporting)
+                                       ana_reporting=args.ana_reporting,
+                                       min_cntlid=args.min_cntlid,
+                                       max_cntlid=args.max_cntlid)
 
     p = subparsers.add_parser('nvmf_create_subsystem', aliases=['nvmf_subsystem_create'],
                               help='Create an NVMe-oF subsystem')
@@ -1954,6 +1983,8 @@ Format: 'user:u1 secret:s1 muser:mu1 msecret:ms1,user:u2 secret:s2 muser:mu2 mse
     p.add_argument("-m", "--max-namespaces", help="Maximum number of namespaces allowed",
                    type=int, default=0)
     p.add_argument("-r", "--ana-reporting", action='store_true', help="Enable ANA reporting feature")
+    p.add_argument("-i", "--min_cntlid", help="Minimum controller ID", type=int, default=1)
+    p.add_argument("-I", "--max_cntlid", help="Maximum controller ID", type=int, default=0xffef)
     p.set_defaults(func=nvmf_create_subsystem)
 
     def nvmf_delete_subsystem(args):
@@ -1969,13 +2000,7 @@ Format: 'user:u1 secret:s1 muser:mu1 msecret:ms1,user:u2 secret:s2 muser:mu2 mse
     p.set_defaults(func=nvmf_delete_subsystem)
 
     def nvmf_subsystem_add_listener(args):
-        rpc.nvmf.nvmf_subsystem_add_listener(args.client,
-                                             nqn=args.nqn,
-                                             trtype=args.trtype,
-                                             traddr=args.traddr,
-                                             tgt_name=args.tgt_name,
-                                             adrfam=args.adrfam,
-                                             trsvcid=args.trsvcid)
+        rpc.nvmf.nvmf_subsystem_add_listener(**vars(args))
 
     p = subparsers.add_parser('nvmf_subsystem_add_listener', help='Add a listener to an NVMe-oF subsystem')
     p.add_argument('nqn', help='NVMe-oF subsystem NQN')
@@ -2136,6 +2161,18 @@ Format: 'user:u1 secret:s1 muser:mu1 msecret:ms1,user:u2 secret:s2 muser:mu2 mse
     p.add_argument('-t', '--tgt_name', help='The name of the parent NVMe-oF target (optional)', type=str)
     p.set_defaults(func=nvmf_get_stats)
 
+    def nvmf_set_crdt(args):
+        print_dict(rpc.nvmf.nvmf_set_crdt(args.client, args.crdt1, args.crdt2, args.crdt3))
+
+    p = subparsers.add_parser(
+        'nvmf_set_crdt',
+        help="""Set the 3 crdt (Command Retry Delay Time) values for NVMf subsystem. All
+        values are in units of 100 milliseconds (same as the NVM Express specification).""")
+    p.add_argument('-t1', '--crdt1', help='Command Retry Delay Time 1, in units of 100 milliseconds', type=int)
+    p.add_argument('-t2', '--crdt2', help='Command Retry Delay Time 2, in units of 100 milliseconds', type=int)
+    p.add_argument('-t3', '--crdt3', help='Command Retry Delay Time 3, in units of 100 milliseconds', type=int)
+    p.set_defaults(func=nvmf_set_crdt)
+
     # pmem
     def bdev_pmem_create_pool(args):
         num_blocks = int((args.total_size * 1024 * 1024) / args.block_size)
diff --git a/scripts/rpc/bdev.py b/scripts/rpc/bdev.py
index 83cb1eb60..94b749c51 100644
--- a/scripts/rpc/bdev.py
+++ b/scripts/rpc/bdev.py
@@ -659,8 +659,57 @@ def bdev_zone_block_delete(client, name):
     return client.call('bdev_zone_block_delete', params)
 
 
+def bdev_rbd_register_cluster(client, name, user=None, config_param=None, config_file=None):
+    """Create a Rados Cluster object of the Ceph RBD backend.
+
+    Args:
+        name: name of Rados Cluster
+        user: Ceph user name (optional)
+        config_param: map of config keys to values (optional)
+        config_file: file path of Ceph configuration file (optional)
+
+    Returns:
+        Name of registered Rados Cluster object.
+    """
+    params = {'name': name}
+
+    if user is not None:
+        params['user_id'] = user
+    if config_param is not None:
+        params['config_param'] = config_param
+    if config_file is not None:
+        params['config_file'] = config_file
+
+    return client.call('bdev_rbd_register_cluster', params)
+
+
+def bdev_rbd_unregister_cluster(client, name):
+    """Remove Rados cluster object from the system.
+
+    Args:
+        name: name of Rados cluster object to unregister
+    """
+    params = {'name': name}
+    return client.call('bdev_rbd_unregister_cluster', params)
+
+
+def bdev_rbd_get_clusters_info(client, name):
+    """Get the cluster(s) info
+
+    Args:
+        name: name of Rados cluster object to query (optional; if omitted, query all clusters)
+
+    Returns:
+        List of registered Rados cluster information objects.
+    """
+    params = {}
+    if name:
+        params['name'] = name
+    return client.call('bdev_rbd_get_clusters_info', params)
+
+
 @deprecated_alias('construct_rbd_bdev')
-def bdev_rbd_create(client, pool_name, rbd_name, block_size, name=None, user=None, config=None):
+def bdev_rbd_create(client, pool_name, rbd_name, block_size, name=None, user=None, config=None, cluster_name=None):
     """Create a Ceph RBD block device.
 
     Args:
@@ -670,6 +719,7 @@ def bdev_rbd_create(client, pool_name, rbd_name, block_size, name=None, user=Non
         name: name of block device (optional)
         user: Ceph user name (optional)
         config: map of config keys to values (optional)
+        cluster_name: Name to identify Rados cluster (optional)
 
     Returns:
         Name of created block device.
@@ -686,6 +736,8 @@ def bdev_rbd_create(client, pool_name, rbd_name, block_size, name=None, user=Non
         params['user_id'] = user
     if config is not None:
         params['config'] = config
+    if cluster_name is not None:
+        params['cluster_name'] = cluster_name
 
     return client.call('bdev_rbd_create', params)
 
diff --git a/scripts/rpc/client.py b/scripts/rpc/client.py
index f84a1cf00..caecda144 100644
--- a/scripts/rpc/client.py
+++ b/scripts/rpc/client.py
@@ -14,6 +14,22 @@ def print_json(s):
     print(json.dumps(s, indent=2).strip('"'))
 
 
+def get_addr_type(addr):
+    try:
+        socket.inet_pton(socket.AF_INET, addr)
+        return socket.AF_INET
+    except Exception as e:
+        pass
+    try:
+        socket.inet_pton(socket.AF_INET6, addr)
+        return socket.AF_INET6
+    except Exception as e:
+        pass
+    if os.path.exists(addr):
+        return socket.AF_UNIX
+    return None
+
+
 class JSONRPCException(Exception):
     def __init__(self, message):
         self.message = message
@@ -54,23 +70,24 @@ class JSONRPCClient(object):
 
     def _connect(self, addr, port):
         try:
-            if os.path.exists(addr):
+            addr_type = get_addr_type(addr)
+
+            if addr_type == socket.AF_UNIX:
                 self._logger.debug("Trying to connect to UNIX socket: %s", addr)
                 self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                 self.sock.connect(addr)
-            elif port:
-                if ':' in addr:
-                    self._logger.debug("Trying to connect to IPv6 address addr:%s, port:%i", addr, port)
-                    for res in socket.getaddrinfo(addr, port, socket.AF_INET6, socket.SOCK_STREAM, socket.SOL_TCP):
-                        af, socktype, proto, canonname, sa = res
-                    self.sock = socket.socket(af, socktype, proto)
-                    self.sock.connect(sa)
-                else:
-                    self._logger.debug("Trying to connect to IPv4 address addr:%s, port:%i'", addr, port)
-                    self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-                    self.sock.connect((addr, port))
+            elif addr_type == socket.AF_INET6:
+                self._logger.debug("Trying to connect to IPv6 address addr:%s, port:%i", addr, port)
+                for res in socket.getaddrinfo(addr, port, socket.AF_INET6, socket.SOCK_STREAM, socket.SOL_TCP):
+                    af, socktype, proto, canonname, sa = res
+                self.sock = socket.socket(af, socktype, proto)
+                self.sock.connect(sa)
+            elif addr_type == socket.AF_INET:
+                self._logger.debug("Trying to connect to IPv4 address addr:%s, port:%i'", addr, port)
+                self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                self.sock.connect((addr, port))
             else:
-                raise socket.error("Unix socket '%s' does not exist" % addr)
+                raise socket.error("Invalid or non-existing address: '%s'" % addr)
         except socket.error as ex:
             raise JSONRPCException("Error while connecting to %s\n"
                                    "Is SPDK application running?\n"
diff --git a/scripts/rpc/cmd_parser.py b/scripts/rpc/cmd_parser.py
new file mode 100644
index 000000000..4cf728f75
--- /dev/null
+++ b/scripts/rpc/cmd_parser.py
@@ -0,0 +1,31 @@
+args_global = ['server_addr', 'port', 'timeout', 'verbose', 'dry_run', 'conn_retries',
+               'is_server', 'rpc_plugin', 'called_rpc_name', 'func', 'client']
+
+
+def strip_globals(kwargs):
+    for arg in args_global:
+        kwargs.pop(arg, None)
+
+
+def remove_null(kwargs):
+    keys = []
+    for key, value in kwargs.items():
+        if value is None:
+            keys.append(key)
+
+    for key in keys:
+        kwargs.pop(key, None)
+
+
+def apply_defaults(kwargs, **defaults):
+    for key, value in defaults.items():
+        if key not in kwargs:
+            kwargs[key] = value
+
+
+def group_as(kwargs, name, values):
+    group = {}
+    for arg in values:
+        if arg in kwargs and kwargs[arg] is not None:
+            group[arg] = kwargs.pop(arg, None)
+    kwargs[name] = group
diff --git a/scripts/rpc/iscsi.py b/scripts/rpc/iscsi.py
index e98126f57..a73df5ff1 100644
--- a/scripts/rpc/iscsi.py
+++ b/scripts/rpc/iscsi.py
@@ -22,7 +22,10 @@ def iscsi_set_options(
         error_recovery_level=None,
         allow_duplicated_isid=None,
         max_large_datain_per_connection=None,
-        max_r2t_per_connection=None):
+        max_r2t_per_connection=None,
+        pdu_pool_size=None,
+        immediate_data_pool_size=None,
+        data_out_pool_size=None):
     """Set iSCSI target options.
 
     Args:
@@ -45,6 +48,9 @@ def iscsi_set_options(
         allow_duplicated_isid: Allow duplicated initiator session ID
         max_large_datain_per_connection: Max number of outstanding split read I/Os per connection (optional)
         max_r2t_per_connection: Max number of outstanding R2Ts per connection (optional)
+        pdu_pool_size: Number of PDUs in the pool (optional)
+        immediate_data_pool_size: Number of immediate data buffers in the pool (optional)
+        data_out_pool_size: Number of data out buffers in the pool (optional)
 
     Returns:
         True or False
@@ -89,6 +95,12 @@ def iscsi_set_options(
         params['max_large_datain_per_connection'] = max_large_datain_per_connection
     if max_r2t_per_connection:
         params['max_r2t_per_connection'] = max_r2t_per_connection
+    if pdu_pool_size:
+        params['pdu_pool_size'] = pdu_pool_size
+    if immediate_data_pool_size:
+        params['immediate_data_pool_size'] = immediate_data_pool_size
+    if data_out_pool_size:
+        params['data_out_pool_size'] = data_out_pool_size
 
     return client.call('iscsi_set_options', params)
 
diff --git a/scripts/rpc/nvmf.py b/scripts/rpc/nvmf.py
index bd8eb3497..96164c0f2 100644
--- a/scripts/rpc/nvmf.py
+++ b/scripts/rpc/nvmf.py
@@ -1,4 +1,5 @@
 from .helpers import deprecated_alias
+from .cmd_parser import *
 
 
 @deprecated_alias('set_nvmf_target_max_subsystems')
@@ -91,28 +92,7 @@ def nvmf_get_targets(client):
     return client.call("nvmf_get_targets")
 
 
-def nvmf_create_transport(client,
-                          trtype,
-                          tgt_name=None,
-                          max_queue_depth=None,
-                          max_qpairs_per_ctrlr=None,
-                          max_io_qpairs_per_ctrlr=None,
-                          in_capsule_data_size=None,
-                          max_io_size=None,
-                          io_unit_size=None,
-                          max_aq_depth=None,
-                          num_shared_buffers=None,
-                          buf_cache_size=None,
-                          num_cqe=None,
-                          max_srq_depth=None,
-                          no_srq=False,
-                          c2h_success=True,
-                          dif_insert_or_strip=None,
-                          sock_priority=None,
-                          acceptor_backlog=None,
-                          abort_timeout_sec=None,
-                          no_wr_batching=None,
-                          control_msg_num=None):
+def nvmf_create_transport(client, **params):
     """NVMf Transport Create options.
 
     Args:
@@ -138,50 +118,14 @@ def nvmf_create_transport(client,
     Returns:
         True or False
     """
-    params = {}
 
-    params['trtype'] = trtype
-    if tgt_name:
-        params['tgt_name'] = tgt_name
-    if max_queue_depth:
-        params['max_queue_depth'] = max_queue_depth
-    if max_qpairs_per_ctrlr:
+    strip_globals(params)
+    apply_defaults(params, no_srq=False, c2h_success=True)
+    remove_null(params)
+
+    if 'max_qpairs_per_ctrlr' in params:
         print("WARNING: max_qpairs_per_ctrlr is deprecated, please use max_io_qpairs_per_ctrlr.")
-        params['max_qpairs_per_ctrlr'] = max_qpairs_per_ctrlr
-    if max_io_qpairs_per_ctrlr:
-        params['max_io_qpairs_per_ctrlr'] = max_io_qpairs_per_ctrlr
-    if in_capsule_data_size is not None:
-        params['in_capsule_data_size'] = in_capsule_data_size
-    if max_io_size:
-        params['max_io_size'] = max_io_size
-    if io_unit_size:
-        params['io_unit_size'] = io_unit_size
-    if max_aq_depth:
-        params['max_aq_depth'] = max_aq_depth
-    if num_shared_buffers:
-        params['num_shared_buffers'] = num_shared_buffers
-    if buf_cache_size is not None:
-        params['buf_cache_size'] = buf_cache_size
-    if num_cqe:
-        params['num_cqe'] = num_cqe
-    if max_srq_depth:
-        params['max_srq_depth'] = max_srq_depth
-    if no_srq:
-        params['no_srq'] = no_srq
-    if c2h_success is not None:
-        params['c2h_success'] = c2h_success
-    if dif_insert_or_strip:
-        params['dif_insert_or_strip'] = dif_insert_or_strip
-    if sock_priority is not None:
-        params['sock_priority'] = sock_priority
-    if acceptor_backlog is not None:
-        params['acceptor_backlog'] = acceptor_backlog
-    if abort_timeout_sec:
-        params['abort_timeout_sec'] = abort_timeout_sec
-    if no_wr_batching is not None:
-        params['no_wr_batching'] = no_wr_batching
-    if control_msg_num is not None:
-        params['control_msg_num'] = control_msg_num
+
     return client.call('nvmf_create_transport', params)
 
 
@@ -233,7 +177,9 @@ def nvmf_create_subsystem(client,
                           model_number='SPDK bdev Controller',
                           allow_any_host=False,
                           max_namespaces=0,
-                          ana_reporting=False):
+                          ana_reporting=False,
+                          min_cntlid=1,
+                          max_cntlid=0xffef):
     """Construct an NVMe over Fabrics target subsystem.
 
     Args:
@@ -244,6 +190,8 @@ def nvmf_create_subsystem(client,
         allow_any_host: Allow any host (True) or enforce allowed host list (False). Default: False.
         max_namespaces: Maximum number of namespaces that can be attached to the subsystem (optional). Default: 0 (Unlimited).
         ana_reporting: Enable ANA reporting feature. Default: False.
+        min_cntlid: Minimum controller ID. Default: 1
+        max_cntlid: Maximum controller ID. Default: 0xffef
 
 
     Returns:
@@ -271,10 +219,17 @@ def nvmf_create_subsystem(client,
     if ana_reporting:
         params['ana_reporting'] = ana_reporting
 
+    if min_cntlid is not None:
+        params['min_cntlid'] = min_cntlid
+
+    if max_cntlid is not None:
+        params['max_cntlid'] = max_cntlid
+
     return client.call('nvmf_create_subsystem', params)
 
 
-def nvmf_subsystem_add_listener(client, nqn, trtype, traddr, trsvcid, adrfam, tgt_name=None):
+def nvmf_subsystem_add_listener(client, **params):
+
     """Add a new listen address to an NVMe-oF subsystem.
 
     Args:
@@ -288,20 +243,11 @@ def nvmf_subsystem_add_listener(client, nqn, trtype, traddr, trsvcid, adrfam, tg
     Returns:
         True or False
     """
-    listen_address = {'trtype': trtype,
-                      'traddr': traddr}
-
-    if trsvcid:
-        listen_address['trsvcid'] = trsvcid
-
-    if adrfam:
-        listen_address['adrfam'] = adrfam
 
-    params = {'nqn': nqn,
-              'listen_address': listen_address}
-
-    if tgt_name:
-        params['tgt_name'] = tgt_name
+    strip_globals(params)
+    apply_defaults(params, tgt_name=None)
+    group_as(params, 'listen_address', ['trtype', 'traddr', 'trsvcid', 'adrfam'])
+    remove_null(params)
 
     return client.call('nvmf_subsystem_add_listener', params)
 
@@ -594,3 +540,25 @@ def nvmf_get_stats(client, tgt_name=None):
         }
 
     return client.call('nvmf_get_stats', params)
+
+
+def nvmf_set_crdt(client, crdt1=None, crdt2=None, crdt3=None):
+    """Set the 3 crdt (Command Retry Delay Time) values
+
+    Args:
+        crdt1: Command Retry Delay Time 1
+        crdt2: Command Retry Delay Time 2
+        crdt3: Command Retry Delay Time 3
+
+    Returns:
+        True or False
+    """
+    params = {}
+    if crdt1 is not None:
+        params['crdt1'] = crdt1
+    if crdt2 is not None:
+        params['crdt2'] = crdt2
+    if crdt3 is not None:
+        params['crdt3'] = crdt3
+
+    return client.call('nvmf_set_crdt', params)
diff --git a/scripts/setup.sh b/scripts/setup.sh
index a2319ec71..4d2346424 100755
--- a/scripts/setup.sh
+++ b/scripts/setup.sh
@@ -509,7 +509,7 @@ function configure_linux() {
 		fi
 	fi
 
-	if [ ! -f /dev/cpu/0/msr ]; then
+	if [ ! -e /dev/cpu/0/msr ]; then
 		# Some distros build msr as a module.  Make sure it's loaded to ensure
 		#  DPDK can easily figure out the TSC rate rather than relying on 100ms
 		#  sleeps.
diff --git a/scripts/vagrant/Vagrantfile b/scripts/vagrant/Vagrantfile
index 912786109..1093eb599 100644
--- a/scripts/vagrant/Vagrantfile
+++ b/scripts/vagrant/Vagrantfile
@@ -140,18 +140,13 @@ def setup_nvme_disk(libvirt, disk, index)
     nvme_drive << ",namespaces=#{nvme_namespaces[index]}"
   end
 
-  cmb_set = "false"
-  pmr_cmdline =
+  pmr_cmdline = ""
 
   if !nvme_cmbs[index].nil? && nvme_cmbs[index] == "true"
     # Fix the size of the buffer to 128M
     nvme_drive << ",cmb_size_mb=128"
-    cmb_set = "true"
   end
   if !nvme_pmrs[index].nil?
-    if cmb_set == "true"
-      abort("CMB and PMR are mutually exclusive, aborting (#{nvme_disk})")
-    end
     pmr_path, pmr_size = nvme_pmrs[index].split(':')
     if !File.exist?(pmr_path)
       abort("#{pmr_path} does not exist, aborting")
@@ -163,7 +158,7 @@ def setup_nvme_disk(libvirt, disk, index)
     pmr_cmdline = "memory-backend-file,id=pmr#{index},share=on,mem-path=#{pmr_path},size=#{pmr_size}"
   end
   libvirt.qemuargs :value => nvme_drive
-  if !pmr_cmdline.nil?
+  if pmr_cmdline != ""
     libvirt.qemuargs :value => "-object"
     libvirt.qemuargs :value => pmr_cmdline
   end
@@ -278,6 +273,14 @@ def setup_libvirt(config, vmcpu, vmram, distro)
       end
     }
 
+    # Add network interface for openstack tests
+    if ENV['SPDK_OPENSTACK_NETWORK'] == "1"
+      libvirt.qemuargs :value => "-device"
+      libvirt.qemuargs :value => "virtio-net,netdev=openstack.0"
+      libvirt.qemuargs :value => "-netdev"
+      libvirt.qemuargs :value => "user,id=openstack.0"
+    end
+
     if ENV['VAGRANT_HUGE_MEM'] == "1"
       libvirt.memorybacking :hugepages
     end
@@ -295,7 +298,6 @@ provider = (ENV['SPDK_VAGRANT_PROVIDER'] || "virtualbox")
 # Get all variables for creating vm
 vmcpu = (ENV['SPDK_VAGRANT_VMCPU'] || 2)
 vmram = (ENV['SPDK_VAGRANT_VMRAM'] || 4096)
-openstack_network = (ENV['SPDK_OPENSTACK_NETWORK'] || false)
 
 # generic/freebsd boxes do not work properly with vagrant-proxyconf and
 # have issues installing rsync and sshfs for syncing files. NFS is
@@ -319,10 +321,6 @@ Vagrant.configure(2) do |config|
   config.vm.box_check_update = false
   config.vm.synced_folder '.', '/vagrant', disabled: true
 
-  # Add network interface for openstack tests
-  if openstack_network == "1"
-    config.vm.network "private_network", ip: "10.0.2.15"
-  end
   # Copy in the .gitconfig if it exists
   copy_gitconfig(config)
 
diff --git a/test/app/bdev_svc/Makefile b/test/app/bdev_svc/Makefile
index 9e560de0f..a13faf085 100644
--- a/test/app/bdev_svc/Makefile
+++ b/test/app/bdev_svc/Makefile
@@ -39,10 +39,10 @@ APP = bdev_svc
 
 C_SRCS := bdev_svc.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev
 
 ifeq ($(OS),Linux)
-SPDK_LIB_LIST += event_nbd
+SPDK_LIB_LIST += event event_nbd
 endif
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/test/app/fuzz/iscsi_fuzz/iscsi_fuzz.c b/test/app/fuzz/iscsi_fuzz/iscsi_fuzz.c
index 8261ae82d..d962039e9 100644
--- a/test/app/fuzz/iscsi_fuzz/iscsi_fuzz.c
+++ b/test/app/fuzz/iscsi_fuzz/iscsi_fuzz.c
@@ -511,7 +511,7 @@ iscsi_fuzz_read_pdu(struct spdk_iscsi_conn *conn)
 				rc = 0;
 			}
 			if (rc == 0) {
-				spdk_trace_record(TRACE_ISCSI_TASK_EXECUTED, 0, 0, (uintptr_t)pdu, 0);
+				spdk_trace_record(TRACE_ISCSI_TASK_EXECUTED, 0, 0, (uintptr_t)pdu);
 				conn->pdu_in_progress = NULL;
 				conn->pdu_recv_state = ISCSI_PDU_RECV_STATE_AWAIT_PDU_READY;
 				return 1;
diff --git a/test/bdev/bdevio/Makefile b/test/bdev/bdevio/Makefile
index a42ded076..166e48096 100644
--- a/test/bdev/bdevio/Makefile
+++ b/test/bdev/bdevio/Makefile
@@ -39,7 +39,7 @@ APP = bdevio
 
 C_SRCS := bdevio.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev
 
 LIBS += -lcunit
 
diff --git a/test/bdev/bdevperf/Makefile b/test/bdev/bdevperf/Makefile
index ef100188d..fac2d2104 100644
--- a/test/bdev/bdevperf/Makefile
+++ b/test/bdev/bdevperf/Makefile
@@ -39,7 +39,7 @@ APP = bdevperf
 
 C_SRCS := bdevperf.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev conf
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev conf
 
 ifeq ($(OS),Linux)
 SPDK_LIB_LIST += event_nbd
diff --git a/test/bdev/bdevperf/bdevperf.c b/test/bdev/bdevperf/bdevperf.c
index 9515f2b7a..c07d7b684 100644
--- a/test/bdev/bdevperf/bdevperf.c
+++ b/test/bdev/bdevperf/bdevperf.c
@@ -45,6 +45,7 @@
 #include "spdk/rpc.h"
 #include "spdk/bit_array.h"
 #include "spdk/conf.h"
+#include "spdk/zipf.h"
 
 #define BDEVPERF_CONFIG_MAX_FILENAME 1024
 #define BDEVPERF_CONFIG_UNDEFINED -1
@@ -91,6 +92,7 @@ static bool g_multithread_mode = false;
 static int g_timeout_in_sec;
 static struct spdk_conf *g_bdevperf_conf = NULL;
 static const char *g_bdevperf_conf_file = NULL;
+static double g_zipf_theta;
 
 static struct spdk_cpuset g_all_cpuset;
 static struct spdk_poller *g_perf_timer = NULL;
@@ -118,6 +120,7 @@ struct bdevperf_job {
 	bool				flush;
 	bool				abort;
 	int				queue_depth;
+	unsigned int			seed;
 
 	uint64_t			io_completed;
 	uint64_t			io_failed;
@@ -135,6 +138,7 @@ struct bdevperf_job {
 	struct spdk_poller		*run_timer;
 	struct spdk_poller		*reset_timer;
 	struct spdk_bit_array		*outstanding;
+	struct spdk_zipf		*zipf;
 	TAILQ_HEAD(, bdevperf_task)	task_list;
 };
 
@@ -405,7 +409,7 @@ bdevperf_test_done(void *ctx)
 		if (job->verify) {
 			spdk_bit_array_free(&job->outstanding);
 		}
-
+		spdk_zipf_free(&job->zipf);
 		free(job->name);
 		free(job);
 	}
@@ -712,7 +716,7 @@ bdevperf_submit_task(void *arg)
 		break;
 	case SPDK_BDEV_IO_TYPE_READ:
 		if (g_zcopy) {
-			rc = spdk_bdev_zcopy_start(desc, ch, task->offset_blocks, job->io_size_blocks,
+			rc = spdk_bdev_zcopy_start(desc, ch, NULL, 0, task->offset_blocks, job->io_size_blocks,
 						   true, bdevperf_zcopy_populate_complete, task);
 		} else {
 			if (spdk_bdev_is_md_separate(job->bdev)) {
@@ -799,7 +803,7 @@ bdevperf_prep_zcopy_write_task(void *arg)
 	struct bdevperf_job	*job = task->job;
 	int			rc;
 
-	rc = spdk_bdev_zcopy_start(job->bdev_desc, job->ch,
+	rc = spdk_bdev_zcopy_start(job->bdev_desc, job->ch, NULL, 0,
 				   task->offset_blocks, job->io_size_blocks,
 				   false, bdevperf_zcopy_get_buf_complete, task);
 	if (rc != 0) {
@@ -826,15 +830,15 @@ bdevperf_job_get_task(struct bdevperf_job *job)
 	return task;
 }
 
-static __thread unsigned int seed = 0;
-
 static void
 bdevperf_submit_single(struct bdevperf_job *job, struct bdevperf_task *task)
 {
 	uint64_t offset_in_ios;
 
-	if (job->is_random) {
-		offset_in_ios = rand_r(&seed) % job->size_in_ios;
+	if (job->zipf) {
+		offset_in_ios = spdk_zipf_generate(job->zipf);
+	} else if (job->is_random) {
+		offset_in_ios = rand_r(&job->seed) % job->size_in_ios;
 	} else {
 		offset_in_ios = job->offset_in_ios++;
 		if (job->offset_in_ios == job->size_in_ios) {
@@ -884,7 +888,7 @@ bdevperf_submit_single(struct bdevperf_job *job, struct bdevperf_task *task)
 	} else if (job->write_zeroes) {
 		task->io_type = SPDK_BDEV_IO_TYPE_WRITE_ZEROES;
 	} else if ((job->rw_percentage == 100) ||
-		   (job->rw_percentage != 0 && ((rand_r(&seed) % 100) < job->rw_percentage))) {
+		   (job->rw_percentage != 0 && ((rand_r(&job->seed) % 100) < job->rw_percentage))) {
 		task->io_type = SPDK_BDEV_IO_TYPE_READ;
 	} else {
 		if (g_zcopy) {
@@ -1172,6 +1176,14 @@ _bdevperf_construct_job(void *ctx)
 		goto end;
 	}
 
+	if (g_zcopy) {
+		if (!spdk_bdev_io_type_supported(job->bdev, SPDK_BDEV_IO_TYPE_ZCOPY)) {
+			printf("Test requires ZCOPY but bdev module does not support ZCOPY\n");
+			g_run_rc = -ENOTSUP;
+			goto end;
+		}
+	}
+
 	job->ch = spdk_bdev_get_io_channel(job->bdev_desc);
 	if (!job->ch) {
 		SPDK_ERRLOG("Could not get io_channel for device %s, error=%d\n", spdk_bdev_get_name(job->bdev),
@@ -1299,6 +1311,10 @@ bdevperf_construct_job(struct spdk_bdev *bdev, struct job_config *config,
 		job->ios_base = 0;
 	}
 
+	if (job->is_random && g_zipf_theta > 0) {
+		job->zipf = spdk_zipf_create(job->size_in_ios, g_zipf_theta, 0);
+	}
+
 	if (job->verify) {
 		job->outstanding = spdk_bit_array_create(job->size_in_ios);
 		if (job->outstanding == NULL) {
@@ -1936,6 +1952,15 @@ bdevperf_parse_arg(int ch, char *arg)
 		g_continue_on_failure = true;
 	} else if (ch == 'j') {
 		g_bdevperf_conf_file = optarg;
+	} else if (ch == 'F') {
+		char *endptr;
+
+		errno = 0;
+		g_zipf_theta = strtod(optarg, &endptr);
+		if (errno || optarg == endptr || g_zipf_theta < 0) {
+			fprintf(stderr, "Illegal zipf theta value %s\n", optarg);
+			return -EINVAL;
+		}
 	} else {
 		tmp = spdk_strtoll(optarg, 10);
 		if (tmp < 0) {
@@ -1993,6 +2018,7 @@ bdevperf_usage(void)
 	printf(" -S <period>               show performance result in real time every <period> seconds\n");
 	printf(" -T <bdev>                 bdev to run against. Default: all available bdevs.\n");
 	printf(" -f                        continue processing I/O even after failures\n");
+	printf(" -F <zipf theta>           use zipf distribution for random I/O\n");
 	printf(" -Z                        enable using zcopy bdev API for read or write I/O\n");
 	printf(" -z                        start bdevperf, but wait for RPC to start tests\n");
 	printf(" -X                        abort timed out I/O\n");
@@ -2103,7 +2129,7 @@ main(int argc, char **argv)
 	opts.rpc_addr = NULL;
 	opts.shutdown_cb = spdk_bdevperf_shutdown_cb;
 
-	if ((rc = spdk_app_parse_args(argc, argv, &opts, "Zzfq:o:t:w:k:CM:P:S:T:Xj:", NULL,
+	if ((rc = spdk_app_parse_args(argc, argv, &opts, "Zzfq:o:t:w:k:CF:M:P:S:T:Xj:", NULL,
 				      bdevperf_parse_arg, bdevperf_usage)) !=
 	    SPDK_APP_PARSE_ARGS_SUCCESS) {
 		return rc;
diff --git a/test/bdev/blockdev.sh b/test/bdev/blockdev.sh
index cfd55f8c6..f8409a63c 100755
--- a/test/bdev/blockdev.sh
+++ b/test/bdev/blockdev.sh
@@ -153,7 +153,11 @@ function nbd_function_test() {
 		local conf=$1
 		local nbd_all=($(ls /dev/nbd* | grep -v p))
 		local bdev_all=($bdevs_name)
-		local nbd_num=${#bdevs_all[@]}
+		local nbd_num=${#bdev_all[@]}
+		if ((nbd_num < 1)); then
+			# There should be at least one bdev and one valid nbd device
+			return 1
+		fi
 		if [ ${#nbd_all[@]} -le $nbd_num ]; then
 			nbd_num=${#nbd_all[@]}
 		fi
diff --git a/test/blobfs/blobfs.sh b/test/blobfs/blobfs.sh
index a741eca37..46975b01b 100755
--- a/test/blobfs/blobfs.sh
+++ b/test/blobfs/blobfs.sh
@@ -142,7 +142,10 @@ jq . <<- JSON > ${conf_file}
 	            "block_size": 512,
 	            "filename": "${tmp_file}"
 	          }
-	        }
+	        },
+	        {
+	          "method": "bdev_wait_for_examine"
+		}
 	      ]
 	    }
 	  ]
diff --git a/test/blobfs/mkfs/Makefile b/test/blobfs/mkfs/Makefile
index 775e49d98..6121cc524 100644
--- a/test/blobfs/mkfs/Makefile
+++ b/test/blobfs/mkfs/Makefile
@@ -39,6 +39,6 @@ APP = mkfs
 
 C_SRCS := mkfs.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/test/common/autotest_common.sh b/test/common/autotest_common.sh
index cec9a9504..4506ccffd 100755
--- a/test/common/autotest_common.sh
+++ b/test/common/autotest_common.sh
@@ -1,5 +1,15 @@
 #!/usr/bin/env bash
 
+function xtrace_fd() {
+	if [[ -n $BASH_XTRACEFD && -e /proc/self/fd/$BASH_XTRACEFD ]]; then
+		# Close it first to make sure it's sane
+		exec {BASH_XTRACEFD}>&-
+	fi
+	exec {BASH_XTRACEFD}>&2
+
+	set -x
+}
+
 function xtrace_disable() {
 	if [ "$XTRACE_DISABLED" != "yes" ]; then
 		PREV_BASH_OPTS="$-"
@@ -18,8 +28,6 @@ xtrace_disable
 set -e
 shopt -s expand_aliases
 
-source "$rootdir/test/common/applications.sh"
-source "$rootdir/scripts/common.sh"
 if [[ -e $rootdir/test/common/build_config.sh ]]; then
 	source "$rootdir/test/common/build_config.sh"
 elif [[ -e $rootdir/mk/config.mk ]]; then
@@ -29,6 +37,10 @@ else
 	source "$rootdir/CONFIG"
 fi
 
+# Source scripts after the config so that the definitions are available.
+source "$rootdir/test/common/applications.sh"
+source "$rootdir/scripts/common.sh"
+
 # Dummy function to be called after restoring xtrace just so that it appears in the
 # xtrace log. This way we can consistently track when xtrace is enabled/disabled.
 function xtrace_enable() {
@@ -77,6 +89,8 @@ export SPDK_TEST_ISCSI_INITIATOR
 export SPDK_TEST_NVME
 : ${SPDK_TEST_NVME_PMR=0}
 export SPDK_TEST_NVME_PMR
+: ${SPDK_TEST_NVME_SCC=0}
+export SPDK_TEST_NVME_SCC
 : ${SPDK_TEST_NVME_CLI=0}
 export SPDK_TEST_NVME_CLI
 : ${SPDK_TEST_NVME_CUSE=0}
@@ -139,6 +153,8 @@ export SPDK_TEST_URING
 export SPDK_TEST_USE_IGB_UIO
 : ${SPDK_TEST_SCHEDULER:=0}
 export SPDK_TEST_SCHEDULER
+: ${SPDK_TEST_SCANBUILD:=0}
+export SPDK_TEST_SCANBUILD
 
 export DPDK_LIB_DIR="${SPDK_RUN_EXTERNAL_DPDK:-$rootdir/dpdk/build}/lib"
 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$SPDK_LIB_DIR:$DPDK_LIB_DIR
@@ -205,10 +221,16 @@ if [ $SPDK_RUN_VALGRIND -eq 0 ]; then
 fi
 
 if [ "$(uname -s)" = "Linux" ]; then
+	export HUGEMEM=4096 CLEAR_HUGE=yes
+	if [[ $SPDK_TEST_CRYPTO -eq 1 || $SPDK_TEST_REDUCE -eq 1 ]]; then
+		# Make sure that memory is distributed across all NUMA nodes - by default, all goes to
+		# node0, but if QAT devices are attached to a different node, all of their VFs will end
+		# up under that node too and memory needs to be available there for the tests.
+		export HUGE_EVEN_ALLOC=yes
+	fi
+
 	MAKE="make"
 	MAKEFLAGS=${MAKEFLAGS:--j$(nproc)}
-	# Override the default HUGEMEM in scripts/setup.sh to allocate 8GB in hugepages.
-	export HUGEMEM=8192
 	if [[ $SPDK_TEST_USE_IGB_UIO -eq 1 ]]; then
 		export DRIVER_OVERRIDE=igb_uio
 		# Building kernel modules requires root privileges
@@ -219,6 +241,11 @@ elif [ "$(uname -s)" = "FreeBSD" ]; then
 	MAKEFLAGS=${MAKEFLAGS:--j$(sysctl -a | grep -E -i 'hw.ncpu' | awk '{print $2}')}
 	# FreeBSD runs a much more limited set of tests, so keep the default 2GB.
 	export HUGEMEM=2048
+elif [ "$(uname -s)" = "Windows" ]; then
+	MAKE="make"
+	MAKEFLAGS=${MAKEFLAGS:--j$(nproc)}
+	# Keep the default 2GB for Windows.
+	export HUGEMEM=2048
 else
 	echo "Unknown OS \"$(uname -s)\""
 	exit 1
@@ -264,17 +291,6 @@ function set_test_storage() {
 	local source fs size avail mount use
 
 	local storage_fallback storage_candidates
-	local storage_fallback_purge
-
-	shopt -s nullglob
-	storage_fallback_purge=("${TMPDIR:-/tmp}/spdk."??????)
-	shopt -u nullglob
-
-	if ((${#storage_fallback_purge[@]} > 0)); then
-		printf '* Purging old temporary test storage (%s)\n' \
-			"${storage_fallback_purge[*]}" >&2
-		sudo rm -rf "${storage_fallback_purge[@]}"
-	fi
 
 	storage_fallback=$(mktemp -udt spdk.XXXXXX)
 	storage_candidates=(
@@ -754,6 +770,8 @@ function waitforbdev() {
 	local bdev_name=$1
 	local i
 
+	$rpc_py bdev_wait_for_examine
+
 	for ((i = 1; i <= 20; i++)); do
 		if $rpc_py bdev_get_bdevs | jq -r '.[] .name' | grep -qw $bdev_name; then
 			return 0
@@ -976,9 +994,11 @@ function print_backtrace() {
 	local args=("${BASH_ARGV[@]}")
 
 	xtrace_disable
+	# Reset IFS in case we were called from an environment where it was modified
+	IFS=" "$'\t'$'\n'
 	echo "========== Backtrace start: =========="
 	echo ""
-	for i in $(seq 1 $((${#FUNCNAME[@]} - 1))); do
+	for ((i = 1; i < ${#FUNCNAME[@]}; i++)); do
 		local func="${FUNCNAME[$i]}"
 		local line_nr="${BASH_LINENO[$((i - 1))]}"
 		local src="${BASH_SOURCE[$i]}"
@@ -1214,6 +1234,16 @@ function autotest_cleanup() {
 		kill "$udevadm_pid" || :
 	fi
 	revert_soft_roce
+
+	shopt -s nullglob
+	local storage_fallback_purge=("${TMPDIR:-/tmp}/spdk."??????)
+	shopt -u nullglob
+
+	if ((${#storage_fallback_purge[@]} > 0)); then
+		printf '* Purging old temporary test storage (%s)\n' \
+			"${storage_fallback_purge[*]}" >&2
+		rm -rf "${storage_fallback_purge[@]}"
+	fi
 }
 
 function freebsd_update_contigmem_mod() {
@@ -1384,7 +1414,7 @@ if $SPDK_AUTOTEST_X; then
 	# explicitly enable xtraces, overriding any tracking information.
 	unset XTRACE_DISABLED
 	unset XTRACE_NESTING_LEVEL
-	set -x
+	xtrace_fd
 	xtrace_enable
 else
 	xtrace_restore
diff --git a/test/common/config/pkgdep/git b/test/common/config/pkgdep/git
index 749819ba7..7e6a65307 100644
--- a/test/common/config/pkgdep/git
+++ b/test/common/config/pkgdep/git
@@ -37,35 +37,9 @@ function install_refspdk() {
 		cp -R "$GIT_REPOS/spdk_repo/spdk" "$output_dir"
 	fi
 
-	lts_2001_fallback=false
-	if [[ "$release" == v20.01* ]]; then
-		# We switch to a .x branch in order to slurp all the backports which fix spdk_abi_lts
-		# build for this particular release. Fetch the branch explicitly as in case of our CI,
-		# jenkins will set remote.*.fetch to a particular ref the build run against, so plain
-		# git fetch won't include any branches from the repo. FIXME: This could be removed
-		# after new LTS is in place.
-		release=${release%%-*}.x
-		git -C "$output_dir" fetch origin "+refs/heads/$release:refs/heads/$release"
-		lts_2001_fallback=true
-	fi
 	git -C "$output_dir" checkout "$release"
 	git -C "$output_dir" submodule update --init
 
-	if [[ "$release" == v20.01* ]]; then
-		makefiles=(
-			"$output_dir/dpdk/drivers/crypto/aesni_gcm/Makefile"
-			"$output_dir/dpdk/drivers/crypto/aesni_mb/Makefile"
-			"$output_dir/dpdk/drivers/crypto/kasumi/Makefile"
-			"$output_dir/dpdk/drivers/crypto/snow3g/Makefile"
-		)
-		# Attempt to replicate dpdk's 2a860943b8 commit which fixes builds under make 4.3
-		# FIXME: Remove this when LTS changes!
-		for makefile in "${makefiles[@]}"; do
-			# This sed call is meant to be compatible with its FreeBSD implementation
-			echo "$(sed -e 's/\\#/\$H/g' -e '/IMB_HDR =/i\
-				H := \\#' "$makefile")" > "$makefile"
-		done
-	fi
 	cat > $HOME/autorun-spdk.conf <<- EOF
 		SPDK_BUILD_SHARED_OBJECT=1
 		SPDK_TEST_AUTOBUILD=1
@@ -82,6 +56,7 @@ function install_refspdk() {
 		SPDK_RUN_ASAN=1
 		SPDK_RUN_UBSAN=1
 		SPDK_TEST_NVME_PMR=1
+		SPDK_TEST_NVME_SCC=1
 		SPDK_TEST_NVME_CUSE=1
 		SPDK_TEST_BLOBFS=1
 		SPDK_TEST_URING=1
@@ -98,25 +73,11 @@ function install_refspdk() {
 		if [[ $OSID == freebsd ]]; then
 			config_params="--enable-debug"
 			config_params+=" --without-isal --with-fio=/usr/src/fio"
-
-			# TODO: Remove this if-block after 21.01 LTS is released and 20.01 LTS is deprecated.
-			if ! "$lts_2001_fallback"; then
-				config_params+=" --with-idxd --disable-unit-tests"
-			fi
+			config_params+=" --with-idxd --disable-unit-tests"
 
 			MAKE=gmake
 		else
-			# TODO: "get_config_params" was not available in 20.01 LTS release.
-			# Remove this if-block after 21.01 release.
-			if "$lts_2001_fallback"; then
-				config_params="--enable-debug --enable-werror --with-rdma"
-				config_params+=" --with-fio=/usr/src/fio --with-iscsi-initiator"
-				config_params+=" --with-nvme-cuse --with-pmdk --with-reduce"
-				config_params+=" --with-rbd --with-crypto --with-ocf --enable-ubsan"
-				config_params+=" --enable-asan --with-fuse --with-uring"
-			else
-				config_params="$(get_config_params)"
-			fi
+			config_params="$(get_config_params)"
 		fi
 		$output_dir/configure $(echo $config_params | sed 's/--enable-coverage//g')
 		if [[ $OSID != freebsd ]]; then
@@ -197,6 +158,12 @@ function install_qat() {
 		# qat_algs_ablkcipher.c doesn't exist in older versions of the driver so simply force the patch
 		patch --force --dir="$GIT_REPOS/QAT" -p1
 	fi < "$rootdir/test/common/config/pkgdep/patches/qat/0001-sha.patch" || :
+
+	# cipher routines were moved from crypto.h to crypto/internal/cipher.h in kernels >= 5.12
+	if ((kernel_ver >= 0x050c00)); then
+		patch --dir="$GIT_REPOS/QAT" -p1
+	fi < "$rootdir/test/common/config/pkgdep/patches/qat/0001-cipher-ns.patch"
+
 	(cd "$GIT_REPOS/QAT" && sudo ./configure --enable-icp-sriov=host && sudo make install)
 
 	if ! sudo service qat_service start; then
@@ -295,6 +262,12 @@ function _install_qemu() {
 	fi
 	opt_params+=("--extra-cflags=${extra_cflags[*]}")
 
+	if [[ $prefix == vanilla ]]; then
+		# Latest qemu seems to take sysconfdir from the prefix and instead of checking /etc
+		# it looks under /usr/local/qemu/vanilla*/bin/../etc which is a bit peculiar. Fix it.
+		opt_params+=("--sysconfdir=/etc/")
+	fi
+
 	# The qemu configure script places several output files in the CWD.
 	(cd "$repo_dir" && ./configure "${opt_params[@]}" --target-list="x86_64-softmmu" --enable-kvm --enable-linux-aio --enable-numa)
 
@@ -313,11 +286,6 @@ function install_qemu() {
 	#  - vfio-user QEMU: A special fork to test libvfio-user components.
 	#  - Vanilla QEMU: Used for the upstream PMR support.
 
-	# Forked QEMUs
-	SPDK_QEMU_BRANCH=spdk-5.0.0
-	VFIO_QEMU_BRANCH=vfio-user-v0.6
-	VANILLA_QEMU_BRANCH=v5.1.0
-
 	_install_qemu $GIT_REPO_QEMU_SPDK $SPDK_QEMU_BRANCH
 	_install_qemu $GIT_REPO_QEMU_VFIO $VFIO_QEMU_BRANCH
 	_install_qemu "$GIT_REPO_QEMU" "$VANILLA_QEMU_BRANCH" vanilla
@@ -559,8 +527,12 @@ function install_sources() {
 }
 
 GIT_VERSION=2.25.1
-IRDMA_VERSION=1.2.21
+IRDMA_VERSION=1.5.2
 ICE_VERSION=1.4.11
+SPDK_QEMU_BRANCH=${SPDK_QEMU_BRANCH:-spdk-5.0.0}
+VFIO_QEMU_BRANCH=${VFIO_QEMU_BRANCH:-vfio-user-v0.9}
+VANILLA_QEMU_BRANCH=${VANILLA_QEMU_BRANCH:-v6.0.0}
+
 : ${GIT_REPO_SPDK=https://github.com/spdk/spdk.git}
 export GIT_REPO_SPDK
 : ${GIT_REPO_DPDK=https://github.com/spdk/dpdk.git}
@@ -575,7 +547,7 @@ export GIT_REPO_FLAMEGRAPH
 export GIT_REPO_QEMU
 : ${GIT_REPO_QEMU_SPDK=https://github.com/spdk/qemu}
 export GIT_REPO_QEMU_SPDK
-: ${GIT_REPO_QEMU_VFIO=https://github.com/tmakatos/qemu}
+: ${GIT_REPO_QEMU_VFIO=https://github.com/oracle/qemu}
 export GIT_REPO_QEMU_VFIO
 : ${GIT_REPO_LIBISCSI=https://github.com/sahlberg/libiscsi}
 export GIT_REPO_LIBISCSI
@@ -587,7 +559,7 @@ export DRIVER_LOCATION_QAT
 export GIT_REPO_GIT
 : ${GIT_REPO_DPDK_KMODS=http://dpdk.org/git/dpdk-kmods}
 export GIT_REPO_DPDK_KMODS
-: ${IRDMA_DRIVER=https://downloadmirror.intel.com/30238/eng/irdma-$IRDMA_VERSION.tgz}
+: ${IRDMA_DRIVER=https://downloadmirror.intel.com/29751/eng/irdma-$IRDMA_VERSION.tgz}
 export IRDMA_DRIVER
 : ${ICE_DRIVER="https://sourceforge.net/projects/e1000/files/ice stable/$ICE_VERSION/ice-$ICE_VERSION.tar.gz"}
 export ICE_DRIVER
diff --git a/test/common/config/pkgdep/patches/qat/0001-cipher-ns.patch b/test/common/config/pkgdep/patches/qat/0001-cipher-ns.patch
new file mode 100644
index 000000000..8c9f29c5a
--- /dev/null
+++ b/test/common/config/pkgdep/patches/qat/0001-cipher-ns.patch
@@ -0,0 +1,28 @@
+Cipher routines were moved to their own header file and the symbol exports
+are now available under a new namespace, CRYPTO_INTERNAL.
+
+Details: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=0eb76ba29d16df2951d37c54ca279c4e5630b071
+---
+
+diff --git a/quickassist/lookaside/access_layer/src/linux/icp_qa_module.c b/quickassist/lookaside/access_layer/src/linux/icp_qa_module.c
+index 413eb07..f7d09bd 100644
+--- a/quickassist/lookaside/access_layer/src/linux/icp_qa_module.c
++++ b/quickassist/lookaside/access_layer/src/linux/icp_qa_module.c
+@@ -120,3 +120,4 @@ module_exit(kapi_mod_exit);
+ MODULE_DESCRIPTION("Intel(R) Quickassist Technology Acceleration Driver");
+ MODULE_AUTHOR("Intel");
+ MODULE_LICENSE("Dual BSD/GPL");
++MODULE_IMPORT_NS(CRYPTO_INTERNAL);
+diff --git a/quickassist/utilities/osal/src/linux/kernel_space/OsalCryptoInterface.c b/quickassist/utilities/osal/src/linux/kernel_space/OsalCryptoInterface.c
+index 9d2e85c..f265422 100644
+--- a/quickassist/utilities/osal/src/linux/kernel_space/OsalCryptoInterface.c
++++ b/quickassist/utilities/osal/src/linux/kernel_space/OsalCryptoInterface.c
+@@ -65,7 +65,7 @@
+  */
+ 
+ #include "Osal.h"
+-#include <linux/crypto.h>
++#include <crypto/internal/cipher.h>
+ #include <linux/version.h>
+ #if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,29))
+ #include <crypto/internal/hash.h>
diff --git a/test/common/config/pkgdep/pkg b/test/common/config/pkgdep/pkg
index 170c7aaf6..3f3f41725 100644
--- a/test/common/config/pkgdep/pkg
+++ b/test/common/config/pkgdep/pkg
@@ -11,7 +11,6 @@ install() {
 }
 
 packages=(
-	etc_os-release
 	pciutils
 	jq
 	gdb
diff --git a/test/common/config/vm_setup.sh b/test/common/config/vm_setup.sh
index bf9ae59d1..59dbff6a7 100755
--- a/test/common/config/vm_setup.sh
+++ b/test/common/config/vm_setup.sh
@@ -57,12 +57,6 @@ function error() {
 }
 
 function set_os_id_version() {
-	if [[ $(uname -s) == FreeBSD ]] && ! pkg info -q etc_os-release; then
-		echo "Please install 'etc_os-release' package" >&2
-		echo "pkg install -y etc_os-release" >&2
-		exit 2
-	fi
-
 	if [[ -f /etc/os-release ]]; then
 		source /etc/os-release
 	elif [[ -f /usr/local/etc/os-release ]]; then
@@ -185,6 +179,7 @@ SPDK_TEST_ISCSI=1
 SPDK_TEST_ISCSI_INITIATOR=1
 SPDK_TEST_NVME=1
 SPDK_TEST_NVME_PMR=1
+SPDK_TEST_NVME_SCC=1
 SPDK_TEST_NVME_CLI=1
 SPDK_TEST_NVMF=1
 SPDK_TEST_VFIOUSER=1
diff --git a/test/common/lib/nvme/common_stubs.h b/test/common/lib/nvme/common_stubs.h
index 1dc22a162..5fd9dd49c 100644
--- a/test/common/lib/nvme/common_stubs.h
+++ b/test/common/lib/nvme/common_stubs.h
@@ -73,17 +73,10 @@ DEFINE_STUB(nvme_request_check_timeout, int, (struct nvme_request *req, uint16_t
 DEFINE_STUB_V(nvme_ctrlr_destruct_finish, (struct spdk_nvme_ctrlr *ctrlr));
 DEFINE_STUB(nvme_ctrlr_construct, int, (struct spdk_nvme_ctrlr *ctrlr), 0);
 DEFINE_STUB_V(nvme_ctrlr_destruct, (struct spdk_nvme_ctrlr *ctrlr));
-DEFINE_STUB_V(nvme_ctrlr_init_cap, (struct spdk_nvme_ctrlr *ctrlr,
-				    const union spdk_nvme_cap_register *cap,
-				    const union spdk_nvme_vs_register *vs));
 DEFINE_STUB(nvme_ctrlr_get_vs, int, (struct spdk_nvme_ctrlr *ctrlr,
 				     union spdk_nvme_vs_register *vs), 0);
 DEFINE_STUB(nvme_ctrlr_get_cap, int, (struct spdk_nvme_ctrlr *ctrlr,
 				      union spdk_nvme_cap_register *cap), 0);
-DEFINE_STUB(nvme_qpair_init, int, (struct spdk_nvme_qpair *qpair, uint16_t id,
-				   struct spdk_nvme_ctrlr *ctrlr,
-				   enum spdk_nvme_qprio qprio,
-				   uint32_t num_requests), 0);
 DEFINE_STUB_V(nvme_qpair_deinit, (struct spdk_nvme_qpair *qpair));
 DEFINE_STUB_V(spdk_nvme_transport_register, (const struct spdk_nvme_transport_ops *ops));
 DEFINE_STUB(nvme_transport_ctrlr_connect_qpair, int, (struct spdk_nvme_ctrlr *ctrlr,
@@ -115,3 +108,18 @@ DEFINE_STUB(nvme_fabric_qpair_connect, int, (struct spdk_nvme_qpair *qpair, uint
 DEFINE_STUB_V(nvme_transport_ctrlr_disconnect_qpair, (struct spdk_nvme_ctrlr *ctrlr,
 		struct spdk_nvme_qpair *qpair));
 DEFINE_STUB(nvme_poll_group_disconnect_qpair, int, (struct spdk_nvme_qpair *qpair), 0);
+
+int
+nvme_qpair_init(struct spdk_nvme_qpair *qpair, uint16_t id,
+		struct spdk_nvme_ctrlr *ctrlr,
+		enum spdk_nvme_qprio qprio,
+		uint32_t num_requests)
+{
+	qpair->ctrlr = ctrlr;
+	qpair->id = id;
+	qpair->qprio = qprio;
+	qpair->trtype = SPDK_NVME_TRANSPORT_TCP;
+	qpair->poll_group = (void *)0xDEADBEEF;
+
+	return 0;
+}
diff --git a/test/common/lib/test_rdma.c b/test/common/lib/test_rdma.c
index 62cd934eb..d89f48745 100644
--- a/test/common/lib/test_rdma.c
+++ b/test/common/lib/test_rdma.c
@@ -39,8 +39,9 @@
 #define RDMA_UT_LKEY 123
 #define RDMA_UT_RKEY 312
 
+struct spdk_rdma_qp g_spdk_rdma_qp = {};
 DEFINE_STUB(spdk_rdma_qp_create, struct spdk_rdma_qp *, (struct rdma_cm_id *cm_id,
-		struct spdk_rdma_qp_init_attr *qp_attr), NULL);
+		struct spdk_rdma_qp_init_attr *qp_attr), &g_spdk_rdma_qp);
 DEFINE_STUB(spdk_rdma_qp_accept, int, (struct spdk_rdma_qp *spdk_rdma_qp,
 				       struct rdma_conn_param *conn_param), 0);
 DEFINE_STUB(spdk_rdma_qp_complete_connect, int, (struct spdk_rdma_qp *spdk_rdma_qp), 0);
diff --git a/test/common/lib/test_sock.c b/test/common/lib/test_sock.c
index d2c83b732..afbcfd044 100644
--- a/test/common/lib/test_sock.c
+++ b/test/common/lib/test_sock.c
@@ -40,8 +40,6 @@ DEFINE_STUB(spdk_sock_getaddr, int, (struct spdk_sock *sock, char *saddr, int sl
 				     char *caddr, int clen, uint16_t *cport), 0);
 DEFINE_STUB(spdk_sock_connect, struct spdk_sock *, (const char *ip, int port, char *impl_name),
 	    NULL);
-DEFINE_STUB(spdk_sock_connect_ext, struct spdk_sock *, (const char *ip, int port, char *impl_name,
-		struct spdk_sock_opts *opts), NULL);
 DEFINE_STUB(spdk_sock_listen, struct spdk_sock *, (const char *ip, int port, char *impl_name),
 	    NULL);
 DEFINE_STUB(spdk_sock_listen_ext, struct spdk_sock *, (const char *ip, int port, char *impl_name,
@@ -49,7 +47,7 @@ DEFINE_STUB(spdk_sock_listen_ext, struct spdk_sock *, (const char *ip, int port,
 DEFINE_STUB_V(spdk_sock_get_default_opts, (struct spdk_sock_opts *opts));
 DEFINE_STUB(spdk_sock_accept, struct spdk_sock *, (struct spdk_sock *sock), NULL);
 DEFINE_STUB(spdk_sock_close, int, (struct spdk_sock **sock), 0);
-DEFINE_STUB(spdk_sock_recv, ssize_t, (struct spdk_sock *sock, void *buf, size_t len), 0);
+DEFINE_STUB(spdk_sock_recv, ssize_t, (struct spdk_sock *sock, void *buf, size_t len), 1);
 DEFINE_STUB(spdk_sock_writev, ssize_t, (struct spdk_sock *sock, struct iovec *iov, int iovcnt), 0);
 DEFINE_STUB(spdk_sock_readv, ssize_t, (struct spdk_sock *sock, struct iovec *iov, int iovcnt), 0);
 DEFINE_STUB(spdk_sock_set_recvlowat, int, (struct spdk_sock *sock, int nbytes), 0);
diff --git a/test/common/skipped_tests.txt b/test/common/skipped_tests.txt
index 294a0aa09..949ac26e3 100644
--- a/test/common/skipped_tests.txt
+++ b/test/common/skipped_tests.txt
@@ -13,6 +13,10 @@ iscsi_tgt_fio_remote_nvme
 nvme_pmr
 nvme_pmr_persistence
 
+# Waiting on SCC support in CI
+nvme_scc
+nvme_simple_copy
+
 # Waiting on significant test rewrite
 nvme_opal
 nvme_opal_bdevio
diff --git a/test/compress/compress.sh b/test/compress/compress.sh
index ae18778d6..7ee601227 100755
--- a/test/compress/compress.sh
+++ b/test/compress/compress.sh
@@ -99,7 +99,7 @@ if [ $RUN_NIGHTLY -eq 1 ]; then
 	# Create an NVMe-oF subsystem and add compress bdevs as namespaces
 	$rpc_py nvmf_create_transport -t $TEST_TRANSPORT -u 8192
 	create_vols
-	$rpc_py nvmf_subsystem_create nqn.2016-06.io.spdk:cnode0 -a -s SPDK0
+	$rpc_py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode0 -a -s SPDK0
 	$rpc_py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode0 COMP_lvs0/lv0
 	$rpc_py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode0 -t $TEST_TRANSPORT -a $NVMF_FIRST_TARGET_IP -s $NVMF_PORT
 
diff --git a/test/dd/common.sh b/test/dd/common.sh
index d2f7defa3..1475274ec 100644
--- a/test/dd/common.sh
+++ b/test/dd/common.sh
@@ -77,7 +77,10 @@ gen_conf() {
 		    {
 		      "subsystem": "bdev",
 		      "config": [
-		        ${config[*]}
+		        ${config[*]},
+			{
+			  "method": "bdev_wait_for_examine"
+			}
 		      ]
 		    }
 		    ${extra_subsystems[*]:+,${extra_subsystems[*]}}
diff --git a/test/dd/dd.sh b/test/dd/dd.sh
index e2b8bb86a..778991104 100755
--- a/test/dd/dd.sh
+++ b/test/dd/dd.sh
@@ -8,6 +8,11 @@ nvmes=($(nvme_in_userspace))
 
 check_liburing
 
+if ((liburing_in_use == 0 && SPDK_TEST_URING == 1)); then
+	printf 'SPDK_TEST_URING is set but spdk_dd is not linked to liburing, aborting\n' >&2
+	exit 1
+fi
+
 run_test "spdk_dd_basic_rw" "$testdir/basic_rw.sh" "${nvmes[@]}"
 run_test "spdk_dd_posix" "$testdir/posix.sh"
 run_test "spdk_dd_bdev_to_bdev" "$testdir/bdev_to_bdev.sh" "${nvmes[@]}"
diff --git a/test/event/app_repeat/Makefile b/test/event/app_repeat/Makefile
index f73b4403c..a0ff1e8dc 100644
--- a/test/event/app_repeat/Makefile
+++ b/test/event/app_repeat/Makefile
@@ -47,6 +47,6 @@ BLOCKDEV_LIST = bdev_malloc bdev_null
 BLOCKDEV_LIST += bdev_aio
 SYS_LIBS += -laio
 
-SPDK_LIB_LIST += $(BLOCKDEV_LIST)
+SPDK_LIB_LIST += $(BLOCKDEV_LIST) event
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/test/event/event_perf/event_perf.c b/test/event/event_perf/event_perf.c
index 631ade161..e344abdc1 100644
--- a/test/event/event_perf/event_perf.c
+++ b/test/event/event_perf/event_perf.c
@@ -35,7 +35,6 @@
 
 #include "spdk/env.h"
 #include "spdk/event.h"
-#include "spdk_internal/event.h"
 #include "spdk/log.h"
 #include "spdk/string.h"
 
diff --git a/test/event/scheduler/Makefile b/test/event/scheduler/Makefile
index 13aa0ab94..cd58c27e8 100644
--- a/test/event/scheduler/Makefile
+++ b/test/event/scheduler/Makefile
@@ -38,6 +38,6 @@ include $(SPDK_ROOT_DIR)/mk/spdk.modules.mk
 APP = scheduler
 C_SRCS := scheduler.c
 
-SPDK_LIB_LIST = $(ALL_MODULES_LIST) event_bdev conf
+SPDK_LIB_LIST = $(ALL_MODULES_LIST) event event_bdev conf
 
 include $(SPDK_ROOT_DIR)/mk/spdk.app.mk
diff --git a/test/external_code/hello_world/Makefile b/test/external_code/hello_world/Makefile
index 4e5de3ebd..96eba7583 100644
--- a/test/external_code/hello_world/Makefile
+++ b/test/external_code/hello_world/Makefile
@@ -34,8 +34,8 @@
 PKG_CONFIG_PATH = $(SPDK_LIB_DIR)/pkgconfig
 
 DPDK_LIB := $(shell PKG_CONFIG_PATH="$(PKG_CONFIG_PATH)" pkg-config --libs spdk_env_dpdk)
-SPDK_BDEV_LIB := $(shell PKG_CONFIG_PATH="$(PKG_CONFIG_PATH)" pkg-config --libs spdk_event_bdev)
-SPDK_DPDK_LIB := $(shell PKG_CONFIG_PATH="$(PKG_CONFIG_PATH)" pkg-config --libs spdk_event_bdev spdk_env_dpdk)
+SPDK_EVENT_LIB := $(shell PKG_CONFIG_PATH="$(PKG_CONFIG_PATH)" pkg-config --libs spdk_event spdk_event_bdev)
+SPDK_DPDK_LIB := $(shell PKG_CONFIG_PATH="$(PKG_CONFIG_PATH)" pkg-config --libs spdk_event spdk_event_bdev spdk_env_dpdk)
 SYS_LIB := $(shell PKG_CONFIG_PATH="$(PKG_CONFIG_PATH)" pkg-config --libs --static spdk_syslibs)
 
 # Shows how to compile both an external bdev and an external application against the SPDK combined shared object and dpdk shared objects.
@@ -46,7 +46,7 @@ bdev_shared_combo:
 # Shows how to compile both an external bdev and an external application against the SPDK individual shared objects and dpdk shared objects.
 bdev_shared_iso:
 	$(CC) $(COMMON_CFLAGS) -L../passthru -Wl,--no-as-needed -o hello_bdev ./hello_bdev.c \
-	-lpassthru_external $(SPDK_BDEV_LIB) \
+	-lpassthru_external $(SPDK_EVENT_LIB) \
 	$(DPDK_LIB)
 
 # Shows how to compile an external application against the SPDK combined shared object and dpdk shared objects.
@@ -56,7 +56,7 @@ alone_shared_combo:
 # Shows how to compile an external application against the SPDK individual shared objects and dpdk shared objects.
 alone_shared_iso:
 	$(CC) $(COMMON_CFLAGS) -Wl,-rpath=$(SPDK_LIB_DIR),--no-as-needed -o hello_bdev ./hello_bdev.c \
-	$(SPDK_BDEV_LIB) $(DPDK_LIB)
+	$(SPDK_EVENT_LIB) $(DPDK_LIB)
 
 # Shows how to compile an external application against the SPDK archives.
 alone_static:
diff --git a/test/external_code/passthru/vbdev_passthru.c b/test/external_code/passthru/vbdev_passthru.c
index 80e41162c..54e010142 100644
--- a/test/external_code/passthru/vbdev_passthru.c
+++ b/test/external_code/passthru/vbdev_passthru.c
@@ -347,7 +347,8 @@ vbdev_passthru_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *b
 				     _pt_complete_io, bdev_io);
 		break;
 	case SPDK_BDEV_IO_TYPE_ZCOPY:
-		rc = spdk_bdev_zcopy_start(pt_node->base_desc, pt_ch->base_ch, bdev_io->u.bdev.offset_blocks,
+		rc = spdk_bdev_zcopy_start(pt_node->base_desc, pt_ch->base_ch, bdev_io->u.bdev.iovs,
+					   bdev_io->u.bdev.iovcnt, bdev_io->u.bdev.offset_blocks,
 					   bdev_io->u.bdev.num_blocks, bdev_io->u.bdev.zcopy.populate,
 					   _pt_complete_zcopy_io, bdev_io);
 		break;
diff --git a/test/iscsi_tgt/common.sh b/test/iscsi_tgt/common.sh
index 7233b099d..ca8d05f7c 100644
--- a/test/iscsi_tgt/common.sh
+++ b/test/iscsi_tgt/common.sh
@@ -143,7 +143,10 @@ function initiator_json_config() {
 		            "url": "iscsi://$TARGET_IP/iqn.2016-06.io.spdk:disk1/0",
 		            "initiator_iqn": "iqn.2016-06.io.spdk:disk1/0"
 		          }
-		        }${*:+,$*}
+		        },
+		        {
+		          "method": "bdev_wait_for_examine"
+		        }
 		      ]
 		    }
 		  ]
diff --git a/test/iscsi_tgt/digests/digests.sh b/test/iscsi_tgt/digests/digests.sh
index 0d46c5dbb..6bbedd3e3 100755
--- a/test/iscsi_tgt/digests/digests.sh
+++ b/test/iscsi_tgt/digests/digests.sh
@@ -47,7 +47,7 @@ MALLOC_BDEV_SIZE=64
 MALLOC_BLOCK_SIZE=512
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 timing_enter start_iscsi_tgt
 
diff --git a/test/iscsi_tgt/fio/fio.sh b/test/iscsi_tgt/fio/fio.sh
index dc072620f..4710de1f6 100755
--- a/test/iscsi_tgt/fio/fio.sh
+++ b/test/iscsi_tgt/fio/fio.sh
@@ -56,7 +56,7 @@ MALLOC_BDEV_SIZE=64
 MALLOC_BLOCK_SIZE=4096
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 timing_enter start_iscsi_tgt
 
diff --git a/test/iscsi_tgt/ip_migration/ip_migration.sh b/test/iscsi_tgt/ip_migration/ip_migration.sh
index 402f676f7..44dd766ec 100755
--- a/test/iscsi_tgt/ip_migration/ip_migration.sh
+++ b/test/iscsi_tgt/ip_migration/ip_migration.sh
@@ -8,7 +8,7 @@ source $rootdir/test/iscsi_tgt/common.sh
 iscsitestinit
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 source "$rootdir/test/common/applications.sh"
 NETMASK=127.0.0.0/24
diff --git a/test/iscsi_tgt/login_redirection/login_redirection.sh b/test/iscsi_tgt/login_redirection/login_redirection.sh
index 824eb2e64..c9fb998cb 100755
--- a/test/iscsi_tgt/login_redirection/login_redirection.sh
+++ b/test/iscsi_tgt/login_redirection/login_redirection.sh
@@ -11,7 +11,7 @@ NULL_BDEV_SIZE=64
 NULL_BLOCK_SIZE=512
 
 rpc_py=$rootdir/scripts/rpc.py
-fio_py=$rootdir/scripts/fio.py
+fio_py=$rootdir/scripts/fio-wrapper
 
 rpc_addr1="/var/tmp/spdk0.sock"
 rpc_addr2="/var/tmp/spdk1.sock"
diff --git a/test/iscsi_tgt/lvol/iscsi_lvol.sh b/test/iscsi_tgt/lvol/iscsi_lvol.sh
index ad975c636..39ff87a41 100755
--- a/test/iscsi_tgt/lvol/iscsi_lvol.sh
+++ b/test/iscsi_tgt/lvol/iscsi_lvol.sh
@@ -18,7 +18,7 @@ else
 fi
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 timing_enter start_iscsi_tgt
 
diff --git a/test/iscsi_tgt/multiconnection/multiconnection.sh b/test/iscsi_tgt/multiconnection/multiconnection.sh
index 09cc9a2fb..a1fc43933 100755
--- a/test/iscsi_tgt/multiconnection/multiconnection.sh
+++ b/test/iscsi_tgt/multiconnection/multiconnection.sh
@@ -8,7 +8,7 @@ source $rootdir/test/iscsi_tgt/common.sh
 iscsitestinit
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 CONNECTION_NUMBER=30
 
diff --git a/test/iscsi_tgt/nvme_remote/fio_remote_nvme.sh b/test/iscsi_tgt/nvme_remote/fio_remote_nvme.sh
index 65a2a1681..47aa5a595 100755
--- a/test/iscsi_tgt/nvme_remote/fio_remote_nvme.sh
+++ b/test/iscsi_tgt/nvme_remote/fio_remote_nvme.sh
@@ -10,7 +10,7 @@ nvmftestinit
 iscsitestinit
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 # Namespaces are NOT used here on purpose. Rxe_cfg utilility used for NVMf tests do not support namespaces.
 TARGET_IP=127.0.0.1
diff --git a/test/iscsi_tgt/pmem/iscsi_pmem.sh b/test/iscsi_tgt/pmem/iscsi_pmem.sh
index da6fd77f4..5765bedfe 100755
--- a/test/iscsi_tgt/pmem/iscsi_pmem.sh
+++ b/test/iscsi_tgt/pmem/iscsi_pmem.sh
@@ -13,7 +13,7 @@ PMEM_BLOCK_SIZE=512
 TGT_NR=10
 PMEM_PER_TGT=1
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 timing_enter start_iscsi_target
 "${ISCSI_APP[@]}" -m $ISCSI_TEST_CORE_MASK --wait-for-rpc &
diff --git a/test/iscsi_tgt/qos/qos.sh b/test/iscsi_tgt/qos/qos.sh
index 6690c1549..31092d5f3 100755
--- a/test/iscsi_tgt/qos/qos.sh
+++ b/test/iscsi_tgt/qos/qos.sh
@@ -53,7 +53,7 @@ MALLOC_BLOCK_SIZE=512
 IOPS_RESULT=
 BANDWIDTH_RESULT=
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 timing_enter start_iscsi_tgt
 
diff --git a/test/iscsi_tgt/rbd/rbd.sh b/test/iscsi_tgt/rbd/rbd.sh
index 7ab0e0352..7e0721fea 100755
--- a/test/iscsi_tgt/rbd/rbd.sh
+++ b/test/iscsi_tgt/rbd/rbd.sh
@@ -13,7 +13,7 @@ trap 'rbd_cleanup; exit 1' SIGINT SIGTERM EXIT
 timing_exit rbd_setup
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 timing_enter start_iscsi_tgt
 
@@ -31,7 +31,9 @@ timing_exit start_iscsi_tgt
 
 $rpc_py iscsi_create_portal_group $PORTAL_TAG $TARGET_IP:$ISCSI_PORT
 $rpc_py iscsi_create_initiator_group $INITIATOR_TAG $INITIATOR_NAME $NETMASK
-rbd_bdev="$($rpc_py bdev_rbd_create $RBD_POOL $RBD_NAME 4096)"
+rbd_cluster_name="$($rpc_py bdev_rbd_register_cluster iscsi_rbd_cluster)"
+$rpc_py bdev_rbd_get_clusters_info -b $rbd_cluster_name
+rbd_bdev="$($rpc_py bdev_rbd_create $RBD_POOL $RBD_NAME 4096 -c $rbd_cluster_name)"
 $rpc_py bdev_get_bdevs
 
 $rpc_py bdev_rbd_resize $rbd_bdev 2000
@@ -64,6 +66,7 @@ trap - SIGINT SIGTERM EXIT
 
 iscsicleanup
 $rpc_py bdev_rbd_delete $rbd_bdev
+$rpc_py bdev_rbd_unregister_cluster $rbd_cluster_name
 killprocess $pid
 rbd_cleanup
 
diff --git a/test/iscsi_tgt/reset/reset.sh b/test/iscsi_tgt/reset/reset.sh
index 7b1d8ada7..5ed96eea8 100755
--- a/test/iscsi_tgt/reset/reset.sh
+++ b/test/iscsi_tgt/reset/reset.sh
@@ -11,7 +11,7 @@ MALLOC_BDEV_SIZE=64
 MALLOC_BLOCK_SIZE=512
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 if ! hash sg_reset; then
 	exit 1
diff --git a/test/iscsi_tgt/trace_record/trace_record.sh b/test/iscsi_tgt/trace_record/trace_record.sh
index 7e13838ba..5a411e3ae 100755
--- a/test/iscsi_tgt/trace_record/trace_record.sh
+++ b/test/iscsi_tgt/trace_record/trace_record.sh
@@ -31,7 +31,7 @@ MALLOC_BDEV_SIZE=64
 MALLOC_BLOCK_SIZE=4096
 
 rpc_py="$rootdir/scripts/rpc.py"
-fio_py="$rootdir/scripts/fio.py"
+fio_py="$rootdir/scripts/fio-wrapper"
 
 timing_enter start_iscsi_tgt
 
diff --git a/test/json_config/config_filter.py b/test/json_config/config_filter.py
index d19a3165b..59ed539b9 100755
--- a/test/json_config/config_filter.py
+++ b/test/json_config/config_filter.py
@@ -28,6 +28,7 @@ def filter_methods(do_remove_global_rpcs):
         'nvmf_set_config',
         'nvmf_set_max_subsystems',
         'nvmf_create_transport',
+        'nvmf_set_crdt',
         'bdev_set_options',
         'bdev_wait_for_examine',
         'bdev_nvme_set_options',
diff --git a/test/json_config/json_config.sh b/test/json_config/json_config.sh
index 5dd95e0c6..396213469 100755
--- a/test/json_config/json_config.sh
+++ b/test/json_config/json_config.sh
@@ -150,6 +150,20 @@ function create_bdev_subsystem_config() {
 
 	local expected_notifications=()
 
+	local event_line event ev_type ev_ctx
+	local rc=""
+
+	# Before testing notifications generated by the operations in this function, we
+	# need add existing notification to the expected list first. Otherwise it would fail.
+	while read -r event_line; do
+		# remove ID
+		event="${event_line%:*}"
+		ev_type=${event%:*}
+		ev_ctx=${event#*:}
+
+		expected_notifications+=(${ev_type}:${ev_ctx})
+	done < <(tgt_rpc notify_get_notifications -i ${last_event_id} | jq -r '.[] | "\(.type):\(.ctx):\(.id)"')
+
 	if [[ $SPDK_TEST_BLOCKDEV -eq 1 ]]; then
 		local lvol_store_base_bdev=Nvme0n1
 		if ! tgt_rpc get_bdevs --name ${lvol_store_base_bdev} > /dev/null; then
@@ -174,7 +188,6 @@ function create_bdev_subsystem_config() {
 		tgt_rpc bdev_malloc_create 16 4096 --name Malloc1
 
 		expected_notifications+=(
-			bdev_register:${lvol_store_base_bdev}
 			bdev_register:${lvol_store_base_bdev}p1
 			bdev_register:${lvol_store_base_bdev}p0
 			bdev_register:Malloc3
diff --git a/test/make/check_so_deps.sh b/test/make/check_so_deps.sh
index 9e3c55963..81f43f8ec 100755
--- a/test/make/check_so_deps.sh
+++ b/test/make/check_so_deps.sh
@@ -2,7 +2,7 @@
 shopt -s extglob
 
 function get_git_tag() {
-	git -C "${1:-$rootdir}" describe --tags --abbrev=0
+	git -C "${1:-$rootdir}" describe --tags --abbrev=0 --exclude=LTS
 }
 
 if [ "$(uname -s)" = "FreeBSD" ]; then
@@ -49,6 +49,10 @@ function confirm_abi_deps() {
 	fi
 
 	cat << EOF > ${suppression_file}
+[suppress_type]
+	name = spdk_nvme_ns_data
+[suppress_type]
+	name = spdk_nvme_ctrlr_data
 EOF
 
 	for object in "$libdir"/libspdk_*.so; do
@@ -132,8 +136,6 @@ EOF
 			fi
 
 			if [[ $so_name_changed == yes ]]; then
-				# After 21.01 LTS all SO major versions were intentionally increased. Disable this check until SPDK 21.04 release.
-				found_abi_change=true
 				if ! $found_abi_change; then
 					echo "SO name for $so_file changed without a change to abi. please revert that change."
 					touch $fail_file
@@ -156,7 +158,7 @@ EOF
 			fi
 
 			if ((abidiff_output == 1)); then
-				"${cmd_args[@]}" --impacted-interfaces
+				"${cmd_args[@]}" --impacted-interfaces || :
 			fi
 		fi
 		processed_so=$((processed_so + 1))
@@ -217,7 +219,7 @@ function confirm_deps() {
 	if [ "$diff" != "" ]; then
 		touch $fail_file
 		echo "there was a dependency mismatch in the library $lib_shortname"
-		echo "The makefile lists: '${lib_make_deps[*]}'"
+		echo "The makefile (spdk.lib_deps.mk) lists: '${lib_make_deps[*]}'"
 		echo "readelf outputs   : '${dep_names[*]}'"
 		echo "---------------------------------------------------------------------"
 	fi
diff --git a/test/nvme/Makefile b/test/nvme/Makefile
index b2ed73a09..0edfb3478 100644
--- a/test/nvme/Makefile
+++ b/test/nvme/Makefile
@@ -35,7 +35,7 @@ SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
 DIRS-y = aer reset sgl e2edp overhead deallocated_value err_injection \
-	startup reserve
+	startup reserve simple_copy
 DIRS-$(CONFIG_NVME_CUSE) += cuse
 
 .PHONY: all clean $(DIRS-y)
diff --git a/test/nvme/aer/aer.c b/test/nvme/aer/aer.c
index 3316f7515..e2674b54a 100644
--- a/test/nvme/aer/aer.c
+++ b/test/nvme/aer/aer.c
@@ -42,8 +42,9 @@
 
 struct dev {
 	struct spdk_nvme_ctrlr				*ctrlr;
+	/* Expected changed NS ID state before AER */
+	bool						ns_test_active;
 	struct spdk_nvme_health_information_page	*health_page;
-	struct spdk_nvme_ns_list			*changed_ns_list;
 	uint32_t					orig_temp_threshold;
 	char						name[SPDK_NVMF_TRADDR_MAX_LEN + 1];
 };
@@ -65,9 +66,7 @@ static char *g_touch_file;
 
 /* Enable AER temperature test */
 static int g_enable_temp_test = 0;
-/* Enable AER namespace attribute notice test, this variable holds
- * the NSID that is expected to be in the Changed NS List.
- */
+/* Expected changed NS ID */
 static uint32_t g_expected_ns_test = 0;
 
 static void
@@ -167,43 +166,6 @@ get_health_log_page_completion(void *cb_arg, const struct spdk_nvme_cpl *cpl)
 	g_aer_done++;
 }
 
-static void
-get_changed_ns_log_page_completion(void *cb_arg, const struct spdk_nvme_cpl *cpl)
-{
-	struct dev *dev = cb_arg;
-	bool found = false;
-	uint32_t i;
-
-	g_outstanding_commands --;
-
-	if (spdk_nvme_cpl_is_error(cpl)) {
-		printf("%s: get log page failed\n", dev->name);
-		g_failed = 1;
-		return;
-	}
-
-	/* Let's compare the expected namespce ID is
-	 * in changed namespace list
-	 */
-	if (dev->changed_ns_list->ns_list[0] != 0xffffffffu) {
-		for (i = 0; i < sizeof(*dev->changed_ns_list) / sizeof(uint32_t); i++) {
-			if (g_expected_ns_test == dev->changed_ns_list->ns_list[i]) {
-				printf("%s: changed NS list contains expected NSID: %u\n",
-				       dev->name, g_expected_ns_test);
-				found = true;
-				break;
-			}
-		}
-	}
-
-	if (!found) {
-		printf("%s: Error: Can't find expected NSID %u\n", dev->name, g_expected_ns_test);
-		g_failed = 1;
-	}
-
-	g_aer_done++;
-}
-
 static int
 get_health_log_page(struct dev *dev)
 {
@@ -220,21 +182,15 @@ get_health_log_page(struct dev *dev)
 	return rc;
 }
 
-static int
-get_changed_ns_log_page(struct dev *dev)
+static void
+get_ns_state_test(struct dev *dev, uint32_t nsid)
 {
-	int rc;
-
-	rc = spdk_nvme_ctrlr_cmd_get_log_page(dev->ctrlr, SPDK_NVME_LOG_CHANGED_NS_LIST,
-					      SPDK_NVME_GLOBAL_NS_TAG, dev->changed_ns_list,
-					      sizeof(*dev->changed_ns_list), 0,
-					      get_changed_ns_log_page_completion, dev);
+	bool new_ns_state;
 
-	if (rc == 0) {
-		g_outstanding_commands++;
+	new_ns_state = spdk_nvme_ctrlr_is_active_ns(dev->ctrlr, nsid);
+	if (new_ns_state == dev->ns_test_active) {
+		g_failed = 1;
 	}
-
-	return rc;
 }
 
 static void
@@ -246,9 +202,6 @@ cleanup(void)
 		if (dev->health_page) {
 			spdk_free(dev->health_page);
 		}
-		if (dev->changed_ns_list) {
-			spdk_free(dev->changed_ns_list);
-		}
 	}
 }
 
@@ -273,7 +226,8 @@ aer_cb(void *arg, const struct spdk_nvme_cpl *cpl)
 		set_temp_threshold(dev, dev->orig_temp_threshold);
 		get_health_log_page(dev);
 	} else if (log_page_id == SPDK_NVME_LOG_CHANGED_NS_LIST) {
-		get_changed_ns_log_page(dev);
+		get_ns_state_test(dev, g_expected_ns_test);
+		g_aer_done++;
 	}
 }
 
@@ -387,12 +341,6 @@ attach_cb(void *cb_ctx, const struct spdk_nvme_transport_id *trid,
 		printf("Allocation error (health page)\n");
 		g_failed = 1;
 	}
-	dev->changed_ns_list = spdk_zmalloc(sizeof(*dev->changed_ns_list), 4096, NULL,
-					    SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA);
-	if (dev->changed_ns_list == NULL) {
-		printf("Allocation error (changed namespace list page)\n");
-		g_failed = 1;
-	}
 }
 
 static void
@@ -501,6 +449,7 @@ spdk_aer_changed_ns_test(void)
 
 	foreach_dev(dev) {
 		get_feature_test(dev);
+		dev->ns_test_active = spdk_nvme_ctrlr_is_active_ns(dev->ctrlr, g_expected_ns_test);
 	}
 
 	if (g_failed) {
diff --git a/test/nvme/cuse/cuse.c b/test/nvme/cuse/cuse.c
index 9a1c38562..9999c51c2 100644
--- a/test/nvme/cuse/cuse.c
+++ b/test/nvme/cuse/cuse.c
@@ -47,6 +47,10 @@ DEFINE_STUB(spdk_nvme_ctrlr_cmd_admin_raw, int, (struct spdk_nvme_ctrlr *ctrlr,
 		struct spdk_nvme_cmd *cmd, void *buf, uint32_t len,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
 
+DEFINE_STUB(spdk_nvme_ctrlr_cmd_io_raw, int, (struct spdk_nvme_ctrlr *ctrlr,
+		struct spdk_nvme_qpair *qpair, struct spdk_nvme_cmd *cmd, void *buf, uint32_t len,
+		spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
+
 DEFINE_STUB(spdk_nvme_ctrlr_get_num_ns, uint32_t, (struct spdk_nvme_ctrlr *ctrlr), 128);
 
 static uint32_t g_active_num_ns = 4;
@@ -60,6 +64,8 @@ spdk_nvme_ctrlr_is_active_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid)
 
 DEFINE_STUB(spdk_nvme_ctrlr_reset, int, (struct spdk_nvme_ctrlr *ctrlr), 0);
 
+DEFINE_STUB(spdk_nvme_ctrlr_reset_subsystem, int, (struct spdk_nvme_ctrlr *ctrlr), 0);
+
 DEFINE_STUB(spdk_nvme_ns_cmd_read, int, (struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 		void *payload,
 		uint64_t lba, uint32_t lba_count, spdk_nvme_cmd_cb cb_fn, void *cb_arg,
@@ -85,6 +91,8 @@ DEFINE_STUB(nvme_io_msg_ctrlr_register, int,
 DEFINE_STUB_V(nvme_io_msg_ctrlr_unregister,
 	      (struct spdk_nvme_ctrlr *ctrlr, struct nvme_io_msg_producer *io_msg_producer));
 
+DEFINE_STUB_V(nvme_ctrlr_update_namespaces, (struct spdk_nvme_ctrlr *ctrlr));
+
 static bool
 wait_for_file(char *filename, bool exists)
 {
diff --git a/test/nvme/nvme_scc.sh b/test/nvme/nvme_scc.sh
new file mode 100755
index 000000000..ce16f6e8a
--- /dev/null
+++ b/test/nvme/nvme_scc.sh
@@ -0,0 +1,12 @@
+#!/usr/bin/env bash
+
+testdir=$(readlink -f $(dirname $0))
+rootdir=$(readlink -f $testdir/../..)
+source $rootdir/scripts/common.sh
+source $rootdir/test/common/autotest_common.sh
+
+if [ $(uname) = Linux ]; then
+	$rootdir/scripts/setup.sh
+fi
+
+run_test "nvme_simple_copy" $testdir/simple_copy/simple_copy
diff --git a/test/nvme/perf/common.sh b/test/nvme/perf/common.sh
index f06e5141c..97951a059 100755
--- a/test/nvme/perf/common.sh
+++ b/test/nvme/perf/common.sh
@@ -85,7 +85,10 @@ function create_spdk_bdev_conf() {
 				{
 					"subsystem": "bdev",
 					"config": [
-						${bdev_json_cfg[*]}
+						${bdev_json_cfg[*]},
+					        {
+					                "method": "bdev_wait_for_examine"
+					        }
 					]
 				}
 			]
diff --git a/test/nvme/simple_copy/.gitignore b/test/nvme/simple_copy/.gitignore
new file mode 100644
index 000000000..e14dcbf9c
--- /dev/null
+++ b/test/nvme/simple_copy/.gitignore
@@ -0,0 +1 @@
+simple_copy
diff --git a/test/nvme/simple_copy/Makefile b/test/nvme/simple_copy/Makefile
new file mode 100644
index 000000000..021861b84
--- /dev/null
+++ b/test/nvme/simple_copy/Makefile
@@ -0,0 +1,38 @@
+#
+#  BSD LICENSE
+#
+#  Copyright (c) Samsung Electronics Co., Ltd.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Samsung Electronics Co., Ltd. nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../..)
+
+APP = simple_copy
+
+include $(SPDK_ROOT_DIR)/mk/nvme.libtest.mk
diff --git a/test/nvme/simple_copy/simple_copy.c b/test/nvme/simple_copy/simple_copy.c
new file mode 100644
index 000000000..585833647
--- /dev/null
+++ b/test/nvme/simple_copy/simple_copy.c
@@ -0,0 +1,444 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Samsung Electronics Co., Ltd.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Samsung Electronics Co., Ltd. nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/stdinc.h"
+#include "spdk/nvme.h"
+#include "spdk/env.h"
+
+#define NUM_LBAS 64
+#define DEST_LBA 256
+
+struct ns_entry {
+	struct spdk_nvme_ctrlr	*ctrlr;
+	struct spdk_nvme_ns	*ns;
+	struct ns_entry		*next;
+	struct spdk_nvme_qpair	*qpair;
+};
+
+struct simple_copy_context {
+	struct ns_entry	*ns_entry;
+	char		**write_bufs;
+	char		**read_bufs;
+	int		writes_completed;
+	int		reads_completed;
+	int		simple_copy_completed;
+	int		matches_written_data;
+	int		error;
+};
+
+static struct ns_entry *g_namespaces = NULL;
+
+static void cleanup(struct simple_copy_context *context);
+
+static void
+fill_random(char *buf, size_t num_bytes)
+{
+	size_t	i;
+
+	srand((unsigned) time(NULL));
+	for (i = 0; i < num_bytes; i++) {
+		buf[i] = rand() % 0x100;
+	}
+}
+
+static void
+register_ns(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns *ns)
+{
+	struct ns_entry				*entry;
+	const struct spdk_nvme_ctrlr_data	*cdata;
+
+	cdata = spdk_nvme_ctrlr_get_data(ctrlr);
+
+	if (!spdk_nvme_ns_is_active(ns)) {
+		printf("Controller %-20.20s (%-20.20s): Skipping inactive NS %u\n",
+		       cdata->mn, cdata->sn,
+		       spdk_nvme_ns_get_id(ns));
+		return;
+	}
+
+	entry = malloc(sizeof(struct ns_entry));
+	if (entry == NULL) {
+		perror("ns_entry malloc");
+		exit(1);
+	}
+
+	entry->ctrlr = ctrlr;
+	entry->ns = ns;
+	entry->next = g_namespaces;
+	g_namespaces = entry;
+
+	printf("  Namespace ID: %d size: %juGB\n", spdk_nvme_ns_get_id(ns),
+	       spdk_nvme_ns_get_size(ns) / 1000000000);
+}
+
+static uint32_t
+get_max_block_size(void)
+{
+	struct ns_entry	*ns;
+	uint32_t	max_block_size, temp_block_size;
+
+	ns = g_namespaces;
+	max_block_size = 0;
+
+	while (ns != NULL) {
+		temp_block_size = spdk_nvme_ns_get_sector_size(ns->ns);
+		max_block_size = temp_block_size > max_block_size ? temp_block_size : max_block_size;
+		ns = ns->next;
+	}
+
+	return max_block_size;
+}
+
+static void
+write_complete(void *arg, const struct spdk_nvme_cpl *cpl)
+{
+	struct simple_copy_context	*context = arg;
+
+	context->writes_completed++;
+
+	if (spdk_nvme_cpl_is_error(cpl)) {
+		printf("write cpl error. SC 0x%x SCT 0x%x\n", cpl->status.sc, cpl->status.sct);
+		context->error++;
+		return;
+	}
+}
+
+static void
+read_complete(void *arg, const struct spdk_nvme_cpl *cpl)
+{
+	struct simple_copy_context	*context = arg;
+	struct ns_entry			*ns_entry = context->ns_entry;
+	int				rc;
+
+	if (spdk_nvme_cpl_is_error(cpl)) {
+		printf("read cpl error. SC 0x%x SCT 0x%x\n", cpl->status.sc, cpl->status.sct);
+		context->reads_completed++;
+		context->error++;
+		return;
+	}
+
+	rc = memcmp(context->write_bufs[context->reads_completed],
+		    context->read_bufs[context->reads_completed], spdk_nvme_ns_get_sector_size(ns_entry->ns));
+	if (rc == 0) {
+		context->matches_written_data++;
+	}
+
+	context->reads_completed++;
+}
+
+static void
+simple_copy_complete(void *arg, const struct spdk_nvme_cpl *cpl)
+{
+	struct simple_copy_context	*context = arg;
+
+	context->simple_copy_completed = 1;
+
+	if (spdk_nvme_cpl_is_error(cpl)) {
+		printf("scc cpl error. SC 0x%x SCT 0x%x\n", cpl->status.sc, cpl->status.sct);
+		context->error++;
+		return;
+	}
+
+	printf("Copied LBAs from 0 - %d to the Destination LBA %d\n", NUM_LBAS - 1, DEST_LBA);
+	context->reads_completed = 0;
+	context->matches_written_data = 0;
+}
+
+static void
+simple_copy_test(void)
+{
+	struct ns_entry				*ns_entry;
+	struct spdk_nvme_ctrlr			*ctrlr;
+	const struct spdk_nvme_ctrlr_data	*data;
+	struct simple_copy_context		context;
+	struct spdk_nvme_scc_source_range	range;
+	uint32_t				max_block_size;
+	int					rc, i;
+
+	memset(&context, 0, sizeof(struct simple_copy_context));
+	max_block_size = get_max_block_size();
+	ns_entry = g_namespaces;
+
+	context.write_bufs = calloc(NUM_LBAS, sizeof(char *));
+	if (context.write_bufs == NULL) {
+		printf("could not allocate write buffer pointers for test\n");
+		cleanup(&context);
+		return;
+	}
+
+	context.read_bufs = calloc(NUM_LBAS, sizeof(char *));
+	if (context.read_bufs == NULL) {
+		printf("could not allocate read buffer pointers for test\n");
+		cleanup(&context);
+		return;
+	}
+
+	for (i = 0; i < NUM_LBAS; i++) {
+		context.write_bufs[i] = spdk_zmalloc(0x1000, max_block_size, NULL, SPDK_ENV_LCORE_ID_ANY,
+						     SPDK_MALLOC_DMA);
+		if (context.write_bufs[i] == NULL) {
+			printf("could not allocate write buffer %d for test\n", i);
+			cleanup(&context);
+			return;
+		}
+
+		fill_random(context.write_bufs[i], 0x1000);
+		context.read_bufs[i] = spdk_zmalloc(0x1000, max_block_size, NULL, SPDK_ENV_LCORE_ID_ANY,
+						    SPDK_MALLOC_DMA);
+		if (context.read_bufs[i] == NULL) {
+			printf("could not allocate read buffer %d for test\n", i);
+			cleanup(&context);
+			return;
+		}
+	}
+
+	while (ns_entry != NULL) {
+
+		ns_entry->qpair = spdk_nvme_ctrlr_alloc_io_qpair(ns_entry->ctrlr, NULL, 0);
+		if (ns_entry->qpair == NULL) {
+			printf("ERROR: spdk_nvme_ctrlr_alloc_io_qpair() failed\n");
+			cleanup(&context);
+			return;
+		}
+
+		ctrlr = spdk_nvme_ns_get_ctrlr(ns_entry->ns);
+		data = spdk_nvme_ctrlr_get_data(ctrlr);
+
+		printf("\nController %-20.20s (%-20.20s)\n", data->mn, data->sn);
+		printf("Controller PCI vendor:%u PCI subsystem vendor:%u\n", data->vid, data->ssvid);
+		printf("Namespace Block Size:%u\n", spdk_nvme_ns_get_sector_size(ns_entry->ns));
+		printf("Writing LBAs 0 to %d with Random Data\n", NUM_LBAS - 1);
+
+		context.ns_entry = ns_entry;
+
+		for (i = 0; i < NUM_LBAS; i++) {
+			rc = spdk_nvme_ns_cmd_write(ns_entry->ns, ns_entry->qpair, context.write_bufs[i],
+						    i,
+						    1,
+						    write_complete, &context, 0);
+			if (rc) {
+				printf("submission of write I/O failed\n");
+			}
+		}
+		while (context.writes_completed < NUM_LBAS) {
+			rc = spdk_nvme_qpair_process_completions(ns_entry->qpair, 0);
+			if (rc < 0) {
+				printf("Error processing write completions, rc: %d\n", rc);
+				break;
+			}
+		}
+
+		if (context.error) {
+			printf("Error : %d Write completions failed\n",
+			       context.error);
+			spdk_nvme_ctrlr_free_io_qpair(ns_entry->qpair);
+			cleanup(&context);
+			exit(1);
+		}
+
+		range.nlb = NUM_LBAS - 1;
+		range.slba = 0;
+
+		rc = spdk_nvme_ns_cmd_copy(ns_entry->ns, ns_entry->qpair,
+					   &range, 1, DEST_LBA, simple_copy_complete, &context);
+
+		if (rc) {
+			printf("submission of copy I/O failed\n");
+		}
+
+		while (!context.simple_copy_completed) {
+			rc = spdk_nvme_qpair_process_completions(ns_entry->qpair, 0);
+			if (rc < 0) {
+				printf("Error processing copy completions, rc: %d\n", rc);
+				break;
+			}
+		}
+
+		if (context.error) {
+			printf("Error : Copy completion failed\n");
+			spdk_nvme_ctrlr_free_io_qpair(ns_entry->qpair);
+			cleanup(&context);
+			exit(1);
+		}
+
+		for (i = 0; i < NUM_LBAS; i++) {
+			rc = spdk_nvme_ns_cmd_read(ns_entry->ns, ns_entry->qpair, context.read_bufs[i],
+						   DEST_LBA + i, /* LBA start */
+						   1, /* number of LBAs */
+						   read_complete, &context, 0);
+			if (rc) {
+				printf("submission of read I/O failed\n");
+			}
+			/* block after each read command so that we can match the block to the write buffer. */
+			while (context.reads_completed <= i) {
+				rc = spdk_nvme_qpair_process_completions(ns_entry->qpair, 0);
+				if (rc < 0) {
+					printf("Error processing read completions, rc: %d\n", rc);
+					break;
+				}
+			}
+		}
+
+		if (context.error) {
+			printf("Error : %d Read completions failed\n",
+			       context.error);
+			spdk_nvme_ctrlr_free_io_qpair(ns_entry->qpair);
+			cleanup(&context);
+			exit(1);
+		}
+
+		printf("LBAs matching Written Data: %d\n", context.matches_written_data);
+
+		if (context.matches_written_data != NUM_LBAS) {
+			printf("Error : %d LBAs are copied correctly out of %d LBAs\n",
+			       context.matches_written_data, NUM_LBAS);
+			spdk_nvme_ctrlr_free_io_qpair(ns_entry->qpair);
+			cleanup(&context);
+			exit(1);
+		}
+
+		/* reset counters in between each namespace. */
+		context.matches_written_data = 0;
+		context.writes_completed = 0;
+		context.reads_completed = 0;
+		context.simple_copy_completed = 0;
+
+		spdk_nvme_ctrlr_free_io_qpair(ns_entry->qpair);
+		ns_entry = ns_entry->next;
+	}
+	cleanup(&context);
+}
+
+static bool
+probe_cb(void *cb_ctx, const struct spdk_nvme_transport_id *trid,
+	 struct spdk_nvme_ctrlr_opts *opts)
+{
+	printf("Attaching to %s\n", trid->traddr);
+
+	return true;
+}
+
+static void
+attach_cb(void *cb_ctx, const struct spdk_nvme_transport_id *trid,
+	  struct spdk_nvme_ctrlr *ctrlr, const struct spdk_nvme_ctrlr_opts *opts)
+{
+	int			num_ns;
+	struct spdk_nvme_ns	*ns;
+	const struct spdk_nvme_ctrlr_data	*cdata;
+
+	cdata = spdk_nvme_ctrlr_get_data(ctrlr);
+
+	if (cdata->oncs.copy) {
+		printf("Controller supports SCC. Attached to %s\n", trid->traddr);
+		/*
+		 * Use only the first namespace from each controller since we are testing controller level functionality.
+		 */
+		num_ns = spdk_nvme_ctrlr_get_num_ns(ctrlr);
+		if (num_ns < 1) {
+			printf("No valid namespaces in controller\n");
+		} else {
+			ns = spdk_nvme_ctrlr_get_ns(ctrlr, 1);
+			register_ns(ctrlr, ns);
+		}
+	} else {
+		printf("Controller doesn't support SCC. Not Attached to %s\n", trid->traddr);
+	}
+}
+
+static void
+cleanup(struct simple_copy_context *context)
+{
+	struct ns_entry	*ns_entry = g_namespaces;
+	struct spdk_nvme_detach_ctx *detach_ctx = NULL;
+	int		i;
+
+	while (ns_entry) {
+		struct ns_entry *next = ns_entry->next;
+
+		detach_ctx = NULL;
+		spdk_nvme_detach_async(ns_entry->ctrlr, &detach_ctx);
+
+		while (detach_ctx && spdk_nvme_detach_poll_async(detach_ctx) == -EAGAIN) {
+			;
+		}
+
+		free(ns_entry);
+		ns_entry = next;
+	}
+	for (i = 0; i < NUM_LBAS; i++) {
+		if (context->write_bufs && context->write_bufs[i]) {
+			spdk_free(context->write_bufs[i]);
+		} else {
+			break;
+		}
+		if (context->read_bufs && context->read_bufs[i]) {
+			spdk_free(context->read_bufs[i]);
+		} else {
+			break;
+		}
+	}
+
+	free(context->write_bufs);
+	free(context->read_bufs);
+}
+
+int main(int argc, char **argv)
+{
+	int			rc;
+	struct spdk_env_opts	opts;
+
+	spdk_env_opts_init(&opts);
+	opts.name = "simple_copy";
+	opts.shm_id = 0;
+	if (spdk_env_init(&opts) < 0) {
+		fprintf(stderr, "Unable to initialize SPDK env\n");
+		return 1;
+	}
+
+	printf("Initializing NVMe Controllers\n");
+
+	rc = spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL);
+	if (rc != 0) {
+		fprintf(stderr, "spdk_nvme_probe() failed\n");
+		return 1;
+	}
+
+	if (g_namespaces == NULL) {
+		fprintf(stderr, "no NVMe controllers found\n");
+		return 1;
+	}
+
+	printf("Initialization complete.\n");
+	simple_copy_test();
+	return 0;
+}
diff --git a/test/nvmf/common.sh b/test/nvmf/common.sh
index bf3dc2746..db7113a80 100644
--- a/test/nvmf/common.sh
+++ b/test/nvmf/common.sh
@@ -280,8 +280,6 @@ function nvmf_veth_init() {
 	"${NVMF_TARGET_NS_CMD[@]}" ip link delete $NVMF_TARGET_INTERFACE2 || true
 	ip netns del $NVMF_TARGET_NAMESPACE || true
 
-	trap 'nvmf_veth_fini; exit 1' SIGINT SIGTERM EXIT
-
 	# Create network namespace
 	ip netns add $NVMF_TARGET_NAMESPACE
 
@@ -368,8 +366,6 @@ function nvmf_tcp_init() {
 	ip -4 addr flush $NVMF_TARGET_INTERFACE || true
 	ip -4 addr flush $NVMF_INITIATOR_INTERFACE || true
 
-	trap 'nvmf_tcp_fini; exit 1' SIGINT SIGTERM
-
 	# Create network namespace
 	ip netns add $NVMF_TARGET_NAMESPACE
 
@@ -401,8 +397,10 @@ function nvmf_tcp_fini() {
 		nvmf_veth_fini
 		return 0
 	fi
-	ip netns del $NVMF_TARGET_NAMESPACE
-	ip -4 addr flush $NVMF_INITIATOR_INTERFACE
+	if [[ -n $NVMF_TARGET_NAMESPACE && -e /var/run/netns/$NVMF_TARGET_NAMESPACE ]]; then
+		ip netns del $NVMF_TARGET_NAMESPACE
+	fi
+	ip -4 addr flush $NVMF_INITIATOR_INTERFACE || :
 }
 
 function nvmftestinit() {
@@ -410,6 +408,9 @@ function nvmftestinit() {
 		echo "transport not specified - use --transport= to specify"
 		return 1
 	fi
+
+	trap 'process_shm --id $NVMF_APP_SHM_ID || :; nvmftestfini' SIGINT SIGTERM EXIT
+
 	if [ "$TEST_MODE" == "iso" ]; then
 		$rootdir/scripts/setup.sh
 		if [[ "$TEST_TRANSPORT" == "rdma" ]]; then
@@ -448,7 +449,6 @@ function nvmfappstart() {
 	timing_enter start_nvmf_tgt
 	"${NVMF_APP[@]}" "$@" &
 	nvmfpid=$!
-	trap 'process_shm --id $NVMF_APP_SHM_ID; nvmftestfini; exit 1' SIGINT SIGTERM EXIT
 	waitforlisten $nvmfpid
 	timing_exit start_nvmf_tgt
 }
@@ -507,19 +507,13 @@ function nvme_connect() {
 }
 
 function get_nvme_devs() {
-	local dev rest
+	local dev _
 
-	nvmes=()
-	while read -r dev rest; do
+	while read -r dev _; do
 		if [[ $dev == /dev/nvme* ]]; then
-			nvmes+=("$dev")
-		fi
-		if [[ $1 == print ]]; then
-			echo "$dev $rest"
+			echo "$dev"
 		fi
 	done < <(nvme list)
-	((${#nvmes[@]})) || return 1
-	echo "${#nvmes[@]}" >&2
 }
 
 function gen_nvmf_target_json() {
@@ -573,7 +567,10 @@ function gen_nvmf_target_json() {
 		        $(
 		IFS=","
 		printf '%s\n' "${config[*]}"
-		)
+		),
+			{
+			  "method": "bdev_wait_for_examine"
+			}
 		      ]
 		    }
 		  ]
diff --git a/test/nvmf/host/target_disconnect.sh b/test/nvmf/host/target_disconnect.sh
index 22c077afb..27bb2a3bc 100755
--- a/test/nvmf/host/target_disconnect.sh
+++ b/test/nvmf/host/target_disconnect.sh
@@ -18,7 +18,7 @@ function disconnect_init() {
 	$rpc_py bdev_malloc_create $MALLOC_BDEV_SIZE $MALLOC_BLOCK_SIZE -b Malloc0
 
 	$rpc_py nvmf_create_transport $NVMF_TRANSPORT_OPTS
-	$rpc_py nvmf_subsystem_create nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001
+	$rpc_py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001
 
 	$rpc_py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0
 	$rpc_py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a $1 -s $NVMF_PORT
diff --git a/test/nvmf/target/dif.sh b/test/nvmf/target/dif.sh
index 7d82edb15..b08bace1e 100755
--- a/test/nvmf/target/dif.sh
+++ b/test/nvmf/target/dif.sh
@@ -15,7 +15,7 @@ create_subsystem() {
 
 	# Make sure NQN matches what's used in gen_nvmf_target_json()
 	rpc_cmd bdev_null_create "bdev_null$sub_id" "$NULL_SIZE" "$NULL_BLOCK_SIZE" --md-size "$NULL_META" --dif-type "$NULL_DIF"
-	rpc_cmd nvmf_subsystem_create "nqn.2016-06.io.spdk:cnode$sub_id" --serial-number "53313233-$sub_id" --allow-any-host
+	rpc_cmd nvmf_create_subsystem "nqn.2016-06.io.spdk:cnode$sub_id" --serial-number "53313233-$sub_id" --allow-any-host
 	rpc_cmd nvmf_subsystem_add_ns "nqn.2016-06.io.spdk:cnode$sub_id" "bdev_null$sub_id"
 	rpc_cmd nvmf_subsystem_add_listener "nqn.2016-06.io.spdk:cnode$sub_id" -t "$TEST_TRANSPORT" -a "$NVMF_FIRST_TARGET_IP" -s "$NVMF_PORT"
 }
diff --git a/test/nvmf/target/fio.sh b/test/nvmf/target/fio.sh
index 4e98d7083..d50853155 100755
--- a/test/nvmf/target/fio.sh
+++ b/test/nvmf/target/fio.sh
@@ -35,15 +35,15 @@ nvme connect -t $TEST_TRANSPORT -n "nqn.2016-06.io.spdk:cnode1" -a "$NVMF_FIRST_
 
 waitforserial $NVMF_SERIAL 3
 
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 1 -t write -r 1 -v
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 1 -t randwrite -r 1 -v
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 128 -t write -r 1 -v
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 128 -t randwrite -r 1 -v
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 1 -t write -r 1 -v
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 1 -t randwrite -r 1 -v
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 128 -t write -r 1 -v
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 128 -t randwrite -r 1 -v
 
 sync
 
 #start hotplug test case
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 1 -t read -r 10 &
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 1 -t read -r 10 &
 fio_pid=$!
 
 sleep 3
diff --git a/test/nvmf/target/initiator_timeout.sh b/test/nvmf/target/initiator_timeout.sh
index 199983be5..12c759c64 100755
--- a/test/nvmf/target/initiator_timeout.sh
+++ b/test/nvmf/target/initiator_timeout.sh
@@ -30,7 +30,7 @@ nvme connect -t $TEST_TRANSPORT -n "nqn.2016-06.io.spdk:cnode1" -a "$NVMF_FIRST_
 waitforserial "$NVMF_SERIAL"
 
 # Once our timed out I/O complete, we will still have 10 sec of I/O.
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 1 -t write -r 60 -v &
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 1 -t write -r 60 -v &
 fio_pid=$!
 
 sleep 3
diff --git a/test/nvmf/target/invalid.sh b/test/nvmf/target/invalid.sh
index 98246efeb..a15115766 100755
--- a/test/nvmf/target/invalid.sh
+++ b/test/nvmf/target/invalid.sh
@@ -55,6 +55,31 @@ out=$("$rpc" nvmf_create_subsystem -s "$(gen_random_s 21)" "$nqn$RANDOM" 2>&1) &
 out=$("$rpc" nvmf_create_subsystem -d "$(gen_random_s 41)" "$nqn$RANDOM" 2>&1) && false
 [[ $out == *"Invalid MN"* ]]
 
+# Attempt to delete non-existing subsytem listener
+$rpc nvmf_create_transport --trtype "$TEST_TRANSPORT"
+$rpc nvmf_create_subsystem $nqn -s SPDK001 -a
+if [[ $TEST_TRANSPORT == "TCP" ]]; then
+	IP="127.0.0.1"
+else
+	IP=$(echo "$RDMA_IP_LIST" | head -n 1)
+fi
+out=$("$rpc" nvmf_subsystem_remove_listener "$nqn" -t "$TEST_TRANSPORT" -a "$IP" -s 4421 2>&1) && false
+[[ $out != *"Unable to stop listener."* ]]
+
+# Attempt to create subsystem with invalid controller ID range - outside [1, 0xffef]
+out=$("$rpc" nvmf_create_subsystem "$nqn$RANDOM" -i 0 2>&1) && false
+[[ $out == *"Invalid cntlid range"* ]]
+out=$("$rpc" nvmf_create_subsystem "$nqn$RANDOM" -i 65520 2>&1) && false
+[[ $out == *"Invalid cntlid range"* ]]
+out=$("$rpc" nvmf_create_subsystem "$nqn$RANDOM" -I 0 2>&1) && false
+[[ $out == *"Invalid cntlid range"* ]]
+out=$("$rpc" nvmf_create_subsystem "$nqn$RANDOM" -I 65520 2>&1) && false
+[[ $out == *"Invalid cntlid range"* ]]
+
+# Attempt to create subsystem with invalid controller ID range - [x, y] where x>y
+out=$("$rpc" nvmf_create_subsystem "$nqn$RANDOM" -i 6 -I 5 2>&1) && false
+[[ $out == *"Invalid cntlid range"* ]]
+
 # Attempt to delete non-existing target
 out=$("$multi_target_rpc" nvmf_delete_target --name "$target" 2>&1) && false
 [[ $out == *"The specified target doesn't exist, cannot delete it."* ]]
diff --git a/test/nvmf/target/multiconnection.sh b/test/nvmf/target/multiconnection.sh
index d7e490861..9befe2543 100755
--- a/test/nvmf/target/multiconnection.sh
+++ b/test/nvmf/target/multiconnection.sh
@@ -37,8 +37,8 @@ for i in $(seq 1 $NVMF_SUBSYS); do
 	waitforserial SPDK$i
 done
 
-$rootdir/scripts/fio.py -p nvmf -i 262144 -d 64 -t read -r 10
-$rootdir/scripts/fio.py -p nvmf -i 262144 -d 64 -t randwrite -r 10
+$rootdir/scripts/fio-wrapper -p nvmf -i 262144 -d 64 -t read -r 10
+$rootdir/scripts/fio-wrapper -p nvmf -i 262144 -d 64 -t randwrite -r 10
 
 sync
 for i in $(seq 1 $NVMF_SUBSYS); do
diff --git a/test/nvmf/target/multipath.sh b/test/nvmf/target/multipath.sh
index 8594b5226..d58293bec 100755
--- a/test/nvmf/target/multipath.sh
+++ b/test/nvmf/target/multipath.sh
@@ -10,6 +10,21 @@ MALLOC_BLOCK_SIZE=512
 
 rpc_py="$rootdir/scripts/rpc.py"
 
+function check_ana_state() {
+	local subsys_id=$1
+	local ctrl_id=$2
+	local ana_state=$3
+	local timeout=3
+
+	while [ $(cat /sys/block/nvme"$subsys_id"c"$ctrl_id"n1/ana_state) != "$ana_state" ]; do
+		sleep 1
+		if ((timeout-- == 0)); then
+			echo "timeout before ANA state (nvme$subsys_id c$ctrl_id) becomes $ana_state"
+			return 1
+		fi
+	done
+}
+
 nvmftestinit
 
 if [ -z $NVMF_SECOND_TARGET_IP ]; then
@@ -44,13 +59,13 @@ subsys_id=$(nvme list-subsys | sed -n 's/nqn.2016-06.io.spdk:cnode1//p' | sed 's
 ctrl1_id=$(nvme list-subsys | sed -n "s/traddr=$NVMF_FIRST_TARGET_IP trsvcid=$NVMF_PORT//p" | sed 's/[^0-9]*//g')
 ctrl2_id=$(nvme list-subsys | sed -n "s/traddr=$NVMF_SECOND_TARGET_IP trsvcid=$NVMF_PORT//p" | sed 's/[^0-9]*//g')
 
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl1_id"n1/ana_state) == "optimized" ]
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl2_id"n1/ana_state) == "optimized" ]
+check_ana_state "$subsys_id" "$ctrl1_id" "optimized"
+check_ana_state "$subsys_id" "$ctrl2_id" "optimized"
 
 # Set IO policy to numa
 echo numa > /sys/class/nvme-subsystem/nvme-subsys$subsys_id/iopolicy
 
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 128 -t randrw -r 6 -v &
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 128 -t randrw -r 6 -v &
 fio_pid=$!
 
 sleep 1
@@ -58,33 +73,27 @@ sleep 1
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_FIRST_TARGET_IP" -s "$NVMF_PORT" -n inaccessible
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_SECOND_TARGET_IP" -s "$NVMF_PORT" -n non_optimized
 
-sleep 1
-
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl1_id"n1/ana_state) == "inaccessible" ]
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl2_id"n1/ana_state) == "non-optimized" ]
+check_ana_state "$subsys_id" "$ctrl1_id" "inaccessible"
+check_ana_state "$subsys_id" "$ctrl2_id" "non-optimized"
 
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_FIRST_TARGET_IP" -s "$NVMF_PORT" -n non_optimized
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_SECOND_TARGET_IP" -s "$NVMF_PORT" -n inaccessible
 
-sleep 1
-
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl1_id"n1/ana_state) == "non-optimized" ]
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl2_id"n1/ana_state) == "inaccessible" ]
+check_ana_state "$subsys_id" "$ctrl1_id" "non-optimized"
+check_ana_state "$subsys_id" "$ctrl2_id" "inaccessible"
 
 wait $fio_pid
 
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_FIRST_TARGET_IP" -s "$NVMF_PORT" -n optimized
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_SECOND_TARGET_IP" -s "$NVMF_PORT" -n optimized
 
-sleep 1
-
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl1_id"n1/ana_state) == "optimized" ]
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl2_id"n1/ana_state) == "optimized" ]
+check_ana_state "$subsys_id" "$ctrl1_id" "optimized"
+check_ana_state "$subsys_id" "$ctrl2_id" "optimized"
 
 # Set IO policy to round-robin
 echo round-robin > /sys/class/nvme-subsystem/nvme-subsys$subsys_id/iopolicy
 
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 128 -t randrw -r 6 -v &
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 128 -t randrw -r 6 -v &
 fio_pid=$!
 
 sleep 1
@@ -92,18 +101,14 @@ sleep 1
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_FIRST_TARGET_IP" -s "$NVMF_PORT" -n inaccessible
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_SECOND_TARGET_IP" -s "$NVMF_PORT" -n non_optimized
 
-sleep 1
-
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl1_id"n1/ana_state) == "inaccessible" ]
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl2_id"n1/ana_state) == "non-optimized" ]
+check_ana_state "$subsys_id" "$ctrl1_id" "inaccessible"
+check_ana_state "$subsys_id" "$ctrl2_id" "non-optimized"
 
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_FIRST_TARGET_IP" -s "$NVMF_PORT" -n non_optimized
 $rpc_py nvmf_subsystem_listener_set_ana_state nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a "$NVMF_SECOND_TARGET_IP" -s "$NVMF_PORT" -n inaccessible
 
-sleep 1
-
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl1_id"n1/ana_state) == "non-optimized" ]
-[ $(cat /sys/block/nvme"$subsys_id"c"$ctrl2_id"n1/ana_state) == "inaccessible" ]
+check_ana_state "$subsys_id" "$ctrl1_id" "non-optimized"
+check_ana_state "$subsys_id" "$ctrl2_id" "inaccessible"
 
 wait $fio_pid
 
diff --git a/test/nvmf/target/nmic.sh b/test/nvmf/target/nmic.sh
index f8501343d..dd0b07479 100755
--- a/test/nvmf/target/nmic.sh
+++ b/test/nvmf/target/nmic.sh
@@ -42,7 +42,7 @@ nvme connect -t $TEST_TRANSPORT -n "nqn.2016-06.io.spdk:cnode1" -a "$NVMF_FIRST_
 
 waitforserial "$NVMF_SERIAL"
 
-$rootdir/scripts/fio.py -p nvmf -i 4096 -d 1 -t write -r 1 -v
+$rootdir/scripts/fio-wrapper -p nvmf -i 4096 -d 1 -t write -r 1 -v
 
 nvme disconnect -n "nqn.2016-06.io.spdk:cnode1"
 
diff --git a/test/nvmf/target/nvme_cli.sh b/test/nvmf/target/nvme_cli.sh
index 24ecd6853..9c6559e12 100755
--- a/test/nvmf/target/nvme_cli.sh
+++ b/test/nvmf/target/nvme_cli.sh
@@ -14,6 +14,7 @@ MALLOC_BDEV_SIZE=64
 MALLOC_BLOCK_SIZE=512
 
 rpc_py="$rootdir/scripts/rpc.py"
+devs=()
 
 nvmftestinit
 nvmfappstart -m 0xF
@@ -23,17 +24,17 @@ $rpc_py nvmf_create_transport $NVMF_TRANSPORT_OPTS -u 8192
 $rpc_py bdev_malloc_create $MALLOC_BDEV_SIZE $MALLOC_BLOCK_SIZE -b Malloc0
 $rpc_py bdev_malloc_create $MALLOC_BDEV_SIZE $MALLOC_BLOCK_SIZE -b Malloc1
 
-$rpc_py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s $NVMF_SERIAL -d SPDK_Controller1
+$rpc_py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s $NVMF_SERIAL -d SPDK_Controller1 -i 291
 $rpc_py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0
 $rpc_py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc1
 $rpc_py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t $TEST_TRANSPORT -a $NVMF_FIRST_TARGET_IP -s $NVMF_PORT
 
 nvme discover -t $TEST_TRANSPORT -a $NVMF_FIRST_TARGET_IP -s "$NVMF_PORT"
-nvme_num_before_connection=$(get_nvme_devs 2>&1 || echo 0)
+devs=($(get_nvme_devs)) nvme_num_before_connection=${#devs[@]}
 nvme connect -t $TEST_TRANSPORT -n "nqn.2016-06.io.spdk:cnode1" -a "$NVMF_FIRST_TARGET_IP" -s "$NVMF_PORT"
 
 waitforserial $NVMF_SERIAL 2
-if ! get_nvme_devs print 2> /dev/null; then
+if [[ -z $(get_nvme_devs) ]]; then
 	echo "Could not find any nvme devices to work with, aborting the test" >&2
 	exit 1
 fi
@@ -46,15 +47,20 @@ for ctrl in "${nvmes[@]}"; do
 		echo "Wrong model number for controller" $nvme_model
 		exit 1
 	fi
+	nvme_cntlid=$(nvme id-ctrl $ctrl | grep -w cntlid | sed 's/^.*: //' | sed 's/ *$//')
+	if [ "$nvme_cntlid" != "0x123" ]; then
+		echo "Wrong controller ID for controller" $nvme_model
+		exit 1
+	fi
 done
 
 for ns in "${nvmes[@]}"; do
 	nvme id-ns $ns
 done
 
-nvme_num=$(get_nvme_devs 2>&1)
+devs=($(get_nvme_devs)) nvme_num=${#devs[@]}
 nvme disconnect -n "nqn.2016-06.io.spdk:cnode1"
-if [ $nvme_num -le $nvme_num_before_connection ]; then
+if ((nvme_num <= nvme_num_before_connection)); then
 	echo "nvme-cli connect target devices failed"
 	exit 1
 fi
diff --git a/test/nvmf/target/nvmf_vfio_user.sh b/test/nvmf/target/nvmf_vfio_user.sh
index f74274236..d53ad7a7b 100755
--- a/test/nvmf/target/nvmf_vfio_user.sh
+++ b/test/nvmf/target/nvmf_vfio_user.sh
@@ -13,7 +13,7 @@ rpc_py="$rootdir/scripts/rpc.py"
 
 export TEST_TRANSPORT=VFIOUSER
 
-rm -rf /var/run/muser
+rm -rf /var/run/vfio-user
 
 # Start the target
 "${NVMF_APP[@]}" -m 0x1 &
@@ -25,32 +25,34 @@ waitforlisten $nvmfpid
 
 sleep 1
 
-$rpc_py nvmf_create_transport -t VFIOUSER
+$rpc_py nvmf_create_transport -t $TEST_TRANSPORT
 
-mkdir -p /var/run/muser
+mkdir -p /var/run/vfio-user
 
 for i in $(seq 1 $NUM_DEVICES); do
-	mkdir -p /var/run/muser/domain/muser$i/$i
+	mkdir -p /var/run/vfio-user/domain/vfio-user$i/$i
 
 	$rpc_py bdev_malloc_create $MALLOC_BDEV_SIZE $MALLOC_BLOCK_SIZE -b Malloc$i
 	$rpc_py nvmf_create_subsystem nqn.2019-07.io.spdk:cnode$i -a -s SPDK$i
 	$rpc_py nvmf_subsystem_add_ns nqn.2019-07.io.spdk:cnode$i Malloc$i
-	$rpc_py nvmf_subsystem_add_listener nqn.2019-07.io.spdk:cnode$i -t VFIOUSER -a "/var/run/muser/domain/muser$i/$i" -s 0
+	$rpc_py nvmf_subsystem_add_listener nqn.2019-07.io.spdk:cnode$i -t $TEST_TRANSPORT -a "/var/run/vfio-user/domain/vfio-user$i/$i" -s 0
 done
 
 for i in $(seq 1 $NUM_DEVICES); do
-	$SPDK_EXAMPLE_DIR/identify -r "trtype:VFIOUSER traddr:/var/run/muser/domain/muser$i/$i" -g -L nvme -L nvme_vfio -L vfio_pci
+	test_traddr=/var/run/vfio-user/domain/vfio-user$i/$i
+	test_subnqn=nqn.2019-07.io.spdk:cnode$i
+	$SPDK_EXAMPLE_DIR/identify -r "trtype:$TEST_TRANSPORT traddr:$test_traddr subnqn:$test_subnqn" -g -L nvme -L nvme_vfio -L vfio_pci
 	sleep 1
-	$SPDK_EXAMPLE_DIR/perf -r "trtype:VFIOUSER traddr:/var/run/muser/domain/muser$i/$i" -s 256 -g -q 128 -o 4096 -w read -t 5 -c 0x2
+	$SPDK_EXAMPLE_DIR/perf -r "trtype:$TEST_TRANSPORT traddr:$test_traddr subnqn:$test_subnqn" -s 256 -g -q 128 -o 4096 -w read -t 5 -c 0x2
 	sleep 1
-	$SPDK_EXAMPLE_DIR/perf -r "trtype:VFIOUSER traddr:/var/run/muser/domain/muser$i/$i" -s 256 -g -q 128 -o 4096 -w write -t 5 -c 0x2
+	$SPDK_EXAMPLE_DIR/perf -r "trtype:$TEST_TRANSPORT traddr:$test_traddr subnqn:$test_subnqn" -s 256 -g -q 128 -o 4096 -w write -t 5 -c 0x2
 	sleep 1
-	$SPDK_EXAMPLE_DIR/reconnect -r "trtype:VFIOUSER traddr:/var/run/muser/domain/muser$i/$i" -g -q 32 -o 4096 -w randrw -M 50 -t 5 -c 0xE
+	$SPDK_EXAMPLE_DIR/reconnect -r "trtype:$TEST_TRANSPORT traddr:$test_traddr subnqn:$test_subnqn" -g -q 32 -o 4096 -w randrw -M 50 -t 5 -c 0xE
 	sleep 1
 done
 
 killprocess $nvmfpid
 
-rm -rf /var/run/muser
+rm -rf /var/run/vfio-user
 
 trap - SIGINT SIGTERM EXIT
diff --git a/test/nvmf/target/srq_overwhelm.sh b/test/nvmf/target/srq_overwhelm.sh
index 98af97aab..832cbce08 100755
--- a/test/nvmf/target/srq_overwhelm.sh
+++ b/test/nvmf/target/srq_overwhelm.sh
@@ -31,7 +31,7 @@ done
 # working even at very high queue depths because the rdma qpair doesn't fail.
 # It is normal to see the initiator timeout and reconnect waiting for completions from an overwhelmmed target,
 # but the connection should come up and FIO should complete without errors.
-$rootdir/scripts/fio.py -p nvmf -i 1048576 -d 128 -t read -r 10 -n 13
+$rootdir/scripts/fio-wrapper -p nvmf -i 1048576 -d 128 -t read -r 10 -n 13
 
 sync
 
diff --git a/test/ocf/integrity/fio-modes.sh b/test/ocf/integrity/fio-modes.sh
index b3801d33c..b8a28501d 100755
--- a/test/ocf/integrity/fio-modes.sh
+++ b/test/ocf/integrity/fio-modes.sh
@@ -68,6 +68,16 @@ for ((d = 0, c = 1; d <= ${#ocf_names[@]} + 2; d += 2, c++)); do
 	)
 done
 
+config+=(
+	"$(
+		cat <<- JSON
+			{
+			  "method": "bdev_wait_for_examine"
+			}
+		JSON
+	)"
+)
+
 # First ']}' closes our config and bdev subsystem blocks
 cat <<- CONFIG > "$curdir/modes.conf"
 	{"subsystems":[
diff --git a/test/ocf/integrity/mallocs.conf b/test/ocf/integrity/mallocs.conf
index 245dd23cf..0e9b7993e 100644
--- a/test/ocf/integrity/mallocs.conf
+++ b/test/ocf/integrity/mallocs.conf
@@ -50,7 +50,10 @@ gen_malloc_ocf_json () {
 		    {
 		      "subsystem": "bdev",
 		      "config": [
-		        $(IFS=","; printf '%s\n' "${config[*]}")
+		        $(IFS=","; printf '%s\n' "${config[*]}"),
+		        {
+		          "method": "bdev_wait_for_examine"
+		        }
 		      ]
 		    }
 		  ]
diff --git a/test/ocf/management/configuration-change.sh b/test/ocf/management/configuration-change.sh
index e307d9b41..faccebc9d 100755
--- a/test/ocf/management/configuration-change.sh
+++ b/test/ocf/management/configuration-change.sh
@@ -5,6 +5,7 @@ rootdir=$(readlink -f $curdir/../../..)
 source $rootdir/test/common/autotest_common.sh
 
 rpc_py=$rootdir/scripts/rpc.py
+cache_line_sizes=(4 8 16 32 64)
 cache_modes=(wt wb pt wa wi wo)
 
 $SPDK_BIN_DIR/iscsi_tgt &
@@ -12,17 +13,43 @@ spdk_pid=$!
 
 waitforlisten $spdk_pid
 
-# Prepare OCF cache
+# Create OCF cache with different cache line sizes
+for cache_line_size in "${cache_line_sizes[@]}"; do
+	$rpc_py bdev_malloc_create 101 512 -b Malloc0
+	$rpc_py bdev_malloc_create 101 512 -b Malloc1
+	$rpc_py bdev_ocf_create Cache0 wt Malloc0 Malloc1 --cache-line-size $cache_line_size
+
+	$rpc_py bdev_ocf_get_bdevs | jq -e \
+		'.[0] | .started and .cache.attached and .core.attached'
+
+	# Check if cache line size values are reported correctly
+	$rpc_py bdev_get_bdevs -b Cache0 | jq -e \
+		".[0] | .driver_specific.cache_line_size == $cache_line_size"
+	$rpc_py save_subsystem_config -n bdev | jq -e \
+		".config | .[] | select(.method == \"bdev_ocf_create\") | .params.cache_line_size == $cache_line_size"
+
+	$rpc_py bdev_ocf_delete Cache0
+	$rpc_py bdev_malloc_delete Malloc0
+	$rpc_py bdev_malloc_delete Malloc1
+done
+
+# Prepare OCF cache for dynamic configuration switching
 $rpc_py bdev_malloc_create 101 512 -b Malloc0
 $rpc_py bdev_malloc_create 101 512 -b Malloc1
-$rpc_py bdev_ocf_create Cache wt Malloc0 Malloc1
+$rpc_py bdev_ocf_create Cache0 wt Malloc0 Malloc1
 
 $rpc_py bdev_ocf_get_bdevs | jq -e \
 	'.[0] | .started and .cache.attached and .core.attached'
 
 # Change cache mode
 for cache_mode in "${cache_modes[@]}"; do
-	$rpc_py bdev_ocf_set_cache_mode Cache $cache_mode
+	$rpc_py bdev_ocf_set_cache_mode Cache0 $cache_mode
+
+	# Check if cache mode values are reported correctly
+	$rpc_py bdev_get_bdevs -b Cache0 | jq -e \
+		".[0] | .driver_specific.mode == \"$cache_mode\""
+	$rpc_py save_subsystem_config -n bdev | jq -e \
+		".config | .[] | select(.method == \"bdev_ocf_create\") | .params.mode == \"$cache_mode\""
 done
 
 trap - SIGINT SIGTERM EXIT
diff --git a/test/ocf/management/create-destruct.sh b/test/ocf/management/create-destruct.sh
index c1fd66b36..162f7a679 100755
--- a/test/ocf/management/create-destruct.sh
+++ b/test/ocf/management/create-destruct.sh
@@ -43,7 +43,7 @@ if bdev_check_claimed Malloc0; then
 	exit 1
 fi
 
-$rpc_py bdev_ocf_create FullCache wt Malloc0 Malloc1 --cache-line-size 8
+$rpc_py bdev_ocf_create FullCache wt Malloc0 Malloc1
 
 $rpc_py bdev_ocf_get_bdevs FullCache | jq -e \
 	'.[0] | .started and .cache.attached and .core.attached'
@@ -59,7 +59,7 @@ if bdev_check_claimed Malloc0 && bdev_check_claimed Malloc1; then
 	exit 1
 fi
 
-$rpc_py bdev_ocf_create HotCache wt Malloc0 Malloc1 --cache-line-size 16
+$rpc_py bdev_ocf_create HotCache wt Malloc0 Malloc1
 
 if ! (bdev_check_claimed Malloc0 && bdev_check_claimed Malloc1); then
 	echo >&2 "Base devices expected to be claimed now"
diff --git a/test/ocf/management/persistent-metadata.sh b/test/ocf/management/persistent-metadata.sh
index 6cecc9739..918e8e3bf 100755
--- a/test/ocf/management/persistent-metadata.sh
+++ b/test/ocf/management/persistent-metadata.sh
@@ -32,6 +32,16 @@ config+=(
 	)"
 )
 
+config+=(
+	"$(
+		cat <<- JSON
+			{
+			  "method": "bdev_wait_for_examine"
+			}
+		JSON
+	)"
+)
+
 # First ']}' closes our config and bdev subsystem blocks
 jq . <<- CONFIG > "$curdir/config"
 	{"subsystems":[
diff --git a/test/ocf/management/remove.sh b/test/ocf/management/remove.sh
index 1302f16cd..c06234080 100755
--- a/test/ocf/management/remove.sh
+++ b/test/ocf/management/remove.sh
@@ -31,6 +31,9 @@ jq . <<- JSON > "$curdir/config"
 	            "block_size": 512,
 	            "filename": "./aio1"
 	          }
+	        },
+	        {
+	          "method": "bdev_wait_for_examine"
 	        }
 	      ]
 	    }
diff --git a/test/openstack/install_devstack.sh b/test/openstack/install_devstack.sh
index 44772c903..c0e64da8d 100755
--- a/test/openstack/install_devstack.sh
+++ b/test/openstack/install_devstack.sh
@@ -2,6 +2,7 @@
 
 testdir=$(readlink -f $(dirname $0))
 rootdir=$(readlink -f $testdir/../..)
+source "$rootdir/test/common/autotest_common.sh"
 
 function usage() {
 	[[ -n $2 ]] && (
@@ -31,25 +32,35 @@ while getopts 'h-:' optchar; do
 	esac
 done
 
-cd /opt/stack/devstack
-su -c "./unstack.sh" -s /bin/bash stack
+if [[ -e /opt/stack/devstack/unstack.sh ]]; then
+	cd /opt/stack/devstack
+	su -c "./unstack.sh" -s /bin/bash stack
+fi
 
-cd /opt/stack
-rm -rf cinder devstack glance keystone heat horizon neutron nova placement requirements tacker tacker-horizon tempest
+mkdir -p /opt/stack
+rm -rf /opt/stack/*
 
 r=0
-until [[ $r -ge 20 ]]; do
+until ((++r >= 20)); do
 	if [[ $branch == "master" ]]; then
-		su -c "git clone --depth 1 https://opendev.org/openstack-dev/devstack" -s /bin/bash stack && break
+		git clone --depth 1 https://opendev.org/openstack-dev/devstack /opt/stack/devstack && break
 	else
-		su -c "git clone --depth 1 https://opendev.org/openstack-dev/devstack -b stable/$branch" -s /bin/bash stack && break
+		git clone --depth 1 https://opendev.org/openstack-dev/devstack -b "stable/$branch" /opt/stack/devstack && break
 	fi
-	r=$((r + 1))
 done
+
+# Check if we reached max retries count
+((r < 20))
+
+git clone https://github.com/openstack/os-brick.git /opt/stack/os-brick
+cd /opt/stack/os-brick
+python3 ./setup.py install
+
 cp $rootdir/scripts/vagrant/local.conf /opt/stack/devstack/local.conf
 
 cd /opt/stack/devstack
-sudo sed -i "s|http://download.cirros-cloud.net|https://download.cirros-cloud.net|g" stackrc
+./tools/create-stack-user.sh
+chown -R stack:stack /opt/stack
 su -c "./stack.sh" -s /bin/bash stack
 source openrc admin admin
 openstack volume type create SPDK --public
diff --git a/test/openstack/run_openstack_tests.sh b/test/openstack/run_openstack_tests.sh
index b96adafbc..781275f5e 100755
--- a/test/openstack/run_openstack_tests.sh
+++ b/test/openstack/run_openstack_tests.sh
@@ -3,11 +3,13 @@
 testdir=$(readlink -f $(dirname $0))
 rootdir=$(readlink -f $testdir/../..)
 rpc_py=$rootdir/scripts/rpc.py
+
+set -- "--iso" "--transport=rdma" "$@"
+
 source $rootdir/test/common/autotest_common.sh
 source $rootdir/test/nvmf/common.sh
-TEST_TRANSPORT='rdma'
 
-nvmftestinit
+HUGE_EVEN_ALLOC=yes HUGEMEM=1024 nvmftestinit
 
 function finish_test() {
 	{
@@ -41,6 +43,8 @@ sudo systemctl restart devstack@c-*
 sleep 10
 timing_exit restart_cinder
 
+rxe_cfg status
+
 # Start testing spdk with openstack using tempest (openstack tool that allow testing an openstack functionalities)
 # In this tests is checked if spdk can correctly cooperate with openstack spdk driver
 timing_enter tempest_tests
diff --git a/test/scheduler/idle.sh b/test/scheduler/idle.sh
index 23f4d2852..830580241 100755
--- a/test/scheduler/idle.sh
+++ b/test/scheduler/idle.sh
@@ -38,13 +38,10 @@ idle() {
 	# The expectation here is that when SPDK app is idle the following is true:
 	# - all threads are assigned to main lcore
 	# - threads are not being moved between lcores
-	# - each thread has a mask pinned to a single cpu
-
-	local all_set
 
 	xtrace_disable
 	while ((samples++ < 5)); do
-		all_set=0 cpusmask=0
+		cpusmask=0
 		reactor_framework=$(rpc_cmd framework_get_reactors | jq -r '.reactors[]')
 		threads=($(
 			jq -r "select(.lcore == $spdk_main_core) | .lw_threads[].name" <<< "$reactor_framework"
@@ -56,10 +53,9 @@ idle() {
 		done
 
 		printf 'SPDK cpumask: %x Threads cpumask: %x\n' "$spdk_cpusmask" "$cpusmask"
-		thread_stats && ((cpusmask == spdk_cpusmask)) && all_set=1
+		thread_stats
 	done
 
-	((all_set == 1))
 	xtrace_restore
 }
 
diff --git a/test/scheduler/load_balancing.sh b/test/scheduler/load_balancing.sh
index a38e14d12..3f867ce93 100755
--- a/test/scheduler/load_balancing.sh
+++ b/test/scheduler/load_balancing.sh
@@ -96,7 +96,7 @@ balanced() {
 
 	thread0_name=thread0
 	thread0=$(create_thread -n "$thread0_name" -m "$(mask_cpus "${selected_cpus[@]}")" -a 0)
-	for cpu in "${selected_cpus[@]}"; do
+	for cpu in "${selected_cpus[@]::${#selected_cpus[@]}-1}"; do
 		extra_threads+=("$(create_thread -n "thread_cpu_$cpu" -m "$(mask_cpus "$cpu")" -a 100)")
 	done
 
@@ -124,13 +124,12 @@ balanced() {
 
 	[[ -n $(jq -r "select(.lcore == $spdk_main_core) | .lw_threads[] | select(.id == $thread0)") ]] <<< "$reactor_framework"
 
-	# thread0 is active, wait for scheduler to run (2x) and check if it is not on main core, nor the core from 2)
+	# thread0 is active, wait for scheduler to run (2x) and check if it is not on main core
 	active_thread "$thread0" 100
 	sleep $((2 * sched_period))
 	reactor_framework=$(rpc_cmd framework_get_reactors | jq -r '.reactors[]')
 
-	[[ -z $(jq -r "select(.lcore == $spdk_main_core) | .lw_threads[] | select(.id == $thread0)") ]] <<< "$reactor_framewrk"
-	[[ -z $(jq -r "select(.lcore == $active_cpu) | .lw_threads[] | select(.id == $thread0)") ]] <<< "$reactor_framewrk"
+	[[ -z $(jq -r "select(.lcore == $spdk_main_core) | .lw_threads[] | select(.id == $thread0)") ]] <<< "$reactor_framework"
 
 	destroy_thread "$thread0"
 	for thread in "${extra_threads[@]}"; do
diff --git a/test/setup/acl.sh b/test/setup/acl.sh
index 4c3b9bc22..088caf9ea 100755
--- a/test/setup/acl.sh
+++ b/test/setup/acl.sh
@@ -11,6 +11,7 @@ collect_setup_devs() {
 
 	while read -r _ dev _ _ _ driver _; do
 		[[ $dev == *:*:*.* ]] || continue
+		[[ $driver == nvme ]] || continue
 		devs+=("$dev") drivers["$dev"]=$driver
 	done < <(setup output status)
 	((${#devs[@]} > 0))
@@ -37,8 +38,7 @@ denied() {
 
 allowed() {
 	PCI_ALLOWED="${devs[0]}" setup output config \
-		| grep "Skipping denied controller at " \
-		| grep -v "${devs[0]}"
+		| grep -E "${devs[0]} .*: ${drivers["${devs[0]}"]} -> .*"
 	verify "${devs[@]:1}"
 	setup reset
 }
diff --git a/test/setup/hugepages.sh b/test/setup/hugepages.sh
index a8460c07d..59cd46b4c 100755
--- a/test/setup/hugepages.sh
+++ b/test/setup/hugepages.sh
@@ -15,6 +15,12 @@ default_huges=$(get_meminfo Hugepagesize)
 default_huge_nr=/sys/kernel/mm/hugepages/hugepages-${default_huges}kB/nr_hugepages
 global_huge_nr=/proc/sys/vm/nr_hugepages
 
+# Make sure environment doesn't affect the tests
+unset -v HUGE_EVEN_ALLOC
+unset -v HUGEMEM
+unset -v HUGENODE
+unset -v NRHUGE
+
 get_nodes() {
 	local node
 
@@ -104,8 +110,8 @@ verify_nr_hugepages() {
 
 # Test cases
 default_setup() {
-	# Default HUGEMEM (8G) alloc on node0
-	get_test_nr_hugepages $((HUGEMEM * 1024)) 0
+	# Default HUGEMEM (2G) alloc on node0
+	get_test_nr_hugepages $((2048 * 1024)) 0
 	setup
 	verify_nr_hugepages
 }
@@ -134,28 +140,30 @@ odd_alloc() {
 }
 
 custom_alloc() {
-	# Custom alloc: node0 == 512 pages [node1 == 1024 pages]
+	# Custom alloc: node0 == 1GB [node1 == 2 GB]
 
 	local IFS=","
 
 	local node
 	local nodes_hp=()
 
-	local nr_hugepages=0
+	local nr_hugepages=0 _nr_hugepages=0
 
-	nodes_hp[0]=512
+	get_test_nr_hugepages $((1024 * 1024))
+	nodes_hp[0]=$nr_hugepages
 	if ((${#nodes_sys[@]} > 1)); then
-		nodes_hp[1]=1024
+		get_test_nr_hugepages $((2048 * 1024))
+		nodes_hp[1]=$nr_hugepages
 	fi
 
 	for node in "${!nodes_hp[@]}"; do
 		HUGENODE+=("nodes_hp[$node]=${nodes_hp[node]}")
-		((nr_hugepages += nodes_hp[node]))
+		((_nr_hugepages += nodes_hp[node]))
 	done
 
 	get_test_nr_hugepages_per_node
 	HUGENODE="${HUGENODE[*]}" setup
-	verify_nr_hugepages
+	nr_hugepages=$_nr_hugepages verify_nr_hugepages
 }
 
 hp_status() {
diff --git a/test/spdkcli/common.sh b/test/spdkcli/common.sh
index fec6d0497..42fad0858 100644
--- a/test/spdkcli/common.sh
+++ b/test/spdkcli/common.sh
@@ -43,3 +43,6 @@ function check_match() {
 	$rootdir/test/app/match/match $testdir/match_files/${MATCH_FILE}.match
 	rm -f $testdir/match_files/${MATCH_FILE}
 }
+
+# Allocate 5GB of hugepages to have some overhead for run_*()s
+HUGEMEM=5120 CLEAR_HUGE=yes "$rootdir/scripts/setup.sh"
diff --git a/test/spdkcli/match_files/spdkcli_iscsi.test.match b/test/spdkcli/match_files/spdkcli_iscsi.test.match
index daa1ec8b9..1cb9219fb 100644
--- a/test/spdkcli/match_files/spdkcli_iscsi.test.match
+++ b/test/spdkcli/match_files/spdkcli_iscsi.test.match
@@ -9,12 +9,14 @@ o- iscsi .......................................................................
   o- global_params ........................................................................................................... [...]
   | o- allow_duplicated_isid: False .......................................................................................... [...]
   | o- chap_group: 1 ......................................................................................................... [...]
+  | o- data_out_pool_size: 2048 .............................................................................................. [...]
   | o- default_time2retain: 20 ............................................................................................... [...]
   | o- default_time2wait: 2 .................................................................................................. [...]
   | o- disable_chap: True .................................................................................................... [...]
   | o- error_recovery_level: 0 ............................................................................................... [...]
   | o- first_burst_length: 8192 .............................................................................................. [...]
   | o- immediate_data: True .................................................................................................. [...]
+  | o- immediate_data_pool_size: 16384 ....................................................................................... [...]
   | o- max_connections_per_session: 2 ........................................................................................ [...]
   | o- max_large_datain_per_connection: 64 ................................................................................... [...]
   | o- max_queue_depth: 64 ................................................................................................... [...]
@@ -24,6 +26,7 @@ o- iscsi .......................................................................
   | o- node_base: iqn.2016-06.io.spdk ........................................................................................ [...]
   | o- nop_in_interval: 30 ................................................................................................... [...]
   | o- nop_timeout: 60 ....................................................................................................... [...]
+  | o- pdu_pool_size: 36864 .................................................................................................. [...]
   | o- require_chap: False ................................................................................................... [...]
   o- initiator_groups ........................................................................................ [Initiator groups: 2]
   | o- initiator_group2 ............................................................................................ [Initiators: 2]
diff --git a/test/unit/lib/Makefile b/test/unit/lib/Makefile
index c7fbc5345..a7ed1eea5 100644
--- a/test/unit/lib/Makefile
+++ b/test/unit/lib/Makefile
@@ -35,7 +35,7 @@ SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
 DIRS-y = accel bdev blob blobfs event ioat iscsi json jsonrpc log lvol
-DIRS-y += notify nvme nvmf scsi sock thread util env_dpdk
+DIRS-y += notify nvme nvmf scsi sock thread util env_dpdk init
 DIRS-$(CONFIG_IDXD) += idxd
 DIRS-$(CONFIG_REDUCE) += reduce
 ifeq ($(OS),Linux)
diff --git a/test/unit/lib/accel/accel.c/accel_engine_ut.c b/test/unit/lib/accel/accel.c/accel_engine_ut.c
index dbd1282db..b137f5ab7 100644
--- a/test/unit/lib/accel/accel.c/accel_engine_ut.c
+++ b/test/unit/lib/accel/accel.c/accel_engine_ut.c
@@ -33,6 +33,7 @@
 
 #include "spdk_cunit.h"
 #include "spdk_internal/mock.h"
+#include "thread/thread_internal.h"
 #include "common/lib/test_env.c"
 
 #include "accel/accel_engine.c"
@@ -185,7 +186,7 @@ test_spdk_accel_get_capabilities(void)
 	SPDK_CU_ASSERT_FATAL(ch != NULL);
 
 	/* Setup a few capabilites and make sure they are reported as expected. */
-	accel_ch = (struct accel_io_channel *)((char *)ch + sizeof(struct spdk_io_channel));
+	accel_ch = (struct accel_io_channel *)spdk_io_channel_get_ctx(ch);
 	accel_ch->engine = &engine;
 	expected_cap = ACCEL_COPY | ACCEL_DUALCAST | ACCEL_CRC32C;
 	accel_ch->engine->capabilities = expected_cap;
diff --git a/test/unit/lib/bdev/bdev.c/bdev_ut.c b/test/unit/lib/bdev/bdev.c/bdev_ut.c
index b5db551f0..3a278d2bd 100644
--- a/test/unit/lib/bdev/bdev.c/bdev_ut.c
+++ b/test/unit/lib/bdev/bdev.c/bdev_ut.c
@@ -42,20 +42,9 @@
 
 #include "bdev/bdev.c"
 
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_owner, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_object, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type,
-		uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_type, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
 DEFINE_STUB(spdk_notify_send, uint64_t, (const char *type, const char *ctx), 0);
 DEFINE_STUB(spdk_notify_type_register, struct spdk_notify_type *, (const char *type), NULL);
 
-
 int g_status;
 int g_count;
 enum spdk_bdev_event_type g_event_type1;
@@ -116,6 +105,11 @@ static void *g_compare_write_buf;
 static uint32_t g_compare_write_buf_len;
 static bool g_abort_done;
 static enum spdk_bdev_io_status g_abort_status;
+static void *g_zcopy_read_buf;
+static uint32_t g_zcopy_read_buf_len;
+static void *g_zcopy_write_buf;
+static uint32_t g_zcopy_write_buf_len;
+static struct spdk_bdev_io *g_zcopy_bdev_io;
 
 static struct ut_expected_io *
 ut_alloc_expected_io(uint8_t type, uint64_t offset, uint64_t length, int iovcnt)
@@ -190,6 +184,43 @@ stub_submit_request(struct spdk_io_channel *_ch, struct spdk_bdev_io *bdev_io)
 		}
 	}
 
+	if (bdev_io->type == SPDK_BDEV_IO_TYPE_ZCOPY) {
+		if (bdev_io->u.bdev.zcopy.start) {
+			g_zcopy_bdev_io = bdev_io;
+			if (bdev_io->u.bdev.zcopy.populate) {
+				/* Start of a read */
+				CU_ASSERT(g_zcopy_read_buf != NULL);
+				CU_ASSERT(g_zcopy_read_buf_len > 0);
+				bdev_io->u.bdev.iovs[0].iov_base = g_zcopy_read_buf;
+				bdev_io->u.bdev.iovs[0].iov_len = g_zcopy_read_buf_len;
+				bdev_io->u.bdev.iovcnt = 1;
+			} else {
+				/* Start of a write */
+				CU_ASSERT(g_zcopy_write_buf != NULL);
+				CU_ASSERT(g_zcopy_write_buf_len > 0);
+				bdev_io->u.bdev.iovs[0].iov_base = g_zcopy_write_buf;
+				bdev_io->u.bdev.iovs[0].iov_len = g_zcopy_write_buf_len;
+				bdev_io->u.bdev.iovcnt = 1;
+			}
+		} else {
+			if (bdev_io->u.bdev.zcopy.commit) {
+				/* End of write */
+				CU_ASSERT(bdev_io->u.bdev.iovs[0].iov_base == g_zcopy_write_buf);
+				CU_ASSERT(bdev_io->u.bdev.iovs[0].iov_len == g_zcopy_write_buf_len);
+				CU_ASSERT(bdev_io->u.bdev.iovcnt == 1);
+				g_zcopy_write_buf = NULL;
+				g_zcopy_write_buf_len = 0;
+			} else {
+				/* End of read */
+				CU_ASSERT(bdev_io->u.bdev.iovs[0].iov_base == g_zcopy_read_buf);
+				CU_ASSERT(bdev_io->u.bdev.iovs[0].iov_len == g_zcopy_read_buf_len);
+				CU_ASSERT(bdev_io->u.bdev.iovcnt == 1);
+				g_zcopy_read_buf = NULL;
+				g_zcopy_read_buf_len = 0;
+			}
+		}
+	}
+
 	TAILQ_INSERT_TAIL(&ch->outstanding_io, bdev_io, module_link);
 	ch->outstanding_io_count++;
 
@@ -856,7 +887,13 @@ io_done(struct spdk_bdev_io *bdev_io, bool success, void *cb_arg)
 {
 	g_io_done = true;
 	g_io_status = bdev_io->internal.status;
-	spdk_bdev_free_io(bdev_io);
+	if ((bdev_io->type == SPDK_BDEV_IO_TYPE_ZCOPY) &&
+	    (bdev_io->u.bdev.zcopy.start)) {
+		g_zcopy_bdev_io = bdev_io;
+	} else {
+		spdk_bdev_free_io(bdev_io);
+		g_zcopy_bdev_io = NULL;
+	}
 }
 
 static void
@@ -1021,6 +1058,7 @@ bdev_io_spans_split_test(void)
 	memset(&bdev, 0, sizeof(bdev));
 	bdev_io.u.bdev.iovs = iov;
 
+	bdev_io.type = SPDK_BDEV_IO_TYPE_READ;
 	bdev.optimal_io_boundary = 0;
 	bdev.max_segment_size = 0;
 	bdev.max_num_segments = 0;
@@ -3441,6 +3479,174 @@ bdev_write_zeroes(void)
 	poll_threads();
 }
 
+static void
+bdev_zcopy_write(void)
+{
+	struct spdk_bdev *bdev;
+	struct spdk_bdev_desc *desc = NULL;
+	struct spdk_io_channel *ioch;
+	struct ut_expected_io *expected_io;
+	uint64_t offset, num_blocks;
+	uint32_t num_completed;
+	char aa_buf[512];
+	struct iovec iov;
+	int rc;
+	const bool populate = false;
+	const bool commit = true;
+
+	memset(aa_buf, 0xaa, sizeof(aa_buf));
+
+	spdk_bdev_initialize(bdev_init_cb, NULL);
+	bdev = allocate_bdev("bdev");
+
+	rc = spdk_bdev_open_ext("bdev", true, bdev_ut_event_cb, NULL, &desc);
+	CU_ASSERT_EQUAL(rc, 0);
+	SPDK_CU_ASSERT_FATAL(desc != NULL);
+	CU_ASSERT(bdev == spdk_bdev_desc_get_bdev(desc));
+	ioch = spdk_bdev_get_io_channel(desc);
+	SPDK_CU_ASSERT_FATAL(ioch != NULL);
+
+	g_io_exp_status = SPDK_BDEV_IO_STATUS_SUCCESS;
+
+	offset = 50;
+	num_blocks = 1;
+	iov.iov_base = NULL;
+	iov.iov_len = 0;
+
+	g_zcopy_read_buf = (void *) 0x1122334455667788UL;
+	g_zcopy_read_buf_len = (uint32_t) -1;
+	/* Do a zcopy start for a write (populate=false) */
+	expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_ZCOPY, offset, num_blocks, 0);
+	TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+	g_io_done = false;
+	g_zcopy_write_buf = aa_buf;
+	g_zcopy_write_buf_len = sizeof(aa_buf);
+	g_zcopy_bdev_io = NULL;
+	rc = spdk_bdev_zcopy_start(desc, ioch, &iov, 1, offset, num_blocks, populate, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	num_completed = stub_complete_io(1);
+	CU_ASSERT_EQUAL(num_completed, 1);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_io_status == SPDK_BDEV_IO_STATUS_SUCCESS);
+	/* Check that the iov has been set up */
+	CU_ASSERT(iov.iov_base == g_zcopy_write_buf);
+	CU_ASSERT(iov.iov_len == g_zcopy_write_buf_len);
+	/* Check that the bdev_io has been saved */
+	CU_ASSERT(g_zcopy_bdev_io != NULL);
+	/* Now do the zcopy end for a write (commit=true) */
+	g_io_done = false;
+	expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_ZCOPY, offset, num_blocks, 0);
+	TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+	rc = spdk_bdev_zcopy_end(g_zcopy_bdev_io, commit, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	num_completed = stub_complete_io(1);
+	CU_ASSERT_EQUAL(num_completed, 1);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_io_status == SPDK_BDEV_IO_STATUS_SUCCESS);
+	/* Check the g_zcopy are reset by io_done */
+	CU_ASSERT(g_zcopy_write_buf == NULL);
+	CU_ASSERT(g_zcopy_write_buf_len == 0);
+	/* Check that io_done has freed the g_zcopy_bdev_io */
+	CU_ASSERT(g_zcopy_bdev_io == NULL);
+
+	/* Check the zcopy read buffer has not been touched which
+	 * ensures that the correct buffers were used.
+	 */
+	CU_ASSERT(g_zcopy_read_buf == (void *) 0x1122334455667788UL);
+	CU_ASSERT(g_zcopy_read_buf_len == (uint32_t) -1);
+
+	spdk_put_io_channel(ioch);
+	spdk_bdev_close(desc);
+	free_bdev(bdev);
+	spdk_bdev_finish(bdev_fini_cb, NULL);
+	poll_threads();
+}
+
+static void
+bdev_zcopy_read(void)
+{
+	struct spdk_bdev *bdev;
+	struct spdk_bdev_desc *desc = NULL;
+	struct spdk_io_channel *ioch;
+	struct ut_expected_io *expected_io;
+	uint64_t offset, num_blocks;
+	uint32_t num_completed;
+	char aa_buf[512];
+	struct iovec iov;
+	int rc;
+	const bool populate = true;
+	const bool commit = false;
+
+	memset(aa_buf, 0xaa, sizeof(aa_buf));
+
+	spdk_bdev_initialize(bdev_init_cb, NULL);
+	bdev = allocate_bdev("bdev");
+
+	rc = spdk_bdev_open_ext("bdev", true, bdev_ut_event_cb, NULL, &desc);
+	CU_ASSERT_EQUAL(rc, 0);
+	SPDK_CU_ASSERT_FATAL(desc != NULL);
+	CU_ASSERT(bdev == spdk_bdev_desc_get_bdev(desc));
+	ioch = spdk_bdev_get_io_channel(desc);
+	SPDK_CU_ASSERT_FATAL(ioch != NULL);
+
+	g_io_exp_status = SPDK_BDEV_IO_STATUS_SUCCESS;
+
+	offset = 50;
+	num_blocks = 1;
+	iov.iov_base = NULL;
+	iov.iov_len = 0;
+
+	g_zcopy_write_buf = (void *) 0x1122334455667788UL;
+	g_zcopy_write_buf_len = (uint32_t) -1;
+
+	/* Do a zcopy start for a read (populate=true) */
+	expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_ZCOPY, offset, num_blocks, 0);
+	TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+	g_io_done = false;
+	g_zcopy_read_buf = aa_buf;
+	g_zcopy_read_buf_len = sizeof(aa_buf);
+	g_zcopy_bdev_io = NULL;
+	rc = spdk_bdev_zcopy_start(desc, ioch, &iov, 1, offset, num_blocks, populate, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	num_completed = stub_complete_io(1);
+	CU_ASSERT_EQUAL(num_completed, 1);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_io_status == SPDK_BDEV_IO_STATUS_SUCCESS);
+	/* Check that the iov has been set up */
+	CU_ASSERT(iov.iov_base == g_zcopy_read_buf);
+	CU_ASSERT(iov.iov_len == g_zcopy_read_buf_len);
+	/* Check that the bdev_io has been saved */
+	CU_ASSERT(g_zcopy_bdev_io != NULL);
+
+	/* Now do the zcopy end for a read (commit=false) */
+	g_io_done = false;
+	expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_ZCOPY, offset, num_blocks, 0);
+	TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+	rc = spdk_bdev_zcopy_end(g_zcopy_bdev_io, commit, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	num_completed = stub_complete_io(1);
+	CU_ASSERT_EQUAL(num_completed, 1);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_io_status == SPDK_BDEV_IO_STATUS_SUCCESS);
+	/* Check the g_zcopy are reset by io_done */
+	CU_ASSERT(g_zcopy_read_buf == NULL);
+	CU_ASSERT(g_zcopy_read_buf_len == 0);
+	/* Check that io_done has freed the g_zcopy_bdev_io */
+	CU_ASSERT(g_zcopy_bdev_io == NULL);
+
+	/* Check the zcopy write buffer has not been touched which
+	 * ensures that the correct buffers were used.
+	 */
+	CU_ASSERT(g_zcopy_write_buf == (void *) 0x1122334455667788UL);
+	CU_ASSERT(g_zcopy_write_buf_len == (uint32_t) -1);
+
+	spdk_put_io_channel(ioch);
+	spdk_bdev_close(desc);
+	free_bdev(bdev);
+	spdk_bdev_finish(bdev_fini_cb, NULL);
+	poll_threads();
+}
+
 static void
 bdev_open_while_hotremove(void)
 {
@@ -4261,6 +4467,207 @@ bdev_io_abort(void)
 	poll_threads();
 }
 
+static void
+bdev_unmap(void)
+{
+	struct spdk_bdev *bdev;
+	struct spdk_bdev_desc *desc = NULL;
+	struct spdk_io_channel *ioch;
+	struct spdk_bdev_channel *bdev_ch;
+	struct ut_expected_io *expected_io;
+	struct spdk_bdev_opts bdev_opts = {};
+	uint32_t i, num_outstanding;
+	uint64_t offset, num_blocks, max_unmap_blocks, num_children;
+	int rc;
+
+	spdk_bdev_get_opts(&bdev_opts, sizeof(bdev_opts));
+	bdev_opts.bdev_io_pool_size = 512;
+	bdev_opts.bdev_io_cache_size = 64;
+	rc = spdk_bdev_set_opts(&bdev_opts);
+	CU_ASSERT(rc == 0);
+
+	spdk_bdev_initialize(bdev_init_cb, NULL);
+	bdev = allocate_bdev("bdev");
+
+	rc = spdk_bdev_open_ext("bdev", true, bdev_ut_event_cb, NULL, &desc);
+	CU_ASSERT_EQUAL(rc, 0);
+	SPDK_CU_ASSERT_FATAL(desc != NULL);
+	CU_ASSERT(bdev == spdk_bdev_desc_get_bdev(desc));
+	ioch = spdk_bdev_get_io_channel(desc);
+	SPDK_CU_ASSERT_FATAL(ioch != NULL);
+	bdev_ch = spdk_io_channel_get_ctx(ioch);
+	CU_ASSERT(TAILQ_EMPTY(&bdev_ch->io_submitted));
+
+	fn_table.submit_request = stub_submit_request;
+	g_io_exp_status = SPDK_BDEV_IO_STATUS_SUCCESS;
+
+	/* Case 1: First test the request won't be split */
+	num_blocks = 32;
+
+	g_io_done = false;
+	expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_UNMAP, 0, num_blocks, 0);
+	TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+	rc = spdk_bdev_unmap_blocks(desc, ioch, 0, num_blocks, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	CU_ASSERT(g_io_done == false);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 1);
+	stub_complete_io(1);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 0);
+
+	/* Case 2: Test the split with 2 children requests */
+	bdev->max_unmap = 8;
+	bdev->max_unmap_segments = 2;
+	max_unmap_blocks = bdev->max_unmap * bdev->max_unmap_segments;
+	num_blocks = max_unmap_blocks * 2;
+	offset = 0;
+
+	g_io_done = false;
+	for (i = 0; i < 2; i++) {
+		expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_UNMAP, offset, max_unmap_blocks, 0);
+		TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+		offset += max_unmap_blocks;
+	}
+
+	rc = spdk_bdev_unmap_blocks(desc, ioch, 0, num_blocks, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	CU_ASSERT(g_io_done == false);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 2);
+	stub_complete_io(2);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 0);
+
+	/* Case 3: Test the split with 15 children requests, will finish 8 requests first */
+	num_children = 15;
+	num_blocks = max_unmap_blocks * num_children;
+	g_io_done = false;
+	offset = 0;
+	for (i = 0; i < num_children; i++) {
+		expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_UNMAP, offset, max_unmap_blocks, 0);
+		TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+		offset += max_unmap_blocks;
+	}
+
+	rc = spdk_bdev_unmap_blocks(desc, ioch, 0, num_blocks, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	CU_ASSERT(g_io_done == false);
+
+	while (num_children > 0) {
+		num_outstanding = spdk_min(num_children, SPDK_BDEV_MAX_CHILDREN_UNMAP_WRITE_ZEROES_REQS);
+		CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == num_outstanding);
+		stub_complete_io(num_outstanding);
+		num_children -= num_outstanding;
+	}
+	CU_ASSERT(g_io_done == true);
+
+	spdk_put_io_channel(ioch);
+	spdk_bdev_close(desc);
+	free_bdev(bdev);
+	spdk_bdev_finish(bdev_fini_cb, NULL);
+	poll_threads();
+}
+
+static void
+bdev_write_zeroes_split_test(void)
+{
+	struct spdk_bdev *bdev;
+	struct spdk_bdev_desc *desc = NULL;
+	struct spdk_io_channel *ioch;
+	struct spdk_bdev_channel *bdev_ch;
+	struct ut_expected_io *expected_io;
+	struct spdk_bdev_opts bdev_opts = {};
+	uint32_t i, num_outstanding;
+	uint64_t offset, num_blocks, max_write_zeroes_blocks, num_children;
+	int rc;
+
+	spdk_bdev_get_opts(&bdev_opts, sizeof(bdev_opts));
+	bdev_opts.bdev_io_pool_size = 512;
+	bdev_opts.bdev_io_cache_size = 64;
+	rc = spdk_bdev_set_opts(&bdev_opts);
+	CU_ASSERT(rc == 0);
+
+	spdk_bdev_initialize(bdev_init_cb, NULL);
+	bdev = allocate_bdev("bdev");
+
+	rc = spdk_bdev_open_ext("bdev", true, bdev_ut_event_cb, NULL, &desc);
+	CU_ASSERT_EQUAL(rc, 0);
+	SPDK_CU_ASSERT_FATAL(desc != NULL);
+	CU_ASSERT(bdev == spdk_bdev_desc_get_bdev(desc));
+	ioch = spdk_bdev_get_io_channel(desc);
+	SPDK_CU_ASSERT_FATAL(ioch != NULL);
+	bdev_ch = spdk_io_channel_get_ctx(ioch);
+	CU_ASSERT(TAILQ_EMPTY(&bdev_ch->io_submitted));
+
+	fn_table.submit_request = stub_submit_request;
+	g_io_exp_status = SPDK_BDEV_IO_STATUS_SUCCESS;
+
+	/* Case 1: First test the request won't be split */
+	num_blocks = 32;
+
+	g_io_done = false;
+	expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_WRITE_ZEROES, 0, num_blocks, 0);
+	TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+	rc = spdk_bdev_write_zeroes_blocks(desc, ioch, 0, num_blocks, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	CU_ASSERT(g_io_done == false);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 1);
+	stub_complete_io(1);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 0);
+
+	/* Case 2: Test the split with 2 children requests */
+	max_write_zeroes_blocks = 8;
+	bdev->max_write_zeroes = max_write_zeroes_blocks;
+	num_blocks = max_write_zeroes_blocks * 2;
+	offset = 0;
+
+	g_io_done = false;
+	for (i = 0; i < 2; i++) {
+		expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_WRITE_ZEROES, offset, max_write_zeroes_blocks,
+						   0);
+		TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+		offset += max_write_zeroes_blocks;
+	}
+
+	rc = spdk_bdev_write_zeroes_blocks(desc, ioch, 0, num_blocks, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	CU_ASSERT(g_io_done == false);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 2);
+	stub_complete_io(2);
+	CU_ASSERT(g_io_done == true);
+	CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == 0);
+
+	/* Case 3: Test the split with 15 children requests, will finish 8 requests first */
+	num_children = 15;
+	num_blocks = max_write_zeroes_blocks * num_children;
+	g_io_done = false;
+	offset = 0;
+	for (i = 0; i < num_children; i++) {
+		expected_io = ut_alloc_expected_io(SPDK_BDEV_IO_TYPE_WRITE_ZEROES, offset, max_write_zeroes_blocks,
+						   0);
+		TAILQ_INSERT_TAIL(&g_bdev_ut_channel->expected_io, expected_io, link);
+		offset += max_write_zeroes_blocks;
+	}
+
+	rc = spdk_bdev_write_zeroes_blocks(desc, ioch, 0, num_blocks, io_done, NULL);
+	CU_ASSERT_EQUAL(rc, 0);
+	CU_ASSERT(g_io_done == false);
+
+	while (num_children > 0) {
+		num_outstanding = spdk_min(num_children, SPDK_BDEV_MAX_CHILDREN_UNMAP_WRITE_ZEROES_REQS);
+		CU_ASSERT(g_bdev_ut_channel->outstanding_io_count == num_outstanding);
+		stub_complete_io(num_outstanding);
+		num_children -= num_outstanding;
+	}
+	CU_ASSERT(g_io_done == true);
+
+	spdk_put_io_channel(ioch);
+	spdk_bdev_close(desc);
+	free_bdev(bdev);
+	spdk_bdev_finish(bdev_fini_cb, NULL);
+	poll_threads();
+}
+
 static void
 bdev_set_options_test(void)
 {
@@ -4298,6 +4705,72 @@ bdev_set_options_test(void)
 	CU_ASSERT(rc == 0);
 }
 
+static uint64_t
+get_ns_time(void)
+{
+	int rc;
+	struct timespec ts;
+
+	rc = clock_gettime(CLOCK_MONOTONIC, &ts);
+	CU_ASSERT(rc == 0);
+	return ts.tv_sec * 1000 * 1000 * 1000 + ts.tv_nsec;
+}
+
+static int
+rb_tree_get_height(struct spdk_bdev_name *bdev_name)
+{
+	int h1, h2;
+
+	if (bdev_name == NULL) {
+		return -1;
+	} else {
+		h1 = rb_tree_get_height(RB_LEFT(bdev_name, node));
+		h2 = rb_tree_get_height(RB_RIGHT(bdev_name, node));
+
+		return spdk_max(h1, h2) + 1;
+	}
+}
+
+static void
+bdev_multi_allocation(void)
+{
+	const int max_bdev_num = 1024 * 16;
+	char name[max_bdev_num][10];
+	char noexist_name[] = "invalid_bdev";
+	struct spdk_bdev *bdev[max_bdev_num];
+	int i, j;
+	uint64_t last_time;
+	int bdev_num;
+	int height;
+
+	for (j = 0; j < max_bdev_num; j++) {
+		snprintf(name[j], sizeof(name[j]), "bdev%d", j);
+	}
+
+	for (i = 0; i < 16; i++) {
+		last_time = get_ns_time();
+		bdev_num = 1024 * (i + 1);
+		for (j = 0; j < bdev_num; j++) {
+			bdev[j] = allocate_bdev(name[j]);
+			height = rb_tree_get_height(&bdev[j]->internal.bdev_name);
+			CU_ASSERT(height <= (int)(spdk_u32log2(j + 1)));
+		}
+		SPDK_NOTICELOG("alloc bdev num %d takes %" PRIu64 " ms\n", bdev_num,
+			       (get_ns_time() - last_time) / 1000 / 1000);
+		for (j = 0; j < bdev_num; j++) {
+			CU_ASSERT(spdk_bdev_get_by_name(name[j]) != NULL);
+		}
+		CU_ASSERT(spdk_bdev_get_by_name(noexist_name) == NULL);
+
+		for (j = 0; j < bdev_num; j++) {
+			free_bdev(bdev[j]);
+		}
+		for (j = 0; j < bdev_num; j++) {
+			CU_ASSERT(spdk_bdev_get_by_name(name[j]) == NULL);
+		}
+	}
+}
+
 int
 main(int argc, char **argv)
 {
@@ -4328,6 +4801,8 @@ main(int argc, char **argv)
 	CU_ADD_TEST(suite, bdev_write_zeroes);
 	CU_ADD_TEST(suite, bdev_compare_and_write);
 	CU_ADD_TEST(suite, bdev_compare);
+	CU_ADD_TEST(suite, bdev_zcopy_write);
+	CU_ADD_TEST(suite, bdev_zcopy_read);
 	CU_ADD_TEST(suite, bdev_open_while_hotremove);
 	CU_ADD_TEST(suite, bdev_close_while_hotremove);
 	CU_ADD_TEST(suite, bdev_open_ext);
@@ -4337,7 +4812,10 @@ main(int argc, char **argv)
 	CU_ADD_TEST(suite, lock_lba_range_with_io_outstanding);
 	CU_ADD_TEST(suite, lock_lba_range_overlapped);
 	CU_ADD_TEST(suite, bdev_io_abort);
+	CU_ADD_TEST(suite, bdev_unmap);
+	CU_ADD_TEST(suite, bdev_write_zeroes_split_test);
 	CU_ADD_TEST(suite, bdev_set_options_test);
+	CU_ADD_TEST(suite, bdev_multi_allocation);
 
 	allocate_cores(1);
 	allocate_threads(1);
diff --git a/test/unit/lib/bdev/compress.c/compress_ut.c b/test/unit/lib/bdev/compress.c/compress_ut.c
index b75d7d4fa..54a1f80c0 100644
--- a/test/unit/lib/bdev/compress.c/compress_ut.c
+++ b/test/unit/lib/bdev/compress.c/compress_ut.c
@@ -36,6 +36,7 @@
 #define UNIT_TEST_NO_VTOPHYS
 #include "common/lib/test_env.c"
 #include "spdk_internal/mock.h"
+#include "thread/thread_internal.h"
 #include "unit/lib/json_mock.c"
 #include "spdk/reduce.h"
 
@@ -590,7 +591,7 @@ test_setup(void)
 	g_bdev_io->bdev = &g_comp_bdev.comp_bdev;
 	g_io_ch = calloc(1, sizeof(struct spdk_io_channel) + sizeof(struct comp_io_channel));
 	g_io_ch->thread = thread;
-	g_comp_ch = (struct comp_io_channel *)((uint8_t *)g_io_ch + sizeof(struct spdk_io_channel));
+	g_comp_ch = (struct comp_io_channel *)spdk_io_channel_get_ctx(g_io_ch);
 	g_io_ctx = (struct comp_bdev_io *)g_bdev_io->driver_ctx;
 
 	g_io_ctx->comp_ch = g_comp_ch;
diff --git a/test/unit/lib/bdev/crypto.c/crypto_ut.c b/test/unit/lib/bdev/crypto.c/crypto_ut.c
index dcf20cdd5..6dbc646cb 100644
--- a/test/unit/lib/bdev/crypto.c/crypto_ut.c
+++ b/test/unit/lib/bdev/crypto.c/crypto_ut.c
@@ -35,6 +35,7 @@
 
 #include "common/lib/test_env.c"
 #include "spdk_internal/mock.h"
+#include "thread/thread_internal.h"
 #include "unit/lib/json_mock.c"
 
 #include <rte_crypto.h>
@@ -317,7 +318,7 @@ test_setup(void)
 	g_bdev_io->u.bdev.iovs = calloc(1, sizeof(struct iovec) * 128);
 	g_bdev_io->bdev = &g_crypto_bdev.crypto_bdev;
 	g_io_ch = calloc(1, sizeof(struct spdk_io_channel) + sizeof(struct crypto_io_channel));
-	g_crypto_ch = (struct crypto_io_channel *)((uint8_t *)g_io_ch + sizeof(struct spdk_io_channel));
+	g_crypto_ch = (struct crypto_io_channel *)spdk_io_channel_get_ctx(g_io_ch);
 	g_io_ctx = (struct crypto_bdev_io *)g_bdev_io->driver_ctx;
 	memset(&g_device, 0, sizeof(struct vbdev_dev));
 	memset(&g_crypto_bdev, 0, sizeof(struct vbdev_crypto));
diff --git a/test/unit/lib/bdev/mt/bdev.c/bdev_ut.c b/test/unit/lib/bdev/mt/bdev.c/bdev_ut.c
index 238823ebe..96ebb009e 100644
--- a/test/unit/lib/bdev/mt/bdev.c/bdev_ut.c
+++ b/test/unit/lib/bdev/mt/bdev.c/bdev_ut.c
@@ -44,16 +44,6 @@
 
 #define BDEV_UT_NUM_THREADS 3
 
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_owner, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_object, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type,
-		uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_type, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
 DEFINE_STUB(spdk_notify_send, uint64_t, (const char *type, const char *ctx), 0);
 DEFINE_STUB(spdk_notify_type_register, struct spdk_notify_type *, (const char *type), NULL);
 DEFINE_STUB_V(spdk_scsi_nvme_translate, (const struct spdk_bdev_io *bdev_io, int *sc, int *sk,
diff --git a/test/unit/lib/bdev/nvme/bdev_nvme.c/bdev_nvme_ut.c b/test/unit/lib/bdev/nvme/bdev_nvme.c/bdev_nvme_ut.c
index 013b26d07..769a4e598 100644
--- a/test/unit/lib/bdev/nvme/bdev_nvme.c/bdev_nvme_ut.c
+++ b/test/unit/lib/bdev/nvme/bdev_nvme.c/bdev_nvme_ut.c
@@ -116,6 +116,8 @@ DEFINE_STUB(spdk_nvme_ns_get_max_io_xfer_size, uint32_t, (struct spdk_nvme_ns *n
 
 DEFINE_STUB(spdk_nvme_ns_get_extended_sector_size, uint32_t, (struct spdk_nvme_ns *ns), 0);
 
+DEFINE_STUB(spdk_nvme_ns_get_sector_size, uint32_t, (struct spdk_nvme_ns *ns), 0);
+
 DEFINE_STUB(spdk_nvme_ns_get_pi_type, enum spdk_nvme_pi_type, (struct spdk_nvme_ns *ns), 0);
 
 DEFINE_STUB(spdk_nvme_ns_supports_compare, bool, (struct spdk_nvme_ns *ns), false);
@@ -127,8 +129,8 @@ DEFINE_STUB(spdk_nvme_ns_get_dealloc_logical_block_read_value,
 
 DEFINE_STUB(spdk_nvme_ns_get_optimal_io_boundary, uint32_t, (struct spdk_nvme_ns *ns), 0);
 
-DEFINE_STUB(spdk_nvme_ns_get_uuid, const struct spdk_uuid *,
-	    (const struct spdk_nvme_ns *ns), NULL);
+DEFINE_STUB(spdk_nvme_ns_get_ana_state, enum spdk_nvme_ana_state,
+	    (const struct spdk_nvme_ns *ns), 0);
 
 DEFINE_STUB(spdk_nvme_ns_get_csi, enum spdk_nvme_csi,
 	    (const struct spdk_nvme_ns *ns), 0);
@@ -204,9 +206,9 @@ DEFINE_STUB_V(bdev_ocssd_depopulate_namespace, (struct nvme_bdev_ns *nvme_ns));
 DEFINE_STUB_V(bdev_ocssd_namespace_config_json, (struct spdk_json_write_ctx *w,
 		struct nvme_bdev_ns *nvme_ns));
 
-DEFINE_STUB(bdev_ocssd_create_io_channel, int, (struct nvme_io_channel *ioch), 0);
+DEFINE_STUB(bdev_ocssd_create_io_channel, int, (struct nvme_io_path *ioch), 0);
 
-DEFINE_STUB_V(bdev_ocssd_destroy_io_channel, (struct nvme_io_channel *ioch));
+DEFINE_STUB_V(bdev_ocssd_destroy_io_channel, (struct nvme_io_path *ioch));
 
 DEFINE_STUB(bdev_ocssd_init_ctrlr, int, (struct nvme_bdev_ctrlr *nvme_bdev_ctrlr), 0);
 
@@ -231,6 +233,7 @@ struct spdk_nvme_ns {
 	struct spdk_nvme_ctrlr		*ctrlr;
 	uint32_t			id;
 	bool				is_active;
+	struct spdk_uuid		uuid;
 };
 
 struct spdk_nvme_qpair {
@@ -742,6 +745,13 @@ spdk_nvme_ns_get_num_sectors(struct spdk_nvme_ns *ns)
 {
 	return _nvme_ns_get_data(ns)->nsze;
 }
+
+const struct spdk_uuid *
+spdk_nvme_ns_get_uuid(const struct spdk_nvme_ns *ns)
+{
+	return &ns->uuid;
+}
+
 int
 spdk_nvme_ns_cmd_read_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair, void *buffer,
 			      void *metadata, uint64_t lba, uint32_t lba_count,
@@ -801,6 +811,15 @@ spdk_nvme_ns_cmd_dataset_management(struct spdk_nvme_ns *ns, struct spdk_nvme_qp
 	return ut_submit_nvme_request(ns, qpair, SPDK_NVME_OPC_DATASET_MANAGEMENT, cb_fn, cb_arg);
 }
 
+int
+spdk_nvme_ns_cmd_write_zeroes(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+			      uint64_t lba, uint32_t lba_count,
+			      spdk_nvme_cmd_cb cb_fn, void *cb_arg,
+			      uint32_t io_flags)
+{
+	return ut_submit_nvme_request(ns, qpair, SPDK_NVME_OPC_WRITE_ZEROES, cb_fn, cb_arg);
+}
+
 struct spdk_nvme_poll_group *
 spdk_nvme_poll_group_create(void *ctx, struct spdk_nvme_accel_fn_table *table)
 {
@@ -986,7 +1005,8 @@ test_create_ctrlr(void)
 
 	ut_init_trid(&trid);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	CU_ASSERT(nvme_bdev_ctrlr_get_by_name("nvme0") != NULL);
 
@@ -1008,7 +1028,7 @@ test_reset_ctrlr(void)
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = NULL;
 	struct nvme_bdev_ctrlr_trid *curr_trid;
 	struct spdk_io_channel *ch1, *ch2;
-	struct nvme_io_channel *nvme_ch1, *nvme_ch2;
+	struct nvme_io_path *io_path1, *io_path2;
 	int rc;
 
 	ut_init_trid(&trid);
@@ -1016,7 +1036,8 @@ test_reset_ctrlr(void)
 
 	set_thread(0);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name("nvme0");
 	SPDK_CU_ASSERT_FATAL(nvme_bdev_ctrlr != NULL);
@@ -1027,16 +1048,16 @@ test_reset_ctrlr(void)
 	ch1 = spdk_get_io_channel(nvme_bdev_ctrlr);
 	SPDK_CU_ASSERT_FATAL(ch1 != NULL);
 
-	nvme_ch1 = spdk_io_channel_get_ctx(ch1);
-	CU_ASSERT(nvme_ch1->qpair != NULL);
+	io_path1 = spdk_io_channel_get_ctx(ch1);
+	CU_ASSERT(io_path1->qpair != NULL);
 
 	set_thread(1);
 
 	ch2 = spdk_get_io_channel(nvme_bdev_ctrlr);
 	SPDK_CU_ASSERT_FATAL(ch2 != NULL);
 
-	nvme_ch2 = spdk_io_channel_get_ctx(ch2);
-	CU_ASSERT(nvme_ch2->qpair != NULL);
+	io_path2 = spdk_io_channel_get_ctx(ch2);
+	CU_ASSERT(io_path2->qpair != NULL);
 
 	/* Reset starts from thread 1. */
 	set_thread(1);
@@ -1062,28 +1083,28 @@ test_reset_ctrlr(void)
 	rc = _bdev_nvme_reset(nvme_bdev_ctrlr);
 	CU_ASSERT(rc == 0);
 	CU_ASSERT(nvme_bdev_ctrlr->resetting == true);
-	CU_ASSERT(nvme_ch1->qpair != NULL);
-	CU_ASSERT(nvme_ch2->qpair != NULL);
+	CU_ASSERT(io_path1->qpair != NULL);
+	CU_ASSERT(io_path2->qpair != NULL);
 
 	poll_thread_times(0, 1);
-	CU_ASSERT(nvme_ch1->qpair == NULL);
-	CU_ASSERT(nvme_ch2->qpair != NULL);
+	CU_ASSERT(io_path1->qpair == NULL);
+	CU_ASSERT(io_path2->qpair != NULL);
 
 	poll_thread_times(1, 1);
-	CU_ASSERT(nvme_ch1->qpair == NULL);
-	CU_ASSERT(nvme_ch2->qpair == NULL);
+	CU_ASSERT(io_path1->qpair == NULL);
+	CU_ASSERT(io_path2->qpair == NULL);
 	CU_ASSERT(ctrlr.is_failed == true);
 
 	poll_thread_times(1, 1);
 	CU_ASSERT(ctrlr.is_failed == false);
 
 	poll_thread_times(0, 1);
-	CU_ASSERT(nvme_ch1->qpair != NULL);
-	CU_ASSERT(nvme_ch2->qpair == NULL);
+	CU_ASSERT(io_path1->qpair != NULL);
+	CU_ASSERT(io_path2->qpair == NULL);
 
 	poll_thread_times(1, 1);
-	CU_ASSERT(nvme_ch1->qpair != NULL);
-	CU_ASSERT(nvme_ch2->qpair != NULL);
+	CU_ASSERT(io_path1->qpair != NULL);
+	CU_ASSERT(io_path2->qpair != NULL);
 	CU_ASSERT(nvme_bdev_ctrlr->resetting == true);
 	CU_ASSERT(curr_trid->is_failed == true);
 
@@ -1121,7 +1142,8 @@ test_race_between_reset_and_destruct_ctrlr(void)
 
 	set_thread(0);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name("nvme0");
 	SPDK_CU_ASSERT_FATAL(nvme_bdev_ctrlr != NULL);
@@ -1197,7 +1219,8 @@ test_failover_ctrlr(void)
 
 	set_thread(0);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid1, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid1, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name("nvme0");
 	SPDK_CU_ASSERT_FATAL(nvme_bdev_ctrlr != NULL);
@@ -1259,7 +1282,8 @@ test_failover_ctrlr(void)
 	set_thread(0);
 
 	/* Second, test two trids case. */
-	bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid2, NULL);
+	rc = bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid2);
+	CU_ASSERT(rc == 0);
 
 	curr_trid = TAILQ_FIRST(&nvme_bdev_ctrlr->trids);
 	SPDK_CU_ASSERT_FATAL(curr_trid != NULL);
@@ -1335,9 +1359,8 @@ test_pending_reset(void)
 	const int STRING_SIZE = 32;
 	const char *attached_names[STRING_SIZE];
 	struct spdk_bdev_io *first_bdev_io, *second_bdev_io;
-	struct nvme_bdev_io *first_bio, *second_bio;
 	struct spdk_io_channel *ch1, *ch2;
-	struct nvme_io_channel *nvme_ch1, *nvme_ch2;
+	struct nvme_io_path *io_path1, *io_path2;
 	int rc;
 
 	memset(attached_names, 0, sizeof(char *) * STRING_SIZE);
@@ -1346,12 +1369,10 @@ test_pending_reset(void)
 	first_bdev_io = calloc(1, sizeof(struct spdk_bdev_io) + sizeof(struct nvme_bdev_io));
 	SPDK_CU_ASSERT_FATAL(first_bdev_io != NULL);
 	first_bdev_io->internal.status = SPDK_BDEV_IO_STATUS_FAILED;
-	first_bio = (struct nvme_bdev_io *)first_bdev_io->driver_ctx;
 
 	second_bdev_io = calloc(1, sizeof(struct spdk_bdev_io) + sizeof(struct nvme_bdev_io));
 	SPDK_CU_ASSERT_FATAL(second_bdev_io != NULL);
 	second_bdev_io->internal.status = SPDK_BDEV_IO_STATUS_FAILED;
-	second_bio = (struct nvme_bdev_io *)second_bdev_io->driver_ctx;
 
 	set_thread(0);
 
@@ -1374,28 +1395,28 @@ test_pending_reset(void)
 	ch1 = spdk_get_io_channel(nvme_bdev_ctrlr);
 	SPDK_CU_ASSERT_FATAL(ch1 != NULL);
 
-	nvme_ch1 = spdk_io_channel_get_ctx(ch1);
+	io_path1 = spdk_io_channel_get_ctx(ch1);
 
 	set_thread(1);
 
 	ch2 = spdk_get_io_channel(nvme_bdev_ctrlr);
 	SPDK_CU_ASSERT_FATAL(ch2 != NULL);
 
-	nvme_ch2 = spdk_io_channel_get_ctx(ch2);
+	io_path2 = spdk_io_channel_get_ctx(ch2);
 
 	/* The first reset request is submitted on thread 1, and the second reset request
 	 * is submitted on thread 0 while processing the first request.
 	 */
-	rc = bdev_nvme_reset(nvme_ch2, first_bio);
+	rc = bdev_nvme_reset(io_path2, first_bdev_io);
 	CU_ASSERT(rc == 0);
 	CU_ASSERT(nvme_bdev_ctrlr->resetting == true);
-	CU_ASSERT(TAILQ_EMPTY(&nvme_ch2->pending_resets));
+	CU_ASSERT(TAILQ_EMPTY(&io_path2->pending_resets));
 
 	set_thread(0);
 
-	rc = bdev_nvme_reset(nvme_ch1, second_bio);
+	rc = bdev_nvme_reset(io_path1, second_bdev_io);
 	CU_ASSERT(rc == 0);
-	CU_ASSERT(TAILQ_FIRST(&nvme_ch1->pending_resets) == second_bdev_io);
+	CU_ASSERT(TAILQ_FIRST(&io_path1->pending_resets) == second_bdev_io);
 
 	poll_threads();
 
@@ -1411,16 +1432,16 @@ test_pending_reset(void)
 	 */
 	set_thread(1);
 
-	rc = bdev_nvme_reset(nvme_ch2, first_bio);
+	rc = bdev_nvme_reset(io_path2, first_bdev_io);
 	CU_ASSERT(rc == 0);
 	CU_ASSERT(nvme_bdev_ctrlr->resetting == true);
-	CU_ASSERT(TAILQ_EMPTY(&nvme_ch2->pending_resets));
+	CU_ASSERT(TAILQ_EMPTY(&io_path2->pending_resets));
 
 	set_thread(0);
 
-	rc = bdev_nvme_reset(nvme_ch1, second_bio);
+	rc = bdev_nvme_reset(io_path1, second_bdev_io);
 	CU_ASSERT(rc == 0);
-	CU_ASSERT(TAILQ_FIRST(&nvme_ch1->pending_resets) == second_bdev_io);
+	CU_ASSERT(TAILQ_FIRST(&io_path1->pending_resets) == second_bdev_io);
 
 	ctrlr->fail_reset = true;
 
@@ -1586,7 +1607,7 @@ test_reconnect_qpair(void)
 	struct spdk_nvme_ctrlr ctrlr = {};
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = NULL;
 	struct spdk_io_channel *ch;
-	struct nvme_io_channel *nvme_ch;
+	struct nvme_io_path *io_path;
 	int rc;
 
 	set_thread(0);
@@ -1594,7 +1615,8 @@ test_reconnect_qpair(void)
 	ut_init_trid(&trid);
 	TAILQ_INIT(&ctrlr.active_io_qpairs);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name("nvme0");
 	SPDK_CU_ASSERT_FATAL(nvme_bdev_ctrlr != NULL);
@@ -1602,26 +1624,26 @@ test_reconnect_qpair(void)
 	ch = spdk_get_io_channel(nvme_bdev_ctrlr);
 	SPDK_CU_ASSERT_FATAL(ch != NULL);
 
-	nvme_ch = spdk_io_channel_get_ctx(ch);
-	CU_ASSERT(nvme_ch->qpair != NULL);
-	CU_ASSERT(nvme_ch->group != NULL);
-	CU_ASSERT(nvme_ch->group->group != NULL);
-	CU_ASSERT(nvme_ch->group->poller != NULL);
+	io_path = spdk_io_channel_get_ctx(ch);
+	CU_ASSERT(io_path->qpair != NULL);
+	CU_ASSERT(io_path->group != NULL);
+	CU_ASSERT(io_path->group->group != NULL);
+	CU_ASSERT(io_path->group->poller != NULL);
 
 	/* Test if the disconnected qpair is reconnected. */
-	nvme_ch->qpair->is_connected = false;
+	io_path->qpair->is_connected = false;
 
 	poll_threads();
 
-	CU_ASSERT(nvme_ch->qpair->is_connected == true);
+	CU_ASSERT(io_path->qpair->is_connected == true);
 
 	/* If the ctrlr is failed, reconnecting qpair should fail too. */
-	nvme_ch->qpair->is_connected = false;
+	io_path->qpair->is_connected = false;
 	ctrlr.is_failed = true;
 
 	poll_threads();
 
-	CU_ASSERT(nvme_ch->qpair->is_connected == false);
+	CU_ASSERT(io_path->qpair->is_connected == false);
 
 	spdk_put_io_channel(ch);
 
@@ -1716,12 +1738,12 @@ static void
 ut_test_submit_nvme_cmd(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io,
 			enum spdk_bdev_io_type io_type)
 {
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
 	struct nvme_bdev *nbdev = (struct nvme_bdev *)bdev_io->bdev->ctxt;
-	struct nvme_bdev_ns *nvme_ns = NULL;
+	struct spdk_nvme_ns *ns = NULL;
 	struct spdk_nvme_qpair *qpair = NULL;
 
-	CU_ASSERT(bdev_nvme_find_io_path(nbdev, nvme_ch, &nvme_ns, &qpair));
+	CU_ASSERT(bdev_nvme_find_io_path(nbdev, io_path, &ns, &qpair));
 
 	bdev_io->type = io_type;
 	bdev_io->internal.in_submit_request = true;
@@ -1742,12 +1764,12 @@ static void
 ut_test_submit_nop(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io,
 		   enum spdk_bdev_io_type io_type)
 {
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
 	struct nvme_bdev *nbdev = (struct nvme_bdev *)bdev_io->bdev->ctxt;
-	struct nvme_bdev_ns *nvme_ns = NULL;
+	struct spdk_nvme_ns *ns = NULL;
 	struct spdk_nvme_qpair *qpair = NULL;
 
-	CU_ASSERT(bdev_nvme_find_io_path(nbdev, nvme_ch, &nvme_ns, &qpair));
+	CU_ASSERT(bdev_nvme_find_io_path(nbdev, io_path, &ns, &qpair));
 
 	bdev_io->type = io_type;
 	bdev_io->internal.in_submit_request = true;
@@ -1762,14 +1784,14 @@ ut_test_submit_nop(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io,
 static void
 ut_test_submit_fused_nvme_cmd(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
 {
-	struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
+	struct nvme_io_path *io_path = spdk_io_channel_get_ctx(ch);
 	struct nvme_bdev_io *bio = (struct nvme_bdev_io *)bdev_io->driver_ctx;
 	struct ut_nvme_req *req;
 	struct nvme_bdev *nbdev = (struct nvme_bdev *)bdev_io->bdev->ctxt;
-	struct nvme_bdev_ns *nvme_ns = NULL;
+	struct spdk_nvme_ns *ns = NULL;
 	struct spdk_nvme_qpair *qpair = NULL;
 
-	CU_ASSERT(bdev_nvme_find_io_path(nbdev, nvme_ch, &nvme_ns, &qpair));
+	CU_ASSERT(bdev_nvme_find_io_path(nbdev, io_path, &ns, &qpair));
 
 	/* Only compare and write now. */
 	bdev_io->type = SPDK_BDEV_IO_TYPE_COMPARE_AND_WRITE;
@@ -1782,7 +1804,7 @@ ut_test_submit_fused_nvme_cmd(struct spdk_io_channel *ch, struct spdk_bdev_io *b
 	CU_ASSERT(bio->first_fused_submitted == true);
 
 	/* First outstanding request is compare operation. */
-	req = TAILQ_FIRST(&nvme_ch->qpair->outstanding_reqs);
+	req = TAILQ_FIRST(&io_path->qpair->outstanding_reqs);
 	SPDK_CU_ASSERT_FATAL(req != NULL);
 	CU_ASSERT(req->opc == SPDK_NVME_OPC_COMPARE);
 	req->cpl.cdw0 = SPDK_NVME_OPC_COMPARE;
@@ -1875,7 +1897,6 @@ test_submit_nvme_cmd(void)
 	ut_test_submit_nvme_cmd(ch, bdev_io, SPDK_BDEV_IO_TYPE_READ);
 	ut_test_submit_nvme_cmd(ch, bdev_io, SPDK_BDEV_IO_TYPE_WRITE);
 	ut_test_submit_nvme_cmd(ch, bdev_io, SPDK_BDEV_IO_TYPE_COMPARE);
-	ut_test_submit_nvme_cmd(ch, bdev_io, SPDK_BDEV_IO_TYPE_WRITE_ZEROES);
 	ut_test_submit_nvme_cmd(ch, bdev_io, SPDK_BDEV_IO_TYPE_UNMAP);
 
 	ut_test_submit_nop(ch, bdev_io, SPDK_BDEV_IO_TYPE_FLUSH);
@@ -1915,12 +1936,14 @@ test_remove_trid(void)
 
 	set_thread(0);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid1, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid1, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name("nvme0");
 	SPDK_CU_ASSERT_FATAL(nvme_bdev_ctrlr != NULL);
 
-	bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid2, NULL);
+	rc = bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid2);
+	CU_ASSERT(rc == 0);
 
 	/* trid3 is not in the registered list. */
 	rc = bdev_nvme_delete("nvme0", &trid3);
@@ -1934,7 +1957,8 @@ test_remove_trid(void)
 		CU_ASSERT(spdk_nvme_transport_id_compare(&ctrid->trid, &trid2) != 0);
 	}
 
-	bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid3, NULL);
+	rc = bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid3);
+	CU_ASSERT(rc == 0);
 
 	/* trid1 is currently used and trid3 is an alternative path.
 	 * If we remove trid1, path is changed to trid3.
@@ -1963,12 +1987,14 @@ test_remove_trid(void)
 
 	CU_ASSERT(nvme_bdev_ctrlr_get_by_name("nvme0") == NULL);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid1, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid1, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name("nvme0");
 	SPDK_CU_ASSERT_FATAL(nvme_bdev_ctrlr != NULL);
 
-	bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid2, NULL);
+	rc = bdev_nvme_add_secondary_trid(nvme_bdev_ctrlr, &ctrlr, &trid2);
+	CU_ASSERT(rc == 0);
 
 	/* If trid is not specified, nvme_bdev_ctrlr itself is removed. */
 	rc = bdev_nvme_delete("nvme0", NULL);
@@ -1991,13 +2017,13 @@ test_abort(void)
 	const char *attached_names[STRING_SIZE];
 	struct nvme_bdev *bdev;
 	struct spdk_bdev_io *write_io, *admin_io, *abort_io;
-	struct spdk_io_channel *ch;
-	struct nvme_io_channel *nvme_ch;
+	struct spdk_io_channel *ch1, *ch2;
+	struct nvme_io_path *io_path1;
 	int rc;
 
 	/* Create ctrlr on thread 1 and submit I/O and admin requests to be aborted on
-	 * thread 0. Abort requests are submitted on thread 0. Aborting I/O requests are
-	 * done on thread 0 but aborting admin requests are done on thread 1.
+	 * thread 0. Aborting I/O requests are submitted on thread 0. Aborting admin requests
+	 * are submitted on thread 1. Both should succeed.
 	 */
 
 	ut_init_trid(&trid);
@@ -2023,8 +2049,6 @@ test_abort(void)
 	bdev = nvme_bdev_ctrlr->namespaces[0]->bdev;
 	SPDK_CU_ASSERT_FATAL(bdev != NULL);
 
-	set_thread(0);
-
 	write_io = calloc(1, sizeof(struct spdk_bdev_io) + sizeof(struct nvme_bdev_io));
 	SPDK_CU_ASSERT_FATAL(write_io != NULL);
 	write_io->bdev = &bdev->disk;
@@ -2042,17 +2066,23 @@ test_abort(void)
 	abort_io->bdev = &bdev->disk;
 	abort_io->type = SPDK_BDEV_IO_TYPE_ABORT;
 
-	ch = spdk_get_io_channel(nvme_bdev_ctrlr);
-	SPDK_CU_ASSERT_FATAL(ch != NULL);
-	nvme_ch = spdk_io_channel_get_ctx(ch);
+	set_thread(0);
 
-	write_io->internal.ch = (struct spdk_bdev_channel *)ch;
-	admin_io->internal.ch = (struct spdk_bdev_channel *)ch;
-	abort_io->internal.ch = (struct spdk_bdev_channel *)ch;
+	ch1 = spdk_get_io_channel(nvme_bdev_ctrlr);
+	SPDK_CU_ASSERT_FATAL(ch1 != NULL);
+	io_path1 = spdk_io_channel_get_ctx(ch1);
+
+	set_thread(1);
+
+	ch2 = spdk_get_io_channel(nvme_bdev_ctrlr);
+	SPDK_CU_ASSERT_FATAL(ch2 != NULL);
+
+	write_io->internal.ch = (struct spdk_bdev_channel *)ch1;
+	abort_io->internal.ch = (struct spdk_bdev_channel *)ch1;
 
 	/* Aborting the already completed request should fail. */
 	write_io->internal.in_submit_request = true;
-	bdev_nvme_submit_request(ch, write_io);
+	bdev_nvme_submit_request(ch1, write_io);
 	poll_threads();
 
 	CU_ASSERT(write_io->internal.in_submit_request == false);
@@ -2060,7 +2090,7 @@ test_abort(void)
 	abort_io->u.abort.bio_to_abort = write_io;
 	abort_io->internal.in_submit_request = true;
 
-	bdev_nvme_submit_request(ch, abort_io);
+	bdev_nvme_submit_request(ch1, abort_io);
 
 	poll_threads();
 
@@ -2068,8 +2098,11 @@ test_abort(void)
 	CU_ASSERT(abort_io->internal.status == SPDK_BDEV_IO_STATUS_FAILED);
 	CU_ASSERT(ctrlr->adminq.num_outstanding_reqs == 0);
 
+	admin_io->internal.ch = (struct spdk_bdev_channel *)ch1;
+	abort_io->internal.ch = (struct spdk_bdev_channel *)ch2;
+
 	admin_io->internal.in_submit_request = true;
-	bdev_nvme_submit_request(ch, admin_io);
+	bdev_nvme_submit_request(ch1, admin_io);
 	spdk_delay_us(10000);
 	poll_threads();
 
@@ -2078,7 +2111,7 @@ test_abort(void)
 	abort_io->u.abort.bio_to_abort = admin_io;
 	abort_io->internal.in_submit_request = true;
 
-	bdev_nvme_submit_request(ch, abort_io);
+	bdev_nvme_submit_request(ch2, abort_io);
 
 	poll_threads();
 
@@ -2088,15 +2121,16 @@ test_abort(void)
 
 	/* Aborting the write request should succeed. */
 	write_io->internal.in_submit_request = true;
-	bdev_nvme_submit_request(ch, write_io);
+	bdev_nvme_submit_request(ch1, write_io);
 
 	CU_ASSERT(write_io->internal.in_submit_request == true);
-	CU_ASSERT(nvme_ch->qpair->num_outstanding_reqs == 1);
+	CU_ASSERT(io_path1->qpair->num_outstanding_reqs == 1);
 
+	abort_io->internal.ch = (struct spdk_bdev_channel *)ch1;
 	abort_io->u.abort.bio_to_abort = write_io;
 	abort_io->internal.in_submit_request = true;
 
-	bdev_nvme_submit_request(ch, abort_io);
+	bdev_nvme_submit_request(ch1, abort_io);
 
 	spdk_delay_us(10000);
 	poll_threads();
@@ -2106,19 +2140,20 @@ test_abort(void)
 	CU_ASSERT(ctrlr->adminq.num_outstanding_reqs == 0);
 	CU_ASSERT(write_io->internal.in_submit_request == false);
 	CU_ASSERT(write_io->internal.status == SPDK_BDEV_IO_STATUS_ABORTED);
-	CU_ASSERT(nvme_ch->qpair->num_outstanding_reqs == 0);
+	CU_ASSERT(io_path1->qpair->num_outstanding_reqs == 0);
 
 	/* Aborting the admin request should succeed. */
 	admin_io->internal.in_submit_request = true;
-	bdev_nvme_submit_request(ch, admin_io);
+	bdev_nvme_submit_request(ch1, admin_io);
 
 	CU_ASSERT(admin_io->internal.in_submit_request == true);
 	CU_ASSERT(ctrlr->adminq.num_outstanding_reqs == 1);
 
+	abort_io->internal.ch = (struct spdk_bdev_channel *)ch2;
 	abort_io->u.abort.bio_to_abort = admin_io;
 	abort_io->internal.in_submit_request = true;
 
-	bdev_nvme_submit_request(ch, abort_io);
+	bdev_nvme_submit_request(ch2, abort_io);
 
 	spdk_delay_us(10000);
 	poll_threads();
@@ -2130,7 +2165,13 @@ test_abort(void)
 	CU_ASSERT(admin_io->internal.status == SPDK_BDEV_IO_STATUS_ABORTED);
 	CU_ASSERT(ctrlr->adminq.num_outstanding_reqs == 0);
 
-	spdk_put_io_channel(ch);
+	set_thread(0);
+
+	spdk_put_io_channel(ch1);
+
+	set_thread(1);
+
+	spdk_put_io_channel(ch2);
 
 	poll_threads();
 
@@ -2155,7 +2196,7 @@ test_get_io_qpair(void)
 	struct spdk_nvme_ctrlr ctrlr = {};
 	struct nvme_bdev_ctrlr *nvme_bdev_ctrlr = NULL;
 	struct spdk_io_channel *ch;
-	struct nvme_io_channel *nvme_ch;
+	struct nvme_io_path *io_path;
 	struct spdk_nvme_qpair *qpair;
 	int rc;
 
@@ -2164,18 +2205,19 @@ test_get_io_qpair(void)
 
 	set_thread(0);
 
-	nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	rc = nvme_bdev_ctrlr_create(&ctrlr, "nvme0", &trid, 0, NULL);
+	CU_ASSERT(rc == 0);
 
 	nvme_bdev_ctrlr = nvme_bdev_ctrlr_get_by_name("nvme0");
 	SPDK_CU_ASSERT_FATAL(nvme_bdev_ctrlr != NULL);
 
 	ch = spdk_get_io_channel(nvme_bdev_ctrlr);
 	SPDK_CU_ASSERT_FATAL(ch != NULL);
-	nvme_ch = spdk_io_channel_get_ctx(ch);
-	CU_ASSERT(nvme_ch->qpair != NULL);
+	io_path = spdk_io_channel_get_ctx(ch);
+	CU_ASSERT(io_path->qpair != NULL);
 
 	qpair = bdev_nvme_get_io_qpair(ch);
-	CU_ASSERT(qpair == nvme_ch->qpair);
+	CU_ASSERT(qpair == io_path->qpair);
 
 	spdk_put_io_channel(ch);
 
@@ -2252,6 +2294,55 @@ test_bdev_unregister(void)
 	CU_ASSERT(nvme_bdev_ctrlr_get_by_name("nvme0") == NULL);
 }
 
+static void
+test_compare_ns(void)
+{
+	struct spdk_nvme_ns_data nsdata1 = {}, nsdata2 = {};
+	struct spdk_nvme_ctrlr ctrlr1 = { .nsdata = &nsdata1, }, ctrlr2 = { .nsdata = &nsdata2, };
+	struct spdk_nvme_ns ns1 = { .id = 1, .ctrlr = &ctrlr1, }, ns2 = { .id = 1, .ctrlr = &ctrlr2, };
+
+	/* No IDs are defined. */
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == true);
+
+	/* Only EUI64 are defined and not matched. */
+	nsdata1.eui64 = 0xABCDEF0123456789;
+	nsdata2.eui64 = 0xBBCDEF0123456789;
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == false);
+
+	/* Only EUI64 are defined and matched. */
+	nsdata2.eui64 = 0xABCDEF0123456789;
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == true);
+
+	/* Only NGUID are defined and not matched. */
+	nsdata1.eui64 = 0x0;
+	nsdata2.eui64 = 0x0;
+	nsdata1.nguid[0] = 0x12;
+	nsdata2.nguid[0] = 0x10;
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == false);
+
+	/* Only NGUID are defined and matched. */
+	nsdata2.nguid[0] = 0x12;
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == true);
+
+	/* Only UUID are defined and not matched. */
+	nsdata1.nguid[0] = 0x0;
+	nsdata2.nguid[0] = 0x0;
+	ns1.uuid.u.raw[0] = 0xAA;
+	ns2.uuid.u.raw[0] = 0xAB;
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == false);
+
+	/* Only UUID are defined and matched. */
+	ns1.uuid.u.raw[0] = 0xAB;
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == true);
+
+	/* All EUI64, NGUID, and UUID are defined and matched. */
+	nsdata1.eui64 = 0x123456789ABCDEF;
+	nsdata2.eui64 = 0x123456789ABCDEF;
+	nsdata1.nguid[15] = 0x34;
+	nsdata2.nguid[15] = 0x34;
+	CU_ASSERT(bdev_nvme_compare_ns(&ns1, &ns2) == true);
+}
+
 static void
 init_accel(void)
 {
@@ -2289,6 +2380,7 @@ main(int argc, const char **argv)
 	CU_ADD_TEST(suite, test_abort);
 	CU_ADD_TEST(suite, test_get_io_qpair);
 	CU_ADD_TEST(suite, test_bdev_unregister);
+	CU_ADD_TEST(suite, test_compare_ns);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 
diff --git a/test/unit/lib/bdev/nvme/bdev_ocssd.c/bdev_ocssd_ut.c b/test/unit/lib/bdev/nvme/bdev_ocssd.c/bdev_ocssd_ut.c
index 91a573ace..5cbb4ec91 100644
--- a/test/unit/lib/bdev/nvme/bdev_ocssd.c/bdev_ocssd_ut.c
+++ b/test/unit/lib/bdev/nvme/bdev_ocssd.c/bdev_ocssd_ut.c
@@ -34,10 +34,10 @@
 #include "spdk/stdinc.h"
 #include "spdk_cunit.h"
 #include "spdk/nvme_ocssd_spec.h"
-#include "spdk/thread.h"
 #include "spdk/bdev_module.h"
 #include "spdk/util.h"
 #include "spdk_internal/mock.h"
+#include "thread/thread_internal.h"
 
 #include "bdev/nvme/bdev_ocssd.c"
 #include "bdev/nvme/common.c"
@@ -540,7 +540,7 @@ delete_nvme_bdev_controller(struct nvme_bdev_ctrlr *nvme_bdev_ctrlr)
 		bdev_ocssd_depopulate_namespace(nvme_bdev_ctrlr->namespaces[nsid]);
 	}
 
-	nvme_bdev_ctrlr_destruct(nvme_bdev_ctrlr);
+	nvme_bdev_ctrlr_release(nvme_bdev_ctrlr);
 	spdk_delay_us(1000);
 
 	while (spdk_thread_poll(g_thread, 0, 0) > 0) {}
@@ -885,7 +885,7 @@ test_get_zone_info(void)
 	struct spdk_bdev *bdev;
 	struct spdk_bdev_io *bdev_io;
 	struct spdk_io_channel *ch;
-	struct nvme_io_channel *nvme_ch;
+	struct nvme_io_path *io_path;
 #define MAX_ZONE_INFO_COUNT 64
 	struct spdk_bdev_zone_info zone_info[MAX_ZONE_INFO_COUNT];
 	struct spdk_ocssd_chunk_information_entry *chunk_info;
@@ -915,12 +915,12 @@ test_get_zone_info(void)
 	bdev = spdk_bdev_get_by_name(bdev_name);
 	SPDK_CU_ASSERT_FATAL(bdev != NULL);
 
-	ch = calloc(1, sizeof(*ch) + sizeof(*nvme_ch));
+	ch = calloc(1, sizeof(*ch) + sizeof(*io_path));
 	SPDK_CU_ASSERT_FATAL(ch != NULL);
 
-	nvme_ch = spdk_io_channel_get_ctx(ch);
-	nvme_ch->ctrlr = nvme_bdev_ctrlr;
-	nvme_ch->qpair = (struct spdk_nvme_qpair *)0x1;
+	io_path = spdk_io_channel_get_ctx(ch);
+	io_path->ctrlr = nvme_bdev_ctrlr;
+	io_path->qpair = (struct spdk_nvme_qpair *)0x1;
 
 	bdev_io = alloc_ocssd_io();
 	bdev_io->internal.cb = get_zone_info_cb;
diff --git a/test/unit/lib/bdev/part.c/part_ut.c b/test/unit/lib/bdev/part.c/part_ut.c
index b2c9363c5..4e3c52889 100644
--- a/test/unit/lib/bdev/part.c/part_ut.c
+++ b/test/unit/lib/bdev/part.c/part_ut.c
@@ -43,16 +43,6 @@
 #include "bdev/bdev.c"
 #include "bdev/part.c"
 
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_owner, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_object, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type,
-		uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_type, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
 DEFINE_STUB(spdk_notify_send, uint64_t, (const char *type, const char *ctx), 0);
 DEFINE_STUB(spdk_notify_type_register, struct spdk_notify_type *, (const char *type), NULL);
 
diff --git a/test/unit/lib/bdev/raid/bdev_raid.c/bdev_raid_ut.c b/test/unit/lib/bdev/raid/bdev_raid.c/bdev_raid_ut.c
index 15eed95a3..574f8b09d 100644
--- a/test/unit/lib/bdev/raid/bdev_raid.c/bdev_raid_ut.c
+++ b/test/unit/lib/bdev/raid/bdev_raid.c/bdev_raid_ut.c
@@ -35,6 +35,7 @@
 #include "spdk_cunit.h"
 #include "spdk/env.h"
 #include "spdk_internal/mock.h"
+#include "thread/thread_internal.h"
 #include "bdev/raid/bdev_raid.c"
 #include "bdev/raid/bdev_raid_rpc.c"
 #include "bdev/raid/raid0.c"
diff --git a/test/unit/lib/bdev/vbdev_lvol.c/vbdev_lvol_ut.c b/test/unit/lib/bdev/vbdev_lvol.c/vbdev_lvol_ut.c
index c66234736..80bb2403f 100644
--- a/test/unit/lib/bdev/vbdev_lvol.c/vbdev_lvol_ut.c
+++ b/test/unit/lib/bdev/vbdev_lvol.c/vbdev_lvol_ut.c
@@ -61,6 +61,12 @@ bool g_examine_done = false;
 bool g_bdev_alias_already_exists = false;
 bool g_lvs_with_name_already_exists = false;
 
+const struct spdk_bdev_aliases_list *
+spdk_bdev_get_aliases(const struct spdk_bdev *bdev)
+{
+	return &bdev->aliases;
+}
+
 int
 spdk_bdev_alias_add(struct spdk_bdev *bdev, const char *alias)
 {
@@ -75,8 +81,8 @@ spdk_bdev_alias_add(struct spdk_bdev *bdev, const char *alias)
 	tmp = calloc(1, sizeof(*tmp));
 	SPDK_CU_ASSERT_FATAL(tmp != NULL);
 
-	tmp->alias = strdup(alias);
-	SPDK_CU_ASSERT_FATAL(tmp->alias != NULL);
+	tmp->alias.name = strdup(alias);
+	SPDK_CU_ASSERT_FATAL(tmp->alias.name != NULL);
 
 	TAILQ_INSERT_TAIL(&bdev->aliases, tmp, tailq);
 
@@ -92,9 +98,9 @@ spdk_bdev_alias_del(struct spdk_bdev *bdev, const char *alias)
 
 	TAILQ_FOREACH(tmp, &bdev->aliases, tailq) {
 		SPDK_CU_ASSERT_FATAL(alias != NULL);
-		if (strncmp(alias, tmp->alias, SPDK_LVOL_NAME_MAX) == 0) {
+		if (strncmp(alias, tmp->alias.name, SPDK_LVOL_NAME_MAX) == 0) {
 			TAILQ_REMOVE(&bdev->aliases, tmp, tailq);
-			free(tmp->alias);
+			free(tmp->alias.name);
 			free(tmp);
 			return 0;
 		}
@@ -110,7 +116,7 @@ spdk_bdev_alias_del_all(struct spdk_bdev *bdev)
 
 	TAILQ_FOREACH_SAFE(p, &bdev->aliases, tailq, tmp) {
 		TAILQ_REMOVE(&bdev->aliases, p, tailq);
-		free(p->alias);
+		free(p->alias.name);
 		free(p);
 	}
 }
@@ -1393,7 +1399,7 @@ ut_lvs_rename(void)
 	vbdev_lvs_rename(lvs, "new_lvs_name", lvol_store_op_complete, NULL);
 	CU_ASSERT(g_lvserrno == 0);
 	CU_ASSERT_STRING_EQUAL(lvs->name, "new_lvs_name");
-	CU_ASSERT_STRING_EQUAL(TAILQ_FIRST(&g_lvol->bdev->aliases)->alias, "new_lvs_name/lvol");
+	CU_ASSERT_STRING_EQUAL(TAILQ_FIRST(&g_lvol->bdev->aliases)->alias.name, "new_lvs_name/lvol");
 
 	/* Trying to rename lvs with name already used by another lvs */
 	/* This is a bdev_lvol test, so g_lvs_with_name_already_exists simulates
@@ -1402,7 +1408,7 @@ ut_lvs_rename(void)
 	vbdev_lvs_rename(lvs, "another_new_lvs_name", lvol_store_op_complete, NULL);
 	CU_ASSERT(g_lvserrno == -EEXIST);
 	CU_ASSERT_STRING_EQUAL(lvs->name, "new_lvs_name");
-	CU_ASSERT_STRING_EQUAL(TAILQ_FIRST(&g_lvol->bdev->aliases)->alias, "new_lvs_name/lvol");
+	CU_ASSERT_STRING_EQUAL(TAILQ_FIRST(&g_lvol->bdev->aliases)->alias.name, "new_lvs_name/lvol");
 	g_lvs_with_name_already_exists = false;
 
 	/* Unload lvol store */
diff --git a/test/unit/lib/bdev/vbdev_zone_block.c/vbdev_zone_block_ut.c b/test/unit/lib/bdev/vbdev_zone_block.c/vbdev_zone_block_ut.c
index bf62922c0..899418b78 100644
--- a/test/unit/lib/bdev/vbdev_zone_block.c/vbdev_zone_block_ut.c
+++ b/test/unit/lib/bdev/vbdev_zone_block.c/vbdev_zone_block_ut.c
@@ -35,7 +35,7 @@
 #include "spdk_cunit.h"
 #include "spdk/env.h"
 #include "spdk_internal/mock.h"
-#include "spdk/thread.h"
+#include "thread/thread_internal.h"
 #include "common/lib/test_env.c"
 #include "bdev/zone_block/vbdev_zone_block.c"
 #include "bdev/zone_block/vbdev_zone_block_rpc.c"
diff --git a/test/unit/lib/blob/blob.c/blob_ut.c b/test/unit/lib/blob/blob.c/blob_ut.c
index a63031bac..b11e002d4 100644
--- a/test/unit/lib/blob/blob.c/blob_ut.c
+++ b/test/unit/lib/blob/blob.c/blob_ut.c
@@ -821,7 +821,7 @@ blob_snapshot_freeze_io(void)
 	/* This is implementation specific.
 	 * Flag 'frozen_io' is set in _spdk_bs_snapshot_freeze_cpl callback.
 	 * Four async I/O operations happen before that. */
-	poll_thread_times(0, 3);
+	poll_thread_times(0, 5);
 
 	CU_ASSERT(TAILQ_EMPTY(&bs_channel->queued_io));
 
@@ -833,9 +833,8 @@ blob_snapshot_freeze_io(void)
 
 	/* Verify that I/O is queued */
 	CU_ASSERT(!TAILQ_EMPTY(&bs_channel->queued_io));
-	/* Verify that payload is not written to disk */
-	CU_ASSERT(memcmp(payload_zero, &g_dev_buffer[blob->active.clusters[0]*SPDK_BS_PAGE_SIZE],
-			 SPDK_BS_PAGE_SIZE) == 0);
+	/* Verify that payload is not written to disk, at this point the blobs already switched */
+	CU_ASSERT(blob->active.clusters[0] == 0);
 
 	/* Finish all operations including spdk_bs_create_snapshot */
 	poll_threads();
diff --git a/test/unit/lib/blob/bs_dev_common.c b/test/unit/lib/blob/bs_dev_common.c
index 4e94fef8b..686d03b6d 100644
--- a/test/unit/lib/blob/bs_dev_common.c
+++ b/test/unit/lib/blob/bs_dev_common.c
@@ -31,7 +31,7 @@
  *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-#include "spdk/thread.h"
+#include "thread/thread_internal.h"
 #include "bs_scheduler.c"
 
 
diff --git a/test/unit/lib/blob/bs_scheduler.c b/test/unit/lib/blob/bs_scheduler.c
index 4b58fa007..0e73120d5 100644
--- a/test/unit/lib/blob/bs_scheduler.c
+++ b/test/unit/lib/blob/bs_scheduler.c
@@ -31,6 +31,8 @@
  *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
+#include "spdk/thread.h"
+
 bool g_scheduler_delay = false;
 
 struct scheduled_ops {
diff --git a/test/unit/lib/blobfs/blobfs_async_ut/blobfs_async_ut.c b/test/unit/lib/blobfs/blobfs_async_ut/blobfs_async_ut.c
index 804bc3a60..1f62b9ab5 100644
--- a/test/unit/lib/blobfs/blobfs_async_ut/blobfs_async_ut.c
+++ b/test/unit/lib/blobfs/blobfs_async_ut/blobfs_async_ut.c
@@ -47,14 +47,6 @@
 struct spdk_filesystem *g_fs;
 struct spdk_file *g_file;
 int g_fserrno;
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type,
-		uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_is_ptr, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
 
 static void
 fs_op_complete(void *ctx, int fserrno)
diff --git a/test/unit/lib/blobfs/blobfs_sync_ut/blobfs_sync_ut.c b/test/unit/lib/blobfs/blobfs_sync_ut/blobfs_sync_ut.c
index ae483dec9..4a5337e3f 100644
--- a/test/unit/lib/blobfs/blobfs_sync_ut/blobfs_sync_ut.c
+++ b/test/unit/lib/blobfs/blobfs_sync_ut/blobfs_sync_ut.c
@@ -36,8 +36,8 @@
 #include "spdk/blobfs.h"
 #include "spdk/env.h"
 #include "spdk/log.h"
-#include "spdk/thread.h"
 #include "spdk/barrier.h"
+#include "thread/thread_internal.h"
 
 #include "spdk_cunit.h"
 #include "unit/lib/blob/bs_dev_common.c"
@@ -49,14 +49,6 @@ struct spdk_filesystem *g_fs;
 struct spdk_file *g_file;
 int g_fserrno;
 struct spdk_thread *g_dispatch_thread = NULL;
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type,
-		uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_is_ptr, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
 
 struct ut_request {
 	fs_request_fn fn;
diff --git a/test/unit/lib/event/Makefile b/test/unit/lib/event/Makefile
index ea411460c..d87b0b1e7 100644
--- a/test/unit/lib/event/Makefile
+++ b/test/unit/lib/event/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-DIRS-y = subsystem.c app.c reactor.c
+DIRS-y = app.c reactor.c
 
 .PHONY: all clean $(DIRS-y)
 
diff --git a/test/unit/lib/event/app.c/app_ut.c b/test/unit/lib/event/app.c/app_ut.c
index 932eef8f7..dfd1ceddf 100644
--- a/test/unit/lib/event/app.c/app_ut.c
+++ b/test/unit/lib/event/app.c/app_ut.c
@@ -51,7 +51,8 @@ DEFINE_STUB_V(spdk_rpc_set_state, (uint32_t state));
 DEFINE_STUB(spdk_rpc_get_state, uint32_t, (void), SPDK_RPC_RUNTIME);
 DEFINE_STUB(spdk_rpc_initialize, int, (const char *listen_addr), 0);
 DEFINE_STUB_V(spdk_rpc_finish, (void));
-DEFINE_STUB_V(spdk_app_json_config_load, (const char *json_config_file, const char *rpc_addr,
+DEFINE_STUB_V(spdk_subsystem_init_from_json_config, (const char *json_config_file,
+		const char *rpc_addr,
 		spdk_subsystem_init_fn cb_fn, void *cb_arg, bool stop_on_error));
 DEFINE_STUB_V(spdk_reactors_start, (void));
 DEFINE_STUB_V(spdk_reactors_stop, (void *arg1));
diff --git a/test/unit/lib/event/reactor.c/reactor_ut.c b/test/unit/lib/event/reactor.c/reactor_ut.c
index 3d5acbb37..0753d3ec4 100644
--- a/test/unit/lib/event/reactor.c/reactor_ut.c
+++ b/test/unit/lib/event/reactor.c/reactor_ut.c
@@ -36,7 +36,7 @@
 #include "spdk_cunit.h"
 #include "common/lib/test_env.c"
 #include "event/reactor.c"
-#include "spdk_internal/thread.h"
+#include "spdk/thread.h"
 #include "event/scheduler_static.c"
 #include "event/scheduler_dynamic.c"
 
@@ -359,6 +359,7 @@ test_reactor_stats(void)
 	struct spdk_thread *thread1, *thread2;
 	struct spdk_reactor *reactor;
 	struct spdk_poller *busy1, *idle1, *busy2, *idle2;
+	struct spdk_thread_stats stats;
 	int rc __attribute__((unused));
 
 	/* Test case is the following:
@@ -370,9 +371,18 @@ test_reactor_stats(void)
 	 * - thread1 for 300 with idle
 	 * - thread2 for 400 with busy.
 	 * Then,
-	 * - both elapsed TSC of thread1 and thread2 should be 1000 (= 100 + 900).
+	 * - both elapsed TSC of thread1 and thread2 should be 1100 (= 100 + 1000).
 	 * - busy TSC of reactor should be 500 (= 100 + 400).
 	 * - idle TSC of reactor should be 500 (= 200 + 300).
+	 *
+	 * After that reactor0 runs with no threads for 900 TSC.
+	 * Create thread1 on reactor0 at TSC = 2000.
+	 * Reactor runs
+	 * - thread1 for 100 with busy
+	 * Then,
+	 * - elapsed TSC of thread1 should be 2100 (= 2000+ 100).
+	 * - busy TSC of reactor should be 600 (= 500 + 100).
+	 * - idle TSC of reactor should be 500 (= 500 + 900).
 	 */
 
 	MOCK_SET(spdk_env_get_current_core, 0);
@@ -383,7 +393,12 @@ test_reactor_stats(void)
 
 	spdk_cpuset_set_cpu(&cpuset, 0, true);
 
+	reactor = spdk_reactor_get(0);
+	SPDK_CU_ASSERT_FATAL(reactor != NULL);
+
+	/* First reactor_run() sets the tsc_last. */
 	MOCK_SET(spdk_get_ticks, 100);
+	reactor->tsc_last = spdk_get_ticks();
 
 	thread1 = spdk_thread_create(NULL, &cpuset);
 	SPDK_CU_ASSERT_FATAL(thread1 != NULL);
@@ -391,11 +406,6 @@ test_reactor_stats(void)
 	thread2 = spdk_thread_create(NULL, &cpuset);
 	SPDK_CU_ASSERT_FATAL(thread2 != NULL);
 
-	reactor = spdk_reactor_get(0);
-	SPDK_CU_ASSERT_FATAL(reactor != NULL);
-
-	reactor->tsc_last = 100;
-
 	spdk_set_thread(thread1);
 	busy1 = spdk_poller_register(poller_run_busy, (void *)100, 0);
 	CU_ASSERT(busy1 != NULL);
@@ -406,16 +416,23 @@ test_reactor_stats(void)
 
 	_reactor_run(reactor);
 
-	CU_ASSERT(thread1->tsc_last == 200);
-	CU_ASSERT(thread1->stats.busy_tsc == 100);
-	CU_ASSERT(thread1->stats.idle_tsc == 0);
-	CU_ASSERT(thread2->tsc_last == 500);
-	CU_ASSERT(thread2->stats.busy_tsc == 0);
-	CU_ASSERT(thread2->stats.idle_tsc == 300);
+	spdk_set_thread(thread1);
+	CU_ASSERT(spdk_thread_get_last_tsc(thread1) == 200);
+	CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+	CU_ASSERT(stats.busy_tsc == 100);
+	CU_ASSERT(stats.idle_tsc == 0);
+	spdk_set_thread(thread2);
+	CU_ASSERT(spdk_thread_get_last_tsc(thread2) == 500);
+	CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+	CU_ASSERT(stats.busy_tsc == 0);
+	CU_ASSERT(stats.idle_tsc == 300);
 
 	CU_ASSERT(reactor->busy_tsc == 100);
 	CU_ASSERT(reactor->idle_tsc == 300);
 
+	/* 100 + 100 + 300 = 500 ticks elapsed */
+	CU_ASSERT(reactor->tsc_last == 500);
+
 	spdk_set_thread(thread1);
 	spdk_poller_unregister(&busy1);
 	idle1 = spdk_poller_register(poller_run_idle, (void *)200, 0);
@@ -428,16 +445,23 @@ test_reactor_stats(void)
 
 	_reactor_run(reactor);
 
-	CU_ASSERT(thread1->tsc_last == 700);
-	CU_ASSERT(thread1->stats.busy_tsc == 100);
-	CU_ASSERT(thread1->stats.idle_tsc == 200);
-	CU_ASSERT(thread2->tsc_last == 1100);
-	CU_ASSERT(thread2->stats.busy_tsc == 400);
-	CU_ASSERT(thread2->stats.idle_tsc == 300);
+	spdk_set_thread(thread1);
+	CU_ASSERT(spdk_thread_get_last_tsc(thread1) == 700);
+	CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+	CU_ASSERT(stats.busy_tsc == 100);
+	CU_ASSERT(stats.idle_tsc == 200);
+	spdk_set_thread(thread2);
+	CU_ASSERT(spdk_thread_get_last_tsc(thread2) == 1100);
+	CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+	CU_ASSERT(stats.busy_tsc == 400);
+	CU_ASSERT(stats.idle_tsc == 300);
 
 	CU_ASSERT(reactor->busy_tsc == 500);
 	CU_ASSERT(reactor->idle_tsc == 500);
 
+	/* 500 + 200 + 400 = 1100 ticks elapsed */
+	CU_ASSERT(reactor->tsc_last == 1100);
+
 	spdk_set_thread(thread1);
 	spdk_poller_unregister(&idle1);
 	spdk_thread_exit(thread1);
@@ -448,8 +472,44 @@ test_reactor_stats(void)
 
 	_reactor_run(reactor);
 
+	/* After 900 ticks new thread is created. */
+	/* 1100 + 900 = 2000 ticks elapsed */
+	MOCK_SET(spdk_get_ticks, 2000);
+	_reactor_run(reactor);
+	CU_ASSERT(reactor->tsc_last == 2000);
+
+	thread1 = spdk_thread_create(NULL, &cpuset);
+	SPDK_CU_ASSERT_FATAL(thread1 != NULL);
+
+	spdk_set_thread(thread1);
+	busy1 = spdk_poller_register(poller_run_busy, (void *)100, 0);
+	CU_ASSERT(busy1 != NULL);
+
+	_reactor_run(reactor);
+
+	spdk_set_thread(thread1);
+	CU_ASSERT(spdk_thread_get_last_tsc(thread1) == 2100);
+	CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+	CU_ASSERT(stats.busy_tsc == 100);
+	CU_ASSERT(stats.idle_tsc == 0);
+
+	CU_ASSERT(reactor->busy_tsc == 600);
+	CU_ASSERT(reactor->idle_tsc == 1400);
+
+	/* 2000 + 100 = 2100 ticks elapsed */
+	CU_ASSERT(reactor->tsc_last == 2100);
+
+	spdk_set_thread(thread1);
+	spdk_poller_unregister(&busy1);
+	spdk_thread_exit(thread1);
+
+	_reactor_run(reactor);
+
 	CU_ASSERT(TAILQ_EMPTY(&reactor->threads));
 
+	/* No further than 2100 ticks elapsed */
+	CU_ASSERT(reactor->tsc_last == 2100);
+
 	spdk_reactors_fini();
 
 	free_cores();
@@ -462,9 +522,10 @@ test_scheduler(void)
 {
 	struct spdk_cpuset cpuset = {};
 	struct spdk_thread *thread[3];
-	struct spdk_lw_thread *lw_thread;
 	struct spdk_reactor *reactor;
 	struct spdk_poller *busy, *idle;
+	uint64_t current_time;
+	struct spdk_thread_stats stats;
 	int i;
 
 	MOCK_SET(spdk_env_get_current_core, 0);
@@ -482,6 +543,7 @@ test_scheduler(void)
 
 	/* Create threads. */
 	for (i = 0; i < 3; i++) {
+		spdk_cpuset_zero(&cpuset);
 		spdk_cpuset_set_cpu(&cpuset, i, true);
 		thread[i] = spdk_thread_create(NULL, &cpuset);
 		CU_ASSERT(thread[i] != NULL);
@@ -500,22 +562,29 @@ test_scheduler(void)
 	MOCK_SET(spdk_env_get_current_core, 0);
 
 	/* Init threads stats (low load) */
+	/* Each reactor starts at 100 tsc,
+	 * ends at 100 + 100 = 200 tsc. */
+	current_time = 100;
 	for (i = 0; i < 3; i++) {
 		spdk_set_thread(thread[i]);
-		busy = spdk_poller_register(poller_run_busy, (void *)10, 0);
-		idle = spdk_poller_register(poller_run_idle, (void *)90, 0);
+		idle = spdk_poller_register(poller_run_idle, (void *)100, 0);
 		reactor = spdk_reactor_get(i);
 		CU_ASSERT(reactor != NULL);
-		reactor->tsc_last = 100;
+		MOCK_SET(spdk_get_ticks, current_time);
+		reactor->tsc_last = spdk_get_ticks();
 		_reactor_run(reactor);
-		spdk_poller_unregister(&busy);
+		CU_ASSERT(reactor->tsc_last == 200);
 		spdk_poller_unregister(&idle);
 
-		/* Update last stats so that we don't have to call scheduler twice */
-		lw_thread = spdk_thread_get_ctx(thread[i]);
-		lw_thread->last_stats.busy_tsc = UINT32_MAX;
-		lw_thread->last_stats.idle_tsc = UINT32_MAX;
+		CU_ASSERT(spdk_thread_get_last_tsc(thread[i]) == 200);
+		CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+		CU_ASSERT(stats.busy_tsc == 0);
+		CU_ASSERT(stats.idle_tsc == 100);
+		CU_ASSERT(reactor->busy_tsc == 0);
+		CU_ASSERT(reactor->idle_tsc == 100);
 	}
+	CU_ASSERT(spdk_get_ticks() == 200);
+	current_time = 200;
 
 	reactor = spdk_reactor_get(0);
 	CU_ASSERT(reactor != NULL);
@@ -536,12 +605,23 @@ test_scheduler(void)
 	MOCK_SET(spdk_env_get_current_core, 0);
 	CU_ASSERT(event_queue_run_batch(reactor) == 1);
 
-	/* Threads were idle, so all of them should be placed on core 0 */
+	/* Threads were idle, so all of them should be placed on core 0.
+	 * All reactors start and end at 200 tsc, since threads are idle. */
 	for (i = 0; i < 3; i++) {
 		reactor = spdk_reactor_get(i);
 		CU_ASSERT(reactor != NULL);
+		MOCK_SET(spdk_get_ticks, current_time);
 		_reactor_run(reactor);
+		CU_ASSERT(reactor->tsc_last == current_time);
+		CU_ASSERT(reactor->busy_tsc == 0);
+		CU_ASSERT(reactor->idle_tsc == 100);
+		spdk_set_thread(thread[i]);
+		CU_ASSERT(spdk_thread_get_last_tsc(thread[i]) == current_time);
+		CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+		CU_ASSERT(stats.busy_tsc == 0);
+		CU_ASSERT(stats.idle_tsc == 100);
 	}
+	CU_ASSERT(spdk_get_ticks() == current_time);
 
 	/* 2 threads should be scheduled to core 0 */
 	reactor = spdk_reactor_get(0);
@@ -562,17 +642,27 @@ test_scheduler(void)
 	/* Make threads busy */
 	reactor = spdk_reactor_get(0);
 	CU_ASSERT(reactor != NULL);
-	reactor->tsc_last = 100;
 
+	/* All threads run on single reactor,
+	 * reactor 0 starts at 200 tsc,
+	 * ending at 200 + (100 * 3) = 500 tsc. */
+	MOCK_SET(spdk_get_ticks, current_time);
 	for (i = 0; i < 3; i++) {
 		spdk_set_thread(thread[i]);
 		busy = spdk_poller_register(poller_run_busy, (void *)100, 0);
 		_reactor_run(reactor);
 		spdk_poller_unregister(&busy);
-	}
 
-	reactor->busy_tsc = 0;
-	reactor->idle_tsc = UINT32_MAX;
+		CU_ASSERT(reactor->tsc_last == (current_time + 100 * (i + 1)));
+		CU_ASSERT(spdk_thread_get_last_tsc(thread[i]) == (current_time + 100 * (i + 1)));
+		CU_ASSERT(spdk_thread_get_stats(&stats) == 0);
+		CU_ASSERT(stats.busy_tsc == 100);
+		CU_ASSERT(stats.idle_tsc == 100);
+	}
+	CU_ASSERT(reactor->busy_tsc == 300);
+	CU_ASSERT(reactor->idle_tsc == 100);
+	CU_ASSERT(spdk_get_ticks() == 500);
+	current_time = 500;
 
 	/* Run scheduler again, this time all threads are busy */
 	_reactors_scheduler_gather_metrics(NULL, NULL);
@@ -700,6 +790,7 @@ test_governor(void)
 
 	/* Create threads. */
 	for (i = 0; i < 2; i++) {
+		spdk_cpuset_zero(&cpuset);
 		spdk_cpuset_set_cpu(&cpuset, i, true);
 		thread[i] = spdk_thread_create(NULL, &cpuset);
 		CU_ASSERT(thread[i] != NULL);
@@ -735,7 +826,7 @@ test_governor(void)
 
 		/* Update last stats so that we don't have to call scheduler twice */
 		lw_thread = spdk_thread_get_ctx(thread[i]);
-		lw_thread->last_stats.idle_tsc = 1;
+		lw_thread->current_stats.idle_tsc = 1;
 	}
 
 	reactor = spdk_reactor_get(0);
diff --git a/test/unit/lib/ftl/common/utils.c b/test/unit/lib/ftl/common/utils.c
index cd834069d..59db375f4 100644
--- a/test/unit/lib/ftl/common/utils.c
+++ b/test/unit/lib/ftl/common/utils.c
@@ -33,6 +33,7 @@
 
 #include "spdk/ftl.h"
 #include "ftl/ftl_core.h"
+#include "thread/thread_internal.h"
 
 struct base_bdev_geometry {
 	size_t write_unit_size;
diff --git a/test/unit/lib/ftl/ftl_io.c/ftl_io_ut.c b/test/unit/lib/ftl/ftl_io.c/ftl_io_ut.c
index ba4211ccf..3a2eb25c8 100644
--- a/test/unit/lib/ftl/ftl_io.c/ftl_io_ut.c
+++ b/test/unit/lib/ftl/ftl_io.c/ftl_io_ut.c
@@ -32,6 +32,7 @@
  */
 
 #include "spdk/stdinc.h"
+#include "thread/thread_internal.h"
 
 #include "spdk_cunit.h"
 #include "common/lib/ut_multithread.c"
diff --git a/test/unit/lib/idxd/Makefile b/test/unit/lib/idxd/Makefile
index e37cb22d9..67640e5d1 100644
--- a/test/unit/lib/idxd/Makefile
+++ b/test/unit/lib/idxd/Makefile
@@ -34,7 +34,7 @@
 SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
-DIRS-y = idxd.c
+DIRS-y = idxd.c idxd_user.c
 
 .PHONY: all clean $(DIRS-y)
 
diff --git a/test/unit/lib/idxd/idxd.c/idxd_ut.c b/test/unit/lib/idxd/idxd.c/idxd_ut.c
index d2dede924..2607fe7c1 100644
--- a/test/unit/lib/idxd/idxd.c/idxd_ut.c
+++ b/test/unit/lib/idxd/idxd.c/idxd_ut.c
@@ -37,210 +37,17 @@
 #include "common/lib/test_env.c"
 
 #include "idxd/idxd.h"
-
-#define FAKE_REG_SIZE 0x800
-#define GRP_CFG_OFFSET 0x400
-#define MAX_TOKENS 0x40
-#define MAX_ARRAY_SIZE 0x20
-
-DEFINE_STUB(spdk_pci_idxd_get_driver, struct spdk_pci_driver *, (void), NULL);
-
-int
-spdk_pci_enumerate(struct spdk_pci_driver *driver, spdk_pci_enum_cb enum_cb, void *enum_ctx)
-{
-	return -1;
-}
-
-int
-spdk_pci_device_map_bar(struct spdk_pci_device *dev, uint32_t bar,
-			void **mapped_addr, uint64_t *phys_addr, uint64_t *size)
-{
-	*mapped_addr = NULL;
-	*phys_addr = 0;
-	*size = 0;
-	return 0;
-}
-
-int
-spdk_pci_device_unmap_bar(struct spdk_pci_device *dev, uint32_t bar, void *addr)
-{
-	return 0;
-}
-
-int
-spdk_pci_device_cfg_read32(struct spdk_pci_device *dev, uint32_t *value,
-			   uint32_t offset)
-{
-	*value = 0xFFFFFFFFu;
-	return 0;
-}
-
-int
-spdk_pci_device_cfg_write32(struct spdk_pci_device *dev, uint32_t value,
-			    uint32_t offset)
-{
-	return 0;
-}
-
-#define movdir64b mock_movdir64b
-static inline void
-mock_movdir64b(void *dst, const void *src)
-{
-	return;
-}
-
 #include "idxd/idxd.c"
 
-#define WQ_CFG_OFFSET 0x500
-#define TOTAL_WQE_SIZE 0x40
-static int
-test_idxd_wq_config(void)
-{
-	struct spdk_idxd_device idxd = {};
-	union idxd_wqcfg wqcfg = {};
-	uint32_t expected[8] = {0x40, 0, 0x11, 0x9e, 0, 0, 0x40000000, 0};
-	uint32_t wq_size;
-	int rc, i, j;
-
-	idxd.reg_base = calloc(1, FAKE_REG_SIZE);
-	SPDK_CU_ASSERT_FATAL(idxd.reg_base != NULL);
-
-	SPDK_CU_ASSERT_FATAL(g_dev_cfg->num_groups <= MAX_ARRAY_SIZE);
-	idxd.groups = calloc(g_dev_cfg->num_groups, sizeof(struct idxd_group));
-	SPDK_CU_ASSERT_FATAL(idxd.groups != NULL);
-
-	idxd.registers.wqcap.total_wq_size = TOTAL_WQE_SIZE;
-	idxd.registers.wqcap.num_wqs = g_dev_cfg->total_wqs;
-	idxd.registers.gencap.max_batch_shift = LOG2_WQ_MAX_BATCH;
-	idxd.registers.gencap.max_xfer_shift = LOG2_WQ_MAX_XFER;
-	idxd.wqcfg_offset = WQ_CFG_OFFSET;
-	wq_size = idxd.registers.wqcap.total_wq_size / g_dev_cfg->total_wqs;
-
-	rc = idxd_wq_config(&idxd);
-	CU_ASSERT(rc == 0);
-	for (i = 0; i < g_dev_cfg->total_wqs; i++) {
-		CU_ASSERT(idxd.queues[i].wqcfg.wq_size == wq_size);
-		CU_ASSERT(idxd.queues[i].wqcfg.mode == WQ_MODE_DEDICATED);
-		CU_ASSERT(idxd.queues[i].wqcfg.max_batch_shift == LOG2_WQ_MAX_BATCH);
-		CU_ASSERT(idxd.queues[i].wqcfg.max_xfer_shift == LOG2_WQ_MAX_XFER);
-		CU_ASSERT(idxd.queues[i].wqcfg.wq_state == WQ_ENABLED);
-		CU_ASSERT(idxd.queues[i].wqcfg.priority == WQ_PRIORITY_1);
-		CU_ASSERT(idxd.queues[i].idxd == &idxd);
-		CU_ASSERT(idxd.queues[i].group == &idxd.groups[i % g_dev_cfg->num_groups]);
-	}
-
-	for (i = 0 ; i < idxd.registers.wqcap.num_wqs; i++) {
-		for (j = 0 ; j < WQCFG_NUM_DWORDS; j++) {
-			wqcfg.raw[j] = spdk_mmio_read_4((uint32_t *)(idxd.reg_base + idxd.wqcfg_offset + i * 32 + j *
-							4));
-			CU_ASSERT(wqcfg.raw[j] == expected[j]);
-		}
-	}
-
-	free(idxd.queues);
-	free(idxd.reg_base);
-	free(idxd.groups);
-
-	return 0;
-}
-
-static int
-test_idxd_group_config(void)
+static void
+user_idxd_set_config(struct device_config *dev_cfg, uint32_t config_num)
 {
-	struct spdk_idxd_device idxd = {};
-	uint64_t wqs[MAX_ARRAY_SIZE] = {};
-	uint64_t engines[MAX_ARRAY_SIZE] = {};
-	union idxd_group_flags flags[MAX_ARRAY_SIZE] = {};
-	int rc, i;
-	uint64_t base_offset;
-
-	idxd.reg_base = calloc(1, FAKE_REG_SIZE);
-	SPDK_CU_ASSERT_FATAL(idxd.reg_base != NULL);
-
-	SPDK_CU_ASSERT_FATAL(g_dev_cfg->num_groups <= MAX_ARRAY_SIZE);
-	idxd.registers.groupcap.num_groups = g_dev_cfg->num_groups;
-	idxd.registers.enginecap.num_engines = g_dev_cfg->total_engines;
-	idxd.registers.wqcap.num_wqs = g_dev_cfg->total_wqs;
-	idxd.registers.groupcap.total_tokens = MAX_TOKENS;
-	idxd.grpcfg_offset = GRP_CFG_OFFSET;
-
-	rc = idxd_group_config(&idxd);
-	CU_ASSERT(rc == 0);
-	for (i = 0 ; i < idxd.registers.groupcap.num_groups; i++) {
-		base_offset = idxd.grpcfg_offset + i * 64;
-
-		wqs[i] = spdk_mmio_read_8((uint64_t *)(idxd.reg_base + base_offset));
-		engines[i] = spdk_mmio_read_8((uint64_t *)(idxd.reg_base + base_offset + CFG_ENGINE_OFFSET));
-		flags[i].raw = spdk_mmio_read_8((uint64_t *)(idxd.reg_base + base_offset + CFG_FLAG_OFFSET));
-	}
-	/* wqe and engine arrays are indexed by group id and are bitmaps of assigned elements. */
-	CU_ASSERT(wqs[0] == 0x1);
-	CU_ASSERT(engines[0] == 0xf);
-	CU_ASSERT(flags[0].tokens_allowed == MAX_TOKENS / g_dev_cfg->num_groups);
-
-	/* groups allocated by code under test. */
-	free(idxd.groups);
-	free(idxd.reg_base);
-
-	return 0;
 }
 
-static int
-test_idxd_reset_dev(void)
-{
-	struct spdk_idxd_device idxd = {};
-	union idxd_cmdsts_reg *fake_cmd_status_reg;
-	int rc;
-
-	idxd.reg_base = calloc(1, FAKE_REG_SIZE);
-	SPDK_CU_ASSERT_FATAL(idxd.reg_base != NULL);
-	fake_cmd_status_reg = idxd.reg_base + IDXD_CMDSTS_OFFSET;
-
-	/* Test happy path */
-	rc = idxd_reset_dev(&idxd);
-	CU_ASSERT(rc == 0);
-
-	/* Test error reported path */
-	fake_cmd_status_reg->err = 1;
-	rc = idxd_reset_dev(&idxd);
-	CU_ASSERT(rc == -EINVAL);
-
-	free(idxd.reg_base);
-
-	return 0;
-}
-
-static int
-test_idxd_wait_cmd(void)
-{
-	struct spdk_idxd_device idxd = {};
-	int timeout = 1;
-	union idxd_cmdsts_reg *fake_cmd_status_reg;
-	int rc;
-
-	idxd.reg_base = calloc(1, FAKE_REG_SIZE);
-	SPDK_CU_ASSERT_FATAL(idxd.reg_base != NULL);
-	fake_cmd_status_reg = idxd.reg_base + IDXD_CMDSTS_OFFSET;
-
-	/* Test happy path. */
-	rc = idxd_wait_cmd(&idxd, timeout);
-	CU_ASSERT(rc == 0);
-
-	/* Setup up our fake register to set the error bit. */
-	fake_cmd_status_reg->err = 1;
-	rc = idxd_wait_cmd(&idxd, timeout);
-	CU_ASSERT(rc == -EINVAL);
-	fake_cmd_status_reg->err = 0;
-
-	/* Setup up our fake register to set the active bit. */
-	fake_cmd_status_reg->active = 1;
-	rc = idxd_wait_cmd(&idxd, timeout);
-	CU_ASSERT(rc == -EBUSY);
-
-	free(idxd.reg_base);
-
-	return 0;
-}
+static struct spdk_idxd_impl g_user_idxd_impl = {
+	.name		= "user",
+	.set_config	= user_idxd_set_config,
+};
 
 static int
 test_spdk_idxd_set_config(void)
@@ -254,35 +61,11 @@ test_spdk_idxd_set_config(void)
 	return 0;
 }
 
-static int
-test_spdk_idxd_reconfigure_chan(void)
-{
-	struct spdk_idxd_io_channel chan = {};
-	struct spdk_idxd_device idxd = {};
-	int rc;
-	uint32_t test_ring_size = 8;
-	uint32_t num_channels = 2;
-
-	chan.ring_slots = spdk_bit_array_create(test_ring_size);
-	chan.ring_size = test_ring_size;
-	chan.completions = spdk_zmalloc(test_ring_size * sizeof(struct idxd_hw_desc), 0, NULL,
-					SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA);
-	SPDK_CU_ASSERT_FATAL(chan.completions != NULL);
-	chan.idxd = &idxd;
-	chan.idxd->num_channels = num_channels;
-
-	rc = spdk_idxd_reconfigure_chan(&chan);
-	CU_ASSERT(rc == 0);
-	CU_ASSERT(chan.max_ring_slots == test_ring_size / num_channels);
-
-	spdk_bit_array_free(&chan.ring_slots);
-	spdk_free(chan.completions);
-	return 0;
-}
-
 static int
 test_setup(void)
 {
+	idxd_impl_register(&g_user_idxd_impl);
+
 	g_dev_cfg = &g_dev_cfg0;
 	return 0;
 }
@@ -297,12 +80,7 @@ int main(int argc, char **argv)
 
 	suite = CU_add_suite("idxd", test_setup, NULL);
 
-	CU_ADD_TEST(suite, test_spdk_idxd_reconfigure_chan);
 	CU_ADD_TEST(suite, test_spdk_idxd_set_config);
-	CU_ADD_TEST(suite, test_idxd_wait_cmd);
-	CU_ADD_TEST(suite, test_idxd_reset_dev);
-	CU_ADD_TEST(suite, test_idxd_group_config);
-	CU_ADD_TEST(suite, test_idxd_wq_config);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/idxd/idxd_user.c/.gitignore b/test/unit/lib/idxd/idxd_user.c/.gitignore
new file mode 100644
index 000000000..b118c291c
--- /dev/null
+++ b/test/unit/lib/idxd/idxd_user.c/.gitignore
@@ -0,0 +1 @@
+idxd_user_ut
diff --git a/test/unit/lib/idxd/idxd_user.c/Makefile b/test/unit/lib/idxd/idxd_user.c/Makefile
new file mode 100644
index 000000000..2bbfb920c
--- /dev/null
+++ b/test/unit/lib/idxd/idxd_user.c/Makefile
@@ -0,0 +1,38 @@
+#
+#  BSD LICENSE
+#
+#  Copyright (c) Intel Corporation.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Intel Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../../../..)
+
+TEST_FILE = idxd_user_ut.c
+
+include $(SPDK_ROOT_DIR)/mk/spdk.unittest.mk
diff --git a/test/unit/lib/idxd/idxd_user.c/idxd_user_ut.c b/test/unit/lib/idxd/idxd_user.c/idxd_user_ut.c
new file mode 100644
index 000000000..c8328c690
--- /dev/null
+++ b/test/unit/lib/idxd/idxd_user.c/idxd_user_ut.c
@@ -0,0 +1,285 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk_cunit.h"
+#include "spdk_internal/mock.h"
+#include "spdk_internal/idxd.h"
+#include "common/lib/test_env.c"
+
+#include "idxd/idxd.h"
+#include "idxd/idxd_user.c"
+
+#define FAKE_REG_SIZE 0x800
+#define GRP_CFG_OFFSET 0x400
+#define MAX_TOKENS 0x40
+#define MAX_ARRAY_SIZE 0x20
+
+DEFINE_STUB(spdk_pci_idxd_get_driver, struct spdk_pci_driver *, (void), NULL);
+DEFINE_STUB_V(idxd_impl_register, (struct spdk_idxd_impl *impl));
+DEFINE_STUB_V(spdk_pci_device_detach, (struct spdk_pci_device *device));
+DEFINE_STUB(spdk_pci_device_claim, int, (struct spdk_pci_device *dev), 0);
+DEFINE_STUB(spdk_pci_device_get_device_id, uint16_t, (struct spdk_pci_device *dev), 0);
+DEFINE_STUB(spdk_pci_device_get_vendor_id, uint16_t, (struct spdk_pci_device *dev), 0);
+
+struct spdk_pci_addr
+spdk_pci_device_get_addr(struct spdk_pci_device *pci_dev)
+{
+	struct spdk_pci_addr pci_addr;
+
+	memset(&pci_addr, 0, sizeof(pci_addr));
+	return pci_addr;
+}
+
+int
+spdk_pci_enumerate(struct spdk_pci_driver *driver, spdk_pci_enum_cb enum_cb, void *enum_ctx)
+{
+	return -1;
+}
+
+int
+spdk_pci_device_map_bar(struct spdk_pci_device *dev, uint32_t bar,
+			void **mapped_addr, uint64_t *phys_addr, uint64_t *size)
+{
+	*mapped_addr = NULL;
+	*phys_addr = 0;
+	*size = 0;
+	return 0;
+}
+
+int
+spdk_pci_device_unmap_bar(struct spdk_pci_device *dev, uint32_t bar, void *addr)
+{
+	return 0;
+}
+
+int
+spdk_pci_device_cfg_read32(struct spdk_pci_device *dev, uint32_t *value,
+			   uint32_t offset)
+{
+	*value = 0xFFFFFFFFu;
+	return 0;
+}
+
+int
+spdk_pci_device_cfg_write32(struct spdk_pci_device *dev, uint32_t value,
+			    uint32_t offset)
+{
+	return 0;
+}
+
+#define WQ_CFG_OFFSET 0x500
+#define TOTAL_WQE_SIZE 0x40
+static int
+test_idxd_wq_config(void)
+{
+	struct spdk_user_idxd_device user_idxd = {};
+	struct spdk_idxd_device *idxd = &user_idxd.idxd;
+	union idxd_wqcfg wqcfg = {};
+	uint32_t expected[8] = {0x40, 0, 0x11, 0x9e, 0, 0, 0x40000000, 0};
+	uint32_t wq_size;
+	int rc, i, j;
+
+	user_idxd.reg_base = calloc(1, FAKE_REG_SIZE);
+	SPDK_CU_ASSERT_FATAL(user_idxd.reg_base != NULL);
+
+	SPDK_CU_ASSERT_FATAL(g_user_dev_cfg.num_groups <= MAX_ARRAY_SIZE);
+	idxd->groups = calloc(g_user_dev_cfg.num_groups, sizeof(struct idxd_group));
+	SPDK_CU_ASSERT_FATAL(idxd->groups != NULL);
+
+	user_idxd.registers.wqcap.total_wq_size = TOTAL_WQE_SIZE;
+	user_idxd.registers.wqcap.num_wqs = g_user_dev_cfg.total_wqs;
+	user_idxd.registers.gencap.max_batch_shift = LOG2_WQ_MAX_BATCH;
+	user_idxd.registers.gencap.max_xfer_shift = LOG2_WQ_MAX_XFER;
+	user_idxd.wqcfg_offset = WQ_CFG_OFFSET;
+	wq_size = user_idxd.registers.wqcap.total_wq_size / g_user_dev_cfg.total_wqs;
+
+	rc = idxd_wq_config(&user_idxd);
+	CU_ASSERT(rc == 0);
+	for (i = 0; i < g_user_dev_cfg.total_wqs; i++) {
+		CU_ASSERT(idxd->queues[i].wqcfg.wq_size == wq_size);
+		CU_ASSERT(idxd->queues[i].wqcfg.mode == WQ_MODE_DEDICATED);
+		CU_ASSERT(idxd->queues[i].wqcfg.max_batch_shift == LOG2_WQ_MAX_BATCH);
+		CU_ASSERT(idxd->queues[i].wqcfg.max_xfer_shift == LOG2_WQ_MAX_XFER);
+		CU_ASSERT(idxd->queues[i].wqcfg.wq_state == WQ_ENABLED);
+		CU_ASSERT(idxd->queues[i].wqcfg.priority == WQ_PRIORITY_1);
+		CU_ASSERT(idxd->queues[i].idxd == idxd);
+		CU_ASSERT(idxd->queues[i].group == &idxd->groups[i % g_user_dev_cfg.num_groups]);
+	}
+
+	for (i = 0 ; i < user_idxd.registers.wqcap.num_wqs; i++) {
+		for (j = 0 ; j < WQCFG_NUM_DWORDS; j++) {
+			wqcfg.raw[j] = spdk_mmio_read_4((uint32_t *)(user_idxd.reg_base + user_idxd.wqcfg_offset + i * 32 +
+							j *
+							4));
+			CU_ASSERT(wqcfg.raw[j] == expected[j]);
+		}
+	}
+
+	free(idxd->queues);
+	free(user_idxd.reg_base);
+	free(idxd->groups);
+
+	return 0;
+}
+
+static int
+test_idxd_group_config(void)
+{
+	struct spdk_user_idxd_device user_idxd = {};
+	struct spdk_idxd_device *idxd = &user_idxd.idxd;
+	uint64_t wqs[MAX_ARRAY_SIZE] = {};
+	uint64_t engines[MAX_ARRAY_SIZE] = {};
+	union idxd_group_flags flags[MAX_ARRAY_SIZE] = {};
+	int rc, i;
+	uint64_t base_offset;
+
+	user_idxd.reg_base = calloc(1, FAKE_REG_SIZE);
+	SPDK_CU_ASSERT_FATAL(user_idxd.reg_base != NULL);
+
+	SPDK_CU_ASSERT_FATAL(g_user_dev_cfg.num_groups <= MAX_ARRAY_SIZE);
+	user_idxd.registers.groupcap.num_groups = g_user_dev_cfg.num_groups;
+	user_idxd.registers.enginecap.num_engines = g_user_dev_cfg.total_engines;
+	user_idxd.registers.wqcap.num_wqs = g_user_dev_cfg.total_wqs;
+	user_idxd.registers.groupcap.total_tokens = MAX_TOKENS;
+	user_idxd.grpcfg_offset = GRP_CFG_OFFSET;
+
+	rc = idxd_group_config(idxd);
+	CU_ASSERT(rc == 0);
+	for (i = 0 ; i < user_idxd.registers.groupcap.num_groups; i++) {
+		base_offset = user_idxd.grpcfg_offset + i * 64;
+
+		wqs[i] = spdk_mmio_read_8((uint64_t *)(user_idxd.reg_base + base_offset));
+		engines[i] = spdk_mmio_read_8((uint64_t *)(user_idxd.reg_base + base_offset + CFG_ENGINE_OFFSET));
+		flags[i].raw = spdk_mmio_read_8((uint64_t *)(user_idxd.reg_base + base_offset + CFG_FLAG_OFFSET));
+	}
+	/* wqe and engine arrays are indexed by group id and are bitmaps of assigned elements. */
+	CU_ASSERT(wqs[0] == 0x1);
+	CU_ASSERT(engines[0] == 0xf);
+	CU_ASSERT(flags[0].tokens_allowed == MAX_TOKENS / g_user_dev_cfg.num_groups);
+
+	/* groups allocated by code under test. */
+	free(idxd->groups);
+	free(user_idxd.reg_base);
+
+	return 0;
+}
+
+static int
+test_idxd_reset_dev(void)
+{
+	struct spdk_user_idxd_device user_idxd = {};
+	union idxd_cmdsts_reg *fake_cmd_status_reg;
+	int rc;
+
+	user_idxd.reg_base = calloc(1, FAKE_REG_SIZE);
+	SPDK_CU_ASSERT_FATAL(user_idxd.reg_base != NULL);
+	fake_cmd_status_reg = user_idxd.reg_base + IDXD_CMDSTS_OFFSET;
+
+	/* Test happy path */
+	rc = idxd_reset_dev(&user_idxd.idxd);
+	CU_ASSERT(rc == 0);
+
+	/* Test error reported path */
+	fake_cmd_status_reg->err = 1;
+	rc = idxd_reset_dev(&user_idxd.idxd);
+	CU_ASSERT(rc == -EINVAL);
+
+	free(user_idxd.reg_base);
+
+	return 0;
+}
+
+static int
+test_idxd_wait_cmd(void)
+{
+	struct spdk_user_idxd_device user_idxd = {};
+	int timeout = 1;
+	union idxd_cmdsts_reg *fake_cmd_status_reg;
+	int rc;
+
+	user_idxd.reg_base = calloc(1, FAKE_REG_SIZE);
+	SPDK_CU_ASSERT_FATAL(user_idxd.reg_base != NULL);
+	fake_cmd_status_reg = user_idxd.reg_base + IDXD_CMDSTS_OFFSET;
+
+	/* Test happy path. */
+	rc = idxd_wait_cmd(&user_idxd.idxd, timeout);
+	CU_ASSERT(rc == 0);
+
+	/* Setup up our fake register to set the error bit. */
+	fake_cmd_status_reg->err = 1;
+	rc = idxd_wait_cmd(&user_idxd.idxd, timeout);
+	CU_ASSERT(rc == -EINVAL);
+	fake_cmd_status_reg->err = 0;
+
+	/* Setup up our fake register to set the active bit. */
+	fake_cmd_status_reg->active = 1;
+	rc = idxd_wait_cmd(&user_idxd.idxd, timeout);
+	CU_ASSERT(rc == -EBUSY);
+
+	free(user_idxd.reg_base);
+
+	return 0;
+}
+
+static int
+test_setup(void)
+{
+	g_user_dev_cfg.config_num = 0;
+	g_user_dev_cfg.num_groups = 1;
+	g_user_dev_cfg.total_wqs = 1;
+	g_user_dev_cfg.total_engines = 4;
+
+	return 0;
+}
+
+int main(int argc, char **argv)
+{
+	CU_pSuite	suite = NULL;
+	unsigned int	num_failures;
+
+	CU_set_error_action(CUEA_ABORT);
+	CU_initialize_registry();
+
+	suite = CU_add_suite("idxd_user", test_setup, NULL);
+
+	CU_ADD_TEST(suite, test_idxd_wait_cmd);
+	CU_ADD_TEST(suite, test_idxd_reset_dev);
+	CU_ADD_TEST(suite, test_idxd_group_config);
+	CU_ADD_TEST(suite, test_idxd_wq_config);
+
+	CU_basic_set_mode(CU_BRM_VERBOSE);
+	CU_basic_run_tests();
+	num_failures = CU_get_number_of_failures();
+	CU_cleanup_registry();
+	return num_failures;
+}
diff --git a/test/unit/lib/init/Makefile b/test/unit/lib/init/Makefile
new file mode 100644
index 000000000..862db3c0d
--- /dev/null
+++ b/test/unit/lib/init/Makefile
@@ -0,0 +1,44 @@
+#
+#  BSD LICENSE
+#
+#  Copyright (c) Intel Corporation.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Intel Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../../..)
+include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
+
+DIRS-y = subsystem.c
+
+.PHONY: all clean $(DIRS-y)
+
+all: $(DIRS-y)
+clean: $(DIRS-y)
+
+include $(SPDK_ROOT_DIR)/mk/spdk.subdirs.mk
diff --git a/test/unit/lib/event/subsystem.c/.gitignore b/test/unit/lib/init/subsystem.c/.gitignore
similarity index 100%
rename from test/unit/lib/event/subsystem.c/.gitignore
rename to test/unit/lib/init/subsystem.c/.gitignore
diff --git a/test/unit/lib/event/subsystem.c/Makefile b/test/unit/lib/init/subsystem.c/Makefile
similarity index 100%
rename from test/unit/lib/event/subsystem.c/Makefile
rename to test/unit/lib/init/subsystem.c/Makefile
diff --git a/test/unit/lib/event/subsystem.c/subsystem_ut.c b/test/unit/lib/init/subsystem.c/subsystem_ut.c
similarity index 99%
rename from test/unit/lib/event/subsystem.c/subsystem_ut.c
rename to test/unit/lib/init/subsystem.c/subsystem_ut.c
index 608f2e4b1..e56664aab 100644
--- a/test/unit/lib/event/subsystem.c/subsystem_ut.c
+++ b/test/unit/lib/init/subsystem.c/subsystem_ut.c
@@ -36,7 +36,7 @@
 #include "spdk_cunit.h"
 
 #include "unit/lib/json_mock.c"
-#include "event/subsystem.c"
+#include "init/subsystem.c"
 #include "common/lib/test_env.c"
 
 static struct spdk_subsystem g_ut_subsystems[8];
diff --git a/test/unit/lib/iscsi/common.c b/test/unit/lib/iscsi/common.c
index baa0bcd45..e5d0f6ad1 100644
--- a/test/unit/lib/iscsi/common.c
+++ b/test/unit/lib/iscsi/common.c
@@ -13,16 +13,6 @@
 
 SPDK_LOG_REGISTER_COMPONENT(iscsi)
 
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_owner, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_object, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type, uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_type, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
-
 TAILQ_HEAD(, spdk_iscsi_pdu) g_write_pdu_list = TAILQ_HEAD_INITIALIZER(g_write_pdu_list);
 
 static bool g_task_pool_is_empty = false;
diff --git a/test/unit/lib/iscsi/conn.c/conn_ut.c b/test/unit/lib/iscsi/conn.c/conn_ut.c
index 2ce45e518..fae4b49d7 100644
--- a/test/unit/lib/iscsi/conn.c/conn_ut.c
+++ b/test/unit/lib/iscsi/conn.c/conn_ut.c
@@ -42,16 +42,6 @@
 
 SPDK_LOG_REGISTER_COMPONENT(iscsi)
 
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_owner, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_object, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type, uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_type, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record,
-	      (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-	       uint32_t size, uint64_t object_id, uint64_t arg1));
 DEFINE_STUB(iscsi_get_pdu, struct spdk_iscsi_pdu *,
 	    (struct spdk_iscsi_conn *conn), NULL);
 DEFINE_STUB(iscsi_param_eq_val, int,
diff --git a/test/unit/lib/lvol/lvol.c/lvol_ut.c b/test/unit/lib/lvol/lvol.c/lvol_ut.c
index d8f16df4b..4bc3f0b72 100644
--- a/test/unit/lib/lvol/lvol.c/lvol_ut.c
+++ b/test/unit/lib/lvol/lvol.c/lvol_ut.c
@@ -33,9 +33,11 @@
 
 #include "spdk_cunit.h"
 #include "spdk/blob.h"
-#include "spdk/thread.h"
 #include "spdk/util.h"
 
+#include "spdk/bdev_module.h"
+#include "thread/thread_internal.h"
+
 #include "common/lib/ut_multithread.c"
 
 #include "lvol/lvol.c"
diff --git a/test/unit/lib/nvme/Makefile b/test/unit/lib/nvme/Makefile
index 11690f326..33697fd89 100644
--- a/test/unit/lib/nvme/Makefile
+++ b/test/unit/lib/nvme/Makefile
@@ -35,7 +35,7 @@ SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../../..)
 include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
 
 DIRS-y = nvme.c nvme_ctrlr.c nvme_ctrlr_cmd.c nvme_ctrlr_ocssd_cmd.c nvme_ns.c nvme_ns_cmd.c nvme_ns_ocssd_cmd.c nvme_pcie.c nvme_poll_group.c nvme_qpair.c \
-	 nvme_quirks.c nvme_tcp.c nvme_transport.c nvme_io_msg.c nvme_pcie_common.c nvme_fabric.c \
+	 nvme_quirks.c nvme_tcp.c nvme_transport.c nvme_io_msg.c nvme_pcie_common.c nvme_fabric.c nvme_opal.c \
 
 DIRS-$(CONFIG_RDMA) += nvme_rdma.c
 DIRS-$(CONFIG_NVME_CUSE) += nvme_cuse.c
diff --git a/test/unit/lib/nvme/nvme_ctrlr.c/nvme_ctrlr_ut.c b/test/unit/lib/nvme/nvme_ctrlr.c/nvme_ctrlr_ut.c
index fa97fd491..75a69a2cb 100644
--- a/test/unit/lib/nvme/nvme_ctrlr.c/nvme_ctrlr_ut.c
+++ b/test/unit/lib/nvme/nvme_ctrlr.c/nvme_ctrlr_ut.c
@@ -2,7 +2,7 @@
  *   BSD LICENSE
  *
  *   Copyright (c) Intel Corporation. All rights reserved.
- *   Copyright (c) 2020 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2020, 2021 Mellanox Technologies LTD. All rights reserved.
  *
  *   Redistribution and use in source and binary forms, with or without
  *   modification, are permitted provided that the following conditions
@@ -53,6 +53,8 @@ struct nvme_driver _g_nvme_driver = {
 struct nvme_driver *g_spdk_nvme_driver = &_g_nvme_driver;
 
 struct spdk_nvme_registers g_ut_nvme_regs = {};
+typedef void (*set_reg_cb)(void);
+set_reg_cb g_set_reg_cb;
 
 __thread int    nvme_thread_ioq_index = -1;
 
@@ -65,9 +67,7 @@ DEFINE_STUB(nvme_ctrlr_cmd_set_host_id, int,
 	     spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
 DEFINE_STUB_V(nvme_ns_set_identify_data, (struct spdk_nvme_ns *ns));
 DEFINE_STUB_V(nvme_ns_set_id_desc_list_data, (struct spdk_nvme_ns *ns));
-DEFINE_STUB_V(nvme_ns_free_zns_specific_data, (struct spdk_nvme_ns *ns));
 DEFINE_STUB_V(nvme_ns_free_iocs_specific_data, (struct spdk_nvme_ns *ns));
-DEFINE_STUB(nvme_ns_has_supported_iocs_specific_data, bool, (struct spdk_nvme_ns *ns), false);
 DEFINE_STUB_V(nvme_qpair_abort_reqs, (struct spdk_nvme_qpair *qpair, uint32_t dnr));
 DEFINE_STUB(spdk_nvme_poll_group_remove, int, (struct spdk_nvme_poll_group *group,
 		struct spdk_nvme_qpair *qpair), 0);
@@ -107,6 +107,9 @@ nvme_transport_ctrlr_set_reg_4(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, u
 {
 	SPDK_CU_ASSERT_FATAL(offset <= sizeof(struct spdk_nvme_registers) - 4);
 	*(uint32_t *)((uintptr_t)&g_ut_nvme_regs + offset) = value;
+	if (g_set_reg_cb) {
+		g_set_reg_cb();
+	}
 	return 0;
 }
 
@@ -115,6 +118,9 @@ nvme_transport_ctrlr_set_reg_8(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, u
 {
 	SPDK_CU_ASSERT_FATAL(offset <= sizeof(struct spdk_nvme_registers) - 8);
 	*(uint64_t *)((uintptr_t)&g_ut_nvme_regs + offset) = value;
+	if (g_set_reg_cb) {
+		g_set_reg_cb();
+	}
 	return 0;
 }
 
@@ -254,13 +260,15 @@ fake_cpl_sc(spdk_nvme_cmd_cb cb_fn, void *cb_arg)
 	cb_fn(cb_arg, &fake_cpl);
 }
 
+static uint32_t g_ut_cdw11;
+
 int
 spdk_nvme_ctrlr_cmd_set_feature(struct spdk_nvme_ctrlr *ctrlr, uint8_t feature,
 				uint32_t cdw11, uint32_t cdw12, void *payload, uint32_t payload_size,
 				spdk_nvme_cmd_cb cb_fn, void *cb_arg)
 {
-	CU_ASSERT(0);
-	return -1;
+	g_ut_cdw11 = cdw11;
+	return 0;
 }
 
 int
@@ -385,27 +393,53 @@ nvme_ctrlr_cmd_set_async_event_config(struct spdk_nvme_ctrlr *ctrlr,
 	return 0;
 }
 
+static uint32_t *g_active_ns_list = NULL;
+static uint32_t g_active_ns_list_length = 0;
+static struct spdk_nvme_ctrlr_data *g_cdata = NULL;
+
 int
 nvme_ctrlr_cmd_identify(struct spdk_nvme_ctrlr *ctrlr, uint8_t cns, uint16_t cntid, uint32_t nsid,
 			uint8_t csi, void *payload, size_t payload_size,
 			spdk_nvme_cmd_cb cb_fn, void *cb_arg)
 {
+	memset(payload, 0, payload_size);
 	if (cns == SPDK_NVME_IDENTIFY_ACTIVE_NS_LIST) {
 		uint32_t count = 0;
 		uint32_t i = 0;
 		struct spdk_nvme_ns_list *ns_list = (struct spdk_nvme_ns_list *)payload;
 
-		for (i = 1; i <= ctrlr->num_ns; i++) {
-			if (i <= nsid) {
-				continue;
-			}
+		if (g_active_ns_list == NULL) {
+			for (i = 1; i <= ctrlr->num_ns; i++) {
+				if (i <= nsid) {
+					continue;
+				}
 
-			ns_list->ns_list[count++] = i;
-			if (count == SPDK_COUNTOF(ns_list->ns_list)) {
-				break;
+				ns_list->ns_list[count++] = i;
+				if (count == SPDK_COUNTOF(ns_list->ns_list)) {
+					break;
+				}
+			}
+		} else {
+			for (i = 0; i < g_active_ns_list_length; i++) {
+				uint32_t cur_nsid = g_active_ns_list[i];
+				if (cur_nsid <= nsid) {
+					continue;
+				}
+
+				ns_list->ns_list[count++] = cur_nsid;
+				if (count == SPDK_COUNTOF(ns_list->ns_list)) {
+					break;
+				}
 			}
 		}
-
+	} else if (cns == SPDK_NVME_IDENTIFY_CTRLR) {
+		if (g_cdata) {
+			memcpy(payload, g_cdata, sizeof(*g_cdata));
+		}
+	} else if (nsid == 99) {
+		return 1;
+	} else if (cns == SPDK_NVME_IDENTIFY_NS_IOCS) {
+		return 0;
 	}
 
 	fake_cpl_sc(cb_fn, cb_arg);
@@ -446,6 +480,7 @@ int
 nvme_ctrlr_cmd_create_ns(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns_data *payload,
 			 spdk_nvme_cmd_cb cb_fn, void *cb_arg)
 {
+	fake_cpl_sc(cb_fn, cb_arg);
 	return 0;
 }
 
@@ -508,6 +543,37 @@ nvme_ctrlr_cmd_fw_image_download(struct spdk_nvme_ctrlr *ctrlr,
 	return 0;
 }
 
+bool
+nvme_ns_has_supported_iocs_specific_data(struct spdk_nvme_ns *ns)
+{
+	switch (ns->csi) {
+	case SPDK_NVME_CSI_NVM:
+		/*
+		 * NVM Command Set Specific Identify Namespace data structure
+		 * is currently all-zeroes, reserved for future use.
+		 */
+		return false;
+	case SPDK_NVME_CSI_ZNS:
+		return true;
+	default:
+		SPDK_WARNLOG("Unsupported CSI: %u for NSID: %u\n", ns->csi, ns->id);
+		return false;
+	}
+}
+
+void
+nvme_ns_free_zns_specific_data(struct spdk_nvme_ns *ns)
+{
+	if (!ns->id) {
+		return;
+	}
+
+	if (ns->nsdata_zns) {
+		spdk_free(ns->nsdata_zns);
+		ns->nsdata_zns = NULL;
+	}
+}
+
 void
 nvme_ns_destruct(struct spdk_nvme_ns *ns)
 {
@@ -557,6 +623,9 @@ test_nvme_ctrlr_init_en_1_rdy_0(void)
 	ctrlr.cdata.nn = 1;
 	ctrlr.page_size = 0x1000;
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_1);
 
@@ -619,6 +688,9 @@ test_nvme_ctrlr_init_en_1_rdy_1(void)
 	ctrlr.cdata.nn = 1;
 	ctrlr.page_size = 0x1000;
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(g_ut_nvme_regs.cc.bits.en == 0);
@@ -684,6 +756,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_rr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_RR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -715,6 +790,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_rr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_WRR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -744,6 +822,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_rr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_VS;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -773,6 +854,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_rr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_VS + 1;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -802,6 +886,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_rr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_RR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -859,6 +946,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_wrr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_RR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -890,6 +980,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_wrr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_WRR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -921,6 +1014,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_wrr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_VS;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -950,6 +1046,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_wrr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_VS + 1;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -979,6 +1078,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_wrr(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_WRR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -1035,6 +1137,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_vs(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_RR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -1066,6 +1171,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_vs(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_WRR;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -1095,6 +1203,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_vs(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_VS;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -1126,6 +1237,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_vs(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_VS + 1;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -1155,6 +1269,9 @@ test_nvme_ctrlr_init_en_0_rdy_0_ams_vs(void)
 	ctrlr.opts.arb_mechanism = SPDK_NVME_CC_AMS_VS;
 
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
@@ -1201,6 +1318,9 @@ test_nvme_ctrlr_init_en_0_rdy_0(void)
 	ctrlr.cdata.nn = 1;
 	ctrlr.page_size = 0x1000;
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 
@@ -1246,6 +1366,9 @@ test_nvme_ctrlr_init_en_0_rdy_1(void)
 	ctrlr.cdata.nn = 1;
 	ctrlr.page_size = 0x1000;
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_INIT);
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 
@@ -1662,7 +1785,7 @@ test_ctrlr_get_default_ctrlr_opts(void)
 	CU_ASSERT(sizeof(opts) > 8);
 	spdk_nvme_ctrlr_get_default_ctrlr_opts(&opts, 8);
 	CU_ASSERT_EQUAL(opts.num_io_queues, DEFAULT_MAX_IO_QUEUES);
-	CU_ASSERT_TRUE(opts.use_cmb_sqs);
+	CU_ASSERT_FALSE(opts.use_cmb_sqs);
 	/* check below fields are not initialized by default value */
 	CU_ASSERT_EQUAL(opts.arb_mechanism, 0);
 	CU_ASSERT_EQUAL(opts.keep_alive_timeout_ms, 0);
@@ -1682,7 +1805,7 @@ test_ctrlr_get_default_ctrlr_opts(void)
 	/* set a consistent opts_size */
 	spdk_nvme_ctrlr_get_default_ctrlr_opts(&opts, sizeof(opts));
 	CU_ASSERT_EQUAL(opts.num_io_queues, DEFAULT_MAX_IO_QUEUES);
-	CU_ASSERT_TRUE(opts.use_cmb_sqs);
+	CU_ASSERT_FALSE(opts.use_cmb_sqs);
 	CU_ASSERT_EQUAL(opts.arb_mechanism, SPDK_NVME_CC_AMS_RR);
 	CU_ASSERT_EQUAL(opts.keep_alive_timeout_ms, 10 * 1000);
 	CU_ASSERT_EQUAL(opts.io_queue_size, DEFAULT_IO_QUEUE_SIZE);
@@ -1880,17 +2003,17 @@ test_nvme_ctrlr_test_active_ns(void)
 		ctrlr.vs.bits.mjr = 1;
 		ctrlr.vs.bits.mnr = minor;
 		ctrlr.vs.bits.ter = 0;
-		ctrlr.num_ns = 1531;
+		ctrlr.num_ns = ctrlr.cdata.nn = 1531;
 		nvme_ctrlr_identify_active_ns(&ctrlr);
 
 		for (nsid = 1; nsid <= ctrlr.num_ns; nsid++) {
 			CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, nsid) == true);
 		}
-		ctrlr.num_ns = 1559;
-		for (; nsid <= ctrlr.num_ns; nsid++) {
+
+		for (; nsid <= 1559; nsid++) {
 			CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, nsid) == false);
 		}
-		ctrlr.num_ns = 1531;
+
 		for (nsid = 0; nsid < ctrlr.num_ns; nsid++) {
 			ctrlr.active_ns_list[nsid] = 0;
 		}
@@ -1938,7 +2061,7 @@ test_nvme_ctrlr_test_active_ns_error_case(void)
 	ctrlr.vs.bits.mjr = 1;
 	ctrlr.vs.bits.mnr = 2;
 	ctrlr.vs.bits.ter = 0;
-	ctrlr.num_ns = 2;
+	ctrlr.cdata.nn = 2;
 
 	set_status_code = SPDK_NVME_SC_INVALID_FIELD;
 	rc = nvme_ctrlr_identify_active_ns(&ctrlr);
@@ -1981,6 +2104,9 @@ test_nvme_ctrlr_init_delay(void)
 
 	/* sleep timeout, start to initialize */
 	spdk_delay_us(2 * spdk_get_ticks_hz());
+	while (ctrlr.state != NVME_CTRLR_STATE_CHECK_EN) {
+		CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0);
 
@@ -2048,10 +2174,12 @@ test_spdk_nvme_ctrlr_set_trid(void)
 static void
 test_nvme_ctrlr_init_set_nvmf_ioccsz(void)
 {
+	struct spdk_nvme_ctrlr_data cdata = {};
 	DECLARE_AND_CONSTRUCT_CTRLR();
 	/* equivalent of 4096 bytes */
-	ctrlr.cdata.nvmf_specific.ioccsz = 260;
-	ctrlr.cdata.nvmf_specific.icdoff = 1;
+	cdata.nvmf_specific.ioccsz = 260;
+	cdata.nvmf_specific.icdoff = 1;
+	g_cdata = &cdata;
 
 	/* Check PCI trtype, */
 	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_construct(&ctrlr) == 0);
@@ -2063,6 +2191,8 @@ test_nvme_ctrlr_init_set_nvmf_ioccsz(void)
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_NUM_QUEUES);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS);
+	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
 
 	CU_ASSERT(ctrlr.ioccsz_bytes == 0);
@@ -2080,6 +2210,8 @@ test_nvme_ctrlr_init_set_nvmf_ioccsz(void)
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_NUM_QUEUES);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS);
+	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
 
 	CU_ASSERT(ctrlr.ioccsz_bytes == 4096);
@@ -2099,6 +2231,8 @@ test_nvme_ctrlr_init_set_nvmf_ioccsz(void)
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_NUM_QUEUES);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS);
+	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
 
 	CU_ASSERT(ctrlr.ioccsz_bytes == 4096);
@@ -2118,6 +2252,8 @@ test_nvme_ctrlr_init_set_nvmf_ioccsz(void)
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_NUM_QUEUES);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS);
+	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
 
 	CU_ASSERT(ctrlr.ioccsz_bytes == 4096);
@@ -2137,12 +2273,16 @@ test_nvme_ctrlr_init_set_nvmf_ioccsz(void)
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_NUM_QUEUES);
 	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS);
+	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0);
 	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
 
 	CU_ASSERT(ctrlr.ioccsz_bytes == 0);
 	CU_ASSERT(ctrlr.icdoff == 0);
 
 	nvme_ctrlr_destruct(&ctrlr);
+
+	g_cdata = NULL;
 }
 
 static void
@@ -2161,8 +2301,8 @@ test_nvme_ctrlr_init_set_num_queues(void)
 	ctrlr.opts.num_io_queues = 64;
 	/* Num queues is zero-based. So, use 31 to get 32 queues */
 	fake_cpl.cdw0 = 31 + (31 << 16);
-	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0); /* -> CONSTRUCT_NS */
-	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
+	CU_ASSERT(nvme_ctrlr_process_init(&ctrlr) == 0); /* -> IDENTIFY_ACTIVE_NS */
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS);
 	CU_ASSERT(ctrlr.opts.num_io_queues == 32);
 	fake_cpl.cdw0 = 0;
 
@@ -2225,6 +2365,7 @@ test_alloc_io_qpair_fail(void)
 	/* Verify that the qpair is removed from the lists */
 	SPDK_CU_ASSERT_FATAL(TAILQ_EMPTY(&ctrlr.active_io_qpairs));
 
+	g_connect_qpair_return_code = 0;
 	cleanup_qpairs(&ctrlr);
 }
 
@@ -2421,6 +2562,500 @@ test_nvme_cmd_map_sgls(void)
 	spdk_free(sgls);
 }
 
+static void
+test_nvme_ctrlr_set_arbitration_feature(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+
+	ctrlr.opts.arbitration_burst = 6;
+	ctrlr.flags |= SPDK_NVME_CTRLR_WRR_SUPPORTED;
+	ctrlr.opts.low_priority_weight = 1;
+	ctrlr.opts.medium_priority_weight = 2;
+	ctrlr.opts.high_priority_weight = 3;
+	/* g_ut_cdw11 used to record value command feature set. */
+	g_ut_cdw11 = 0;
+
+	/* arbitration_burst count available. */
+	nvme_ctrlr_set_arbitration_feature(&ctrlr);
+	CU_ASSERT((uint8_t)g_ut_cdw11 == 6);
+	CU_ASSERT((uint8_t)(g_ut_cdw11 >> 8) == 1);
+	CU_ASSERT((uint8_t)(g_ut_cdw11 >> 16) == 2);
+	CU_ASSERT((uint8_t)(g_ut_cdw11 >> 24) == 3);
+
+	/* arbitration_burst unavailable. */
+	g_ut_cdw11 = 0;
+	ctrlr.opts.arbitration_burst = 8;
+
+	nvme_ctrlr_set_arbitration_feature(&ctrlr);
+	CU_ASSERT(g_ut_cdw11 == 0);
+}
+
+static void
+test_nvme_ctrlr_set_state(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	MOCK_SET(spdk_get_ticks, 0);
+
+	nvme_ctrlr_set_state(&ctrlr, NVME_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT, 1000);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT);
+	CU_ASSERT(ctrlr.state_timeout_tsc == 1000000);
+
+	nvme_ctrlr_set_state(&ctrlr, NVME_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT, 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT);
+	CU_ASSERT(ctrlr.state_timeout_tsc == NVME_TIMEOUT_INFINITE);
+
+	/* Time out ticks causes integer overflow. */
+	MOCK_SET(spdk_get_ticks, UINT64_MAX);
+
+	nvme_ctrlr_set_state(&ctrlr, NVME_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT, 1000);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT);
+	CU_ASSERT(ctrlr.state_timeout_tsc == NVME_TIMEOUT_INFINITE);
+	MOCK_CLEAR(spdk_get_ticks);
+}
+
+static void
+test_nvme_ctrlr_active_ns_list_v0(void)
+{
+	DECLARE_AND_CONSTRUCT_CTRLR();
+
+	ctrlr.vs.bits.mjr = 1;
+	ctrlr.vs.bits.mnr = 0;
+	ctrlr.vs.bits.ter = 0;
+	ctrlr.cdata.nn = 1024;
+
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0); /* -> CONSTRUCT_NS */
+	SPDK_CU_ASSERT_FATAL(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1));
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1024));
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1025));
+	CU_ASSERT(spdk_nvme_ctrlr_get_first_active_ns(&ctrlr) == 1);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1023) == 1024);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1024) == 0);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1025) == 0);
+
+	nvme_ctrlr_destruct(&ctrlr);
+}
+
+static void
+test_nvme_ctrlr_active_ns_list_v2(void)
+{
+	uint32_t i;
+	uint32_t active_ns_list[1024];
+	DECLARE_AND_CONSTRUCT_CTRLR();
+
+	ctrlr.vs.bits.mjr = 1;
+	ctrlr.vs.bits.mnr = 2;
+	ctrlr.vs.bits.ter = 0;
+	ctrlr.cdata.nn = 4096;
+
+	g_active_ns_list = active_ns_list;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list);
+
+	/* No active namespaces */
+	memset(active_ns_list, 0, sizeof(active_ns_list));
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0); /* -> CONSTRUCT_NS */
+	SPDK_CU_ASSERT_FATAL(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1));
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1024));
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1025));
+	CU_ASSERT(spdk_nvme_ctrlr_get_first_active_ns(&ctrlr) == 0);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1024) == 0);
+
+	nvme_ctrlr_destruct(&ctrlr);
+
+	/* 1024 active namespaces - one full page */
+	memset(active_ns_list, 0, sizeof(active_ns_list));
+	for (i = 0; i < 1024; ++i) {
+		active_ns_list[i] = i + 1;
+	}
+
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	g_active_ns_list = active_ns_list;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list);
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0); /* -> CONSTRUCT_NS */
+	SPDK_CU_ASSERT_FATAL(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1));
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1024));
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1025));
+	CU_ASSERT(spdk_nvme_ctrlr_get_first_active_ns(&ctrlr) == 1);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1023) == 1024);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1024) == 0);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1025) == 0);
+
+	nvme_ctrlr_destruct(&ctrlr);
+
+	/* 1023 active namespaces - full page minus one	 */
+	memset(active_ns_list, 0, sizeof(active_ns_list));
+	for (i = 0; i < 1023; ++i) {
+		active_ns_list[i] = i + 1;
+	}
+
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0); /* -> CONSTRUCT_NS */
+	SPDK_CU_ASSERT_FATAL(ctrlr.state == NVME_CTRLR_STATE_CONSTRUCT_NS);
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1));
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1023));
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1024));
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 1025));
+	CU_ASSERT(spdk_nvme_ctrlr_get_first_active_ns(&ctrlr) == 1);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1023) == 0);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1024) == 0);
+	CU_ASSERT(spdk_nvme_ctrlr_get_next_active_ns(&ctrlr, 1025) == 0);
+
+	nvme_ctrlr_destruct(&ctrlr);
+
+	g_active_ns_list = NULL;
+	g_active_ns_list_length = 0;
+}
+
+static void
+test_nvme_ctrlr_ns_mgmt(void)
+{
+	DECLARE_AND_CONSTRUCT_CTRLR();
+	uint32_t active_ns_list[] = { 1, 2, 100, 1024 };
+	uint32_t active_ns_list2[] = { 1, 2, 3, 100, 1024 };
+	struct spdk_nvme_ns_data nsdata = {};
+	struct spdk_nvme_ctrlr_list ctrlr_list = {};
+	uint32_t nsid;
+
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_construct(&ctrlr) == 0);
+
+	ctrlr.vs.bits.mjr = 1;
+	ctrlr.vs.bits.mnr = 2;
+	ctrlr.vs.bits.ter = 0;
+	ctrlr.cdata.nn = 4096;
+
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	g_active_ns_list = active_ns_list;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list);
+	while (ctrlr.state != NVME_CTRLR_STATE_READY) {
+		SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
+
+	fake_cpl.cdw0 = 3;
+	nsid = spdk_nvme_ctrlr_create_ns(&ctrlr, &nsdata);
+	fake_cpl.cdw0 = 0;
+	CU_ASSERT(nsid == 3);
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 3));
+	CU_ASSERT(spdk_nvme_ctrlr_get_ns(&ctrlr, 3) != NULL);
+
+	g_active_ns_list = active_ns_list2;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list2);
+	CU_ASSERT(spdk_nvme_ctrlr_attach_ns(&ctrlr, 3, &ctrlr_list) == 0);
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 3));
+	CU_ASSERT(spdk_nvme_ctrlr_get_ns(&ctrlr, 3) != NULL);
+
+	g_active_ns_list = active_ns_list;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list);
+	CU_ASSERT(spdk_nvme_ctrlr_detach_ns(&ctrlr, 3, &ctrlr_list) == 0);
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 3));
+	CU_ASSERT(spdk_nvme_ctrlr_get_ns(&ctrlr, 3) != NULL);
+
+	CU_ASSERT(spdk_nvme_ctrlr_delete_ns(&ctrlr, 3) == 0);
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 3));
+	CU_ASSERT(spdk_nvme_ctrlr_get_ns(&ctrlr, 3) != NULL);
+	g_active_ns_list = 0;
+	g_active_ns_list_length = 0;
+
+	nvme_ctrlr_destruct(&ctrlr);
+}
+
+static void check_en_set_rdy(void)
+{
+	if (g_ut_nvme_regs.cc.bits.en == 1) {
+		g_ut_nvme_regs.csts.bits.rdy = 1;
+	}
+}
+
+static void
+test_nvme_ctrlr_reset(void)
+{
+	DECLARE_AND_CONSTRUCT_CTRLR();
+	struct spdk_nvme_ctrlr_data cdata = { .nn = 4096 };
+	uint32_t active_ns_list[] = { 1, 2, 100, 1024 };
+	uint32_t active_ns_list2[] = { 1, 100, 1024 };
+
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_construct(&ctrlr) == 0);
+
+	g_ut_nvme_regs.vs.bits.mjr = 1;
+	g_ut_nvme_regs.vs.bits.mnr = 2;
+	g_ut_nvme_regs.vs.bits.ter = 0;
+	nvme_ctrlr_get_vs(&ctrlr, &ctrlr.vs);
+	ctrlr.cdata.nn = 2048;
+
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	g_active_ns_list = active_ns_list;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list);
+	while (ctrlr.state != NVME_CTRLR_STATE_READY) {
+		SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
+	CU_ASSERT(spdk_nvme_ctrlr_get_num_ns(&ctrlr) == 2048);
+	CU_ASSERT(spdk_nvme_ctrlr_get_ns(&ctrlr, 2) != NULL);
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 2));
+
+	/* Reset controller with changed number of namespaces */
+	g_cdata = &cdata;
+	g_active_ns_list = active_ns_list2;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list2);
+	STAILQ_INSERT_HEAD(&adminq.free_req, &req, stailq);
+	g_ut_nvme_regs.cc.raw = 0;
+	g_ut_nvme_regs.csts.raw = 0;
+	g_set_reg_cb = check_en_set_rdy;
+	CU_ASSERT(spdk_nvme_ctrlr_reset(&ctrlr) == 0);
+	g_set_reg_cb = NULL;
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_READY);
+	g_cdata = NULL;
+	g_active_ns_list = 0;
+	g_active_ns_list_length = 0;
+
+	CU_ASSERT(spdk_nvme_ctrlr_get_num_ns(&ctrlr) == 4096);
+	CU_ASSERT(spdk_nvme_ctrlr_get_ns(&ctrlr, 2) != NULL);
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 2));
+
+	g_ut_nvme_regs.csts.bits.shst = SPDK_NVME_SHST_COMPLETE;
+	nvme_ctrlr_destruct(&ctrlr);
+}
+
+static uint32_t g_aer_cb_counter;
+
+static void
+aer_cb(void *aer_cb_arg, const struct spdk_nvme_cpl *cpl)
+{
+	g_aer_cb_counter++;
+}
+
+static void
+test_nvme_ctrlr_aer_callback(void)
+{
+	DECLARE_AND_CONSTRUCT_CTRLR();
+	uint32_t active_ns_list[] = { 1, 2, 100, 1024 };
+	union spdk_nvme_async_event_completion	aer_event = {
+		.bits.async_event_type = SPDK_NVME_ASYNC_EVENT_TYPE_NOTICE,
+		.bits.async_event_info = SPDK_NVME_ASYNC_EVENT_NS_ATTR_CHANGED
+	};
+	struct spdk_nvme_cpl aer_cpl = {
+		.status.sct = SPDK_NVME_SCT_GENERIC,
+		.status.sc = SPDK_NVME_SC_SUCCESS,
+		.cdw0 = aer_event.raw
+	};
+
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_construct(&ctrlr) == 0);
+
+	ctrlr.vs.bits.mjr = 1;
+	ctrlr.vs.bits.mnr = 2;
+	ctrlr.vs.bits.ter = 0;
+	ctrlr.cdata.nn = 4096;
+
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	g_active_ns_list = active_ns_list;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list);
+	while (ctrlr.state != NVME_CTRLR_STATE_READY) {
+		SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
+
+	CU_ASSERT(nvme_ctrlr_add_process(&ctrlr, NULL) == 0);
+	spdk_nvme_ctrlr_register_aer_callback(&ctrlr, aer_cb, NULL);
+
+	/* Async event */
+	g_aer_cb_counter = 0;
+	nvme_ctrlr_async_event_cb(&ctrlr.aer[0], &aer_cpl);
+	nvme_ctrlr_complete_queued_async_events(&ctrlr);
+	CU_ASSERT(g_aer_cb_counter == 1);
+	g_active_ns_list = 0;
+	g_active_ns_list_length = 0;
+
+	nvme_ctrlr_free_processes(&ctrlr);
+	nvme_ctrlr_destruct(&ctrlr);
+}
+
+static void
+test_nvme_ctrlr_ns_attr_changed(void)
+{
+	DECLARE_AND_CONSTRUCT_CTRLR();
+	uint32_t active_ns_list[] = { 1, 2, 100, 1024 };
+	uint32_t active_ns_list2[] = { 1, 2, 1024 };
+	uint32_t active_ns_list3[] = { 1, 2, 101, 1024 };
+	union spdk_nvme_async_event_completion	aer_event = {
+		.bits.async_event_type = SPDK_NVME_ASYNC_EVENT_TYPE_NOTICE,
+		.bits.async_event_info = SPDK_NVME_ASYNC_EVENT_NS_ATTR_CHANGED
+	};
+	struct spdk_nvme_cpl aer_cpl = {
+		.status.sct = SPDK_NVME_SCT_GENERIC,
+		.status.sc = SPDK_NVME_SC_SUCCESS,
+		.cdw0 = aer_event.raw
+	};
+
+	SPDK_CU_ASSERT_FATAL(nvme_ctrlr_construct(&ctrlr) == 0);
+
+	ctrlr.vs.bits.mjr = 1;
+	ctrlr.vs.bits.mnr = 3;
+	ctrlr.vs.bits.ter = 0;
+	ctrlr.cap.bits.css |= SPDK_NVME_CAP_CSS_IOCS;
+	ctrlr.cdata.nn = 4096;
+
+	ctrlr.state = NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS;
+	g_active_ns_list = active_ns_list;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list);
+	while (ctrlr.state != NVME_CTRLR_STATE_READY) {
+		SPDK_CU_ASSERT_FATAL(nvme_ctrlr_process_init(&ctrlr) == 0);
+	}
+
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 100));
+
+	CU_ASSERT(nvme_ctrlr_add_process(&ctrlr, NULL) == 0);
+	spdk_nvme_ctrlr_register_aer_callback(&ctrlr, aer_cb, NULL);
+
+	/* Remove NS 100 */
+	g_aer_cb_counter = 0;
+	g_active_ns_list = active_ns_list2;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list2);
+	nvme_ctrlr_async_event_cb(&ctrlr.aer[0], &aer_cpl);
+	nvme_ctrlr_complete_queued_async_events(&ctrlr);
+	CU_ASSERT(g_aer_cb_counter == 1);
+	CU_ASSERT(!spdk_nvme_ctrlr_is_active_ns(&ctrlr, 100));
+
+	/* Add NS 101 */
+	g_active_ns_list = active_ns_list3;
+	g_active_ns_list_length = SPDK_COUNTOF(active_ns_list3);
+	nvme_ctrlr_async_event_cb(&ctrlr.aer[0], &aer_cpl);
+	nvme_ctrlr_complete_queued_async_events(&ctrlr);
+	CU_ASSERT(g_aer_cb_counter == 2);
+	CU_ASSERT(spdk_nvme_ctrlr_is_active_ns(&ctrlr, 101));
+
+	g_active_ns_list = 0;
+	g_active_ns_list_length = 0;
+	nvme_ctrlr_free_processes(&ctrlr);
+	nvme_ctrlr_destruct(&ctrlr);
+}
+
+static void
+test_nvme_ctrlr_identify_namespaces_iocs_specific_next(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	uint32_t prev_nsid;
+	uint32_t active_ns_list[5] = {1, 2, 3, 4, 5};
+	struct spdk_nvme_ns ns[5] = {};
+	struct spdk_nvme_ctrlr ns_ctrlr[5] = {};
+	int rc = 0;
+
+	ctrlr.ns = ns;
+	ctrlr.cdata.nn = 5;
+	ctrlr.max_active_ns_idx = 5;
+	ctrlr.num_ns = 5;
+	/* case 1: No first/next active NS, move on to the next state, expect: pass */
+	prev_nsid = 0;
+	ctrlr.active_ns_list = NULL;
+	ctrlr.opts.admin_timeout_ms = NVME_TIMEOUT_INFINITE;
+	rc = nvme_ctrlr_identify_namespaces_iocs_specific_next(&ctrlr, prev_nsid);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONFIGURE_AER);
+	CU_ASSERT(ctrlr.state_timeout_tsc == NVME_TIMEOUT_INFINITE);
+
+	/* case 2: move on to the next active NS, and no namespace with (supported) iocs specific data found , expect: pass */
+	memset(&ctrlr.state, 0x00, sizeof(ctrlr.state));
+	memset(&ctrlr.state_timeout_tsc, 0x00, sizeof(ctrlr.state_timeout_tsc));
+	prev_nsid = 1;
+	ctrlr.active_ns_list = active_ns_list;
+	ns[1].csi = SPDK_NVME_CSI_NVM;
+	ns[1].id = 2;
+	rc = nvme_ctrlr_identify_namespaces_iocs_specific_next(&ctrlr, prev_nsid);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_CONFIGURE_AER);
+	CU_ASSERT(ctrlr.state_timeout_tsc == NVME_TIMEOUT_INFINITE);
+
+	/* case 3: ns.csi is SPDK_NVME_CSI_ZNS, do not loop, expect: pass */
+	memset(&ctrlr.state, 0x00, sizeof(ctrlr.state));
+	memset(&ctrlr.state_timeout_tsc, 0x00, sizeof(ctrlr.state_timeout_tsc));
+	ctrlr.opts.admin_timeout_ms = NVME_TIMEOUT_INFINITE;
+	prev_nsid = 0;
+	ctrlr.active_ns_list = active_ns_list;
+
+	for (int i = 0; i < 5; i++) {
+		ns[i].csi = SPDK_NVME_CSI_NVM;
+		ns[i].id = i + 1;
+		ns[i].ctrlr = &ns_ctrlr[i];
+	}
+	ns[4].csi = SPDK_NVME_CSI_ZNS;
+	ns_ctrlr[4].opts.admin_timeout_ms = NVME_TIMEOUT_INFINITE;
+
+	rc = nvme_ctrlr_identify_namespaces_iocs_specific_next(&ctrlr, prev_nsid);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(ctrlr.state == 0);
+	CU_ASSERT(ctrlr.state_timeout_tsc == NVME_TIMEOUT_INFINITE);
+	CU_ASSERT(ns_ctrlr[4].state == NVME_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS_IOCS_SPECIFIC);
+	CU_ASSERT(ns_ctrlr[4].state_timeout_tsc == NVME_TIMEOUT_INFINITE);
+
+	for (int i = 0; i < 5; i++) {
+		nvme_ns_free_zns_specific_data(&ns[i]);
+	}
+
+	/* case 4: nvme_ctrlr_identify_ns_iocs_specific_async return 1, expect: false */
+	memset(&ctrlr.state, 0x00, sizeof(ctrlr.state));
+	memset(&ctrlr.state_timeout_tsc, 0x00, sizeof(ctrlr.state_timeout_tsc));
+	prev_nsid = 1;
+	ctrlr.active_ns_list = active_ns_list;
+	ns[1].csi = SPDK_NVME_CSI_ZNS;
+	ns[1].id = 99;
+	rc = nvme_ctrlr_identify_namespaces_iocs_specific_next(&ctrlr, prev_nsid);
+	CU_ASSERT(rc == 1);
+	CU_ASSERT(ctrlr.state == NVME_CTRLR_STATE_ERROR);
+	CU_ASSERT(ctrlr.state_timeout_tsc == NVME_TIMEOUT_INFINITE);
+}
+
+static void
+test_nvme_ctrlr_set_supported_log_pages(void)
+{
+	int rc;
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct spdk_nvme_intel_log_page_directory *log_page_directory = NULL;
+
+	/* Intel device */
+	ctrlr.cdata.lpa.celp = true;
+	ctrlr.cdata.vid = SPDK_PCI_VID_INTEL;
+	ctrlr.quirks |= NVME_INTEL_QUIRK_READ_LATENCY;
+	ctrlr.quirks |= NVME_INTEL_QUIRK_WRITE_LATENCY;
+	log_page_directory = spdk_zmalloc(sizeof(struct spdk_nvme_intel_log_page_directory),
+					  64, NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_DMA);
+	SPDK_CU_ASSERT_FATAL(log_page_directory != NULL);
+
+	log_page_directory->temperature_statistics_log_len = 1;
+	log_page_directory->smart_log_len = 1;
+	log_page_directory->marketing_description_log_len = 1;
+	MOCK_SET(spdk_zmalloc, log_page_directory);
+
+	rc = nvme_ctrlr_set_supported_log_pages(&ctrlr);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_ERROR] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_HEALTH_INFORMATION] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_FIRMWARE_SLOT] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_COMMAND_EFFECTS_LOG] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_INTEL_LOG_READ_CMD_LATENCY] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_INTEL_LOG_WRITE_CMD_LATENCY] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_INTEL_LOG_TEMPERATURE] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_INTEL_LOG_SMART] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_INTEL_MARKETING_DESCRIPTION] == true);
+	MOCK_CLEAR(spdk_zmalloc);
+
+	/* ana supported */
+	memset(&ctrlr, 0, sizeof(ctrlr));
+	ctrlr.cdata.cmic.ana_reporting = true;
+	ctrlr.cdata.lpa.celp = 1;
+	ctrlr.cdata.nanagrpid = 1;
+	ctrlr.cdata.nn = 1;
+
+	rc = nvme_ctrlr_set_supported_log_pages(&ctrlr);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_ERROR] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_HEALTH_INFORMATION] == true);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_FIRMWARE_SLOT] == true);
+	CU_ASSERT(ctrlr.ana_log_page_size == sizeof(struct spdk_nvme_ana_page) +
+		  sizeof(struct spdk_nvme_ana_group_descriptor) * 1 + sizeof(uint32_t) * 1);
+	CU_ASSERT(ctrlr.log_page_supported[SPDK_NVME_LOG_ASYMMETRIC_NAMESPACE_ACCESS] == true);
+	spdk_free(ctrlr.ana_log_page);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -2463,6 +3098,16 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_nvme_ctrlr_add_remove_process);
 	CU_ADD_TEST(suite, test_nvme_cmd_map_prps);
 	CU_ADD_TEST(suite, test_nvme_cmd_map_sgls);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_set_arbitration_feature);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_set_state);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_active_ns_list_v0);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_active_ns_list_v2);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_ns_mgmt);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_reset);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_aer_callback);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_ns_attr_changed);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_identify_namespaces_iocs_specific_next);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_set_supported_log_pages);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_ctrlr_cmd.c/nvme_ctrlr_cmd_ut.c b/test/unit/lib/nvme/nvme_ctrlr_cmd.c/nvme_ctrlr_cmd_ut.c
index 610ce9903..8077ee7d9 100644
--- a/test/unit/lib/nvme/nvme_ctrlr_cmd.c/nvme_ctrlr_cmd_ut.c
+++ b/test/unit/lib/nvme/nvme_ctrlr_cmd.c/nvme_ctrlr_cmd_ut.c
@@ -2,8 +2,8 @@
 /*-
  *   BSD LICENSE
  *
- *   Copyright (c) Intel Corporation.
- *   All rights reserved.
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2021 Mellanox Technologies LTD. All rights reserved.
  *
  *   Redistribution and use in source and binary forms, with or without
  *   modification, are permitted provided that the following conditions
@@ -368,7 +368,17 @@ nvme_ctrlr_submit_admin_request(struct spdk_nvme_ctrlr *ctrlr, struct nvme_reque
 	return 0;
 }
 
-#define DECLARE_AND_CONSTRUCT_CTRLR()	\
+struct spdk_nvme_ns *
+spdk_nvme_ctrlr_get_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid)
+{
+	if (nsid < 1 || nsid > ctrlr->num_ns) {
+		return NULL;
+	}
+
+	return &ctrlr->ns[nsid - 1];
+}
+
+#define DECLARE_AND_CONSTRUCT_CTRLR()		\
 	struct spdk_nvme_ctrlr	ctrlr = {};	\
 	struct spdk_nvme_qpair	adminq = {};	\
 	struct nvme_request	req;		\
@@ -918,6 +928,66 @@ test_spdk_nvme_ctrlr_cmd_abort(void)
 	CU_ASSERT(pthread_mutex_destroy(&ctrlr.ctrlr_lock) == 0);
 }
 
+static void
+test_nvme_ctrlr_cmd_identify(void)
+{
+	DECLARE_AND_CONSTRUCT_CTRLR();
+	struct nvme_payload payload = {};
+	int rc;
+	MOCK_SET(nvme_ctrlr_submit_admin_request, 0);
+
+	rc = nvme_ctrlr_cmd_identify(&ctrlr, SPDK_NVME_IDENTIFY_NS, 2, 1, 0, &payload, 4096, NULL, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_IDENTIFY);
+	CU_ASSERT(req.cmd.cdw10_bits.identify.cns == SPDK_NVME_IDENTIFY_NS);
+	CU_ASSERT(req.cmd.cdw10_bits.identify.cntid == 2);
+	CU_ASSERT(req.cmd.cdw11_bits.identify.csi == 0);
+	CU_ASSERT(req.cmd.nsid == 1);
+	CU_ASSERT(STAILQ_EMPTY(&ctrlr.adminq->free_req));
+	DECONSTRUCT_CTRLR();
+	MOCK_CLEAR(nvme_ctrlr_submit_admin_request);
+}
+
+static void
+test_spdk_nvme_ctrlr_cmd_security_receive_send(void)
+{
+	DECLARE_AND_CONSTRUCT_CTRLR();
+	struct nvme_payload payload = {};
+	int rc;
+	MOCK_SET(nvme_ctrlr_submit_admin_request, 0);
+
+	rc = spdk_nvme_ctrlr_cmd_security_send(&ctrlr, 0xea, 0xaabb, 0xcc, &payload, 4096, NULL, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_SECURITY_SEND);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.nssf == 0xcc);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.spsp0 == 0xbb);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.spsp1 == 0xaa);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.secp == 0xea);
+	CU_ASSERT(req.cmd.cdw11 == 4096);
+	SPDK_CU_ASSERT_FATAL(STAILQ_EMPTY(&ctrlr.adminq->free_req));
+
+	memset(&req, 0, sizeof(req));
+	STAILQ_INSERT_HEAD(&ctrlr.adminq->free_req, &req, stailq);
+	rc = spdk_nvme_ctrlr_cmd_security_receive(&ctrlr, 0xea, 0xaabb, 0xcc, &payload, 4096, NULL, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_SECURITY_RECEIVE);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.nssf == 0xcc);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.spsp0 == 0xbb);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.spsp1 == 0xaa);
+	CU_ASSERT(req.cmd.cdw10_bits.sec_send_recv.secp == 0xea);
+	CU_ASSERT(req.cmd.cdw11 == 4096);
+	SPDK_CU_ASSERT_FATAL(STAILQ_EMPTY(&ctrlr.adminq->free_req));
+	MOCK_CLEAR(nvme_ctrlr_submit_admin_request);
+
+	/* Without request valid. */
+	rc = spdk_nvme_ctrlr_cmd_security_send(&ctrlr, 0xea, 0xaabb, 0xcc, &payload, 4096, NULL, NULL);
+	CU_ASSERT(rc == -ENOMEM);
+
+	rc = spdk_nvme_ctrlr_cmd_security_receive(&ctrlr, 0xea, 0xaabb, 0xcc, &payload, 4096, NULL, NULL);
+	CU_ASSERT(rc == -ENOMEM);
+	DECONSTRUCT_CTRLR();
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -950,6 +1020,8 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_directive);
 	CU_ADD_TEST(suite, test_nvme_request_add_abort);
 	CU_ADD_TEST(suite, test_spdk_nvme_ctrlr_cmd_abort);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_cmd_identify);
+	CU_ADD_TEST(suite, test_spdk_nvme_ctrlr_cmd_security_receive_send);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_cuse.c/nvme_cuse_ut.c b/test/unit/lib/nvme/nvme_cuse.c/nvme_cuse_ut.c
index 1ec7a5190..20fb266ed 100644
--- a/test/unit/lib/nvme/nvme_cuse.c/nvme_cuse_ut.c
+++ b/test/unit/lib/nvme/nvme_cuse.c/nvme_cuse_ut.c
@@ -45,11 +45,14 @@ DEFINE_STUB(spdk_nvme_ctrlr_cmd_admin_raw, int, (struct spdk_nvme_ctrlr *ctrlr,
 		struct spdk_nvme_cmd *cmd, void *buf, uint32_t len,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
 
-DEFINE_STUB(spdk_nvme_ctrlr_get_num_ns, uint32_t,
-	    (struct spdk_nvme_ctrlr *ctrlr), 128);
+DEFINE_STUB(spdk_nvme_ctrlr_cmd_io_raw, int, (struct spdk_nvme_ctrlr *ctrlr,
+		struct spdk_nvme_qpair *qpair, struct spdk_nvme_cmd *cmd, void *buf, uint32_t len,
+		spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
 
 DEFINE_STUB(spdk_nvme_ctrlr_reset, int, (struct spdk_nvme_ctrlr *ctrlr), 0);
 
+DEFINE_STUB(spdk_nvme_ctrlr_reset_subsystem, int, (struct spdk_nvme_ctrlr *ctrlr), 0);
+
 DEFINE_STUB(spdk_nvme_ns_cmd_read, int,
 	    (struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 	     void *payload, uint64_t lba, uint32_t lba_count,
@@ -63,14 +66,8 @@ DEFINE_STUB(spdk_nvme_ns_cmd_write, int,
 DEFINE_STUB(spdk_nvme_ns_get_num_sectors, uint64_t,
 	    (struct spdk_nvme_ns *ns), 0);
 
-DEFINE_STUB(spdk_nvme_ns_get_sector_size, uint32_t,
-	    (struct spdk_nvme_ns *ns), 0);
-
 DEFINE_STUB_V(spdk_unaffinitize_thread, (void));
 
-DEFINE_STUB(spdk_nvme_ctrlr_get_ns, struct spdk_nvme_ns *,
-	    (struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid), NULL);
-
 DEFINE_STUB(nvme_io_msg_ctrlr_register, int,
 	    (struct spdk_nvme_ctrlr *ctrlr,
 	     struct nvme_io_msg_producer *io_msg_producer), 0);
@@ -84,7 +81,17 @@ DEFINE_STUB(spdk_nvme_ctrlr_is_active_ns, bool,
 
 DEFINE_STUB(fuse_reply_err, int, (fuse_req_t req, int err), 0);
 
+DEFINE_STUB_V(nvme_ctrlr_update_namespaces, (struct spdk_nvme_ctrlr *ctrlr));
+
 struct cuse_io_ctx *g_ut_ctx;
+struct spdk_nvme_ctrlr *g_ut_ctrlr;
+uint32_t g_ut_nsid;
+
+uint32_t
+spdk_nvme_ctrlr_get_num_ns(struct spdk_nvme_ctrlr *ctrlr)
+{
+	return ctrlr->num_ns;
+}
 
 DEFINE_RETURN_MOCK(nvme_io_msg_send, int);
 int
@@ -92,11 +99,37 @@ nvme_io_msg_send(struct spdk_nvme_ctrlr *ctrlr,
 		 uint32_t nsid, spdk_nvme_io_msg_fn fn, void *arg)
 {
 	g_ut_ctx = arg;
+	g_ut_nsid = nsid;
+	g_ut_ctrlr = ctrlr;
 
 	HANDLE_RETURN_MOCK(nvme_io_msg_send);
 	return 0;
 }
 
+uint32_t
+spdk_nvme_ns_get_sector_size(struct spdk_nvme_ns *ns)
+{
+	return ns->sector_size;
+}
+
+struct spdk_nvme_ns *
+spdk_nvme_ctrlr_get_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid)
+{
+	if (nsid < 1 || nsid > ctrlr->num_ns) {
+		return NULL;
+	}
+
+	return &ctrlr->ns[nsid - 1];
+}
+
+struct cuse_device *g_cuse_device;
+DEFINE_RETURN_MOCK(fuse_req_userdata, void *);
+void *
+fuse_req_userdata(fuse_req_t req)
+{
+	return g_cuse_device;
+}
+
 static void
 test_cuse_nvme_submit_io_read_write(void)
 {
@@ -147,6 +180,172 @@ test_cuse_nvme_submit_io_read_write(void)
 	free(user_io);
 }
 
+static void
+test_cuse_nvme_submit_passthru_cmd(void)
+{
+	struct nvme_passthru_cmd *passthru_cmd = NULL;
+	fuse_req_t req = (void *)0xDEEACDFF;
+
+	passthru_cmd = calloc(1, sizeof(struct nvme_passthru_cmd));
+	g_cuse_device = calloc(1, sizeof(struct cuse_device));
+
+	/* Use fatal or we'll segfault if we didn't get memory */
+	SPDK_CU_ASSERT_FATAL(passthru_cmd != NULL);
+	SPDK_CU_ASSERT_FATAL(g_cuse_device != NULL);
+	g_cuse_device->ctrlr = (void *)0xDEADBEEF;
+
+	g_ut_ctx = NULL;
+	/* Passthrough command */
+	passthru_cmd->opcode   = SPDK_NVME_DATA_CONTROLLER_TO_HOST;
+	passthru_cmd->nsid     = 1;
+	passthru_cmd->data_len = 512;
+	passthru_cmd->cdw10    = 0xc0de1010;
+	passthru_cmd->cdw11    = 0xc0de1111;
+	passthru_cmd->cdw12    = 0xc0de1212;
+	passthru_cmd->cdw13    = 0xc0de1313;
+	passthru_cmd->cdw14    = 0xc0de1414;
+	passthru_cmd->cdw15    = 0xc0de1515;
+
+	/* Send IO Command IOCTL */
+	cuse_nvme_passthru_cmd_send(req, passthru_cmd, NULL, NVME_IOCTL_IO_CMD);
+	SPDK_CU_ASSERT_FATAL(g_ut_ctx != NULL);
+	CU_ASSERT(g_ut_ctx->data != NULL);
+	CU_ASSERT(g_ut_ctx->req               == req);
+	CU_ASSERT(g_ut_ctx->data_len          == 512);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.opc      == SPDK_NVME_DATA_CONTROLLER_TO_HOST);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.nsid     == 1);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.cdw10    == 0xc0de1010);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.cdw11    == 0xc0de1111);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.cdw12    == 0xc0de1212);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.cdw13    == 0xc0de1313);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.cdw14    == 0xc0de1414);
+	CU_ASSERT(g_ut_ctx->nvme_cmd.cdw15    == 0xc0de1515);
+
+	cuse_io_ctx_free(g_ut_ctx);
+	free(passthru_cmd);
+	free(g_cuse_device);
+}
+
+static void
+test_nvme_cuse_get_cuse_ns_device(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct cuse_device ctrlr_device = {};
+	struct cuse_device ns_devices[3] = {};
+	struct cuse_device *cuse_dev = NULL;
+
+	ctrlr.num_ns = 3;
+	ctrlr_device.ctrlr = &ctrlr;
+	ctrlr_device.ns_devices = ns_devices;
+	ns_devices[0].is_started = true;
+	ns_devices[1].is_started = false;
+
+	SPDK_CU_ASSERT_FATAL(TAILQ_EMPTY(&g_ctrlr_ctx_head));
+	TAILQ_INSERT_TAIL(&g_ctrlr_ctx_head, &ctrlr_device, tailq);
+
+	cuse_dev = nvme_cuse_get_cuse_ns_device(&ctrlr, 1);
+	CU_ASSERT(cuse_dev == &ns_devices[0]);
+
+	/* nsid 2 was not started */
+	cuse_dev = nvme_cuse_get_cuse_ns_device(&ctrlr, 2);
+	CU_ASSERT(cuse_dev == NULL);
+
+	/* nsid invalid */
+	cuse_dev = nvme_cuse_get_cuse_ns_device(&ctrlr, 0);
+	CU_ASSERT(cuse_dev == NULL);
+
+	TAILQ_REMOVE(&g_ctrlr_ctx_head, &ctrlr_device, tailq);
+}
+
+static void
+test_cuse_nvme_submit_io(void)
+{
+	struct cuse_device cuse_device = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct fuse_file_info fi = {};
+	struct spdk_nvme_ns ns = {};
+	struct nvme_user_io *user_io = NULL;
+	char arg[1024] = {};
+	fuse_req_t req = (void *)0xDEEACDFF;
+
+	/* Allocate memory to avoid stack buffer overflow */
+	user_io = calloc(3, 4096);
+	SPDK_CU_ASSERT_FATAL(user_io != NULL);
+
+	cuse_device.ctrlr = &ctrlr;
+	ctrlr.ns = &ns;
+	ctrlr.num_ns = 1;
+	ns.sector_size = 4096;
+	ns.id = 1;
+	user_io->slba = 1024;
+	user_io->nblocks = 1;
+	cuse_device.nsid = 1;
+	g_cuse_device = &cuse_device;
+
+	/* Read */
+	user_io->opcode = SPDK_NVME_OPC_READ;
+	g_ut_ctx = NULL;
+
+	cuse_nvme_submit_io(req, 0, arg, &fi, FUSE_IOCTL_DIR, user_io, 4096, 4096);
+	SPDK_CU_ASSERT_FATAL(g_ut_ctx != NULL);
+	CU_ASSERT(g_ut_nsid == 1);
+	CU_ASSERT(g_ut_ctx->req == (void *)0xDEEACDFF);
+	CU_ASSERT(g_ut_ctx->lba = 1024);
+	CU_ASSERT(g_ut_ctx->lba_count == 2);
+	CU_ASSERT(g_ut_ctx->data_len == 2 * 4096);
+	CU_ASSERT(g_ut_ctx->data != NULL);
+	cuse_io_ctx_free(g_ut_ctx);
+
+	/* Write */
+	user_io->opcode = SPDK_NVME_OPC_WRITE;
+	g_ut_ctx = NULL;
+
+	cuse_nvme_submit_io(req, 0, arg, &fi, FUSE_IOCTL_DIR, user_io, 4096, 4096);
+	SPDK_CU_ASSERT_FATAL(g_ut_ctx != NULL);
+	CU_ASSERT(g_ut_nsid == 1);
+	CU_ASSERT(g_ut_ctx->req == req);
+	CU_ASSERT(g_ut_ctx->lba = 1024);
+	CU_ASSERT(g_ut_ctx->lba_count == 2);
+	CU_ASSERT(g_ut_ctx->data_len == 2 * 4096);
+	CU_ASSERT(g_ut_ctx->data != NULL);
+	cuse_io_ctx_free(g_ut_ctx);
+
+	/* Invalid */
+	g_ut_ctx = NULL;
+	user_io->opcode = SPDK_NVME_OPC_FLUSH;
+
+	cuse_nvme_submit_io(req, 0, arg, &fi, FUSE_IOCTL_DIR, user_io, 4096, 4096);
+	SPDK_CU_ASSERT_FATAL(g_ut_ctx == NULL);
+
+	free(user_io);
+}
+
+static void
+test_cuse_nvme_reset(void)
+{
+	struct cuse_device cuse_device = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	fuse_req_t req = (void *)0xDEADBEEF;
+
+	cuse_device.ctrlr = &ctrlr;
+	g_cuse_device = &cuse_device;
+
+	/* Invalid nsid  */
+	cuse_device.nsid = 1;
+	g_ut_ctx = NULL;
+
+	cuse_nvme_reset(req, 0, NULL, NULL, 0, NULL, 4096, 4096);
+	CU_ASSERT(g_ut_ctx == NULL);
+
+	/* Valid nsid, check IO message sent value */
+	cuse_device.nsid = 0;
+
+	cuse_nvme_reset(req, 0, NULL, NULL, 0, NULL, 4096, 4096);
+	CU_ASSERT(g_ut_ctx == (void *)0xDEADBEEF);
+	CU_ASSERT(g_ut_ctrlr == &ctrlr);
+	CU_ASSERT(g_ut_nsid == 0);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -157,6 +356,10 @@ int main(int argc, char **argv)
 
 	suite = CU_add_suite("nvme_cuse", NULL, NULL);
 	CU_ADD_TEST(suite, test_cuse_nvme_submit_io_read_write);
+	CU_ADD_TEST(suite, test_cuse_nvme_submit_passthru_cmd);
+	CU_ADD_TEST(suite, test_nvme_cuse_get_cuse_ns_device);
+	CU_ADD_TEST(suite, test_cuse_nvme_submit_io);
+	CU_ADD_TEST(suite, test_cuse_nvme_reset);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_fabric.c/nvme_fabric_ut.c b/test/unit/lib/nvme/nvme_fabric.c/nvme_fabric_ut.c
index 1df04c010..0b5c8a6c1 100644
--- a/test/unit/lib/nvme/nvme_fabric.c/nvme_fabric_ut.c
+++ b/test/unit/lib/nvme/nvme_fabric.c/nvme_fabric_ut.c
@@ -38,15 +38,12 @@
 
 SPDK_LOG_REGISTER_COMPONENT(nvme)
 
+pid_t g_spdk_nvme_pid;
 struct spdk_nvmf_fabric_prop_set_cmd g_ut_cmd = {};
 struct spdk_nvmf_fabric_prop_get_rsp g_ut_response = {};
 
 DEFINE_STUB_V(nvme_completion_poll_cb, (void *arg, const struct spdk_nvme_cpl *cpl));
 
-DEFINE_STUB(nvme_wait_for_completion, int,
-	    (struct spdk_nvme_qpair *qpair,
-	     struct nvme_completion_poll_status *status), 0);
-
 DEFINE_STUB_V(spdk_nvme_ctrlr_get_default_ctrlr_opts,
 	      (struct spdk_nvme_ctrlr_opts *opts, size_t opts_size));
 
@@ -71,9 +68,6 @@ DEFINE_STUB(spdk_nvme_ctrlr_cmd_get_log_page, int,
 	     uint32_t nsid, void *payload, uint32_t payload_size,
 	     uint64_t offset, spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
 
-DEFINE_STUB(spdk_nvme_transport_id_populate_trstring, int,
-	    (struct spdk_nvme_transport_id *trid, const char *trstring), 0);
-
 DEFINE_STUB(spdk_nvme_transport_available_by_name, bool,
 	    (const char *transport_name), true);
 
@@ -82,24 +76,121 @@ DEFINE_STUB(nvme_transport_ctrlr_construct, struct spdk_nvme_ctrlr *,
 	     const struct spdk_nvme_ctrlr_opts *opts,
 	     void *devhandle), NULL);
 
-DEFINE_STUB(nvme_ctrlr_probe, int,
-	    (const struct spdk_nvme_transport_id *trid,
-	     struct spdk_nvme_probe_ctx *probe_ctx, void *devhandle), 0);
+DEFINE_STUB(spdk_nvme_transport_id_adrfam_str, const char *,
+	    (enum spdk_nvmf_adrfam adrfam), NULL);
 
-DEFINE_STUB(spdk_nvme_ctrlr_cmd_io_raw, int, (struct spdk_nvme_ctrlr *ctrlr,
-		struct spdk_nvme_qpair *qpair, struct spdk_nvme_cmd *cmd, void *buf,
-		uint32_t len, spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
+DEFINE_STUB(nvme_ctrlr_process_init, int, (struct spdk_nvme_ctrlr *ctrlr), 0);
 
-DEFINE_STUB(nvme_wait_for_completion_timeout, int,
-	    (struct spdk_nvme_qpair *qpair,
-	     struct nvme_completion_poll_status *status,
-	     uint64_t timeout_in_usecs), 0);
+static struct spdk_nvmf_fabric_connect_data g_nvmf_data;
 
-DEFINE_STUB(spdk_nvme_transport_id_trtype_str, const char *,
-	    (enum spdk_nvme_transport_type trtype), NULL);
+int
+spdk_nvme_ctrlr_cmd_io_raw(struct spdk_nvme_ctrlr *ctrlr,
+			   struct spdk_nvme_qpair *qpair,
+			   struct spdk_nvme_cmd *cmd,
+			   void *buf, uint32_t len,
+			   spdk_nvme_cmd_cb cb_fn, void *cb_arg)
+{
+	struct nvme_request	*req;
 
-DEFINE_STUB(spdk_nvme_transport_id_adrfam_str, const char *,
-	    (enum spdk_nvmf_adrfam adrfam), NULL);
+	req = nvme_allocate_request_contig(qpair, buf, len, cb_fn, cb_arg);
+
+	if (req == NULL) {
+		return -ENOMEM;
+	}
+
+	memcpy(&req->cmd, cmd, sizeof(req->cmd));
+	memcpy(&g_nvmf_data, buf, sizeof(g_nvmf_data));
+
+	return 0;
+}
+
+DEFINE_RETURN_MOCK(nvme_wait_for_completion_timeout, int);
+int
+nvme_wait_for_completion_timeout(struct spdk_nvme_qpair *qpair,
+				 struct nvme_completion_poll_status *status,
+				 uint64_t timeout_in_usecs)
+{
+	struct spdk_nvmf_fabric_connect_rsp *rsp = (void *)&status->cpl;
+
+	if (nvme_qpair_is_admin_queue(qpair)) {
+		rsp->status_code_specific.success.cntlid = 1;
+	}
+
+	status->timed_out = false;
+	HANDLE_RETURN_MOCK(nvme_wait_for_completion_timeout);
+
+	return 0;
+}
+
+int
+spdk_nvme_transport_id_populate_trstring(struct spdk_nvme_transport_id *trid, const char *trstring)
+{
+	int len, i, rc;
+
+	if (trstring == NULL) {
+		return -EINVAL;
+	}
+
+	len = strnlen(trstring, SPDK_NVMF_TRSTRING_MAX_LEN);
+	if (len == SPDK_NVMF_TRSTRING_MAX_LEN) {
+		return -EINVAL;
+	}
+
+	rc = snprintf(trid->trstring, SPDK_NVMF_TRSTRING_MAX_LEN, "%s", trstring);
+	if (rc < 0) {
+		return rc;
+	}
+
+	/* cast official trstring to uppercase version of input. */
+	for (i = 0; i < len; i++) {
+		trid->trstring[i] = toupper(trid->trstring[i]);
+	}
+	return 0;
+}
+
+static struct spdk_nvme_transport_id g_ut_trid;
+static bool g_ut_ctrlr_is_probed;
+
+int
+nvme_ctrlr_probe(const struct spdk_nvme_transport_id *trid,
+		 struct spdk_nvme_probe_ctx *probe_ctx, void *devhandle)
+{
+	g_ut_trid = *trid;
+	g_ut_ctrlr_is_probed = true;
+
+	return 0;
+}
+
+const char *
+spdk_nvme_transport_id_trtype_str(enum spdk_nvme_transport_type trtype)
+{
+	switch (trtype) {
+	case SPDK_NVME_TRANSPORT_PCIE:
+		return "PCIe";
+	case SPDK_NVME_TRANSPORT_RDMA:
+		return "RDMA";
+	case SPDK_NVME_TRANSPORT_FC:
+		return "FC";
+	case SPDK_NVME_TRANSPORT_TCP:
+		return "TCP";
+	case SPDK_NVME_TRANSPORT_VFIOUSER:
+		return "VFIOUSER";
+	case SPDK_NVME_TRANSPORT_CUSTOM:
+		return "CUSTOM";
+	default:
+		return NULL;
+	}
+}
+
+DEFINE_RETURN_MOCK(nvme_wait_for_completion, int);
+int
+nvme_wait_for_completion(struct spdk_nvme_qpair *qpair,
+			 struct nvme_completion_poll_status *status)
+{
+	status->timed_out = false;
+	HANDLE_RETURN_MOCK(nvme_wait_for_completion);
+	return 0;
+}
 
 DEFINE_RETURN_MOCK(spdk_nvme_ctrlr_cmd_admin_raw, int);
 int
@@ -177,6 +268,152 @@ test_nvme_fabric_prop_get_cmd(void)
 	CU_ASSERT(g_ut_response.value.u64 == value);
 }
 
+static void
+test_nvme_fabric_get_discovery_log_page(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	char buffer[4096] = {};
+	uint64_t offset = 0;
+	int rc;
+
+	rc = nvme_fabric_get_discovery_log_page(&ctrlr, buffer, sizeof(buffer), offset);
+	CU_ASSERT(rc == 0);
+
+	/* Get log page fail */
+	MOCK_SET(spdk_nvme_ctrlr_cmd_get_log_page, -EINVAL);
+
+	rc = nvme_fabric_get_discovery_log_page(&ctrlr, buffer, sizeof(buffer), offset);
+	CU_ASSERT(rc == -1);
+	MOCK_CLEAR(spdk_nvme_ctrlr_cmd_get_log_page);
+
+	/* Completion time out */
+	MOCK_SET(nvme_wait_for_completion, -1);
+
+	rc = nvme_fabric_get_discovery_log_page(&ctrlr, buffer, sizeof(buffer), offset);
+	CU_ASSERT(rc == -1);
+	MOCK_CLEAR(nvme_wait_for_completion);
+}
+
+static void
+test_nvme_fabric_discover_probe(void)
+{
+	struct spdk_nvmf_discovery_log_page_entry entry = {};
+	struct spdk_nvme_probe_ctx probe_ctx = {};
+	char hostnqn[256] = "nqn.2016-06.io.spdk:cnode1";
+	char traddr[SPDK_NVMF_TRADDR_MAX_LEN] = "192.168.100.8";
+	char trsvcid[SPDK_NVMF_TRSVCID_MAX_LEN] = "4420";
+	char trstring[SPDK_NVMF_TRSTRING_MAX_LEN + 1] = "RDMA";
+
+	entry.trtype = SPDK_NVME_TRANSPORT_RDMA;
+	entry.subtype = SPDK_NVMF_SUBTYPE_NVME;
+	entry.adrfam = SPDK_NVMF_ADRFAM_IPV4;
+
+	memcpy(entry.subnqn, hostnqn, 256);
+	memcpy(entry.traddr, traddr, SPDK_NVMF_TRADDR_MAX_LEN);
+	memcpy(entry.trsvcid, trsvcid, SPDK_NVMF_TRSVCID_MAX_LEN);
+	memcpy(probe_ctx.trid.trstring, trstring, sizeof(probe_ctx.trid.trstring));
+
+	nvme_fabric_discover_probe(&entry, &probe_ctx, 1);
+	CU_ASSERT(g_ut_ctrlr_is_probed == true);
+	CU_ASSERT(g_ut_trid.trtype == SPDK_NVME_TRANSPORT_RDMA);
+	CU_ASSERT(g_ut_trid.adrfam == SPDK_NVMF_ADRFAM_IPV4);
+	CU_ASSERT(!strncmp(g_ut_trid.trstring, trstring, sizeof(trstring)));
+	CU_ASSERT(!strncmp(g_ut_trid.subnqn, hostnqn, sizeof(hostnqn)));
+	CU_ASSERT(!strncmp(g_ut_trid.traddr, traddr, sizeof(traddr)));
+	CU_ASSERT(!strncmp(g_ut_trid.trsvcid, trsvcid, sizeof(trsvcid)));
+
+	g_ut_ctrlr_is_probed = false;
+	memset(&g_ut_trid, 0, sizeof(g_ut_trid));
+
+	/* Entry type unsupported */
+	entry.subtype = SPDK_NVMF_SUBTYPE_DISCOVERY;
+
+	nvme_fabric_discover_probe(&entry, &probe_ctx, 1);
+	CU_ASSERT(g_ut_ctrlr_is_probed == false);
+
+	/* Entry type invalid */
+	entry.subtype = 3;
+
+	nvme_fabric_discover_probe(&entry, &probe_ctx, 1);
+	CU_ASSERT(g_ut_ctrlr_is_probed == false);
+}
+
+static void
+test_nvme_fabric_qpair_connect(void)
+{
+	struct spdk_nvme_qpair qpair = {};
+	struct nvme_request	req = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct spdk_nvmf_fabric_connect_cmd *cmd = NULL;
+	int rc;
+	char hostnqn[SPDK_NVMF_NQN_MAX_LEN + 1] = "nqn.2016-06.io.spdk:host1";
+	char subnqn[SPDK_NVMF_NQN_MAX_LEN + 1] = "nqn.2016-06.io.spdk:subsystem1";
+	const uint8_t hostid[16] = {
+		0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,
+		0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F
+	};
+
+	cmd = (void *)&req.cmd;
+	qpair.ctrlr = &ctrlr;
+	req.qpair = &qpair;
+	STAILQ_INIT(&qpair.free_req);
+	STAILQ_INSERT_HEAD(&qpair.free_req, &req, stailq);
+	memset(&g_nvmf_data, 0, sizeof(g_nvmf_data));
+
+	qpair.id = 1;
+	ctrlr.opts.keep_alive_timeout_ms = 100;
+	ctrlr.cntlid = 2;
+	memcpy(ctrlr.opts.extended_host_id, hostid, sizeof(hostid));
+	memcpy(ctrlr.opts.hostnqn, hostnqn, sizeof(hostnqn));
+	memcpy(ctrlr.trid.subnqn, subnqn, sizeof(subnqn));
+
+	rc = nvme_fabric_qpair_connect(&qpair, 1);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(cmd->opcode == SPDK_NVME_OPC_FABRIC);
+	CU_ASSERT(cmd->fctype == SPDK_NVMF_FABRIC_COMMAND_CONNECT);
+	CU_ASSERT(cmd->qid == 1);
+	CU_ASSERT(cmd->sqsize == 0);
+	CU_ASSERT(cmd->kato == 100);
+	CU_ASSERT(g_nvmf_data.cntlid == 2);
+	CU_ASSERT(!strncmp(g_nvmf_data.hostid, ctrlr.opts.extended_host_id, sizeof(g_nvmf_data.hostid)));
+	CU_ASSERT(!strncmp(g_nvmf_data.hostnqn, ctrlr.opts.hostnqn, sizeof(ctrlr.opts.hostnqn)));
+	CU_ASSERT(!strncmp(g_nvmf_data.subnqn, ctrlr.trid.subnqn, sizeof(ctrlr.trid.subnqn)));
+	CU_ASSERT(STAILQ_EMPTY(&qpair.free_req));
+
+	/* qid is adminq */
+	memset(&g_nvmf_data, 0, sizeof(g_nvmf_data));
+	memset(&req, 0, sizeof(req));
+	STAILQ_INSERT_HEAD(&qpair.free_req, &req, stailq);
+	qpair.id = 0;
+	ctrlr.cntlid = 0;
+
+	rc = nvme_fabric_qpair_connect(&qpair, 1);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(cmd->opcode == SPDK_NVME_OPC_FABRIC);
+	CU_ASSERT(cmd->fctype == SPDK_NVMF_FABRIC_COMMAND_CONNECT);
+	CU_ASSERT(cmd->qid == 0);
+	CU_ASSERT(cmd->sqsize == 0);
+	CU_ASSERT(cmd->kato == 100);
+	CU_ASSERT(ctrlr.cntlid == 1);
+	CU_ASSERT(g_nvmf_data.cntlid == 0xffff);
+	CU_ASSERT(!strncmp(g_nvmf_data.hostid, ctrlr.opts.extended_host_id, sizeof(g_nvmf_data.hostid)));
+	CU_ASSERT(!strncmp(g_nvmf_data.hostnqn, ctrlr.opts.hostnqn, sizeof(ctrlr.opts.hostnqn)));
+	CU_ASSERT(!strncmp(g_nvmf_data.subnqn, ctrlr.trid.subnqn, sizeof(ctrlr.trid.subnqn)));
+	CU_ASSERT(STAILQ_EMPTY(&qpair.free_req));
+
+	/* Wait_for completion timeout */
+	STAILQ_INSERT_HEAD(&qpair.free_req, &req, stailq);
+	MOCK_SET(nvme_wait_for_completion_timeout, 1);
+
+	rc = nvme_fabric_qpair_connect(&qpair, 1);
+	CU_ASSERT(rc == -EIO);
+	MOCK_CLEAR(nvme_wait_for_completion_timeout);
+
+	/* Input parameters invalid */
+	rc = nvme_fabric_qpair_connect(&qpair, 0);
+	CU_ASSERT(rc == -EINVAL);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -188,6 +425,9 @@ int main(int argc, char **argv)
 	suite = CU_add_suite("nvme_fabric", NULL, NULL);
 	CU_ADD_TEST(suite, test_nvme_fabric_prop_set_cmd);
 	CU_ADD_TEST(suite, test_nvme_fabric_prop_get_cmd);
+	CU_ADD_TEST(suite, test_nvme_fabric_get_discovery_log_page);
+	CU_ADD_TEST(suite, test_nvme_fabric_discover_probe);
+	CU_ADD_TEST(suite, test_nvme_fabric_qpair_connect);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_ns.c/nvme_ns_ut.c b/test/unit/lib/nvme/nvme_ns.c/nvme_ns_ut.c
index 39f03ed4f..fa8468fd1 100644
--- a/test/unit/lib/nvme/nvme_ns.c/nvme_ns_ut.c
+++ b/test/unit/lib/nvme/nvme_ns.c/nvme_ns_ut.c
@@ -63,11 +63,15 @@ static struct spdk_nvme_zns_ns_data nsdata_zns = {
 	.mor = 1024,
 };
 
+struct spdk_nvme_cmd g_ut_cmd = {};
+
 int
 nvme_ctrlr_cmd_identify(struct spdk_nvme_ctrlr *ctrlr, uint8_t cns, uint16_t cntid, uint32_t nsid,
 			uint8_t csi, void *payload, size_t payload_size,
 			spdk_nvme_cmd_cb cb_fn, void *cb_arg)
 {
+	memset(&g_ut_cmd, 0, sizeof(g_ut_cmd));
+
 	if (cns == SPDK_NVME_IDENTIFY_NS) {
 		assert(payload_size == sizeof(struct spdk_nvme_ns_data));
 		if (fake_nsdata) {
@@ -81,6 +85,12 @@ nvme_ctrlr_cmd_identify(struct spdk_nvme_ctrlr *ctrlr, uint8_t cns, uint16_t cnt
 		assert(payload_size == sizeof(struct spdk_nvme_zns_ns_data));
 		memcpy(payload, &nsdata_zns, sizeof(struct spdk_nvme_zns_ns_data));
 		return 0;
+	} else if (cns == SPDK_NVME_IDENTIFY_NS_ID_DESCRIPTOR_LIST) {
+		g_ut_cmd.cdw10_bits.identify.cns = cns;
+		g_ut_cmd.cdw10_bits.identify.cntid = cntid;
+		g_ut_cmd.cdw11_bits.identify.csi = csi;
+		g_ut_cmd.nsid = nsid;
+		return 0;
 	}
 	return -1;
 }
@@ -453,6 +463,64 @@ test_nvme_ctrlr_identify_ns_iocs_specific(void)
 	CU_ASSERT(ns.nsdata_zns == NULL);
 }
 
+static void
+test_nvme_ctrlr_identify_id_desc(void)
+{
+	struct spdk_nvme_ns ns = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	int rc;
+
+	ns.ctrlr = &ctrlr;
+	ns.ctrlr->vs.raw = SPDK_NVME_VERSION(1, 3, 0);
+	ns.ctrlr->cap.bits.css |= SPDK_NVME_CAP_CSS_IOCS;
+	ns.id = 1;
+
+	rc = nvme_ctrlr_identify_id_desc(&ns);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(g_ut_cmd.cdw10_bits.identify.cns == SPDK_NVME_IDENTIFY_NS_ID_DESCRIPTOR_LIST);
+	CU_ASSERT(g_ut_cmd.cdw10_bits.identify.cntid == 0);
+	CU_ASSERT(g_ut_cmd.cdw11_bits.identify.csi == spdk_nvme_ns_get_csi(&ns));
+	CU_ASSERT(g_ut_cmd.nsid == 1);
+
+	/* NVME version and css unsupported */
+	ns.ctrlr->vs.raw = SPDK_NVME_VERSION(1, 2, 0);
+	ns.ctrlr->cap.bits.css &= ~SPDK_NVME_CAP_CSS_IOCS;
+
+	rc = nvme_ctrlr_identify_id_desc(&ns);
+	CU_ASSERT(rc == 0);
+}
+
+static void
+test_nvme_ns_find_id_desc(void)
+{
+	struct spdk_nvme_ns ns = {};
+	struct spdk_nvme_ns_id_desc *desc = NULL;
+	const uint8_t *csi = NULL;
+	size_t length = 0;
+
+	desc = (void *)ns.id_desc_list;
+	desc->nidl = 4;
+	desc->nidt = SPDK_NVME_NIDT_CSI;
+
+	/* Case 1: get id descriptor successfully */
+	csi = nvme_ns_find_id_desc(&ns, SPDK_NVME_NIDT_CSI, &length);
+	CU_ASSERT(csi == desc->nid);
+	CU_ASSERT(length == 4);
+
+	/* Case 2: ns_id length invalid, expect fail */
+	desc->nidl = 0;
+
+	csi = nvme_ns_find_id_desc(&ns, SPDK_NVME_NIDT_CSI, &length);
+	CU_ASSERT(csi == NULL);
+
+	/* Case 3: No correct id descriptor type entry, expect fail */
+	desc->nidl = 4;
+	desc->nidt = SPDK_NVME_NIDT_CSI;
+
+	csi = nvme_ns_find_id_desc(&ns, SPDK_NVME_NIDT_UUID, &length);
+	CU_ASSERT(csi == NULL);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -473,6 +541,8 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, spdk_nvme_ns_supports);
 	CU_ADD_TEST(suite, test_nvme_ns_has_supported_iocs_specific_data);
 	CU_ADD_TEST(suite, test_nvme_ctrlr_identify_ns_iocs_specific);
+	CU_ADD_TEST(suite, test_nvme_ctrlr_identify_id_desc);
+	CU_ADD_TEST(suite, test_nvme_ns_find_id_desc);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_ns_cmd.c/nvme_ns_cmd_ut.c b/test/unit/lib/nvme/nvme_ns_cmd.c/nvme_ns_cmd_ut.c
index 0b40d3779..5899579dc 100644
--- a/test/unit/lib/nvme/nvme_ns_cmd.c/nvme_ns_cmd_ut.c
+++ b/test/unit/lib/nvme/nvme_ns_cmd.c/nvme_ns_cmd_ut.c
@@ -694,6 +694,59 @@ test_nvme_ns_cmd_dataset_management(void)
 	cleanup_after_test(&qpair);
 }
 
+static void
+test_nvme_ns_cmd_copy(void)
+{
+	struct spdk_nvme_ns	ns;
+	struct spdk_nvme_ctrlr	ctrlr;
+	struct spdk_nvme_qpair	qpair;
+	spdk_nvme_cmd_cb	cb_fn = NULL;
+	void			*cb_arg = NULL;
+	uint16_t		i;
+	int			rc = 0;
+	uint64_t		cmd_dest_lba;
+	uint32_t		cmd_range_count;
+	struct spdk_nvme_scc_source_range	ranges[64];
+
+	prepare_for_test(&ns, &ctrlr, &qpair, 512, 0, 128 * 1024, 0, false);
+
+	for (i = 0; i < 64; i++) {
+		ranges[i].slba = i;
+		ranges[i].nlb = 1;
+	}
+
+	/* COPY one LBA */
+	rc = spdk_nvme_ns_cmd_copy(&ns, &qpair, ranges,
+				   1, 64, cb_fn, cb_arg);
+	CU_ASSERT(rc == 0);
+	SPDK_CU_ASSERT_FATAL(g_request != NULL);
+	CU_ASSERT(g_request->cmd.opc == SPDK_NVME_OPC_COPY);
+	CU_ASSERT(g_request->cmd.nsid == ns.id);
+	nvme_cmd_interpret_rw(&g_request->cmd, &cmd_dest_lba, &cmd_range_count);
+	CU_ASSERT_EQUAL(cmd_dest_lba, 64);
+	CU_ASSERT_EQUAL(cmd_range_count, 1);
+	spdk_free(g_request->payload.contig_or_cb_arg);
+	nvme_free_request(g_request);
+
+	/* COPY 64 LBAs */
+	rc = spdk_nvme_ns_cmd_copy(&ns, &qpair, ranges,
+				   64, 64, cb_fn, cb_arg);
+	CU_ASSERT(rc == 0);
+	SPDK_CU_ASSERT_FATAL(g_request != NULL);
+	CU_ASSERT(g_request->cmd.opc == SPDK_NVME_OPC_COPY);
+	CU_ASSERT(g_request->cmd.nsid == ns.id);
+	nvme_cmd_interpret_rw(&g_request->cmd, &cmd_dest_lba, &cmd_range_count);
+	CU_ASSERT_EQUAL(cmd_dest_lba, 64);
+	CU_ASSERT_EQUAL(cmd_range_count, 64);
+	spdk_free(g_request->payload.contig_or_cb_arg);
+	nvme_free_request(g_request);
+
+	rc = spdk_nvme_ns_cmd_copy(&ns, &qpair, ranges,
+				   0, 64, cb_fn, cb_arg);
+	CU_ASSERT(rc != 0);
+	cleanup_after_test(&qpair);
+}
+
 static void
 test_nvme_ns_cmd_readv(void)
 {
@@ -1972,6 +2025,70 @@ test_nvme_ns_cmd_compare_with_md(void)
 	free(metadata);
 }
 
+static void
+test_nvme_ns_cmd_setup_request(void)
+{
+	struct spdk_nvme_ns ns = {};
+	struct nvme_request req = {};
+
+	ns.id = 1;
+	ns.pi_type = SPDK_NVME_FMT_NVM_PROTECTION_TYPE1;
+	ns.flags = SPDK_NVME_NS_DPS_PI_SUPPORTED;
+
+	_nvme_ns_cmd_setup_request(&ns, &req, SPDK_NVME_OPC_READ,
+				   1024, 256, SPDK_NVME_IO_FLAGS_PRACT, 1, 1);
+	CU_ASSERT(req.cmd.cdw10 == 1024);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_READ);
+	CU_ASSERT(req.cmd.nsid == 1);
+	CU_ASSERT(req.cmd.cdw14  == 1024);
+	CU_ASSERT(req.cmd.fuse == 0);
+	CU_ASSERT(req.cmd.cdw12 == (255 | SPDK_NVME_IO_FLAGS_PRACT));
+	CU_ASSERT(req.cmd.cdw15 == (1 << 16 | 1));
+}
+
+static void
+test_spdk_nvme_ns_cmd_readv_with_md(void)
+{
+	struct spdk_nvme_ns		ns;
+	struct spdk_nvme_ctrlr		ctrlr;
+	struct spdk_nvme_qpair		qpair;
+	int				rc = 0;
+	char				*metadata = NULL;
+	uint32_t			lba_count = 256;
+	uint32_t			sector_size = 512;
+	uint32_t			md_size = 128;
+	uint64_t			sge_length = lba_count * sector_size;
+
+	metadata = (void *)0xDEADBEEF;
+	prepare_for_test(&ns, &ctrlr, &qpair, sector_size,
+			 md_size, 128 * 1024, 0, false);
+
+	rc = spdk_nvme_ns_cmd_readv_with_md(&ns, &qpair, 0x1000, lba_count, NULL,
+					    &sge_length, 0, nvme_request_reset_sgl,
+					    nvme_request_next_sge, metadata, 0, 0);
+	CU_ASSERT(rc == 0);
+	SPDK_CU_ASSERT_FATAL(g_request != NULL);
+	CU_ASSERT(g_request->cmd.opc == SPDK_NVME_OPC_READ);
+	CU_ASSERT(nvme_payload_type(&g_request->payload) == NVME_PAYLOAD_TYPE_SGL);
+	CU_ASSERT(g_request->payload.reset_sgl_fn == nvme_request_reset_sgl);
+	CU_ASSERT(g_request->payload.next_sge_fn == nvme_request_next_sge);
+	CU_ASSERT(g_request->payload.contig_or_cb_arg == &sge_length);
+	CU_ASSERT(g_request->payload.md == (void *)0xDEADBEEF);
+	CU_ASSERT(g_request->cmd.nsid == ns.id);
+	CU_ASSERT(g_request->payload_size == 256 * 512);
+	CU_ASSERT(g_request->qpair == &qpair);
+	CU_ASSERT(g_request->md_offset == 0);
+	CU_ASSERT(g_request->payload_offset == 0);
+
+	rc = spdk_nvme_ns_cmd_readv_with_md(&ns, &qpair, 0x1000, lba_count, NULL,
+					    NULL, 0, nvme_request_reset_sgl, NULL,
+					    metadata, 0, 0);
+	CU_ASSERT(rc == -EINVAL);
+
+	nvme_free_request(g_request);
+	cleanup_after_test(&qpair);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -1988,6 +2105,7 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, split_test4);
 	CU_ADD_TEST(suite, test_nvme_ns_cmd_flush);
 	CU_ADD_TEST(suite, test_nvme_ns_cmd_dataset_management);
+	CU_ADD_TEST(suite, test_nvme_ns_cmd_copy);
 	CU_ADD_TEST(suite, test_io_flags);
 	CU_ADD_TEST(suite, test_nvme_ns_cmd_write_zeroes);
 	CU_ADD_TEST(suite, test_nvme_ns_cmd_write_uncorrectable);
@@ -2006,6 +2124,8 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_nvme_ns_cmd_compare_and_write);
 	CU_ADD_TEST(suite, test_nvme_ns_cmd_compare_with_md);
 	CU_ADD_TEST(suite, test_nvme_ns_cmd_comparev_with_md);
+	CU_ADD_TEST(suite, test_nvme_ns_cmd_setup_request);
+	CU_ADD_TEST(suite, test_spdk_nvme_ns_cmd_readv_with_md);
 
 	g_spdk_nvme_driver = &_g_nvme_driver;
 
diff --git a/test/unit/lib/nvme/nvme_opal.c/.gitignore b/test/unit/lib/nvme/nvme_opal.c/.gitignore
new file mode 100644
index 000000000..8d75c8d36
--- /dev/null
+++ b/test/unit/lib/nvme/nvme_opal.c/.gitignore
@@ -0,0 +1 @@
+nvme_opal_ut
diff --git a/test/unit/lib/nvme/nvme_opal.c/Makefile b/test/unit/lib/nvme/nvme_opal.c/Makefile
new file mode 100644
index 000000000..e8818ecb0
--- /dev/null
+++ b/test/unit/lib/nvme/nvme_opal.c/Makefile
@@ -0,0 +1,38 @@
+#
+#  BSD LICENSE
+#
+#  Copyright (c) Intel Corporation.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Intel Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../../../../..)
+
+TEST_FILE = nvme_opal_ut.c
+
+include $(SPDK_ROOT_DIR)/mk/spdk.unittest.mk
diff --git a/test/unit/lib/nvme/nvme_opal.c/nvme_opal_ut.c b/test/unit/lib/nvme/nvme_opal.c/nvme_opal_ut.c
new file mode 100644
index 000000000..bf68bf575
--- /dev/null
+++ b/test/unit/lib/nvme/nvme_opal.c/nvme_opal_ut.c
@@ -0,0 +1,142 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/stdinc.h"
+#include "spdk_cunit.h"
+#include "nvme/nvme_opal.c"
+#include "common/lib/test_env.c"
+
+SPDK_LOG_REGISTER_COMPONENT(nvme)
+
+DEFINE_STUB(spdk_nvme_ctrlr_cmd_security_receive, int,
+	    (struct spdk_nvme_ctrlr *ctrlr, uint8_t secp, uint16_t spsp,
+	     uint8_t nssf, void *payload, uint32_t payload_size,
+	     spdk_nvme_cmd_cb cb_fn, void *cb_arg), 1);
+
+DEFINE_STUB(spdk_nvme_ctrlr_security_receive, int,
+	    (struct spdk_nvme_ctrlr *ctrlr, uint8_t secp,
+	     uint16_t spsp, uint8_t nssf, void *payload, size_t size), 0);
+
+DEFINE_STUB(spdk_nvme_ctrlr_process_admin_completions, int,
+	    (struct spdk_nvme_ctrlr *ctrlr), 0);
+
+DEFINE_STUB(spdk_nvme_ctrlr_cmd_security_send, int,
+	    (struct spdk_nvme_ctrlr *ctrlr, uint8_t secp,
+	     uint16_t spsp, uint8_t nssf, void *payload,
+	     uint32_t payload_size, spdk_nvme_cmd_cb cb_fn, void *cb_arg), 0);
+
+static int g_ut_recv_status = 0;
+static void *g_ut_sess_ctx;
+
+static void
+ut_opal_sess_cb(struct opal_session *sess, int status, void *ctx)
+{
+	g_ut_recv_status = status;
+	g_ut_sess_ctx = ctx;
+}
+
+static void
+reset_ut_global_variables(void)
+{
+	g_ut_recv_status = 0;
+	g_ut_sess_ctx = NULL;
+}
+
+static void
+test_opal_nvme_security_recv_send_done(void)
+{
+	struct spdk_nvme_cpl cpl = {};
+	struct spdk_opal_compacket header = {};
+	struct spdk_opal_dev dev = {};
+	struct opal_session sess = {};
+
+	sess.sess_cb = ut_opal_sess_cb;
+	sess.cb_arg = (void *)0xDEADBEEF;
+	sess.dev = &dev;
+	memcpy(sess.resp, &header, sizeof(header));
+
+	/* Case 1: receive/send IO error */
+	reset_ut_global_variables();
+	cpl.status.sct = SPDK_NVME_SCT_MEDIA_ERROR;
+
+	opal_nvme_security_recv_done(&sess, &cpl);
+	CU_ASSERT(g_ut_recv_status == -EIO);
+	CU_ASSERT(g_ut_sess_ctx == (void *)0xDEADBEEF);
+
+	reset_ut_global_variables();
+	opal_nvme_security_send_done(&sess, &cpl);
+	CU_ASSERT(g_ut_recv_status == -EIO);
+	CU_ASSERT(g_ut_sess_ctx == (void *)0xDEADBEEF);
+
+	/* Case 2: receive with opal header no outstanding data */
+	reset_ut_global_variables();
+	memset(&header, 0, sizeof(header));
+	cpl.status.sct = SPDK_NVME_SCT_GENERIC;
+
+	opal_nvme_security_recv_done(&sess, &cpl);
+	CU_ASSERT(g_ut_recv_status == 0);
+	CU_ASSERT(g_ut_sess_ctx == (void *)0xDEADBEEF);
+
+	/* Case 3: receive with opal header outstanding data and send done success */
+	reset_ut_global_variables();
+	header.outstanding_data = 0xff;
+	memcpy(sess.resp, &header, sizeof(header));
+	cpl.status.sct = SPDK_NVME_SCT_GENERIC;
+
+	opal_nvme_security_recv_done(&sess, &cpl);
+	CU_ASSERT(g_ut_recv_status == 1);
+	CU_ASSERT(g_ut_sess_ctx == (void *)0xDEADBEEF);
+
+	reset_ut_global_variables();
+	opal_nvme_security_send_done(&sess, &cpl);
+	CU_ASSERT(g_ut_recv_status == 1);
+	CU_ASSERT(g_ut_sess_ctx == (void *)0xDEADBEEF);
+}
+
+int main(int argc, char **argv)
+{
+	CU_pSuite	suite = NULL;
+	unsigned int	num_failures;
+
+	CU_set_error_action(CUEA_ABORT);
+	CU_initialize_registry();
+
+	suite = CU_add_suite("nvme_opal", NULL, NULL);
+	CU_ADD_TEST(suite, test_opal_nvme_security_recv_send_done);
+
+	CU_basic_set_mode(CU_BRM_VERBOSE);
+	CU_basic_run_tests();
+	num_failures = CU_get_number_of_failures();
+	CU_cleanup_registry();
+	return num_failures;
+}
diff --git a/test/unit/lib/nvme/nvme_pcie.c/nvme_pcie_ut.c b/test/unit/lib/nvme/nvme_pcie.c/nvme_pcie_ut.c
index b988f795a..25faec557 100644
--- a/test/unit/lib/nvme/nvme_pcie.c/nvme_pcie_ut.c
+++ b/test/unit/lib/nvme/nvme_pcie.c/nvme_pcie_ut.c
@@ -58,9 +58,6 @@ DEFINE_STUB(nvme_ctrlr_submit_admin_request, int, (struct spdk_nvme_ctrlr *ctrlr
 DEFINE_STUB_V(nvme_ctrlr_free_processes, (struct spdk_nvme_ctrlr *ctrlr));
 DEFINE_STUB(nvme_ctrlr_proc_get_devhandle, struct spdk_pci_device *,
 	    (struct spdk_nvme_ctrlr *ctrlr), NULL);
-
-DEFINE_STUB(spdk_pci_device_map_bar, int, (struct spdk_pci_device *dev, uint32_t bar,
-		void **mapped_addr, uint64_t *phys_addr, uint64_t *size), 0);
 DEFINE_STUB(spdk_pci_device_unmap_bar, int, (struct spdk_pci_device *dev, uint32_t bar, void *addr),
 	    0);
 DEFINE_STUB(spdk_pci_device_attach, int, (struct spdk_pci_driver *driver, spdk_pci_enum_cb enum_cb,
@@ -86,8 +83,32 @@ DEFINE_STUB(nvme_transport_get_name, const char *, (const struct spdk_nvme_trans
 
 SPDK_LOG_REGISTER_COMPONENT(nvme)
 
+struct dev_mem_resource {
+	uint64_t phys_addr;
+	uint64_t len;
+	void *addr;
+};
+
+struct nvme_pcie_ut_bdev_io {
+	struct iovec iovs[NVME_MAX_SGL_DESCRIPTORS];
+	int iovpos;
+};
+
 struct nvme_driver *g_spdk_nvme_driver = NULL;
 
+int
+spdk_pci_device_map_bar(struct spdk_pci_device *dev, uint32_t bar,
+			void **mapped_addr, uint64_t *phys_addr, uint64_t *size)
+{
+	struct dev_mem_resource *dev_mem_res = (void *)dev;
+
+	*mapped_addr = dev_mem_res->addr;
+	*phys_addr = dev_mem_res->phys_addr;
+	*size = dev_mem_res->len;
+
+	return 0;
+}
+
 void
 nvme_ctrlr_fail(struct spdk_nvme_ctrlr *ctrlr, bool hot_remove)
 {
@@ -139,7 +160,9 @@ prp_list_prep(struct nvme_tracker *tr, struct nvme_request *req, uint32_t *prp_i
 	memset(tr, 0, sizeof(*tr));
 	tr->req = req;
 	tr->prp_sgl_bus_addr = 0xDEADBEEF;
-	*prp_index = 0;
+	if (prp_index) {
+		*prp_index = 0;
+	}
 }
 
 static void
@@ -147,47 +170,54 @@ test_prp_list_append(void)
 {
 	struct nvme_request req;
 	struct nvme_tracker tr;
+	struct spdk_nvme_ctrlr ctrlr = {};
 	uint32_t prp_index;
 
+	ctrlr.trid.trtype = SPDK_NVME_TRANSPORT_PCIE;
 	/* Non-DWORD-aligned buffer (invalid) */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100001, 0x1000, 0x1000) == -EFAULT);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100001, 0x1000,
+					    0x1000) == -EFAULT);
 
 	/* 512-byte buffer, 4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000, 0x200, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000, 0x200, 0x1000) == 0);
 	CU_ASSERT(prp_index == 1);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
 
 	/* 512-byte buffer, non-4K-aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x108000, 0x200, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x108000, 0x200, 0x1000) == 0);
 	CU_ASSERT(prp_index == 1);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x108000);
 
 	/* 4K buffer, 4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000, 0x1000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000, 0x1000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 1);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
 
 	/* 4K buffer, non-4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100800, 0x1000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100800, 0x1000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 2);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100800);
 	CU_ASSERT(req.cmd.dptr.prp.prp2 == 0x101000);
 
 	/* 8K buffer, 4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000, 0x2000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000, 0x2000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 2);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
 	CU_ASSERT(req.cmd.dptr.prp.prp2 == 0x101000);
 
 	/* 8K buffer, non-4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100800, 0x2000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100800, 0x2000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 3);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100800);
 	CU_ASSERT(req.cmd.dptr.prp.prp2 == tr.prp_sgl_bus_addr);
@@ -196,7 +226,8 @@ test_prp_list_append(void)
 
 	/* 12K buffer, 4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000, 0x3000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000, 0x3000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 3);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
 	CU_ASSERT(req.cmd.dptr.prp.prp2 == tr.prp_sgl_bus_addr);
@@ -205,7 +236,8 @@ test_prp_list_append(void)
 
 	/* 12K buffer, non-4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100800, 0x3000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100800, 0x3000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 4);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100800);
 	CU_ASSERT(req.cmd.dptr.prp.prp2 == tr.prp_sgl_bus_addr);
@@ -215,18 +247,22 @@ test_prp_list_append(void)
 
 	/* Two 4K buffers, both 4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000, 0x1000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000, 0x1000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 1);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x900000, 0x1000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x900000, 0x1000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 2);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
 	CU_ASSERT(req.cmd.dptr.prp.prp2 == 0x900000);
 
 	/* Two 4K buffers, first non-4K aligned, second 4K aligned */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100800, 0x1000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100800, 0x1000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 2);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x900000, 0x1000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x900000, 0x1000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 3);
 	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100800);
 	CU_ASSERT(req.cmd.dptr.prp.prp2 == tr.prp_sgl_bus_addr);
@@ -235,37 +271,40 @@ test_prp_list_append(void)
 
 	/* Two 4K buffers, both non-4K aligned (invalid) */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100800, 0x1000, 0x1000) == 0);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100800, 0x1000,
+					    0x1000) == 0);
 	CU_ASSERT(prp_index == 2);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x900800, 0x1000, 0x1000) == -EFAULT);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x900800, 0x1000,
+					    0x1000) == -EFAULT);
 	CU_ASSERT(prp_index == 2);
 
 	/* 4K buffer, 4K aligned, but vtophys fails */
 	MOCK_SET(spdk_vtophys, SPDK_VTOPHYS_ERROR);
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000, 0x1000, 0x1000) == -EFAULT);
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000, 0x1000,
+					    0x1000) == -EFAULT);
 	MOCK_CLEAR(spdk_vtophys);
 
 	/* Largest aligned buffer that can be described in NVME_MAX_PRP_LIST_ENTRIES (plus PRP1) */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000,
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000,
 					    (NVME_MAX_PRP_LIST_ENTRIES + 1) * 0x1000, 0x1000) == 0);
 	CU_ASSERT(prp_index == NVME_MAX_PRP_LIST_ENTRIES + 1);
 
 	/* Largest non-4K-aligned buffer that can be described in NVME_MAX_PRP_LIST_ENTRIES (plus PRP1) */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100800,
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100800,
 					    NVME_MAX_PRP_LIST_ENTRIES * 0x1000, 0x1000) == 0);
 	CU_ASSERT(prp_index == NVME_MAX_PRP_LIST_ENTRIES + 1);
 
 	/* Buffer too large to be described in NVME_MAX_PRP_LIST_ENTRIES */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100000,
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100000,
 					    (NVME_MAX_PRP_LIST_ENTRIES + 2) * 0x1000, 0x1000) == -EFAULT);
 
 	/* Non-4K-aligned buffer too large to be described in NVME_MAX_PRP_LIST_ENTRIES */
 	prp_list_prep(&tr, &req, &prp_index);
-	CU_ASSERT(nvme_pcie_prp_list_append(&tr, &prp_index, (void *)0x100800,
+	CU_ASSERT(nvme_pcie_prp_list_append(&ctrlr, &tr, &prp_index, (void *)0x100800,
 					    (NVME_MAX_PRP_LIST_ENTRIES + 1) * 0x1000, 0x1000) == -EFAULT);
 }
 
@@ -406,8 +445,11 @@ test_build_contig_hw_sgl_request(void)
 	struct spdk_nvme_qpair qpair = {};
 	struct nvme_request req = {};
 	struct nvme_tracker tr = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
 	int rc;
 
+	ctrlr.trid.trtype = SPDK_NVME_TRANSPORT_PCIE;
+	qpair.ctrlr = &ctrlr;
 	/* Test 1: Payload covered by a single mapping */
 	req.payload_size = 100;
 	req.payload = NVME_PAYLOAD_CONTIG(0, 0);
@@ -427,6 +469,7 @@ test_build_contig_hw_sgl_request(void)
 	memset(&tr, 0, sizeof(tr));
 
 	/* Test 2: Payload covered by a single mapping, but request is at an offset */
+	qpair.ctrlr = &ctrlr;
 	req.payload_size = 100;
 	req.payload_offset = 50;
 	req.payload = NVME_PAYLOAD_CONTIG(0, 0);
@@ -446,6 +489,7 @@ test_build_contig_hw_sgl_request(void)
 	memset(&tr, 0, sizeof(tr));
 
 	/* Test 3: Payload spans two mappings */
+	qpair.ctrlr = &ctrlr;
 	req.payload_size = 100;
 	req.payload = NVME_PAYLOAD_CONTIG(0, 0);
 	g_vtophys_size = 60;
@@ -480,6 +524,7 @@ test_nvme_pcie_qpair_build_metadata(void)
 	struct spdk_nvme_ctrlr	ctrlr = {};
 	int rc;
 
+	ctrlr.trid.trtype = SPDK_NVME_TRANSPORT_PCIE;
 	tr.req = &req;
 	qpair.ctrlr = &ctrlr;
 
@@ -509,6 +554,483 @@ test_nvme_pcie_qpair_build_metadata(void)
 	MOCK_CLEAR(spdk_vtophys);
 }
 
+static int
+nvme_pcie_ut_next_sge(void *cb_arg, void **address, uint32_t *length)
+{
+	struct nvme_pcie_ut_bdev_io *bio = cb_arg;
+	struct iovec *iov;
+
+	SPDK_CU_ASSERT_FATAL(bio->iovpos < NVME_MAX_SGL_DESCRIPTORS);
+
+	iov = &bio->iovs[bio->iovpos];
+
+	*address = iov->iov_base;
+	*length = iov->iov_len;
+	bio->iovpos++;
+
+	return 0;
+}
+
+static void
+nvme_pcie_ut_reset_sgl(void *cb_arg, uint32_t offset)
+{
+	struct nvme_pcie_ut_bdev_io *bio = cb_arg;
+	struct iovec *iov;
+
+	for (bio->iovpos = 0; bio->iovpos < NVME_MAX_SGL_DESCRIPTORS; bio->iovpos++) {
+		iov = &bio->iovs[bio->iovpos];
+		/* Offset must be aligned with the start of any SGL entry */
+		if (offset == 0) {
+			break;
+		}
+
+		SPDK_CU_ASSERT_FATAL(offset >= iov->iov_len);
+		offset -= iov->iov_len;
+	}
+
+	SPDK_CU_ASSERT_FATAL(offset == 0);
+	SPDK_CU_ASSERT_FATAL(bio->iovpos < NVME_MAX_SGL_DESCRIPTORS);
+}
+
+static void
+test_nvme_pcie_qpair_build_prps_sgl_request(void)
+{
+	struct spdk_nvme_qpair qpair = {};
+	struct nvme_request req = {};
+	struct nvme_tracker tr = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct nvme_pcie_ut_bdev_io bio = {};
+	int rc;
+
+	tr.req = &req;
+	qpair.ctrlr = &ctrlr;
+	req.payload.contig_or_cb_arg = &bio;
+
+	req.payload.reset_sgl_fn = nvme_pcie_ut_reset_sgl;
+	req.payload.next_sge_fn = nvme_pcie_ut_next_sge;
+	req.payload_size = 4096;
+	ctrlr.page_size = 4096;
+	bio.iovs[0].iov_base = (void *)0x100000;
+	bio.iovs[0].iov_len = 4096;
+
+	rc = nvme_pcie_qpair_build_prps_sgl_request(&qpair, &req, &tr, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
+}
+
+static void
+test_nvme_pcie_qpair_build_hw_sgl_request(void)
+{
+	struct spdk_nvme_qpair qpair = {};
+	struct nvme_request req = {};
+	struct nvme_tracker tr = {};
+	struct nvme_pcie_ut_bdev_io bio = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	int rc;
+
+	ctrlr.trid.trtype = SPDK_NVME_TRANSPORT_PCIE;
+	qpair.ctrlr = &ctrlr;
+	req.payload.contig_or_cb_arg = &bio;
+	req.payload.reset_sgl_fn = nvme_pcie_ut_reset_sgl;
+	req.payload.next_sge_fn = nvme_pcie_ut_next_sge;
+	req.cmd.opc = SPDK_NVME_OPC_WRITE;
+	tr.prp_sgl_bus_addr =  0xDAADBEE0;
+	g_vtophys_size = 4096;
+
+	/* Multiple vectors, 2k + 4k + 2k */
+	req.payload_size = 8192;
+	bio.iovpos = 3;
+	bio.iovs[0].iov_base = (void *)0xDBADBEE0;
+	bio.iovs[0].iov_len = 2048;
+	bio.iovs[1].iov_base = (void *)0xDCADBEE0;
+	bio.iovs[1].iov_len = 4096;
+	bio.iovs[2].iov_base = (void *)0xDDADBEE0;
+	bio.iovs[2].iov_len = 2048;
+
+	rc = nvme_pcie_qpair_build_hw_sgl_request(&qpair, &req, &tr, true);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(tr.u.sgl[0].unkeyed.type == SPDK_NVME_SGL_TYPE_DATA_BLOCK);
+	CU_ASSERT(tr.u.sgl[0].unkeyed.length == 2048);
+	CU_ASSERT(tr.u.sgl[0].address == 0xDBADBEE0);
+	CU_ASSERT(tr.u.sgl[0].unkeyed.subtype == 0);
+	CU_ASSERT(tr.u.sgl[1].unkeyed.type == SPDK_NVME_SGL_TYPE_DATA_BLOCK);
+	CU_ASSERT(tr.u.sgl[1].unkeyed.length == 4096);
+	CU_ASSERT(tr.u.sgl[1].address == 0xDCADBEE0);
+	CU_ASSERT(tr.u.sgl[2].unkeyed.type == SPDK_NVME_SGL_TYPE_DATA_BLOCK);
+	CU_ASSERT(tr.u.sgl[2].unkeyed.length == 2048);
+	CU_ASSERT(tr.u.sgl[2].unkeyed.length == 2048);
+	CU_ASSERT(tr.u.sgl[2].address == 0xDDADBEE0);
+	CU_ASSERT(req.cmd.psdt == SPDK_NVME_PSDT_SGL_MPTR_CONTIG);
+	CU_ASSERT(req.cmd.dptr.sgl1.unkeyed.subtype == 0);
+	CU_ASSERT(req.cmd.dptr.sgl1.unkeyed.type == SPDK_NVME_SGL_TYPE_LAST_SEGMENT);
+	CU_ASSERT(req.cmd.dptr.sgl1.address == 0xDAADBEE0);
+	CU_ASSERT(req.cmd.dptr.sgl1.unkeyed.length == 48);
+
+	/* Single vector */
+	memset(&tr, 0, sizeof(tr));
+	memset(&bio, 0, sizeof(bio));
+	memset(&req, 0, sizeof(req));
+	req.payload.contig_or_cb_arg = &bio;
+	req.payload.reset_sgl_fn = nvme_pcie_ut_reset_sgl;
+	req.payload.next_sge_fn = nvme_pcie_ut_next_sge;
+	req.cmd.opc = SPDK_NVME_OPC_WRITE;
+	req.payload_size = 4096;
+	bio.iovpos = 1;
+	bio.iovs[0].iov_base = (void *)0xDBADBEE0;
+	bio.iovs[0].iov_len = 4096;
+
+	rc = nvme_pcie_qpair_build_hw_sgl_request(&qpair, &req, &tr, true);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(tr.u.sgl[0].unkeyed.type == SPDK_NVME_SGL_TYPE_DATA_BLOCK);
+	CU_ASSERT(tr.u.sgl[0].unkeyed.length == 4096);
+	CU_ASSERT(tr.u.sgl[0].address == 0xDBADBEE0);
+	CU_ASSERT(tr.u.sgl[0].unkeyed.subtype == 0);
+	CU_ASSERT(req.cmd.psdt == SPDK_NVME_PSDT_SGL_MPTR_CONTIG);
+	CU_ASSERT(req.cmd.dptr.sgl1.unkeyed.subtype == 0);
+	CU_ASSERT(req.cmd.dptr.sgl1.unkeyed.type == SPDK_NVME_SGL_TYPE_DATA_BLOCK);
+	CU_ASSERT(req.cmd.dptr.sgl1.address == 0xDBADBEE0);
+	CU_ASSERT(req.cmd.dptr.sgl1.unkeyed.length == 4096);
+}
+
+static void
+test_nvme_pcie_qpair_build_contig_request(void)
+{
+	struct nvme_pcie_qpair pqpair = {};
+	struct nvme_request req = {};
+	struct nvme_tracker tr = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	int rc;
+
+	pqpair.qpair.ctrlr = &ctrlr;
+	ctrlr.page_size = 0x1000;
+
+	/* 1 prp, 4k-aligned */
+	prp_list_prep(&tr, &req, NULL);
+	req.payload_size = 0x1000;
+	req.payload.contig_or_cb_arg = (void *)0x100000;
+
+	rc = nvme_pcie_qpair_build_contig_request(&pqpair.qpair, &req, &tr, true);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
+
+	/* 2 prps, non-4K-aligned */
+	prp_list_prep(&tr, &req, NULL);
+	req.payload_size = 0x1000;
+	req.payload_offset = 0x800;
+	req.payload.contig_or_cb_arg = (void *)0x100000;
+
+	rc = nvme_pcie_qpair_build_contig_request(&pqpair.qpair, &req, &tr, true);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100800);
+	CU_ASSERT(req.cmd.dptr.prp.prp2 == 0x101000);
+
+	/* 3 prps, 4k-aligned */
+	prp_list_prep(&tr, &req, NULL);
+	req.payload_size = 0x3000;
+	req.payload.contig_or_cb_arg = (void *)0x100000;
+
+	rc = nvme_pcie_qpair_build_contig_request(&pqpair.qpair, &req, &tr, true);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0x100000);
+	CU_ASSERT(req.cmd.dptr.prp.prp2 == tr.prp_sgl_bus_addr);
+	CU_ASSERT(tr.u.prp[0] == 0x101000);
+	CU_ASSERT(tr.u.prp[1] == 0x102000);
+
+	/* address not dword aligned */
+	prp_list_prep(&tr, &req, NULL);
+	req.payload_size = 0x3000;
+	req.payload.contig_or_cb_arg = (void *)0x100001;
+	req.qpair = &pqpair.qpair;
+	TAILQ_INIT(&pqpair.outstanding_tr);
+	TAILQ_INSERT_TAIL(&pqpair.outstanding_tr, &tr, tq_list);
+
+	rc = nvme_pcie_qpair_build_contig_request(&pqpair.qpair, &req, &tr, true);
+	CU_ASSERT(rc == -EFAULT);
+}
+
+static void
+test_nvme_pcie_ctrlr_regs_get_set(void)
+{
+	struct nvme_pcie_ctrlr pctrlr = {};
+	volatile struct spdk_nvme_registers regs = {};
+	uint32_t value_4;
+	uint64_t value_8;
+	int rc;
+
+	pctrlr.regs = &regs;
+
+	rc = nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, 8, 4);
+	CU_ASSERT(rc == 0);
+
+	rc = nvme_pcie_ctrlr_get_reg_4(&pctrlr.ctrlr, 8, &value_4);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(value_4 == 4);
+
+	rc = nvme_pcie_ctrlr_set_reg_8(&pctrlr.ctrlr, 0, 0x100000000);
+	CU_ASSERT(rc == 0);
+
+	rc = nvme_pcie_ctrlr_get_reg_8(&pctrlr.ctrlr, 0, &value_8);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(value_8 == 0x100000000);
+}
+
+static void
+test_nvme_pcie_ctrlr_map_unmap_cmb(void)
+{
+	struct nvme_pcie_ctrlr pctrlr = {};
+	volatile struct spdk_nvme_registers regs = {};
+	union spdk_nvme_cmbsz_register cmbsz = {};
+	union spdk_nvme_cmbloc_register cmbloc = {};
+	struct dev_mem_resource cmd_res = {};
+	int rc;
+
+	pctrlr.regs = &regs;
+	pctrlr.devhandle = (void *)&cmd_res;
+	cmd_res.addr = (void *)0x7f7c0080d000;
+	cmd_res.len = 0x800000;
+	cmd_res.phys_addr = 0xFC800000;
+	/* Configure cmb size with unit size 4k, offset 100, unsupported SQ */
+	cmbsz.bits.sz = 512;
+	cmbsz.bits.szu = 0;
+	cmbsz.bits.sqs = 0;
+	cmbloc.bits.bir = 0;
+	cmbloc.bits.ofst = 100;
+
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, cmbsz.raw),
+				  cmbsz.raw);
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, cmbloc.raw),
+				  cmbloc.raw);
+
+	nvme_pcie_ctrlr_map_cmb(&pctrlr);
+	CU_ASSERT(pctrlr.cmb.bar_va == (void *)0x7f7c0080d000);
+	CU_ASSERT(pctrlr.cmb.bar_pa == 0xFC800000);
+	CU_ASSERT(pctrlr.cmb.size == 512 * 4096);
+	CU_ASSERT(pctrlr.cmb.current_offset == 4096 * 100);
+	CU_ASSERT(pctrlr.ctrlr.opts.use_cmb_sqs == false);
+
+	rc = nvme_pcie_ctrlr_unmap_cmb(&pctrlr);
+	CU_ASSERT(rc == 0);
+
+	/* Invalid mapping information */
+	memset(&pctrlr.cmb, 0, sizeof(pctrlr.cmb));
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, cmbsz.raw), 0);
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, cmbloc.raw), 0);
+
+	nvme_pcie_ctrlr_map_cmb(&pctrlr);
+	CU_ASSERT(pctrlr.cmb.bar_va == NULL);
+	CU_ASSERT(pctrlr.cmb.bar_pa == 0);
+	CU_ASSERT(pctrlr.cmb.size == 0);
+	CU_ASSERT(pctrlr.cmb.current_offset == 0);
+	CU_ASSERT(pctrlr.ctrlr.opts.use_cmb_sqs == false);
+}
+
+
+static void
+prepare_map_io_cmd(struct nvme_pcie_ctrlr *pctrlr)
+{
+	union spdk_nvme_cmbsz_register cmbsz = {};
+	union spdk_nvme_cmbloc_register cmbloc = {};
+
+	cmbsz.bits.sz = 512;
+	cmbsz.bits.wds = 1;
+	cmbsz.bits.rds = 1;
+
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr->ctrlr, offsetof(struct spdk_nvme_registers, cmbsz.raw),
+				  cmbsz.raw);
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr->ctrlr, offsetof(struct spdk_nvme_registers, cmbloc.raw),
+				  cmbloc.raw);
+
+	pctrlr->cmb.bar_va = (void *)0x7F7C0080D000;
+	pctrlr->cmb.bar_pa = 0xFC800000;
+	pctrlr->cmb.current_offset = 1ULL << 22;
+	pctrlr->cmb.size = (1ULL << 22) * 512;
+	pctrlr->cmb.mem_register_addr = NULL;
+	pctrlr->ctrlr.opts.use_cmb_sqs = false;
+}
+
+static void
+test_nvme_pcie_ctrlr_map_io_cmb(void)
+{
+	struct nvme_pcie_ctrlr pctrlr = {};
+	volatile struct spdk_nvme_registers regs = {};
+	union spdk_nvme_cmbsz_register cmbsz = {};
+	void *mem_reg_addr = NULL;
+	size_t size;
+	int rc;
+
+	pctrlr.regs = &regs;
+	prepare_map_io_cmd(&pctrlr);
+
+	mem_reg_addr = nvme_pcie_ctrlr_map_io_cmb(&pctrlr.ctrlr, &size);
+	/* Ceil the current cmb vaddr and cmb size to 2MB_aligned */
+	CU_ASSERT(mem_reg_addr == (void *)0x7F7C00E00000);
+	CU_ASSERT(size == 0x7FE00000);
+
+	rc = nvme_pcie_ctrlr_unmap_io_cmb(&pctrlr.ctrlr);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(pctrlr.cmb.mem_register_addr == NULL);
+	CU_ASSERT(pctrlr.cmb.mem_register_size == 0);
+
+	/* cmb mem_register_addr not NULL */
+	prepare_map_io_cmd(&pctrlr);
+	pctrlr.cmb.mem_register_addr = (void *)0xDEADBEEF;
+	pctrlr.cmb.mem_register_size = 1024;
+
+	mem_reg_addr = nvme_pcie_ctrlr_map_io_cmb(&pctrlr.ctrlr, &size);
+	CU_ASSERT(size == 1024);
+	CU_ASSERT(mem_reg_addr == (void *)0xDEADBEEF);
+
+	/* cmb.bar_va is NULL */
+	prepare_map_io_cmd(&pctrlr);
+	pctrlr.cmb.bar_va = NULL;
+
+	mem_reg_addr = nvme_pcie_ctrlr_map_io_cmb(&pctrlr.ctrlr, &size);
+	CU_ASSERT(mem_reg_addr == NULL);
+	CU_ASSERT(size == 0);
+
+	/* submission queue already used */
+	prepare_map_io_cmd(&pctrlr);
+	pctrlr.ctrlr.opts.use_cmb_sqs = true;
+
+	mem_reg_addr = nvme_pcie_ctrlr_map_io_cmb(&pctrlr.ctrlr, &size);
+	CU_ASSERT(mem_reg_addr == NULL);
+	CU_ASSERT(size == 0);
+
+	pctrlr.ctrlr.opts.use_cmb_sqs = false;
+
+	/* Only SQS is supported */
+	prepare_map_io_cmd(&pctrlr);
+	cmbsz.bits.wds = 0;
+	cmbsz.bits.rds = 0;
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, cmbsz.raw),
+				  cmbsz.raw);
+
+	mem_reg_addr = nvme_pcie_ctrlr_map_io_cmb(&pctrlr.ctrlr, &size);
+	CU_ASSERT(mem_reg_addr == NULL);
+	CU_ASSERT(size == 0);
+
+	/* CMB size is less than 4MB */
+	prepare_map_io_cmd(&pctrlr);
+	pctrlr.cmb.size = 1ULL << 16;
+
+	mem_reg_addr = nvme_pcie_ctrlr_map_io_cmb(&pctrlr.ctrlr, &size);
+	CU_ASSERT(mem_reg_addr == NULL);
+	CU_ASSERT(size == 0);
+}
+
+static void
+test_nvme_pcie_ctrlr_map_unmap_pmr(void)
+{
+	struct nvme_pcie_ctrlr pctrlr = {};
+	volatile struct spdk_nvme_registers regs = {};
+	union spdk_nvme_pmrcap_register pmrcap = {};
+	struct dev_mem_resource cmd_res = {};
+	int rc;
+
+	pctrlr.regs = &regs;
+	pctrlr.devhandle = (void *)&cmd_res;
+	regs.cap.bits.pmrs = 1;
+	cmd_res.addr = (void *)0x7F7C0080d000;
+	cmd_res.len = 0x800000;
+	cmd_res.phys_addr = 0xFC800000;
+	pmrcap.bits.bir = 2;
+	pmrcap.bits.cmss = 1;
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr,
+				  offsetof(struct spdk_nvme_registers, pmrcap.raw),
+				  pmrcap.raw);
+
+	nvme_pcie_ctrlr_map_pmr(&pctrlr);
+	CU_ASSERT(pctrlr.regs->pmrmscu == 0);
+	/* Controller memory space enable, bit 1 */
+	CU_ASSERT(pctrlr.regs->pmrmscl.raw == 0xFC800002);
+	CU_ASSERT(pctrlr.regs->pmrsts.raw == 0);
+	CU_ASSERT(pctrlr.pmr.bar_va == (void *)0x7F7C0080d000);
+	CU_ASSERT(pctrlr.pmr.bar_pa == 0xFC800000);
+	CU_ASSERT(pctrlr.pmr.size == 0x800000);
+
+	rc = nvme_pcie_ctrlr_unmap_pmr(&pctrlr);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(pctrlr.regs->pmrmscu == 0);
+	CU_ASSERT(pctrlr.regs->pmrmscl.raw == 0);
+
+	/* pmrcap value invalid */
+	memset(&pctrlr, 0, sizeof(pctrlr));
+	memset((void *)&regs, 0, sizeof(regs));
+	memset(&cmd_res, 0, sizeof(cmd_res));
+
+	pctrlr.regs = &regs;
+	pctrlr.devhandle = (void *)&cmd_res;
+	regs.cap.bits.pmrs = 1;
+	cmd_res.addr = (void *)0x7F7C0080d000;
+	cmd_res.len = 0x800000;
+	cmd_res.phys_addr = 0xFC800000;
+	pmrcap.raw = 0;
+	nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr,
+				  offsetof(struct spdk_nvme_registers, pmrcap.raw),
+				  pmrcap.raw);
+
+	nvme_pcie_ctrlr_map_pmr(&pctrlr);
+	CU_ASSERT(pctrlr.pmr.bar_va == NULL);
+	CU_ASSERT(pctrlr.pmr.bar_pa == 0);
+	CU_ASSERT(pctrlr.pmr.size == 0);
+}
+
+static void
+test_nvme_pcie_ctrlr_config_pmr(void)
+{
+	struct nvme_pcie_ctrlr pctrlr = {};
+	union spdk_nvme_pmrcap_register pmrcap = {};
+	union spdk_nvme_pmrsts_register pmrsts = {};
+	union spdk_nvme_cap_register	cap = {};
+	union spdk_nvme_pmrctl_register pmrctl = {};
+	volatile struct spdk_nvme_registers regs = {};
+	int rc;
+
+	/* pmrctl enable */
+	pctrlr.regs = &regs;
+	pmrcap.bits.pmrtu = 0;
+	pmrcap.bits.pmrto = 1;
+	pmrsts.bits.nrdy = false;
+	pmrctl.bits.en = 0;
+	cap.bits.pmrs = 1;
+
+	rc = nvme_pcie_ctrlr_set_pmrctl(&pctrlr, &pmrctl);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+	rc = nvme_pcie_ctrlr_set_reg_8(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, cap.raw),
+				       cap.raw);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+	rc = nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, pmrcap.raw),
+				       pmrcap.raw);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+	rc = nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, pmrsts.raw),
+				       pmrsts.raw);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+
+	rc = nvme_pcie_ctrlr_config_pmr(&pctrlr.ctrlr, true);
+	CU_ASSERT(rc == 0);
+	rc = nvme_pcie_ctrlr_get_pmrctl(&pctrlr, &pmrctl);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(pmrctl.bits.en == true);
+
+	/* pmrctl disable */
+	pmrsts.bits.nrdy = true;
+	rc = nvme_pcie_ctrlr_set_reg_4(&pctrlr.ctrlr, offsetof(struct spdk_nvme_registers, pmrsts.raw),
+				       pmrsts.raw);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+	rc = nvme_pcie_ctrlr_set_pmrctl(&pctrlr, &pmrctl);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+
+	rc = nvme_pcie_ctrlr_config_pmr(&pctrlr.ctrlr, false);
+	CU_ASSERT(rc == 0);
+	rc = nvme_pcie_ctrlr_get_pmrctl(&pctrlr, &pmrctl);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(pmrctl.bits.en == false);
+
+	/* configuration exist */
+	rc = nvme_pcie_ctrlr_config_pmr(&pctrlr.ctrlr, false);
+	CU_ASSERT(rc == -EINVAL);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -523,6 +1045,14 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_shadow_doorbell_update);
 	CU_ADD_TEST(suite, test_build_contig_hw_sgl_request);
 	CU_ADD_TEST(suite, test_nvme_pcie_qpair_build_metadata);
+	CU_ADD_TEST(suite, test_nvme_pcie_qpair_build_prps_sgl_request);
+	CU_ADD_TEST(suite, test_nvme_pcie_qpair_build_hw_sgl_request);
+	CU_ADD_TEST(suite, test_nvme_pcie_qpair_build_contig_request);
+	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_regs_get_set);
+	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_map_unmap_cmb);
+	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_map_io_cmb);
+	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_map_unmap_pmr);
+	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_config_pmr);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_pcie_common.c/nvme_pcie_common_ut.c b/test/unit/lib/nvme/nvme_pcie_common.c/nvme_pcie_common_ut.c
index 411dfbe1e..8160860be 100644
--- a/test/unit/lib/nvme/nvme_pcie_common.c/nvme_pcie_common_ut.c
+++ b/test/unit/lib/nvme/nvme_pcie_common.c/nvme_pcie_common_ut.c
@@ -99,6 +99,336 @@ test_nvme_pcie_ctrlr_alloc_cmb(void)
 	CU_ASSERT(vaddr == NULL);
 }
 
+static void
+test_nvme_pcie_qpair_construct_destroy(void)
+{
+	struct spdk_nvme_io_qpair_opts opts = {};
+	struct nvme_pcie_ctrlr pctrlr = {};
+	struct spdk_nvme_cpl cpl[2] = {};
+	struct nvme_pcie_qpair *pqpair = NULL;
+	size_t page_align = sysconf(_SC_PAGESIZE);
+	uint64_t cmb_offset;
+	int rc;
+
+	opts.sq.paddr = 0xDEADBEEF;
+	opts.cq.paddr = 0xDBADBEEF;
+	opts.sq.vaddr = (void *)0xDCADBEEF;
+	opts.cq.vaddr = cpl;
+
+	pctrlr.ctrlr.trid.trtype = SPDK_NVME_TRANSPORT_PCIE;
+	pctrlr.ctrlr.opts.transport_retry_count = 1;
+	pctrlr.cmb.mem_register_addr = NULL;
+	pctrlr.cmb.bar_va = (void *)0xF9000000;
+	pctrlr.cmb.bar_pa = 0xF8000000;
+	pctrlr.cmb.current_offset = 0x10;
+	cmb_offset = pctrlr.cmb.current_offset;
+	/* Make sure that CMB size is big enough and includes page alignment */
+	pctrlr.cmb.size = (1 << 16) + page_align;
+	pctrlr.doorbell_base = (void *)0xF7000000;
+	pctrlr.doorbell_stride_u32 = 1;
+
+	/* Allocate memory for destroying. */
+	pqpair = spdk_zmalloc(sizeof(*pqpair), 64, NULL,
+			      SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
+	SPDK_CU_ASSERT_FATAL(pqpair != NULL);
+	pqpair->qpair.ctrlr = &pctrlr.ctrlr;
+	pqpair->num_entries = 2;
+	pqpair->qpair.id = 1;
+	pqpair->cpl = cpl;
+
+	/* Enable submission queue in controller memory buffer. */
+	pctrlr.ctrlr.opts.use_cmb_sqs = true;
+
+	rc = nvme_pcie_qpair_construct(&pqpair->qpair, &opts);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(pqpair->sq_vaddr == (void *)0xDCADBEEF);
+	CU_ASSERT(pqpair->cq_vaddr == cpl);
+	CU_ASSERT(pqpair->retry_count == 1);
+	CU_ASSERT(pqpair->max_completions_cap == 1);
+	CU_ASSERT(pqpair->sq_in_cmb == true);
+	CU_ASSERT(pqpair->cmd != NULL && pqpair->cmd != (void *)0xDCADBEEF);
+	CU_ASSERT(pqpair->cmd_bus_addr == (((pctrlr.cmb.bar_pa + cmb_offset) + page_align - 1) & ~
+					   (page_align - 1)));
+	CU_ASSERT(pqpair->sq_tdbl == (void *)0xF7000008);
+	CU_ASSERT(pqpair->cq_hdbl == (void *)0xF700000C);
+	CU_ASSERT(pqpair->flags.phase = 1);
+	CU_ASSERT(pqpair->tr != NULL);
+	CU_ASSERT(pqpair->tr == TAILQ_FIRST(&pqpair->free_tr));
+	CU_ASSERT(pctrlr.cmb.current_offset == (uintptr_t)pqpair->cmd + (pqpair->num_entries * sizeof(
+				struct spdk_nvme_cmd)) - (uintptr_t)pctrlr.cmb.bar_va);
+	cmb_offset = pctrlr.cmb.current_offset;
+	nvme_pcie_qpair_destroy(&pqpair->qpair);
+
+	/* Disable submission queue in controller memory buffer. */
+	pctrlr.ctrlr.opts.use_cmb_sqs = false;
+	pqpair = spdk_zmalloc(sizeof(*pqpair), 64, NULL,
+			      SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
+	SPDK_CU_ASSERT_FATAL(pqpair != NULL);
+	pqpair->qpair.ctrlr = &pctrlr.ctrlr;
+	pqpair->num_entries = 2;
+	pqpair->qpair.id = 1;
+	pqpair->cpl = cpl;
+
+	rc = nvme_pcie_qpair_construct(&pqpair->qpair, &opts);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(pqpair->sq_vaddr == (void *)0xDCADBEEF);
+	CU_ASSERT(pqpair->cq_vaddr == cpl);
+	CU_ASSERT(pqpair->retry_count == 1);
+	CU_ASSERT(pqpair->max_completions_cap == 1);
+	CU_ASSERT(pqpair->sq_in_cmb == false);
+	CU_ASSERT(pqpair->cmd == (void *)0xDCADBEEF);
+	CU_ASSERT(pqpair->cmd_bus_addr == 0xDEADBEEF);
+	CU_ASSERT(pqpair->sq_tdbl == (void *)0xF7000008);
+	CU_ASSERT(pqpair->cq_hdbl == (void *)0xF700000C);
+	CU_ASSERT(pqpair->flags.phase = 1);
+	CU_ASSERT(pqpair->tr != NULL);
+	CU_ASSERT(pqpair->tr == TAILQ_FIRST(&pqpair->free_tr));
+	nvme_pcie_qpair_destroy(&pqpair->qpair);
+
+	/* Disable submission queue in controller memory buffer, sq_vaddr and cq_vaddr invalid. */
+	pctrlr.ctrlr.opts.use_cmb_sqs = false;
+	pqpair = spdk_zmalloc(sizeof(*pqpair), 64, NULL,
+			      SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
+	SPDK_CU_ASSERT_FATAL(pqpair != NULL);
+	pqpair->qpair.ctrlr = &pctrlr.ctrlr;
+	pqpair->num_entries = 2;
+	pqpair->qpair.id = 1;
+	pqpair->cpl = cpl;
+	MOCK_SET(spdk_vtophys, 0xDAADBEEF);
+
+	rc = nvme_pcie_qpair_construct(&pqpair->qpair, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(pqpair->retry_count == 1);
+	CU_ASSERT(pqpair->max_completions_cap == 1);
+	CU_ASSERT(pqpair->cmd != NULL && pqpair->cmd != (void *)0xDCADBEEF);
+	CU_ASSERT(pqpair->sq_in_cmb == false);
+	CU_ASSERT(pqpair->cmd_bus_addr == 0xDAADBEEF);
+	CU_ASSERT(pqpair->sq_tdbl == (void *)0xF7000008);
+	CU_ASSERT(pqpair->cq_hdbl == (void *)0xF700000c);
+	CU_ASSERT(pqpair->flags.phase = 1);
+	CU_ASSERT(pqpair->tr != NULL);
+	CU_ASSERT(pqpair->tr == TAILQ_FIRST(&pqpair->free_tr));
+	nvme_pcie_qpair_destroy(&pqpair->qpair);
+	MOCK_CLEAR(spdk_vtophys);
+}
+
+static void
+test_nvme_pcie_ctrlr_cmd_create_delete_io_queue(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct nvme_pcie_qpair pqpair = {};
+	struct spdk_nvme_qpair adminq = {};
+	struct nvme_request req = {};
+	int rc;
+
+	ctrlr.adminq = &adminq;
+	STAILQ_INIT(&ctrlr.adminq->free_req);
+	STAILQ_INSERT_HEAD(&ctrlr.adminq->free_req, &req, stailq);
+	pqpair.qpair.id = 1;
+	pqpair.num_entries = 1;
+	pqpair.cpl_bus_addr = 0xDEADBEEF;
+	pqpair.cmd_bus_addr = 0xDDADBEEF;
+	pqpair.qpair.qprio = SPDK_NVME_QPRIO_HIGH;
+
+	rc = nvme_pcie_ctrlr_cmd_create_io_cq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_CREATE_IO_CQ);
+	CU_ASSERT(req.cmd.cdw10_bits.create_io_q.qid == 1);
+	CU_ASSERT(req.cmd.cdw10_bits.create_io_q.qsize == 0);
+	CU_ASSERT(req.cmd.cdw11_bits.create_io_cq.pc == 1);
+	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0xDEADBEEF);
+	CU_ASSERT(STAILQ_EMPTY(&ctrlr.adminq->free_req));
+
+	memset(&req, 0, sizeof(req));
+	STAILQ_INSERT_HEAD(&ctrlr.adminq->free_req, &req, stailq);
+
+	rc = nvme_pcie_ctrlr_cmd_create_io_sq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_CREATE_IO_SQ);
+	CU_ASSERT(req.cmd.cdw10_bits.create_io_q.qid == 1);
+	CU_ASSERT(req.cmd.cdw10_bits.create_io_q.qsize == 0);
+	CU_ASSERT(req.cmd.cdw11_bits.create_io_sq.pc == 1);
+	CU_ASSERT(req.cmd.cdw11_bits.create_io_sq.qprio == SPDK_NVME_QPRIO_HIGH);
+	CU_ASSERT(req.cmd.cdw11_bits.create_io_sq.cqid = 1);
+	CU_ASSERT(req.cmd.dptr.prp.prp1 == 0xDDADBEEF);
+	CU_ASSERT(STAILQ_EMPTY(&ctrlr.adminq->free_req));
+
+	/* No free request available */
+	rc = nvme_pcie_ctrlr_cmd_create_io_cq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == -ENOMEM);
+
+	rc = nvme_pcie_ctrlr_cmd_create_io_sq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == -ENOMEM);
+
+	/* Delete cq or sq */
+	memset(&req, 0, sizeof(req));
+	STAILQ_INSERT_HEAD(&ctrlr.adminq->free_req, &req, stailq);
+
+	rc = nvme_pcie_ctrlr_cmd_delete_io_cq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_DELETE_IO_CQ);
+	CU_ASSERT(req.cmd.cdw10_bits.delete_io_q.qid == 1);
+	CU_ASSERT(STAILQ_EMPTY(&ctrlr.adminq->free_req));
+
+	memset(&req, 0, sizeof(req));
+	STAILQ_INSERT_HEAD(&ctrlr.adminq->free_req, &req, stailq);
+
+	rc = nvme_pcie_ctrlr_cmd_delete_io_sq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req.cmd.opc == SPDK_NVME_OPC_DELETE_IO_SQ);
+	CU_ASSERT(req.cmd.cdw10_bits.delete_io_q.qid == 1);
+	CU_ASSERT(STAILQ_EMPTY(&ctrlr.adminq->free_req));
+
+	/* No free request available */
+	rc = nvme_pcie_ctrlr_cmd_delete_io_cq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == -ENOMEM);
+
+	rc = nvme_pcie_ctrlr_cmd_delete_io_sq(&ctrlr, &pqpair.qpair, NULL, NULL);
+	CU_ASSERT(rc == -ENOMEM);
+}
+
+static void
+test_nvme_pcie_ctrlr_connect_qpair(void)
+{
+	struct nvme_pcie_ctrlr	pctrlr = {};
+	struct nvme_pcie_qpair	pqpair = {};
+	struct spdk_nvme_transport_poll_group poll_group = {};
+	struct spdk_nvme_cpl cpl = {};
+	struct spdk_nvme_qpair adminq = {};
+	struct nvme_request req[2] = {};
+	int rc;
+
+	pqpair.cpl = &cpl;
+	pqpair.num_entries = 1;
+	pqpair.qpair.ctrlr = &pctrlr.ctrlr;
+	pqpair.qpair.id = 1;
+	pqpair.num_entries = 1;
+	pqpair.cpl_bus_addr = 0xDEADBEEF;
+	pqpair.cmd_bus_addr = 0xDDADBEEF;
+	pqpair.qpair.qprio = SPDK_NVME_QPRIO_HIGH;
+	pqpair.stat = NULL;
+	pqpair.qpair.poll_group = &poll_group;
+	pctrlr.ctrlr.page_size = 4096;
+
+	/* Shadow doorbell available */
+	pctrlr.doorbell_stride_u32 = 1;
+	pctrlr.ctrlr.shadow_doorbell = spdk_zmalloc(pctrlr.ctrlr.page_size, pctrlr.ctrlr.page_size,
+				       NULL, SPDK_ENV_LCORE_ID_ANY,
+				       SPDK_MALLOC_DMA | SPDK_MALLOC_SHARE);
+	pctrlr.ctrlr.eventidx = spdk_zmalloc(pctrlr.ctrlr.page_size, pctrlr.ctrlr.page_size,
+					     NULL, SPDK_ENV_LCORE_ID_ANY,
+					     SPDK_MALLOC_DMA | SPDK_MALLOC_SHARE);
+	pctrlr.ctrlr.adminq = &adminq;
+	STAILQ_INIT(&pctrlr.ctrlr.adminq->free_req);
+	for (int i = 0; i < 2; i++) {
+		STAILQ_INSERT_TAIL(&pctrlr.ctrlr.adminq->free_req, &req[i], stailq);
+	}
+
+	rc = nvme_pcie_ctrlr_connect_qpair(&pctrlr.ctrlr, &pqpair.qpair);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req[0].cmd.opc == SPDK_NVME_OPC_CREATE_IO_CQ);
+	CU_ASSERT(req[0].cmd.cdw10_bits.create_io_q.qid == 1);
+	CU_ASSERT(req[0].cmd.cdw10_bits.create_io_q.qsize == 0);
+	CU_ASSERT(req[0].cmd.cdw11_bits.create_io_cq.pc == 1);
+	CU_ASSERT(req[0].cmd.dptr.prp.prp1 == 0xDEADBEEF);
+	CU_ASSERT(req[1].cmd.opc == SPDK_NVME_OPC_CREATE_IO_SQ);
+	CU_ASSERT(req[1].cmd.cdw10_bits.create_io_q.qid == 1);
+	CU_ASSERT(req[1].cmd.cdw10_bits.create_io_q.qsize == 0);
+	CU_ASSERT(req[1].cmd.cdw11_bits.create_io_sq.pc == 1);
+	CU_ASSERT(req[1].cmd.cdw11_bits.create_io_sq.qprio == SPDK_NVME_QPRIO_HIGH);
+	CU_ASSERT(req[1].cmd.cdw11_bits.create_io_sq.cqid = 1);
+	CU_ASSERT(req[1].cmd.dptr.prp.prp1 == 0xDDADBEEF);
+
+	/* doorbell stride and qid are 1 */
+	CU_ASSERT(pqpair.shadow_doorbell.sq_tdbl == pctrlr.ctrlr.shadow_doorbell + 2);
+	CU_ASSERT(pqpair.shadow_doorbell.cq_hdbl == pctrlr.ctrlr.shadow_doorbell + 3);
+	CU_ASSERT(pqpair.shadow_doorbell.sq_eventidx == pctrlr.ctrlr.eventidx + 2);
+	CU_ASSERT(pqpair.shadow_doorbell.cq_eventidx == pctrlr.ctrlr.eventidx + 3);
+	CU_ASSERT(pqpair.flags.has_shadow_doorbell == 1);
+	CU_ASSERT(STAILQ_EMPTY(&pctrlr.ctrlr.adminq->free_req));
+
+	spdk_free(pctrlr.ctrlr.shadow_doorbell);
+	spdk_free(pctrlr.ctrlr.eventidx);
+	pctrlr.ctrlr.shadow_doorbell = NULL;
+	pctrlr.ctrlr.eventidx = NULL;
+
+	/* Shadow doorbell 0 */
+	memset(req, 0, sizeof(struct nvme_request) * 2);
+	memset(&pqpair, 0, sizeof(pqpair));
+	pqpair.cpl = &cpl;
+	pqpair.qpair.ctrlr = &pctrlr.ctrlr;
+	pqpair.qpair.id = 1;
+	pqpair.num_entries = 1;
+	pqpair.cpl_bus_addr = 0xDEADBEEF;
+	pqpair.cmd_bus_addr = 0xDDADBEEF;
+	pqpair.qpair.qprio = SPDK_NVME_QPRIO_HIGH;
+	pqpair.stat = NULL;
+	pqpair.qpair.poll_group = &poll_group;
+	for (int i = 0; i < 2; i++) {
+		STAILQ_INSERT_TAIL(&pctrlr.ctrlr.adminq->free_req, &req[i], stailq);
+	}
+
+	rc = nvme_pcie_ctrlr_connect_qpair(&pctrlr.ctrlr, &pqpair.qpair);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(req[0].cmd.opc == SPDK_NVME_OPC_CREATE_IO_CQ);
+	CU_ASSERT(req[0].cmd.cdw10_bits.create_io_q.qid == 1);
+	CU_ASSERT(req[0].cmd.cdw10_bits.create_io_q.qsize == 0);
+	CU_ASSERT(req[0].cmd.cdw11_bits.create_io_cq.pc == 1);
+	CU_ASSERT(req[0].cmd.dptr.prp.prp1 == 0xDEADBEEF);
+	CU_ASSERT(req[1].cmd.opc == SPDK_NVME_OPC_CREATE_IO_SQ);
+	CU_ASSERT(req[1].cmd.cdw10_bits.create_io_q.qid == 1);
+	CU_ASSERT(req[1].cmd.cdw10_bits.create_io_q.qsize == 0);
+	CU_ASSERT(req[1].cmd.cdw11_bits.create_io_sq.pc == 1);
+	CU_ASSERT(req[1].cmd.cdw11_bits.create_io_sq.qprio == SPDK_NVME_QPRIO_HIGH);
+	CU_ASSERT(req[1].cmd.cdw11_bits.create_io_sq.cqid = 1);
+	CU_ASSERT(req[1].cmd.dptr.prp.prp1 == 0xDDADBEEF);
+
+	CU_ASSERT(pqpair.shadow_doorbell.sq_tdbl == NULL);
+	CU_ASSERT(pqpair.shadow_doorbell.sq_eventidx == NULL);
+	CU_ASSERT(pqpair.flags.has_shadow_doorbell == 0);
+	CU_ASSERT(STAILQ_EMPTY(&pctrlr.ctrlr.adminq->free_req));
+
+	/* Completion error */
+	memset(req, 0, sizeof(struct nvme_request) * 2);
+	memset(&pqpair, 0, sizeof(pqpair));
+	pqpair.cpl = &cpl;
+	pqpair.qpair.ctrlr = &pctrlr.ctrlr;
+	pqpair.qpair.id = 1;
+	pqpair.num_entries = 1;
+	pqpair.cpl_bus_addr = 0xDEADBEEF;
+	pqpair.cmd_bus_addr = 0xDDADBEEF;
+	pqpair.qpair.qprio = SPDK_NVME_QPRIO_HIGH;
+	pqpair.stat = NULL;
+	pqpair.qpair.poll_group = &poll_group;
+	for (int i = 0; i < 2; i++) {
+		STAILQ_INSERT_TAIL(&pctrlr.ctrlr.adminq->free_req, &req[i], stailq);
+	}
+	MOCK_SET(nvme_wait_for_completion, -EIO);
+
+	rc = nvme_pcie_ctrlr_connect_qpair(&pctrlr.ctrlr, &pqpair.qpair);
+	CU_ASSERT(rc == -1);
+	/* Remove unused request */
+	STAILQ_REMOVE_HEAD(&pctrlr.ctrlr.adminq->free_req, stailq);
+	CU_ASSERT(STAILQ_EMPTY(&pctrlr.ctrlr.adminq->free_req));
+	MOCK_CLEAR(nvme_wait_for_completion);
+
+	/* No available request used */
+	memset(req, 0, sizeof(struct nvme_request) * 2);
+	memset(&pqpair, 0, sizeof(pqpair));
+	pqpair.cpl = &cpl;
+	pqpair.qpair.ctrlr = &pctrlr.ctrlr;
+	pqpair.qpair.id = 1;
+	pqpair.num_entries = 1;
+	pqpair.cpl_bus_addr = 0xDEADBEEF;
+	pqpair.cmd_bus_addr = 0xDDADBEEF;
+	pqpair.qpair.qprio = SPDK_NVME_QPRIO_HIGH;
+	pqpair.stat = NULL;
+	pqpair.qpair.poll_group = &poll_group;
+
+	rc = nvme_pcie_ctrlr_connect_qpair(&pctrlr.ctrlr, &pqpair.qpair);
+	CU_ASSERT(rc == -ENOMEM);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -109,6 +439,9 @@ int main(int argc, char **argv)
 
 	suite = CU_add_suite("nvme_pcie_common", NULL, NULL);
 	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_alloc_cmb);
+	CU_ADD_TEST(suite, test_nvme_pcie_qpair_construct_destroy);
+	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_cmd_create_delete_io_queue);
+	CU_ADD_TEST(suite, test_nvme_pcie_ctrlr_connect_qpair);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_qpair.c/nvme_qpair_ut.c b/test/unit/lib/nvme/nvme_qpair.c/nvme_qpair_ut.c
index 7d214e2cf..dc51b32c2 100644
--- a/test/unit/lib/nvme/nvme_qpair.c/nvme_qpair_ut.c
+++ b/test/unit/lib/nvme/nvme_qpair.c/nvme_qpair_ut.c
@@ -711,6 +711,46 @@ test_nvme_qpair_init_deinit(void)
 	CU_ASSERT(TAILQ_EMPTY(&qpair.err_cmd_head));
 }
 
+static void
+test_nvme_get_sgl_print_info(void)
+{
+	char buf[NVME_CMD_DPTR_STR_SIZE] = {};
+	struct spdk_nvme_cmd cmd = {};
+
+	cmd.dptr.sgl1.keyed.length = 0x1000;
+	cmd.dptr.sgl1.keyed.key = 0xababccdd;
+
+	nvme_get_sgl_keyed(buf, NVME_CMD_DPTR_STR_SIZE, &cmd);
+	CU_ASSERT(!strncmp(buf, " len:0x1000 key:0xababccdd", NVME_CMD_DPTR_STR_SIZE));
+
+	memset(&cmd.dptr.sgl1, 0, sizeof(cmd.dptr.sgl1));
+	cmd.dptr.sgl1.unkeyed.length = 0x1000;
+
+	nvme_get_sgl_unkeyed(buf, NVME_CMD_DPTR_STR_SIZE, &cmd);
+	CU_ASSERT(!strncmp(buf, " len:0x1000", NVME_CMD_DPTR_STR_SIZE));
+
+	memset(&cmd.dptr.sgl1, 0, sizeof(cmd.dptr.sgl1));
+	cmd.dptr.sgl1.generic.type = SPDK_NVME_SGL_TYPE_DATA_BLOCK;
+	cmd.dptr.sgl1.generic.subtype = 0;
+	cmd.dptr.sgl1.address = 0xdeadbeef;
+	cmd.dptr.sgl1.keyed.length = 0x1000;
+	cmd.dptr.sgl1.keyed.key = 0xababccdd;
+
+	nvme_get_sgl(buf, NVME_CMD_DPTR_STR_SIZE, &cmd);
+	CU_ASSERT(!strncmp(buf, "SGL DATA BLOCK ADDRESS 0xdeadbeef len:0x1000 key:0xababccdd",
+			   NVME_CMD_DPTR_STR_SIZE));
+
+	memset(&cmd.dptr.sgl1, 0, sizeof(cmd.dptr.sgl1));
+	cmd.dptr.sgl1.generic.type = SPDK_NVME_SGL_TYPE_KEYED_DATA_BLOCK;
+	cmd.dptr.sgl1.generic.subtype = 0;
+	cmd.dptr.sgl1.address = 0xdeadbeef;
+	cmd.dptr.sgl1.unkeyed.length = 0x1000;
+
+	nvme_get_sgl(buf, NVME_CMD_DPTR_STR_SIZE, &cmd);
+	CU_ASSERT(!strncmp(buf, "SGL RESERVED ADDRESS 0xdeadbeef len:0x1000",
+			   NVME_CMD_DPTR_STR_SIZE));
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -734,6 +774,7 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_nvme_qpair_resubmit_request_with_transport_failed);
 	CU_ADD_TEST(suite, test_nvme_qpair_manual_complete_request);
 	CU_ADD_TEST(suite, test_nvme_qpair_init_deinit);
+	CU_ADD_TEST(suite, test_nvme_get_sgl_print_info);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_rdma.c/nvme_rdma_ut.c b/test/unit/lib/nvme/nvme_rdma.c/nvme_rdma_ut.c
index b0297d2e6..b31245630 100644
--- a/test/unit/lib/nvme/nvme_rdma.c/nvme_rdma_ut.c
+++ b/test/unit/lib/nvme/nvme_rdma.c/nvme_rdma_ut.c
@@ -61,6 +61,11 @@ DEFINE_STUB_V(rdma_destroy_event_channel, (struct rdma_event_channel *channel));
 
 DEFINE_STUB(ibv_dereg_mr, int, (struct ibv_mr *mr), 0);
 
+int ibv_resize_cq(struct ibv_cq *cq, int cqe)
+{
+	return 0;
+}
+
 /* ibv_reg_mr can be a macro, need to undefine it */
 #ifdef ibv_reg_mr
 #undef ibv_reg_mr
@@ -958,6 +963,117 @@ test_nvme_rdma_register_and_unregister_reqs(void)
 	CU_ASSERT(rqpair.cmd_mr.mr == NULL);
 }
 
+static void
+test_nvme_rdma_poll_group_connect_disconnect_qpair(void)
+{
+	int					rc;
+	struct nvme_rdma_poll_group		group = {};
+	struct rdma_cm_id			cm_id = {};
+	struct nvme_rdma_qpair			*rqpair = NULL;
+	struct nvme_rdma_destroyed_qpair	*qpair_tracker = NULL;
+	struct ibv_context			*contexts = (void *)0xDEADBEEF;
+
+	/* Allocate memory for deleting qpair to free */
+	rqpair = calloc(1, sizeof(struct nvme_rdma_qpair));
+	rqpair->cm_id =  &cm_id;
+	rqpair->qpair.trtype = SPDK_NVME_TRANSPORT_RDMA;
+	rqpair->qpair.poll_group = &group.group;
+	rqpair->qpair.state = NVME_QPAIR_DESTROYING;
+	cm_id.verbs = (void *)0xDEADBEEF;
+
+	STAILQ_INIT(&group.destroyed_qpairs);
+	STAILQ_INIT(&group.pollers);
+	rc = nvme_rdma_poller_create(&group, contexts);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+
+	rc = nvme_rdma_poll_group_connect_qpair(&rqpair->qpair);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(rqpair->cq == (void *)0xFEEDBEEF);
+	CU_ASSERT(rqpair->poller != NULL);
+
+	rc = nvme_rdma_poll_group_disconnect_qpair(&rqpair->qpair);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(rqpair->defer_deletion_to_pg == true);
+	CU_ASSERT(rqpair->poll_group_disconnect_in_progress == false);
+	CU_ASSERT(rqpair->cq == NULL);
+	CU_ASSERT(!STAILQ_EMPTY(&group.destroyed_qpairs));
+
+	qpair_tracker = STAILQ_FIRST(&group.destroyed_qpairs);
+	CU_ASSERT(qpair_tracker->destroyed_qpair_tracker == rqpair);
+	CU_ASSERT(qpair_tracker->completed_cycles == 0);
+
+	nvme_rdma_poll_group_delete_qpair(&group, qpair_tracker);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(STAILQ_EMPTY(&group.destroyed_qpairs));
+
+	nvme_rdma_poll_group_free_pollers(&group);
+	CU_ASSERT(STAILQ_EMPTY(&group.pollers));
+
+	/* No available poller */
+	rqpair = calloc(1, sizeof(struct nvme_rdma_qpair));
+
+	rqpair->cm_id =  &cm_id;
+	rqpair->qpair.trtype = SPDK_NVME_TRANSPORT_RDMA;
+	rqpair->qpair.poll_group = &group.group;
+	rqpair->qpair.state = NVME_QPAIR_DESTROYING;
+	cm_id.verbs = (void *)0xDEADBEEF;
+
+	rc = nvme_rdma_poll_group_connect_qpair(&rqpair->qpair);
+	CU_ASSERT(rc == -EINVAL);
+	CU_ASSERT(rqpair->cq == NULL);
+
+	/* Poll group disconnect in progress */
+	rqpair->poll_group_disconnect_in_progress = true;
+
+	rc = nvme_rdma_poll_group_disconnect_qpair(&rqpair->qpair);
+	CU_ASSERT(rc == -EINPROGRESS);
+	free(rqpair);
+}
+
+static void
+test_nvme_rdma_parse_addr(void)
+{
+	struct sockaddr_storage dst_addr;
+	int rc = 0;
+
+	memset(&dst_addr, 0, sizeof(dst_addr));
+	/* case1: getaddrinfo failed */
+	rc = nvme_rdma_parse_addr(&dst_addr, AF_INET, NULL, NULL);
+	CU_ASSERT(rc != 0);
+
+	/* case2: res->ai_addrlen < sizeof(*sa). Expect: Pass. */
+	rc = nvme_rdma_parse_addr(&dst_addr, AF_INET, "12.34.56.78", "23");
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(dst_addr.ss_family == AF_INET);
+}
+
+static void
+test_nvme_rdma_qpair_init(void)
+{
+	struct nvme_rdma_qpair		rqpair = {};
+	struct rdma_cm_id		 cm_id = {};
+	struct ibv_qp			    qp = {};
+	struct nvme_rdma_ctrlr	rctrlr = {};
+	int rc = 0;
+
+	rctrlr.ctrlr.trid.trtype = SPDK_NVME_TRANSPORT_RDMA;
+	rqpair.cm_id = &cm_id;
+	g_nvme_hooks.get_ibv_pd = NULL;
+	rqpair.qpair.poll_group = NULL;
+	rqpair.qpair.ctrlr = &rctrlr.ctrlr;
+	g_spdk_rdma_qp.qp = &qp;
+
+	rc = nvme_rdma_qpair_init(&rqpair);
+	CU_ASSERT(rc == 0);
+
+	CU_ASSERT(rqpair.cm_id->context == &rqpair.qpair);
+	CU_ASSERT(rqpair.max_send_sge == NVME_RDMA_DEFAULT_TX_SGE);
+	CU_ASSERT(rqpair.max_recv_sge == NVME_RDMA_DEFAULT_RX_SGE);
+	CU_ASSERT(rqpair.current_num_sends == 0);
+	CU_ASSERT(rqpair.current_num_recvs == 0);
+	CU_ASSERT(rqpair.cq == (struct ibv_cq *)0xFEEDBEEF);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -982,6 +1098,9 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_nvme_rdma_req_init);
 	CU_ADD_TEST(suite, test_nvme_rdma_validate_cm_event);
 	CU_ADD_TEST(suite, test_nvme_rdma_register_and_unregister_reqs);
+	CU_ADD_TEST(suite, test_nvme_rdma_poll_group_connect_disconnect_qpair);
+	CU_ADD_TEST(suite, test_nvme_rdma_parse_addr);
+	CU_ADD_TEST(suite, test_nvme_rdma_qpair_init);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvme/nvme_tcp.c/nvme_tcp_ut.c b/test/unit/lib/nvme/nvme_tcp.c/nvme_tcp_ut.c
index 07787cca0..5d17a0792 100644
--- a/test/unit/lib/nvme/nvme_tcp.c/nvme_tcp_ut.c
+++ b/test/unit/lib/nvme/nvme_tcp.c/nvme_tcp_ut.c
@@ -586,7 +586,7 @@ test_nvme_tcp_req_get(void)
 	struct nvme_tcp_qpair tqpair = {};
 	struct nvme_tcp_pdu send_pdu = {};
 
-	tcp_req.send_pdu = &send_pdu;
+	tcp_req.pdu = &send_pdu;
 	tcp_req.state = NVME_TCP_REQ_FREE;
 
 	TAILQ_INIT(&tqpair.free_reqs);
@@ -623,7 +623,7 @@ test_nvme_tcp_qpair_capsule_cmd_send(void)
 	memset(iov_base0, 0xFF, 4096);
 	memset(iov_base1, 0xFF, 4096);
 	tcp_req.req = &req;
-	tcp_req.send_pdu = &pdu;
+	tcp_req.pdu = &pdu;
 	TAILQ_INIT(&tqpair.send_queue);
 
 	tcp_req.iov[0].iov_base = (void *)iov_base0;
@@ -800,6 +800,9 @@ test_nvme_tcp_qpair_set_recv_state(void)
 {
 	struct nvme_tcp_qpair tqpair = {};
 	enum nvme_tcp_pdu_recv_state state;
+	struct nvme_tcp_pdu recv_pdu = {};
+
+	tqpair.recv_pdu = &recv_pdu;
 
 	/* case1: The recv state of tqpair is same with the state to be set */
 	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_ERROR;
@@ -809,17 +812,17 @@ test_nvme_tcp_qpair_set_recv_state(void)
 
 	/* case2: The recv state of tqpair is different with the state to be set */
 	/* state is NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY or NVME_TCP_PDU_RECV_STATE_ERROR, tqpair->recv_pdu will be cleared */
-	tqpair.recv_pdu.cb_arg = (void *)0xDEADBEEF;
+	tqpair.recv_pdu->cb_arg = (void *)0xDEADBEEF;
 	state = NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY;
 	nvme_tcp_qpair_set_recv_state(&tqpair, state);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
-	CU_ASSERT(tqpair.recv_pdu.cb_arg == (void *)0x0);
+	CU_ASSERT(tqpair.recv_pdu->cb_arg == (void *)0x0);
 
-	tqpair.recv_pdu.cb_arg = (void *)0xDEADBEEF;
+	tqpair.recv_pdu->cb_arg = (void *)0xDEADBEEF;
 	state = NVME_TCP_PDU_RECV_STATE_ERROR;
 	nvme_tcp_qpair_set_recv_state(&tqpair, state);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
-	CU_ASSERT(tqpair.recv_pdu.cb_arg == (void *)0x0);
+	CU_ASSERT(tqpair.recv_pdu->cb_arg == (void *)0x0);
 
 	/* state is NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_CH or NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH or NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD or default */
 	state = NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_CH;
@@ -851,7 +854,7 @@ test_nvme_tcp_alloc_reqs(void)
 	CU_ASSERT(rc == 0);
 	CU_ASSERT(tqpair.tcp_reqs[0].cid == 0);
 	CU_ASSERT(tqpair.tcp_reqs[0].tqpair == &tqpair);
-	CU_ASSERT(tqpair.tcp_reqs[0].send_pdu == &tqpair.send_pdus[0]);
+	CU_ASSERT(tqpair.tcp_reqs[0].pdu == &tqpair.send_pdus[0]);
 	CU_ASSERT(tqpair.send_pdu == &tqpair.send_pdus[tqpair.num_entries]);
 	free(tqpair.tcp_reqs);
 	spdk_free(tqpair.send_pdus);
@@ -863,7 +866,7 @@ test_nvme_tcp_alloc_reqs(void)
 	for (int i = 0; i < tqpair.num_entries; i++) {
 		CU_ASSERT(tqpair.tcp_reqs[i].cid == i);
 		CU_ASSERT(tqpair.tcp_reqs[i].tqpair == &tqpair);
-		CU_ASSERT(tqpair.tcp_reqs[i].send_pdu == &tqpair.send_pdus[i]);
+		CU_ASSERT(tqpair.tcp_reqs[i].pdu == &tqpair.send_pdus[i]);
 	}
 	CU_ASSERT(tqpair.send_pdu == &tqpair.send_pdus[tqpair.num_entries]);
 
@@ -894,14 +897,14 @@ static void
 test_nvme_tcp_qpair_send_h2c_term_req(void)
 {
 	struct nvme_tcp_qpair tqpair = {};
-	struct nvme_tcp_pdu pdu = {};
-	struct nvme_tcp_pdu send_pdu = {};
+	struct nvme_tcp_pdu pdu = {}, recv_pdu = {}, send_pdu = {};
 	enum spdk_nvme_tcp_term_req_fes fes = SPDK_NVME_TCP_TERM_REQ_FES_INVALID_HEADER_FIELD;
 	uint32_t error_offset = 1;
 
 	tqpair.send_pdu = &send_pdu;
+	tqpair.recv_pdu = &recv_pdu;
 	TAILQ_INIT(&tqpair.send_queue);
-	/* case1: hlen < SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE, Except: copy_len == hlen */
+	/* case1: hlen < SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE, Expect: copy_len == hlen */
 	pdu.hdr.common.hlen = 64;
 	nvme_tcp_qpair_send_h2c_term_req(&tqpair, &pdu, fes, error_offset);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
@@ -910,7 +913,7 @@ test_nvme_tcp_qpair_send_h2c_term_req(void)
 		  pdu.hdr.common.hlen);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
 
-	/* case2: hlen > SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE, Except: copy_len == SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE */
+	/* case2: hlen > SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE, Expect: copy_len == SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE */
 	pdu.hdr.common.hlen = 255;
 	nvme_tcp_qpair_send_h2c_term_req(&tqpair, &pdu, fes, error_offset);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
@@ -924,12 +927,13 @@ static void
 test_nvme_tcp_pdu_ch_handle(void)
 {
 	struct nvme_tcp_qpair tqpair = {};
-	struct nvme_tcp_pdu send_pdu = {};
+	struct nvme_tcp_pdu send_pdu = {}, recv_pdu = {};
 
 	tqpair.send_pdu = &send_pdu;
+	tqpair.recv_pdu = &recv_pdu;
 	TAILQ_INIT(&tqpair.send_queue);
 	/* case 1: Already received IC_RESP PDU. Expect: fail */
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
 	tqpair.state = NVME_TCP_QPAIR_STATE_INITIALIZING;
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
@@ -938,10 +942,10 @@ test_nvme_tcp_pdu_ch_handle(void)
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.plen == tqpair.send_pdu->hdr.term_req.common.hlen);
 
 	/* case 2: Expected PDU header length and received are different. Expect: fail */
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
 	tqpair.state = NVME_TCP_QPAIR_STATE_INVALID;
-	tqpair.recv_pdu.hdr.common.plen = sizeof(struct spdk_nvme_tcp_ic_resp);
-	tqpair.recv_pdu.hdr.common.hlen = 0;
+	tqpair.recv_pdu->hdr.common.plen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->hdr.common.hlen = 0;
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -950,10 +954,10 @@ test_nvme_tcp_pdu_ch_handle(void)
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.fei[0] == 2);
 
 	/* case 3: The TCP/IP tqpair connection is not negotitated. Expect: fail */
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_CAPSULE_RESP;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_CAPSULE_RESP;
 	tqpair.state = NVME_TCP_QPAIR_STATE_INVALID;
-	tqpair.recv_pdu.hdr.common.plen = sizeof(struct spdk_nvme_tcp_ic_resp);
-	tqpair.recv_pdu.hdr.common.hlen = 0;
+	tqpair.recv_pdu->hdr.common.plen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->hdr.common.hlen = 0;
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -961,10 +965,10 @@ test_nvme_tcp_pdu_ch_handle(void)
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.plen == tqpair.send_pdu->hdr.term_req.common.hlen);
 
 	/* case 4: Unexpected PDU type. Expect: fail */
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_REQ;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_REQ;
 	tqpair.state = NVME_TCP_QPAIR_STATE_RUNNING;
-	tqpair.recv_pdu.hdr.common.plen = 0;
-	tqpair.recv_pdu.hdr.common.hlen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->hdr.common.plen = 0;
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_ic_resp);
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -973,10 +977,10 @@ test_nvme_tcp_pdu_ch_handle(void)
 		  (unsigned)SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE);
 
 	/* case 5: plen error. Expect: fail */
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
 	tqpair.state = NVME_TCP_QPAIR_STATE_INVALID;
-	tqpair.recv_pdu.hdr.common.plen = 0;
-	tqpair.recv_pdu.hdr.common.hlen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->hdr.common.plen = 0;
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_ic_resp);
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -985,11 +989,11 @@ test_nvme_tcp_pdu_ch_handle(void)
 		  (unsigned)SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.fei[0] == 4);
 
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_CAPSULE_RESP;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_CAPSULE_RESP;
 	tqpair.state = NVME_TCP_QPAIR_STATE_RUNNING;
-	tqpair.recv_pdu.hdr.common.flags = SPDK_NVME_TCP_CH_FLAGS_HDGSTF;
-	tqpair.recv_pdu.hdr.common.plen = 0;
-	tqpair.recv_pdu.hdr.common.hlen = sizeof(struct spdk_nvme_tcp_rsp);
+	tqpair.recv_pdu->hdr.common.flags = SPDK_NVME_TCP_CH_FLAGS_HDGSTF;
+	tqpair.recv_pdu->hdr.common.plen = 0;
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_rsp);
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -998,11 +1002,11 @@ test_nvme_tcp_pdu_ch_handle(void)
 		  (unsigned)sizeof(struct spdk_nvme_tcp_term_req_hdr));
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.fei[0] == 4);
 
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_C2H_DATA;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_C2H_DATA;
 	tqpair.state = NVME_TCP_QPAIR_STATE_RUNNING;
-	tqpair.recv_pdu.hdr.common.plen = 0;
-	tqpair.recv_pdu.hdr.common.pdo = 64;
-	tqpair.recv_pdu.hdr.common.hlen = sizeof(struct spdk_nvme_tcp_c2h_data_hdr);
+	tqpair.recv_pdu->hdr.common.plen = 0;
+	tqpair.recv_pdu->hdr.common.pdo = 64;
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_c2h_data_hdr);
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -1011,10 +1015,10 @@ test_nvme_tcp_pdu_ch_handle(void)
 		  (unsigned)sizeof(struct spdk_nvme_tcp_term_req_hdr));
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.fei[0] == 4);
 
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_C2H_TERM_REQ;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_C2H_TERM_REQ;
 	tqpair.state = NVME_TCP_QPAIR_STATE_RUNNING;
-	tqpair.recv_pdu.hdr.common.plen = 0;
-	tqpair.recv_pdu.hdr.common.hlen = sizeof(struct spdk_nvme_tcp_term_req_hdr);
+	tqpair.recv_pdu->hdr.common.plen = 0;
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_term_req_hdr);
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -1023,11 +1027,11 @@ test_nvme_tcp_pdu_ch_handle(void)
 		  (unsigned)sizeof(struct spdk_nvme_tcp_term_req_hdr));
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.fei[0] == 4);
 
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_R2T;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_R2T;
 	tqpair.state = NVME_TCP_QPAIR_STATE_RUNNING;
-	tqpair.recv_pdu.hdr.common.flags = SPDK_NVME_TCP_CH_FLAGS_HDGSTF;
-	tqpair.recv_pdu.hdr.common.plen = 0;
-	tqpair.recv_pdu.hdr.common.hlen = sizeof(struct spdk_nvme_tcp_r2t_hdr);
+	tqpair.recv_pdu->hdr.common.flags = SPDK_NVME_TCP_CH_FLAGS_HDGSTF;
+	tqpair.recv_pdu->hdr.common.plen = 0;
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_r2t_hdr);
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ);
@@ -1037,16 +1041,484 @@ test_nvme_tcp_pdu_ch_handle(void)
 	CU_ASSERT(tqpair.send_pdu->hdr.term_req.fei[0] == 4);
 
 	/* case 6: Expect:  PASS */
-	tqpair.recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
 	tqpair.state = NVME_TCP_QPAIR_STATE_INVALID;
-	tqpair.recv_pdu.hdr.common.plen = sizeof(struct spdk_nvme_tcp_ic_resp);
-	tqpair.recv_pdu.hdr.common.hlen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->hdr.common.plen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_ic_resp);
 	nvme_tcp_pdu_ch_handle(&tqpair);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH);
-	CU_ASSERT(tqpair.recv_pdu.psh_len == tqpair.recv_pdu.hdr.common.hlen - sizeof(
+	CU_ASSERT(tqpair.recv_pdu->psh_len == tqpair.recv_pdu->hdr.common.hlen - sizeof(
 			  struct spdk_nvme_tcp_common_pdu_hdr));
 }
 
+DEFINE_RETURN_MOCK(spdk_sock_connect_ext, struct spdk_sock *);
+struct spdk_sock *
+spdk_sock_connect_ext(const char *ip, int port,
+		      char *_impl_name, struct spdk_sock_opts *opts)
+{
+	HANDLE_RETURN_MOCK(spdk_sock_connect_ext);
+	CU_ASSERT(port == 23);
+	CU_ASSERT(opts->opts_size == sizeof(*opts));
+	CU_ASSERT(opts->priority == 1);
+	CU_ASSERT(opts->zcopy == true);
+	CU_ASSERT(!strcmp(ip, "192.168.1.78"));
+	return (struct spdk_sock *)0xDDADBEEF;
+}
+
+static void
+test_nvme_tcp_qpair_connect_sock(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct nvme_tcp_qpair tqpair = {};
+	int rc;
+
+	tqpair.qpair.trtype = SPDK_NVME_TRANSPORT_TCP;
+	tqpair.qpair.id = 1;
+	tqpair.qpair.poll_group = (void *)0xDEADBEEF;
+	ctrlr.trid.priority = 1;
+	ctrlr.trid.adrfam = SPDK_NVMF_ADRFAM_IPV4;
+	memcpy(ctrlr.trid.traddr, "192.168.1.78", sizeof("192.168.1.78"));
+	memcpy(ctrlr.trid.trsvcid, "23", sizeof("23"));
+	memcpy(ctrlr.opts.src_addr, "192.168.1.77", sizeof("192.168.1.77"));
+	memcpy(ctrlr.opts.src_svcid, "23", sizeof("23"));
+
+	rc = nvme_tcp_qpair_connect_sock(&ctrlr, &tqpair.qpair);
+	CU_ASSERT(rc == 0);
+
+	/* Unsupported family of the transport address */
+	ctrlr.trid.adrfam = SPDK_NVMF_ADRFAM_IB;
+
+	rc = nvme_tcp_qpair_connect_sock(&ctrlr, &tqpair.qpair);
+	SPDK_CU_ASSERT_FATAL(rc == -1);
+
+	/* Invalid dst_port, INT_MAX is 2147483647 */
+	ctrlr.trid.adrfam = SPDK_NVMF_ADRFAM_IPV4;
+	memcpy(ctrlr.trid.trsvcid, "2147483647", sizeof("2147483647"));
+
+	rc = nvme_tcp_qpair_connect_sock(&ctrlr, &tqpair.qpair);
+	SPDK_CU_ASSERT_FATAL(rc == -1);
+
+	/* Parse invalid address */
+	memcpy(ctrlr.trid.trsvcid, "23", sizeof("23"));
+	memcpy(ctrlr.trid.traddr, "192.168.1.256", sizeof("192.168.1.256"));
+
+	rc = nvme_tcp_qpair_connect_sock(&ctrlr, &tqpair.qpair);
+	SPDK_CU_ASSERT_FATAL(rc != 0);
+}
+
+static void
+test_nvme_tcp_qpair_icreq_send(void)
+{
+	struct nvme_tcp_qpair tqpair = {};
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct nvme_tcp_pdu pdu = {};
+	struct nvme_tcp_poll_group poll_group = {};
+	struct spdk_nvme_tcp_ic_req *ic_req = NULL;
+	int rc;
+
+	tqpair.send_pdu = &pdu;
+	tqpair.qpair.ctrlr = &ctrlr;
+	tqpair.qpair.poll_group = &poll_group.group;
+	ic_req = &pdu.hdr.ic_req;
+
+	tqpair.state = NVME_TCP_QPAIR_STATE_RUNNING;
+	tqpair.qpair.ctrlr->opts.header_digest = true;
+	tqpair.qpair.ctrlr->opts.data_digest = true;
+	TAILQ_INIT(&tqpair.send_queue);
+
+	rc = nvme_tcp_qpair_icreq_send(&tqpair);
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(ic_req->common.hlen == sizeof(*ic_req));
+	CU_ASSERT(ic_req->common.plen == sizeof(*ic_req));
+	CU_ASSERT(ic_req->common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_IC_REQ);
+	CU_ASSERT(ic_req->pfv == 0);
+	CU_ASSERT(ic_req->maxr2t == NVME_TCP_MAX_R2T_DEFAULT - 1);
+	CU_ASSERT(ic_req->hpda == NVME_TCP_HPDA_DEFAULT);
+	CU_ASSERT(ic_req->dgst.bits.hdgst_enable == true);
+	CU_ASSERT(ic_req->dgst.bits.ddgst_enable == true);
+}
+
+static void
+test_nvme_tcp_c2h_payload_handle(void)
+{
+	struct nvme_tcp_qpair tqpair = {};
+	struct nvme_tcp_pdu pdu = {};
+	struct nvme_tcp_req tcp_req = {};
+	struct nvme_request	req = {};
+	struct nvme_tcp_pdu recv_pdu = {};
+	uint32_t reaped = 1;
+
+	tcp_req.req = &req;
+	tcp_req.req->qpair = &tqpair.qpair;
+	tcp_req.req->cb_fn = ut_nvme_complete_request;
+	tcp_req.tqpair = &tqpair;
+	tcp_req.cid = 1;
+
+	TAILQ_INIT(&tcp_req.tqpair->outstanding_reqs);
+
+	pdu.req = &tcp_req;
+	pdu.hdr.c2h_data.common.flags = SPDK_NVME_TCP_C2H_DATA_FLAGS_SUCCESS;
+	pdu.data_len = 1024;
+
+	tqpair.qpair.id = 1;
+	tqpair.recv_pdu = &recv_pdu;
+
+	/* case 1: nvme_tcp_c2h_data_payload_handle: tcp_req->datao != tcp_req->req->payload_size */
+	tcp_req.datao = 1024;
+	tcp_req.req->payload_size = 2048;
+	tcp_req.state = NVME_TCP_REQ_ACTIVE;
+	tcp_req.ordering.bits.send_ack = 1;
+	memset(&tcp_req.rsp, 0, sizeof(tcp_req.rsp));
+	tcp_req.ordering.bits.data_recv = 0;
+	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_ERROR;
+	TAILQ_INSERT_TAIL(&tcp_req.tqpair->outstanding_reqs, &tcp_req, link);
+
+	nvme_tcp_c2h_data_payload_handle(&tqpair, &pdu, &reaped);
+
+	CU_ASSERT(tcp_req.rsp.status.p == 0);
+	CU_ASSERT(tcp_req.rsp.cid == tcp_req.cid);
+	CU_ASSERT(tcp_req.rsp.sqid == tqpair.qpair.id);
+	CU_ASSERT(tcp_req.ordering.bits.data_recv == 1);
+	CU_ASSERT(reaped == 2);
+
+	/* case 2: nvme_tcp_c2h_data_payload_handle: tcp_req->datao == tcp_req->req->payload_size */
+	tcp_req.datao = 1024;
+	tcp_req.req->payload_size = 1024;
+	tcp_req.state = NVME_TCP_REQ_ACTIVE;
+	tcp_req.ordering.bits.send_ack = 1;
+	memset(&tcp_req.rsp, 0, sizeof(tcp_req.rsp));
+	tcp_req.ordering.bits.data_recv = 0;
+	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_ERROR;
+	TAILQ_INSERT_TAIL(&tcp_req.tqpair->outstanding_reqs, &tcp_req, link);
+
+	nvme_tcp_c2h_data_payload_handle(&tqpair, &pdu, &reaped);
+
+	CU_ASSERT(tcp_req.rsp.status.p == 1);
+	CU_ASSERT(tcp_req.rsp.cid == tcp_req.cid);
+	CU_ASSERT(tcp_req.rsp.sqid == tqpair.qpair.id);
+	CU_ASSERT(tcp_req.ordering.bits.data_recv == 1);
+	CU_ASSERT(reaped == 3);
+
+	/* case 3: nvme_tcp_c2h_data_payload_handle: flag does not have SPDK_NVME_TCP_C2H_DATA_FLAGS_SUCCESS */
+	pdu.hdr.c2h_data.common.flags = SPDK_NVME_TCP_C2H_DATA_FLAGS_LAST_PDU;
+	tcp_req.datao = 1024;
+	tcp_req.req->payload_size = 1024;
+	tcp_req.state = NVME_TCP_REQ_ACTIVE;
+	tcp_req.ordering.bits.send_ack = 1;
+	memset(&tcp_req.rsp, 0, sizeof(tcp_req.rsp));
+	tcp_req.ordering.bits.data_recv = 0;
+	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_ERROR;
+	TAILQ_INSERT_TAIL(&tcp_req.tqpair->outstanding_reqs, &tcp_req, link);
+
+	nvme_tcp_c2h_data_payload_handle(&tqpair, &pdu, &reaped);
+
+	CU_ASSERT(reaped == 3);
+
+	/* case 4: nvme_tcp_c2h_term_req_payload_handle: recv_state is NVME_TCP_PDU_RECV_STATE_ERROR */
+	pdu.hdr.term_req.fes = SPDK_NVME_TCP_TERM_REQ_FES_INVALID_HEADER_FIELD;
+	nvme_tcp_c2h_term_req_payload_handle(&tqpair, &pdu);
+
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
+}
+
+static void
+test_nvme_tcp_icresp_handle(void)
+{
+	struct nvme_tcp_qpair tqpair = {};
+	struct nvme_tcp_pdu pdu = {};
+	struct nvme_tcp_pdu send_pdu = {};
+	struct nvme_tcp_pdu recv_pdu = {};
+
+	tqpair.send_pdu = &send_pdu;
+	tqpair.recv_pdu = &recv_pdu;
+	TAILQ_INIT(&tqpair.send_queue);
+
+	/* case 1: Expected ICResp PFV and got are different. */
+	pdu.hdr.ic_resp.pfv = 1;
+
+	nvme_tcp_icresp_handle(&tqpair, &pdu);
+
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
+
+	/* case 2: Expected ICResp maxh2cdata and got are different. */
+	pdu.hdr.ic_resp.pfv = 0;
+	pdu.hdr.ic_resp.maxh2cdata = 2048;
+
+	nvme_tcp_icresp_handle(&tqpair, &pdu);
+
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
+
+	/* case 3: Expected ICResp cpda and got are different. */
+	pdu.hdr.ic_resp.maxh2cdata = NVME_TCP_PDU_H2C_MIN_DATA_SIZE;
+	pdu.hdr.ic_resp.cpda = 64;
+
+	nvme_tcp_icresp_handle(&tqpair, &pdu);
+
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
+
+	/* case 4: waiting icreq ack. */
+	pdu.hdr.ic_resp.maxh2cdata = NVME_TCP_PDU_H2C_MIN_DATA_SIZE;
+	pdu.hdr.ic_resp.cpda = 30;
+	pdu.hdr.ic_resp.dgst.bits.hdgst_enable = true;
+	pdu.hdr.ic_resp.dgst.bits.ddgst_enable = true;
+	tqpair.flags.icreq_send_ack = 0;
+
+	nvme_tcp_icresp_handle(&tqpair, &pdu);
+
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
+	CU_ASSERT(tqpair.state == NVME_TCP_QPAIR_STATE_INITIALIZING);
+	CU_ASSERT(tqpair.maxh2cdata == pdu.hdr.ic_resp.maxh2cdata);
+	CU_ASSERT(tqpair.cpda == pdu.hdr.ic_resp.cpda);
+	CU_ASSERT(tqpair.flags.host_hdgst_enable == pdu.hdr.ic_resp.dgst.bits.hdgst_enable);
+	CU_ASSERT(tqpair.flags.host_ddgst_enable == pdu.hdr.ic_resp.dgst.bits.ddgst_enable);
+
+	/* case 5: Expect: PASS. */
+	tqpair.flags.icreq_send_ack = 1;
+
+	nvme_tcp_icresp_handle(&tqpair, &pdu);
+
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
+	CU_ASSERT(tqpair.state == NVME_TCP_QPAIR_STATE_RUNNING);
+	CU_ASSERT(tqpair.maxh2cdata == pdu.hdr.ic_resp.maxh2cdata);
+	CU_ASSERT(tqpair.cpda == pdu.hdr.ic_resp.cpda);
+	CU_ASSERT(tqpair.flags.host_hdgst_enable == pdu.hdr.ic_resp.dgst.bits.hdgst_enable);
+	CU_ASSERT(tqpair.flags.host_ddgst_enable == pdu.hdr.ic_resp.dgst.bits.ddgst_enable);
+}
+
+static void
+test_nvme_tcp_pdu_payload_handle(void)
+{
+	struct nvme_tcp_qpair	tqpair = {};
+	struct nvme_tcp_pdu	recv_pdu = {};
+	struct nvme_tcp_req	tcp_req = {};
+	struct nvme_request	req = {};
+	uint32_t		reaped = 0;
+
+	tqpair.recv_pdu = &recv_pdu;
+	tcp_req.tqpair = &tqpair;
+	tcp_req.req = &req;
+	tcp_req.req->qpair = &tqpair.qpair;
+
+	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD;
+	tqpair.qpair.id = 1;
+	recv_pdu.ddgst_enable = false;
+	recv_pdu.req = &tcp_req;
+	recv_pdu.hdr.c2h_data.common.flags = SPDK_NVME_TCP_C2H_DATA_FLAGS_SUCCESS;
+	recv_pdu.data_len = 1024;
+	tcp_req.ordering.bits.data_recv = 0;
+	tcp_req.req->cb_fn = ut_nvme_complete_request;
+	tcp_req.cid = 1;
+	TAILQ_INIT(&tcp_req.tqpair->outstanding_reqs);
+	TAILQ_INSERT_TAIL(&tcp_req.tqpair->outstanding_reqs, &tcp_req, link);
+
+	/* C2H_DATA */
+	recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_C2H_DATA;
+	tcp_req.datao = 1024;
+	tcp_req.req->payload_size = 2048;
+	tcp_req.state = NVME_TCP_REQ_ACTIVE;
+	tcp_req.ordering.bits.send_ack = 1;
+
+	nvme_tcp_pdu_payload_handle(&tqpair, &reaped);
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
+	CU_ASSERT(tcp_req.rsp.status.p == 0);
+	CU_ASSERT(tcp_req.rsp.cid == 1);
+	CU_ASSERT(tcp_req.rsp.sqid == 1);
+	CU_ASSERT(tcp_req.ordering.bits.data_recv == 1);
+	CU_ASSERT(reaped == 1);
+
+	/* TermResp */
+	recv_pdu.hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_C2H_TERM_REQ;
+	recv_pdu.hdr.term_req.fes = SPDK_NVME_TCP_TERM_REQ_FES_INVALID_HEADER_FIELD;
+	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD;
+
+	nvme_tcp_pdu_payload_handle(&tqpair, &reaped);
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
+}
+
+static void
+test_nvme_tcp_capsule_resp_hdr_handle(void)
+{
+	struct nvme_tcp_qpair	tqpair = {};
+	struct nvme_request	req = {};
+	struct spdk_nvme_cpl	rccqe_tgt = {};
+	struct nvme_tcp_req	*tcp_req = NULL;
+	uint32_t		reaped = 0;
+	int			rc;
+
+	/* Initialize requests and pdus */
+	tqpair.num_entries = 1;
+	req.qpair = &tqpair.qpair;
+
+	rc = nvme_tcp_alloc_reqs(&tqpair);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+	tcp_req = nvme_tcp_req_get(&tqpair);
+	SPDK_CU_ASSERT_FATAL(tcp_req != NULL);
+	rc = nvme_tcp_req_init(&tqpair, &req, tcp_req);
+	SPDK_CU_ASSERT_FATAL(rc == 0);
+	tcp_req->ordering.bits.send_ack = 1;
+	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH;
+	/* tqpair.recv_pdu will be reseted after handling */
+	memset(&rccqe_tgt, 0xff, sizeof(rccqe_tgt));
+	rccqe_tgt.cid = 0;
+	memcpy(&tqpair.recv_pdu->hdr.capsule_resp.rccqe, &rccqe_tgt, sizeof(rccqe_tgt));
+
+	nvme_tcp_capsule_resp_hdr_handle(&tqpair, tqpair.recv_pdu, &reaped);
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_READY);
+	CU_ASSERT(!memcmp(&tcp_req->rsp, &rccqe_tgt, sizeof(rccqe_tgt)));
+	CU_ASSERT(tcp_req->ordering.bits.data_recv == 1);
+	CU_ASSERT(reaped == 1);
+	CU_ASSERT(TAILQ_EMPTY(&tcp_req->tqpair->outstanding_reqs));
+
+	/* Get tcp request error, expect fail */
+	reaped = 0;
+	tqpair.recv_pdu->hdr.capsule_resp.rccqe.cid = 1;
+	tqpair.recv_state = NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PSH;
+
+	nvme_tcp_capsule_resp_hdr_handle(&tqpair, tqpair.recv_pdu, &reaped);
+	CU_ASSERT(reaped == 0);
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_ERROR);
+	nvme_tcp_free_reqs(&tqpair);
+}
+
+static void
+test_nvme_tcp_ctrlr_connect_qpair(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct spdk_nvme_qpair *qpair;
+	struct nvme_tcp_qpair tqpair = {};
+	struct nvme_tcp_pdu pdu = {};
+	struct nvme_tcp_pdu recv_pdu = {};
+	struct spdk_nvme_tcp_ic_req *ic_req = NULL;
+	int rc;
+
+	tqpair.qpair.trtype = SPDK_NVME_TRANSPORT_TCP;
+	tqpair.recv_pdu = &recv_pdu;
+	qpair = &tqpair.qpair;
+	tqpair.sock = (struct spdk_sock *)0xDEADBEEF;
+	tqpair.send_pdu = &pdu;
+	tqpair.qpair.ctrlr = &ctrlr;
+	ic_req = &pdu.hdr.ic_req;
+
+	tqpair.recv_pdu->hdr.common.pdu_type = SPDK_NVME_TCP_PDU_TYPE_IC_RESP;
+	tqpair.recv_pdu->hdr.common.plen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->hdr.common.hlen = sizeof(struct spdk_nvme_tcp_ic_resp);
+	tqpair.recv_pdu->ch_valid_bytes = 8;
+	tqpair.recv_pdu->psh_valid_bytes = tqpair.recv_pdu->hdr.common.hlen;
+	tqpair.recv_pdu->hdr.ic_resp.maxh2cdata = 4096;
+	tqpair.recv_pdu->hdr.ic_resp.cpda = 1;
+	tqpair.flags.icreq_send_ack = 1;
+	tqpair.qpair.ctrlr->opts.header_digest = true;
+	tqpair.qpair.ctrlr->opts.data_digest = true;
+	TAILQ_INIT(&tqpair.send_queue);
+
+
+	rc = nvme_tcp_ctrlr_connect_qpair(&ctrlr, qpair);
+
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(tqpair.maxr2t == NVME_TCP_MAX_R2T_DEFAULT);
+	CU_ASSERT(tqpair.state == NVME_TCP_QPAIR_STATE_RUNNING);
+	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_CH);
+	CU_ASSERT(ic_req->common.hlen == sizeof(*ic_req));
+	CU_ASSERT(ic_req->common.plen == sizeof(*ic_req));
+	CU_ASSERT(ic_req->common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_IC_REQ);
+	CU_ASSERT(ic_req->pfv == 0);
+	CU_ASSERT(ic_req->maxr2t == NVME_TCP_MAX_R2T_DEFAULT - 1);
+	CU_ASSERT(ic_req->hpda == NVME_TCP_HPDA_DEFAULT);
+	CU_ASSERT(ic_req->dgst.bits.hdgst_enable == true);
+	CU_ASSERT(ic_req->dgst.bits.ddgst_enable == true);
+}
+
+static void
+test_nvme_tcp_ctrlr_disconnect_qpair(void)
+{
+	struct spdk_nvme_ctrlr ctrlr = {};
+	struct spdk_nvme_qpair *qpair;
+	struct nvme_tcp_qpair tqpair = {
+		.qpair.trtype = SPDK_NVME_TRANSPORT_TCP,
+	};
+	struct nvme_tcp_poll_group tgroup = {};
+	struct nvme_tcp_pdu pdu = {};
+
+	qpair = &tqpair.qpair;
+	qpair->poll_group = &tgroup.group;
+	tqpair.sock = (struct spdk_sock *)0xDEADBEEF;
+	tqpair.needs_poll = true;
+	TAILQ_INIT(&tgroup.needs_poll);
+	TAILQ_INIT(&tqpair.send_queue);
+	TAILQ_INSERT_TAIL(&tgroup.needs_poll, &tqpair, link);
+	TAILQ_INSERT_TAIL(&tqpair.send_queue, &pdu, tailq);
+
+	nvme_tcp_ctrlr_disconnect_qpair(&ctrlr, qpair);
+
+	CU_ASSERT(tqpair.needs_poll == false);
+	CU_ASSERT(tqpair.sock == NULL);
+	CU_ASSERT(TAILQ_EMPTY(&tqpair.send_queue) == true);
+}
+
+static void
+test_nvme_tcp_ctrlr_create_io_qpair(void)
+{
+	struct spdk_nvme_qpair *qpair = NULL;
+	struct spdk_nvme_ctrlr ctrlr = {};
+	uint16_t qid = 1;
+	const struct spdk_nvme_io_qpair_opts opts = {
+		.io_queue_size = 1,
+		.qprio = SPDK_NVME_QPRIO_URGENT,
+		.io_queue_requests = 1,
+	};
+	struct nvme_tcp_qpair *tqpair;
+
+	ctrlr.trid.priority = 1;
+	ctrlr.trid.adrfam = SPDK_NVMF_ADRFAM_IPV4;
+	memcpy(ctrlr.trid.traddr, "192.168.1.78", sizeof("192.168.1.78"));
+	memcpy(ctrlr.trid.trsvcid, "23", sizeof("23"));
+	memcpy(ctrlr.opts.src_addr, "192.168.1.77", sizeof("192.168.1.77"));
+	memcpy(ctrlr.opts.src_svcid, "23", sizeof("23"));
+
+	qpair = nvme_tcp_ctrlr_create_io_qpair(&ctrlr, qid, &opts);
+	tqpair = nvme_tcp_qpair(qpair);
+
+	CU_ASSERT(qpair != NULL);
+	CU_ASSERT(qpair->id == 1);
+	CU_ASSERT(qpair->ctrlr == &ctrlr);
+	CU_ASSERT(qpair->qprio == SPDK_NVME_QPRIO_URGENT);
+	CU_ASSERT(qpair->trtype == SPDK_NVME_TRANSPORT_TCP);
+	CU_ASSERT(qpair->poll_group == (void *)0xDEADBEEF);
+	CU_ASSERT(tqpair->num_entries = 1);
+
+	free(tqpair->tcp_reqs);
+	spdk_free(tqpair->send_pdus);
+	free(tqpair);
+}
+
+static void
+test_nvme_tcp_ctrlr_delete_io_qpair(void)
+{
+	struct spdk_nvme_ctrlr *ctrlr = (struct spdk_nvme_ctrlr *)0xdeadbeef;
+	struct spdk_nvme_qpair *qpair;
+	struct nvme_tcp_qpair *tqpair;
+	struct nvme_tcp_req tcp_req = {};
+	struct nvme_request	req = {};
+	int rc;
+
+	tqpair = calloc(1, sizeof(struct nvme_tcp_qpair));
+	tqpair->tcp_reqs = calloc(1, sizeof(struct nvme_tcp_req));
+	tqpair->send_pdus = calloc(1, sizeof(struct nvme_tcp_pdu));
+	tqpair->qpair.trtype = SPDK_NVME_TRANSPORT_TCP;
+	qpair = &tqpair->qpair;
+	tcp_req.req = &req;
+	tcp_req.req->qpair = &tqpair->qpair;
+	tcp_req.req->cb_fn = ut_nvme_complete_request;
+	tcp_req.tqpair = tqpair;
+	tcp_req.state = NVME_TCP_REQ_ACTIVE;
+	TAILQ_INIT(&tqpair->outstanding_reqs);
+	TAILQ_INSERT_TAIL(&tcp_req.tqpair->outstanding_reqs, &tcp_req, link);
+
+	rc = nvme_tcp_ctrlr_delete_io_qpair(ctrlr, qpair);
+
+	CU_ASSERT(rc == 0);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -1071,6 +1543,16 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_nvme_tcp_parse_addr);
 	CU_ADD_TEST(suite, test_nvme_tcp_qpair_send_h2c_term_req);
 	CU_ADD_TEST(suite, test_nvme_tcp_pdu_ch_handle);
+	CU_ADD_TEST(suite, test_nvme_tcp_qpair_connect_sock);
+	CU_ADD_TEST(suite, test_nvme_tcp_qpair_icreq_send);
+	CU_ADD_TEST(suite, test_nvme_tcp_c2h_payload_handle);
+	CU_ADD_TEST(suite, test_nvme_tcp_icresp_handle);
+	CU_ADD_TEST(suite, test_nvme_tcp_pdu_payload_handle);
+	CU_ADD_TEST(suite, test_nvme_tcp_capsule_resp_hdr_handle);
+	CU_ADD_TEST(suite, test_nvme_tcp_ctrlr_connect_qpair);
+	CU_ADD_TEST(suite, test_nvme_tcp_ctrlr_disconnect_qpair);
+	CU_ADD_TEST(suite, test_nvme_tcp_ctrlr_create_io_qpair);
+	CU_ADD_TEST(suite, test_nvme_tcp_ctrlr_delete_io_qpair);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvmf/ctrlr.c/ctrlr_ut.c b/test/unit/lib/nvmf/ctrlr.c/ctrlr_ut.c
index aa2912f6f..af85fecdd 100644
--- a/test/unit/lib/nvmf/ctrlr.c/ctrlr_ut.c
+++ b/test/unit/lib/nvmf/ctrlr.c/ctrlr_ut.c
@@ -35,6 +35,7 @@
 
 #include "spdk_cunit.h"
 #include "spdk_internal/mock.h"
+#include "thread/thread_internal.h"
 
 #include "common/lib/ut_multithread.c"
 #include "nvmf/ctrlr.c"
@@ -1373,6 +1374,7 @@ test_reservation_notification_log_page(void)
 	union nvmf_c2h_msg rsp = {};
 	union spdk_nvme_async_event_completion event = {};
 	struct spdk_nvme_reservation_notification_log logs[3];
+	struct iovec iov;
 
 	memset(&ctrlr, 0, sizeof(ctrlr));
 	ctrlr.thread = spdk_get_thread();
@@ -1422,7 +1424,9 @@ test_reservation_notification_log_page(void)
 	SPDK_CU_ASSERT_FATAL(ctrlr.num_avail_log_pages == 3);
 
 	/* Test Case: Get Log Page to clear the log pages */
-	nvmf_get_reservation_notification_log_page(&ctrlr, (void *)logs, 0, sizeof(logs), 0);
+	iov.iov_base = &logs[0];
+	iov.iov_len = sizeof(logs);
+	nvmf_get_reservation_notification_log_page(&ctrlr, &iov, 1, 0, sizeof(logs), 0);
 	SPDK_CU_ASSERT_FATAL(ctrlr.num_avail_log_pages == 0);
 
 	cleanup_pending_async_events(&ctrlr);
@@ -1499,8 +1503,10 @@ test_get_dif_ctx(void)
 static void
 test_identify_ctrlr(void)
 {
+	struct spdk_nvmf_tgt tgt = {};
 	struct spdk_nvmf_subsystem subsystem = {
-		.subtype = SPDK_NVMF_SUBTYPE_NVME
+		.subtype = SPDK_NVMF_SUBTYPE_NVME,
+		.tgt = &tgt,
 	};
 	struct spdk_nvmf_transport_ops tops = {};
 	struct spdk_nvmf_transport transport = {
@@ -1787,6 +1793,7 @@ test_get_ana_log_page(void)
 	int i;
 	char expected_page[UT_ANA_LOG_PAGE_SIZE] = {0};
 	char actual_page[UT_ANA_LOG_PAGE_SIZE] = {0};
+	struct iovec iov, iovs[2];
 	struct spdk_nvme_ana_page *ana_hdr;
 	char _ana_desc[UT_ANA_DESC_SIZE];
 	struct spdk_nvme_ana_group_descriptor *ana_desc;
@@ -1825,12 +1832,25 @@ test_get_ana_log_page(void)
 	offset = 0;
 	while (offset < UT_ANA_LOG_PAGE_SIZE) {
 		length = spdk_min(16, UT_ANA_LOG_PAGE_SIZE - offset);
-		nvmf_get_ana_log_page(&ctrlr, &actual_page[offset], offset, length, 0);
+		iov.iov_base = &actual_page[offset];
+		iov.iov_len = length;
+		nvmf_get_ana_log_page(&ctrlr, &iov, 1, offset, length, 0);
 		offset += length;
 	}
 
 	/* compare expected page and actual page */
 	CU_ASSERT(memcmp(expected_page, actual_page, UT_ANA_LOG_PAGE_SIZE) == 0);
+
+	memset(&actual_page[0], 0, UT_ANA_LOG_PAGE_SIZE);
+	offset = 0;
+	iovs[0].iov_base = &actual_page[offset];
+	iovs[0].iov_len = UT_ANA_LOG_PAGE_SIZE - UT_ANA_DESC_SIZE + 4;
+	offset += UT_ANA_LOG_PAGE_SIZE - UT_ANA_DESC_SIZE + 4;
+	iovs[1].iov_base = &actual_page[offset];
+	iovs[1].iov_len = UT_ANA_LOG_PAGE_SIZE - offset;
+	nvmf_get_ana_log_page(&ctrlr, &iovs[0], 2, 0, UT_ANA_LOG_PAGE_SIZE, 0);
+
+	CU_ASSERT(memcmp(expected_page, actual_page, UT_ANA_LOG_PAGE_SIZE) == 0);
 }
 
 static void
@@ -1998,6 +2018,111 @@ test_rae(void)
 	cleanup_pending_async_events(&ctrlr);
 }
 
+static void
+test_nvmf_ctrlr_create_destruct(void)
+{
+	struct spdk_nvmf_fabric_connect_data connect_data = {};
+	struct spdk_nvmf_poll_group group = {};
+	struct spdk_nvmf_subsystem_poll_group sgroups[2] = {};
+	struct spdk_nvmf_transport transport = {};
+	struct spdk_nvmf_transport_ops tops = {};
+	struct spdk_nvmf_subsystem subsystem = {};
+	struct spdk_nvmf_request req = {};
+	struct spdk_nvmf_qpair qpair = {};
+	struct spdk_nvmf_ctrlr *ctrlr = NULL;
+	struct spdk_nvmf_tgt tgt = {};
+	union nvmf_h2c_msg cmd = {};
+	union nvmf_c2h_msg rsp = {};
+	const uint8_t hostid[16] = {
+		0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,
+		0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F
+	};
+	const char subnqn[] = "nqn.2016-06.io.spdk:subsystem1";
+	const char hostnqn[] = "nqn.2016-06.io.spdk:host1";
+
+	group.thread = spdk_get_thread();
+	transport.ops = &tops;
+	transport.opts.max_aq_depth = 32;
+	transport.opts.max_queue_depth = 64;
+	transport.opts.max_qpairs_per_ctrlr = 3;
+	transport.opts.dif_insert_or_strip = true;
+	transport.tgt = &tgt;
+	qpair.transport = &transport;
+	qpair.group = &group;
+	qpair.state = SPDK_NVMF_QPAIR_ACTIVE;
+	TAILQ_INIT(&qpair.outstanding);
+
+	memcpy(connect_data.hostid, hostid, sizeof(hostid));
+	connect_data.cntlid = 0xFFFF;
+	snprintf(connect_data.subnqn, sizeof(connect_data.subnqn), "%s", subnqn);
+	snprintf(connect_data.hostnqn, sizeof(connect_data.hostnqn), "%s", hostnqn);
+
+	subsystem.thread = spdk_get_thread();
+	subsystem.id = 1;
+	TAILQ_INIT(&subsystem.ctrlrs);
+	subsystem.tgt = &tgt;
+	subsystem.subtype = SPDK_NVMF_SUBTYPE_NVME;
+	subsystem.state = SPDK_NVMF_SUBSYSTEM_ACTIVE;
+	snprintf(subsystem.subnqn, sizeof(subsystem.subnqn), "%s", subnqn);
+
+	group.sgroups = sgroups;
+
+	cmd.connect_cmd.opcode = SPDK_NVME_OPC_FABRIC;
+	cmd.connect_cmd.cid = 1;
+	cmd.connect_cmd.fctype = SPDK_NVMF_FABRIC_COMMAND_CONNECT;
+	cmd.connect_cmd.recfmt = 0;
+	cmd.connect_cmd.qid = 0;
+	cmd.connect_cmd.sqsize = 31;
+	cmd.connect_cmd.cattr = 0;
+	cmd.connect_cmd.kato = 120000;
+
+	req.qpair = &qpair;
+	req.length = sizeof(connect_data);
+	req.xfer = SPDK_NVME_DATA_HOST_TO_CONTROLLER;
+	req.data = &connect_data;
+	req.cmd = &cmd;
+	req.rsp = &rsp;
+
+	TAILQ_INSERT_TAIL(&qpair.outstanding, &req, link);
+	sgroups[subsystem.id].mgmt_io_outstanding++;
+
+	ctrlr = nvmf_ctrlr_create(&subsystem, &req, &req.cmd->connect_cmd, req.data);
+	poll_threads();
+	SPDK_CU_ASSERT_FATAL(ctrlr != NULL);
+	CU_ASSERT(req.qpair->ctrlr == ctrlr);
+	CU_ASSERT(ctrlr->subsys == &subsystem);
+	CU_ASSERT(ctrlr->thread == req.qpair->group->thread);
+	CU_ASSERT(ctrlr->disconnect_in_progress == false);
+	CU_ASSERT(ctrlr->qpair_mask != NULL);
+	CU_ASSERT(ctrlr->feat.keep_alive_timer.bits.kato == 120000);
+	CU_ASSERT(ctrlr->feat.async_event_configuration.bits.ns_attr_notice == 1);
+	CU_ASSERT(ctrlr->feat.volatile_write_cache.bits.wce == 1);
+	CU_ASSERT(ctrlr->feat.number_of_queues.bits.ncqr == 1);
+	CU_ASSERT(ctrlr->feat.number_of_queues.bits.nsqr == 1);
+	CU_ASSERT(!strncmp((void *)&ctrlr->hostid, hostid, 16));
+	CU_ASSERT(ctrlr->vcprop.cap.bits.cqr == 1);
+	CU_ASSERT(ctrlr->vcprop.cap.bits.mqes == 63);
+	CU_ASSERT(ctrlr->vcprop.cap.bits.ams == 0);
+	CU_ASSERT(ctrlr->vcprop.cap.bits.to == 1);
+	CU_ASSERT(ctrlr->vcprop.cap.bits.dstrd == 0);
+	CU_ASSERT(ctrlr->vcprop.cap.bits.css == SPDK_NVME_CAP_CSS_NVM);
+	CU_ASSERT(ctrlr->vcprop.cap.bits.mpsmin == 0);
+	CU_ASSERT(ctrlr->vcprop.cap.bits.mpsmax == 0);
+	CU_ASSERT(ctrlr->vcprop.vs.bits.mjr == 1);
+	CU_ASSERT(ctrlr->vcprop.vs.bits.mnr == 3);
+	CU_ASSERT(ctrlr->vcprop.vs.bits.ter == 0);
+	CU_ASSERT(ctrlr->vcprop.cc.raw == 0);
+	CU_ASSERT(ctrlr->vcprop.cc.bits.en == 0);
+	CU_ASSERT(ctrlr->vcprop.csts.raw == 0);
+	CU_ASSERT(ctrlr->vcprop.csts.bits.rdy == 0);
+	CU_ASSERT(ctrlr->dif_insert_or_strip == true);
+
+	nvmf_ctrlr_destruct(ctrlr);
+	poll_threads();
+	CU_ASSERT(TAILQ_EMPTY(&subsystem.ctrlrs));
+	CU_ASSERT(TAILQ_EMPTY(&qpair.outstanding));
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -2026,6 +2151,7 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_get_ana_log_page);
 	CU_ADD_TEST(suite, test_multi_async_events);
 	CU_ADD_TEST(suite, test_rae);
+	CU_ADD_TEST(suite, test_nvmf_ctrlr_create_destruct);
 
 	allocate_threads(1);
 	set_thread(0);
diff --git a/test/unit/lib/nvmf/ctrlr_bdev.c/ctrlr_bdev_ut.c b/test/unit/lib/nvmf/ctrlr_bdev.c/ctrlr_bdev_ut.c
index e3801b57d..b21fb56b9 100644
--- a/test/unit/lib/nvmf/ctrlr_bdev.c/ctrlr_bdev_ut.c
+++ b/test/unit/lib/nvmf/ctrlr_bdev.c/ctrlr_bdev_ut.c
@@ -36,9 +36,11 @@
 #include "spdk_cunit.h"
 
 #include "spdk_internal/mock.h"
+#include "thread/thread_internal.h"
 
 #include "nvmf/ctrlr_bdev.c"
 
+#include "spdk/bdev_module.h"
 
 SPDK_LOG_REGISTER_COMPONENT(nvmf)
 
@@ -46,10 +48,8 @@ DEFINE_STUB(spdk_nvmf_request_complete, int, (struct spdk_nvmf_request *req), -1
 
 DEFINE_STUB(spdk_bdev_get_name, const char *, (const struct spdk_bdev *bdev), "test");
 
-DEFINE_STUB(spdk_bdev_get_acwu, uint16_t, (const struct spdk_bdev *bdev), 0);
-
-DEFINE_STUB(spdk_bdev_get_data_block_size, uint32_t,
-	    (const struct spdk_bdev *bdev), 512);
+DEFINE_STUB(spdk_bdev_get_physical_block_size, uint32_t,
+	    (const struct spdk_bdev *bdev), 4096);
 
 DEFINE_STUB(nvmf_ctrlr_process_admin_cmd, int, (struct spdk_nvmf_request *req), 0);
 
@@ -67,35 +67,69 @@ DEFINE_STUB(spdk_bdev_abort, int,
 	    (struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
 	     void *bio_cb_arg, spdk_bdev_io_completion_cb cb, void *cb_arg), 0);
 
-struct spdk_bdev {
-	uint32_t blocklen;
-	uint64_t num_blocks;
-	uint32_t md_len;
-};
+uint32_t
+spdk_bdev_get_optimal_io_boundary(const struct spdk_bdev *bdev)
+{
+	return bdev->optimal_io_boundary;
+}
 
 uint32_t
-spdk_bdev_get_block_size(const struct spdk_bdev *bdev)
+spdk_bdev_get_md_size(const struct spdk_bdev *bdev)
 {
-	return bdev->blocklen;
+	return bdev->md_len;
 }
 
-uint64_t
-spdk_bdev_get_num_blocks(const struct spdk_bdev *bdev)
+bool
+spdk_bdev_is_md_interleaved(const struct spdk_bdev *bdev)
 {
-	return bdev->num_blocks;
+	return (bdev->md_len != 0) && bdev->md_interleave;
+}
+
+enum spdk_dif_type spdk_bdev_get_dif_type(const struct spdk_bdev *bdev)
+{
+	if (bdev->md_len != 0) {
+		return bdev->dif_type;
+	} else {
+		return SPDK_DIF_DISABLE;
+	}
+}
+
+bool
+spdk_bdev_is_dif_head_of_md(const struct spdk_bdev *bdev)
+{
+	if (spdk_bdev_get_dif_type(bdev) != SPDK_DIF_DISABLE) {
+		return bdev->dif_is_head_of_md;
+	} else {
+		return false;
+	}
 }
 
 uint32_t
-spdk_bdev_get_optimal_io_boundary(const struct spdk_bdev *bdev)
+spdk_bdev_get_data_block_size(const struct spdk_bdev *bdev)
 {
-	abort();
-	return 0;
+	if (spdk_bdev_is_md_interleaved(bdev)) {
+		return bdev->blocklen - bdev->md_len;
+	} else {
+		return bdev->blocklen;
+	}
+}
+
+uint16_t
+spdk_bdev_get_acwu(const struct spdk_bdev *bdev)
+{
+	return bdev->acwu;
 }
 
 uint32_t
-spdk_bdev_get_md_size(const struct spdk_bdev *bdev)
+spdk_bdev_get_block_size(const struct spdk_bdev *bdev)
 {
-	return bdev->md_len;
+	return bdev->blocklen;
+}
+
+uint64_t
+spdk_bdev_get_num_blocks(const struct spdk_bdev *bdev)
+{
+	return bdev->blockcnt;
 }
 
 DEFINE_STUB(spdk_bdev_comparev_and_writev_blocks, int,
@@ -111,13 +145,6 @@ DEFINE_STUB(nvmf_ctrlr_process_io_cmd, int, (struct spdk_nvmf_request *req), 0);
 DEFINE_STUB_V(spdk_bdev_io_get_nvme_fused_status, (const struct spdk_bdev_io *bdev_io,
 		uint32_t *cdw0, int *cmp_sct, int *cmp_sc, int *wr_sct, int *wr_sc));
 
-DEFINE_STUB(spdk_bdev_is_md_interleaved, bool, (const struct spdk_bdev *bdev), false);
-
-DEFINE_STUB(spdk_bdev_get_dif_type, enum spdk_dif_type,
-	    (const struct spdk_bdev *bdev), SPDK_DIF_DISABLE);
-
-DEFINE_STUB(spdk_bdev_is_dif_head_of_md, bool, (const struct spdk_bdev *bdev), false);
-
 DEFINE_STUB(spdk_bdev_is_dif_check_enabled, bool,
 	    (const struct spdk_bdev *bdev, enum spdk_dif_check_type check_type), false);
 
@@ -311,7 +338,7 @@ test_spdk_nvmf_bdev_ctrlr_compare_and_write_cmd(void)
 	struct spdk_nvmf_subsystem_pg_ns_info ns_info = {};
 
 	bdev.blocklen = 512;
-	bdev.num_blocks = 10;
+	bdev.blockcnt = 10;
 	ns.bdev = &bdev;
 
 	subsystem.id = 0;
@@ -412,6 +439,81 @@ test_spdk_nvmf_bdev_ctrlr_compare_and_write_cmd(void)
 	CU_ASSERT(write_rsp.nvme_cpl.status.sc == SPDK_NVME_SC_DATA_SGL_LENGTH_INVALID);
 }
 
+static void
+test_nvmf_bdev_ctrlr_identify_ns(void)
+{
+	struct spdk_nvmf_ns ns = {};
+	struct spdk_nvme_ns_data nsdata = {};
+	struct spdk_bdev bdev = {};
+	uint8_t ns_g_id[16] = "abcdefgh";
+	uint8_t eui64[8] = "12345678";
+
+	ns.bdev = &bdev;
+	ns.ptpl_file = (void *)0xDEADBEEF;
+	memcpy(ns.opts.nguid, ns_g_id, 16);
+	memcpy(ns.opts.eui64, eui64, 8);
+
+	bdev.blockcnt = 10;
+	bdev.acwu = 0;
+	bdev.md_len = 512;
+	bdev.dif_type = SPDK_DIF_TYPE1;
+	bdev.blocklen = 4096;
+	bdev.md_interleave = 0;
+	bdev.optimal_io_boundary = BDEV_IO_NUM_CHILD_IOV;
+	bdev.dif_is_head_of_md = true;
+
+	nvmf_bdev_ctrlr_identify_ns(&ns, &nsdata, false);
+	CU_ASSERT(nsdata.nsze == 10);
+	CU_ASSERT(nsdata.ncap == 10);
+	CU_ASSERT(nsdata.nuse == 10);
+	CU_ASSERT(nsdata.nlbaf == 0);
+	CU_ASSERT(nsdata.flbas.format == 0);
+	CU_ASSERT(nsdata.nacwu == 0);
+	CU_ASSERT(nsdata.lbaf[0].lbads == spdk_u32log2(4096));
+	CU_ASSERT(nsdata.lbaf[0].ms == 512);
+	CU_ASSERT(nsdata.dpc.pit1 == 1);
+	CU_ASSERT(nsdata.dps.pit == SPDK_NVME_FMT_NVM_PROTECTION_TYPE1);
+	CU_ASSERT(nsdata.noiob == BDEV_IO_NUM_CHILD_IOV);
+	CU_ASSERT(nsdata.nmic.can_share == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.persist == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.write_exclusive == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.exclusive_access == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.write_exclusive_reg_only == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.exclusive_access_reg_only == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.write_exclusive_all_reg == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.exclusive_access_all_reg == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.ignore_existing_key == 1);
+	CU_ASSERT(nsdata.flbas.extended == 1);
+	CU_ASSERT(nsdata.mc.extended == 1);
+	CU_ASSERT(nsdata.mc.pointer == 0);
+	CU_ASSERT(nsdata.dps.md_start == true);
+	CU_ASSERT(!strncmp(nsdata.nguid, ns_g_id, 16));
+	CU_ASSERT(!strncmp((uint8_t *)&nsdata.eui64, eui64, 8));
+
+	memset(&nsdata, 0, sizeof(nsdata));
+	nvmf_bdev_ctrlr_identify_ns(&ns, &nsdata, true);
+	CU_ASSERT(nsdata.nsze == 10);
+	CU_ASSERT(nsdata.ncap == 10);
+	CU_ASSERT(nsdata.nuse == 10);
+	CU_ASSERT(nsdata.nlbaf == 0);
+	CU_ASSERT(nsdata.flbas.format == 0);
+	CU_ASSERT(nsdata.nacwu == 0);
+	CU_ASSERT(nsdata.lbaf[0].lbads == spdk_u32log2(4096));
+	CU_ASSERT(nsdata.noiob == BDEV_IO_NUM_CHILD_IOV);
+	CU_ASSERT(nsdata.nmic.can_share == 1);
+	CU_ASSERT(nsdata.lbaf[0].ms == 0);
+	CU_ASSERT(nsdata.nsrescap.rescap.persist == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.write_exclusive == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.exclusive_access == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.write_exclusive_reg_only == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.exclusive_access_reg_only == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.write_exclusive_all_reg == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.exclusive_access_all_reg == 1);
+	CU_ASSERT(nsdata.nsrescap.rescap.ignore_existing_key == 1);
+	CU_ASSERT(!strncmp(nsdata.nguid, ns_g_id, 16));
+	CU_ASSERT(!strncmp((uint8_t *)&nsdata.eui64, eui64, 8));
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -425,6 +527,7 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_get_rw_params);
 	CU_ADD_TEST(suite, test_lba_in_range);
 	CU_ADD_TEST(suite, test_get_dif_ctx);
+	CU_ADD_TEST(suite, test_nvmf_bdev_ctrlr_identify_ns);
 
 	CU_ADD_TEST(suite, test_spdk_nvmf_bdev_ctrlr_compare_and_write_cmd);
 
diff --git a/test/unit/lib/nvmf/ctrlr_discovery.c/ctrlr_discovery_ut.c b/test/unit/lib/nvmf/ctrlr_discovery.c/ctrlr_discovery_ut.c
index a21489a57..ad627818d 100644
--- a/test/unit/lib/nvmf/ctrlr_discovery.c/ctrlr_discovery_ut.c
+++ b/test/unit/lib/nvmf/ctrlr_discovery.c/ctrlr_discovery_ut.c
@@ -37,6 +37,7 @@
 #include "spdk_internal/mock.h"
 
 #include "common/lib/test_env.c"
+#include "spdk/bdev_module.h"
 #include "nvmf/ctrlr_discovery.c"
 #include "nvmf/subsystem.c"
 
diff --git a/test/unit/lib/nvmf/fc.c/fc_ut.c b/test/unit/lib/nvmf/fc.c/fc_ut.c
index a7603a7bc..be9f9cc07 100644
--- a/test/unit/lib/nvmf/fc.c/fc_ut.c
+++ b/test/unit/lib/nvmf/fc.c/fc_ut.c
@@ -51,6 +51,7 @@
 #include "json/json_write.c"
 #include "nvmf/nvmf.c"
 #include "nvmf/transport.c"
+#include "spdk/bdev_module.h"
 #include "nvmf/subsystem.c"
 #include "nvmf/fc.c"
 #include "nvmf/fc_ls.c"
@@ -91,19 +92,9 @@ const struct spdk_nvmf_transport_ops spdk_nvmf_transport_tcp = {
 	.type = SPDK_NVME_TRANSPORT_TCP,
 };
 
-struct spdk_trace_histories *g_trace_histories;
-
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
 DEFINE_STUB(spdk_nvme_transport_id_compare, int,
 	    (const struct spdk_nvme_transport_id *trid1,
 	     const struct spdk_nvme_transport_id *trid2), 0);
-DEFINE_STUB_V(spdk_trace_register_object, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_description,
-	      (const char *name, uint16_t tpoint_id, uint8_t owner_type,
-	       uint8_t object_type, uint8_t new_object, uint8_t arg1_type,
-	       const char *arg1_name));
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
 DEFINE_STUB(spdk_bdev_get_name, const char *, (const struct spdk_bdev *bdev), "fc_ut_test");
 DEFINE_STUB_V(nvmf_ctrlr_destruct, (struct spdk_nvmf_ctrlr *ctrlr));
 DEFINE_STUB_V(nvmf_qpair_free_aer, (struct spdk_nvmf_qpair *qpair));
@@ -111,9 +102,6 @@ DEFINE_STUB(spdk_bdev_get_io_channel, struct spdk_io_channel *, (struct spdk_bde
 	    NULL);
 DEFINE_STUB_V(spdk_nvmf_request_exec, (struct spdk_nvmf_request *req));
 DEFINE_STUB_V(nvmf_ctrlr_ns_changed, (struct spdk_nvmf_ctrlr *ctrlr, uint32_t nsid));
-DEFINE_STUB(spdk_bdev_open, int, (struct spdk_bdev *bdev, bool write,
-				  spdk_bdev_remove_cb_t remove_cb,
-				  void *remove_ctx, struct spdk_bdev_desc **desc), 0);
 DEFINE_STUB_V(spdk_bdev_close, (struct spdk_bdev_desc *desc));
 DEFINE_STUB(spdk_bdev_module_claim_bdev, int,
 	    (struct spdk_bdev *bdev, struct spdk_bdev_desc *desc,
diff --git a/test/unit/lib/nvmf/rdma.c/rdma_ut.c b/test/unit/lib/nvmf/rdma.c/rdma_ut.c
index 007f34421..7ad1508d2 100644
--- a/test/unit/lib/nvmf/rdma.c/rdma_ut.c
+++ b/test/unit/lib/nvmf/rdma.c/rdma_ut.c
@@ -63,15 +63,6 @@ DEFINE_STUB(spdk_nvmf_qpair_get_listen_trid, int,
 	    (struct spdk_nvmf_qpair *qpair, struct spdk_nvme_transport_id *trid), 0);
 DEFINE_STUB_V(spdk_mem_map_free, (struct spdk_mem_map **pmap));
 
-struct spdk_trace_histories *g_trace_histories;
-DEFINE_STUB_V(spdk_trace_add_register_fn, (struct spdk_trace_register_fn *reg_fn));
-DEFINE_STUB_V(spdk_trace_register_object, (uint8_t type, char id_prefix));
-DEFINE_STUB_V(spdk_trace_register_description, (const char *name,
-		uint16_t tpoint_id, uint8_t owner_type, uint8_t object_type, uint8_t new_object,
-		uint8_t arg1_type, const char *arg1_name));
-DEFINE_STUB_V(_spdk_trace_record, (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-				   uint32_t size, uint64_t object_id, uint64_t arg1));
-
 DEFINE_STUB_V(spdk_nvmf_ctrlr_data_init, (struct spdk_nvmf_transport_opts *opts,
 		struct spdk_nvmf_ctrlr_data *cdata));
 DEFINE_STUB_V(spdk_nvmf_request_exec, (struct spdk_nvmf_request *req));
@@ -132,6 +123,7 @@ static void reset_nvmf_rdma_request(struct spdk_nvmf_rdma_request *rdma_req)
 	rdma_req->data.wr.num_sge = 0;
 	rdma_req->data.wr.wr.rdma.remote_addr = 0;
 	rdma_req->data.wr.wr.rdma.rkey = 0;
+	rdma_req->offset = 0;
 	memset(&rdma_req->req.dif, 0, sizeof(rdma_req->req.dif));
 
 	for (i = 0; i < SPDK_NVMF_MAX_SGL_ENTRIES; i++) {
@@ -162,6 +154,8 @@ test_spdk_nvmf_rdma_request_parse_sgl(void)
 	struct spdk_nvme_sgl_descriptor sgl_desc[SPDK_NVMF_MAX_SGL_ENTRIES] = {{0}};
 	struct spdk_nvmf_rdma_request_data data;
 	int rc, i;
+	uint32_t sgl_length;
+	uintptr_t aligned_buffer_address;
 
 	data.wr.sg_list = data.sgl;
 	STAILQ_INIT(&group.group.buf_cache);
@@ -362,7 +356,7 @@ test_spdk_nvmf_rdma_request_parse_sgl(void)
 	CU_ASSERT(rc == 0);
 	CU_ASSERT(rdma_req.req.data_from_pool == true);
 	CU_ASSERT(rdma_req.req.length == rtransport.transport.opts.io_unit_size * 16);
-	CU_ASSERT(rdma_req.req.iovcnt == 17);
+	CU_ASSERT(rdma_req.req.iovcnt == 16);
 	CU_ASSERT(rdma_req.data.wr.num_sge == 16);
 	for (i = 0; i < 15; i++) {
 		CU_ASSERT(rdma_req.data.sgl[i].length == rtransport.transport.opts.io_unit_size);
@@ -378,6 +372,39 @@ test_spdk_nvmf_rdma_request_parse_sgl(void)
 	CU_ASSERT(data.wr.num_sge == 1);
 	CU_ASSERT(data.wr.next == &rdma_req.rsp.wr);
 
+	/* part 4: 2 SGL descriptors, each length is transport buffer / 2
+	 * 1 transport buffers should be allocated */
+	reset_nvmf_rdma_request(&rdma_req);
+	aligned_buffer_address = ((uintptr_t)(&data) + NVMF_DATA_BUFFER_MASK) & ~NVMF_DATA_BUFFER_MASK;
+	sgl->unkeyed.length = 2 * sizeof(struct spdk_nvme_sgl_descriptor);
+	sgl_length = rtransport.transport.opts.io_unit_size / 2;
+	for (i = 0; i < 2; i++) {
+		sgl_desc[i].keyed.length = sgl_length;
+		sgl_desc[i].address = 0x4000 + i * sgl_length;
+	}
+
+	rc = nvmf_rdma_request_parse_sgl(&rtransport, &device, &rdma_req);
+
+	CU_ASSERT(rc == 0);
+	CU_ASSERT(rdma_req.req.data_from_pool == true);
+	CU_ASSERT(rdma_req.req.length == rtransport.transport.opts.io_unit_size);
+	CU_ASSERT(rdma_req.req.iovcnt == 1);
+
+	CU_ASSERT(rdma_req.data.sgl[0].length == sgl_length);
+	/* We mocked mempool_get to return address of data variable. Mempool is used
+	 * to get both additional WRs and data buffers, so data points to &data */
+	CU_ASSERT(rdma_req.data.sgl[0].addr == aligned_buffer_address);
+	CU_ASSERT(rdma_req.data.wr.wr.rdma.rkey == 0x44);
+	CU_ASSERT(rdma_req.data.wr.wr.rdma.remote_addr == 0x4000);
+	CU_ASSERT(rdma_req.data.wr.num_sge == 1);
+	CU_ASSERT(rdma_req.data.wr.next == &data.wr);
+
+	CU_ASSERT(data.wr.wr.rdma.rkey == 0x44);
+	CU_ASSERT(data.wr.wr.rdma.remote_addr == 0x4000 + sgl_length);
+	CU_ASSERT(data.sgl[0].length == sgl_length);
+	CU_ASSERT(data.sgl[0].addr == aligned_buffer_address + sgl_length);
+	CU_ASSERT(data.wr.num_sge == 1);
+
 	/* Test 4: use PG buffer cache */
 	sgl->generic.type = SPDK_NVME_SGL_TYPE_KEYED_DATA_BLOCK;
 	sgl->keyed.subtype = SPDK_NVME_SGL_SUBTYPE_ADDRESS;
@@ -877,7 +904,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = data_bs * 8;
 	sgl->keyed.length = data_bs * 4;
 
@@ -907,7 +934,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = data_bs * 4;
 	sgl->keyed.length = data_bs * 4;
 
@@ -944,7 +971,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = data_bs;
 	sgl->keyed.length = data_bs;
 
@@ -979,7 +1006,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = (data_bs + md_size) * 4;
 	sgl->keyed.length = data_bs * 4;
 
@@ -1009,7 +1036,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = (data_bs + md_size) * 2;
 	sgl->keyed.length = data_bs * 4;
 
@@ -1042,7 +1069,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = data_bs * 4;
 	sgl->keyed.length = data_bs * 6;
 
@@ -1090,7 +1117,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = data_bs * 16;
 	sgl->keyed.length = data_bs * 16;
 
@@ -1106,10 +1133,26 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	CU_ASSERT(rdma_req.data.wr.num_sge == 16);
 	CU_ASSERT(rdma_req.data.wr.wr.rdma.rkey == 0xEEEE);
 	CU_ASSERT(rdma_req.data.wr.wr.rdma.remote_addr == 0xFFFF);
+
+	for (i = 0; i < 15; ++i) {
+		CU_ASSERT(rdma_req.data.wr.sg_list[i].addr == (uintptr_t)aligned_buffer + i * (data_bs + md_size));
+		CU_ASSERT(rdma_req.data.wr.sg_list[i].length == data_bs);
+		CU_ASSERT(rdma_req.data.wr.sg_list[i].lkey == RDMA_UT_LKEY);
+	}
+
+	/* 8192 - (512 + 8) * 15 = 392 */
+	CU_ASSERT(rdma_req.data.wr.sg_list[i].addr == (uintptr_t)aligned_buffer + i * (data_bs + md_size));
+	CU_ASSERT(rdma_req.data.wr.sg_list[i].length == 392);
+	CU_ASSERT(rdma_req.data.wr.sg_list[i].lkey == RDMA_UT_LKEY);
+
 	/* additional wr from pool */
 	CU_ASSERT(rdma_req.data.wr.next == (void *)&data2->wr);
 	CU_ASSERT(rdma_req.data.wr.next->num_sge == 1);
 	CU_ASSERT(rdma_req.data.wr.next->next == &rdma_req.rsp.wr);
+	/* 2nd IO buffer */
+	CU_ASSERT(data2->wr.sg_list[0].addr == (uintptr_t)aligned_buffer);
+	CU_ASSERT(data2->wr.sg_list[0].length == 120);
+	CU_ASSERT(data2->wr.sg_list[0].lkey == RDMA_UT_LKEY);
 
 	/* Part 8: simple I/O, data with metadata do not fit to 1 io_buffer */
 	MOCK_SET(spdk_mempool_get, (void *)0x2000);
@@ -1117,7 +1160,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1, SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK,
 			  0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = 516;
 	sgl->keyed.length = data_bs * 2;
 
@@ -1158,7 +1201,7 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	spdk_dif_ctx_init(&rdma_req.req.dif.dif_ctx, data_bs + md_size, md_size, true, false,
 			  SPDK_DIF_TYPE1,
 			  SPDK_DIF_FLAGS_GUARD_CHECK | SPDK_DIF_FLAGS_REFTAG_CHECK, 0, 0, 0, 0, 0);
-	rdma_req.req.dif.dif_insert_or_strip = true;
+	rdma_req.req.dif_enabled = true;
 	rtransport.transport.opts.io_unit_size = (data_bs + md_size) * 4;
 	sgl->unkeyed.length = 2 * sizeof(struct spdk_nvme_sgl_descriptor);
 
@@ -1199,6 +1242,56 @@ test_spdk_nvmf_rdma_request_parse_sgl_with_md(void)
 	CU_ASSERT(data->wr.next == &rdma_req.rsp.wr);
 }
 
+static void
+test_nvmf_rdma_opts_init(void)
+{
+	struct spdk_nvmf_transport_opts	opts = {};
+
+	nvmf_rdma_opts_init(&opts);
+	CU_ASSERT(opts.max_queue_depth == SPDK_NVMF_RDMA_DEFAULT_MAX_QUEUE_DEPTH);
+	CU_ASSERT(opts.max_qpairs_per_ctrlr ==	SPDK_NVMF_RDMA_DEFAULT_MAX_QPAIRS_PER_CTRLR);
+	CU_ASSERT(opts.in_capsule_data_size ==	SPDK_NVMF_RDMA_DEFAULT_IN_CAPSULE_DATA_SIZE);
+	CU_ASSERT(opts.max_io_size == SPDK_NVMF_RDMA_DEFAULT_MAX_IO_SIZE);
+	CU_ASSERT(opts.io_unit_size == SPDK_NVMF_RDMA_MIN_IO_BUFFER_SIZE);
+	CU_ASSERT(opts.max_aq_depth == SPDK_NVMF_RDMA_DEFAULT_AQ_DEPTH);
+	CU_ASSERT(opts.num_shared_buffers == SPDK_NVMF_RDMA_DEFAULT_NUM_SHARED_BUFFERS);
+	CU_ASSERT(opts.buf_cache_size == SPDK_NVMF_RDMA_DEFAULT_BUFFER_CACHE_SIZE);
+	CU_ASSERT(opts.dif_insert_or_strip == SPDK_NVMF_RDMA_DIF_INSERT_OR_STRIP);
+	CU_ASSERT(opts.abort_timeout_sec == SPDK_NVMF_RDMA_DEFAULT_ABORT_TIMEOUT_SEC);
+	CU_ASSERT(opts.transport_specific == NULL);
+}
+
+static void
+test_nvmf_rdma_request_free_data(void)
+{
+	struct spdk_nvmf_rdma_request rdma_req = {};
+	struct spdk_nvmf_rdma_transport rtransport = {};
+	struct spdk_nvmf_rdma_request_data *next_request_data = NULL;
+
+	MOCK_CLEAR(spdk_mempool_get);
+	rtransport.data_wr_pool = spdk_mempool_create("spdk_nvmf_rdma_wr_data",
+				  SPDK_NVMF_MAX_SGL_ENTRIES,
+				  sizeof(struct spdk_nvmf_rdma_request_data),
+				  SPDK_MEMPOOL_DEFAULT_CACHE_SIZE,
+				  SPDK_ENV_SOCKET_ID_ANY);
+	next_request_data = spdk_mempool_get(rtransport.data_wr_pool);
+	SPDK_CU_ASSERT_FATAL(((struct test_mempool *)rtransport.data_wr_pool)->count ==
+			     SPDK_NVMF_MAX_SGL_ENTRIES - 1);
+	next_request_data->wr.wr_id = 1;
+	next_request_data->wr.num_sge = 2;
+	next_request_data->wr.next = NULL;
+	rdma_req.data.wr.next = &next_request_data->wr;
+	rdma_req.data.wr.wr_id = 1;
+	rdma_req.data.wr.num_sge = 2;
+
+	nvmf_rdma_request_free_data(&rdma_req, &rtransport);
+	/* Check if next_request_data put into memory pool */
+	CU_ASSERT(((struct test_mempool *)rtransport.data_wr_pool)->count == SPDK_NVMF_MAX_SGL_ENTRIES);
+	CU_ASSERT(rdma_req.data.wr.num_sge == 0);
+
+	spdk_mempool_free(rtransport.data_wr_pool);
+}
+
 int main(int argc, char **argv)
 {
 	CU_pSuite	suite = NULL;
@@ -1213,6 +1306,8 @@ int main(int argc, char **argv)
 	CU_ADD_TEST(suite, test_spdk_nvmf_rdma_request_process);
 	CU_ADD_TEST(suite, test_nvmf_rdma_get_optimal_poll_group);
 	CU_ADD_TEST(suite, test_spdk_nvmf_rdma_request_parse_sgl_with_md);
+	CU_ADD_TEST(suite, test_nvmf_rdma_opts_init);
+	CU_ADD_TEST(suite, test_nvmf_rdma_request_free_data);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/nvmf/subsystem.c/subsystem_ut.c b/test/unit/lib/nvmf/subsystem.c/subsystem_ut.c
index 3263eeb66..2c113acd5 100644
--- a/test/unit/lib/nvmf/subsystem.c/subsystem_ut.c
+++ b/test/unit/lib/nvmf/subsystem.c/subsystem_ut.c
@@ -37,6 +37,7 @@
 #include "spdk_cunit.h"
 #include "spdk_internal/mock.h"
 
+#include "spdk/bdev_module.h"
 #include "nvmf/subsystem.c"
 
 SPDK_LOG_REGISTER_COMPONENT(nvmf)
diff --git a/test/unit/lib/nvmf/tcp.c/tcp_ut.c b/test/unit/lib/nvmf/tcp.c/tcp_ut.c
index b96f9e854..b9ac4595d 100644
--- a/test/unit/lib/nvmf/tcp.c/tcp_ut.c
+++ b/test/unit/lib/nvmf/tcp.c/tcp_ut.c
@@ -246,8 +246,6 @@ DEFINE_STUB(spdk_nvmf_bdev_ctrlr_nvme_passthru_admin,
 	     spdk_nvmf_nvme_passthru_cmd_cb cb_fn),
 	    0)
 
-struct spdk_trace_histories *g_trace_histories;
-
 struct spdk_bdev {
 	int ut_mock;
 	uint64_t blockcnt;
@@ -260,25 +258,6 @@ spdk_nvme_transport_id_compare(const struct spdk_nvme_transport_id *trid1,
 	return 0;
 }
 
-void
-spdk_trace_register_object(uint8_t type, char id_prefix)
-{
-}
-
-void
-spdk_trace_register_description(const char *name,
-				uint16_t tpoint_id, uint8_t owner_type,
-				uint8_t object_type, uint8_t new_object,
-				uint8_t arg1_type, const char *arg1_name)
-{
-}
-
-void
-_spdk_trace_record(uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-		   uint32_t size, uint64_t object_id, uint64_t arg1)
-{
-}
-
 const char *
 spdk_nvme_transport_id_trtype_str(enum spdk_nvme_transport_type trtype)
 {
@@ -367,11 +346,6 @@ spdk_nvmf_subsystem_get_mn(const struct spdk_nvmf_subsystem *subsystem)
 	return subsystem->mn;
 }
 
-void
-spdk_trace_add_register_fn(struct spdk_trace_register_fn *reg_fn)
-{
-}
-
 static void
 test_nvmf_tcp_create(void)
 {
@@ -559,6 +533,7 @@ test_nvmf_tcp_send_c2h_data(void)
 
 	tcp_req.pdu = &pdu;
 	tcp_req.req.length = 300;
+	tcp_req.req.qpair = &tqpair.qpair;
 
 	tqpair.qpair.transport = &ttransport.transport;
 
@@ -681,7 +656,7 @@ test_nvmf_tcp_incapsule_data_handle(void)
 {
 	struct spdk_nvmf_tcp_transport ttransport = {};
 	struct spdk_nvmf_tcp_qpair tqpair = {};
-	struct nvme_tcp_pdu *pdu;
+	struct nvme_tcp_pdu *pdu, pdu_in_progress = {};
 	union nvmf_c2h_msg rsp0 = {};
 	union nvmf_c2h_msg rsp = {};
 
@@ -697,6 +672,7 @@ test_nvmf_tcp_incapsule_data_handle(void)
 	struct spdk_nvmf_tcp_poll_group tcp_group = {};
 	struct spdk_sock_group grp = {};
 
+	tqpair.pdu_in_progress = &pdu_in_progress;
 	ttransport.transport.opts.max_io_size = UT_MAX_IO_SIZE;
 	ttransport.transport.opts.io_unit_size = UT_IO_UNIT_SIZE;
 
@@ -732,7 +708,7 @@ test_nvmf_tcp_incapsule_data_handle(void)
 	tqpair.state_cntr[TCP_REQUEST_STATE_NEW]++;
 
 	/* init pdu, make pdu need sgl buff */
-	pdu = &tqpair.pdu_in_progress;
+	pdu = tqpair.pdu_in_progress;
 	capsule_data = &pdu->hdr.capsule_cmd;
 	nvmf_capsule_data = (struct spdk_nvmf_capsule_cmd *)&pdu->hdr.capsule_cmd.ccsqe;
 	sgl = &capsule_data->ccsqe.dptr.sgl1;
@@ -755,7 +731,7 @@ test_nvmf_tcp_incapsule_data_handle(void)
 	sgl->unkeyed.length = UT_IO_UNIT_SIZE - 1;
 
 	/* process tqpair capsule req. but we still remain req in pending_buff. */
-	nvmf_tcp_capsule_cmd_hdr_handle(&ttransport, &tqpair, &tqpair.pdu_in_progress);
+	nvmf_tcp_capsule_cmd_hdr_handle(&ttransport, &tqpair, tqpair.pdu_in_progress);
 	CU_ASSERT(tqpair.recv_state == NVME_TCP_PDU_RECV_STATE_AWAIT_PDU_PAYLOAD);
 	CU_ASSERT(STAILQ_FIRST(&group->pending_buf_queue) == &tcp_req1.req);
 	STAILQ_FOREACH(req_temp, &group->pending_buf_queue, buf_link) {
@@ -764,7 +740,7 @@ test_nvmf_tcp_incapsule_data_handle(void)
 		}
 	}
 	CU_ASSERT(req_temp == NULL);
-	CU_ASSERT(tqpair.pdu_in_progress.req == (void *)&tcp_req2);
+	CU_ASSERT(tqpair.pdu_in_progress->req == (void *)&tcp_req2);
 }
 
 int main(int argc, char **argv)
diff --git a/test/unit/lib/scsi/lun.c/lun_ut.c b/test/unit/lib/scsi/lun.c/lun_ut.c
index 5501537c0..8acc7654f 100644
--- a/test/unit/lib/scsi/lun.c/lun_ut.c
+++ b/test/unit/lib/scsi/lun.c/lun_ut.c
@@ -51,18 +51,10 @@ struct spdk_bdev {
 
 SPDK_LOG_REGISTER_COMPONENT(scsi)
 
-struct spdk_scsi_globals g_spdk_scsi;
-
 static bool g_lun_execute_fail = false;
 static int g_lun_execute_status = SPDK_SCSI_TASK_PENDING;
 static uint32_t g_task_count = 0;
 
-struct spdk_trace_histories *g_trace_histories;
-
-DEFINE_STUB_V(_spdk_trace_record,
-	      (uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
-	       uint32_t size, uint64_t object_id, uint64_t arg1));
-
 DEFINE_STUB(bdev_scsi_get_dif_ctx, bool,
 	    (struct spdk_bdev *bdev, struct spdk_scsi_task *task,
 	     struct spdk_dif_ctx *dif_ctx), false);
@@ -94,11 +86,6 @@ spdk_bdev_free_io(struct spdk_bdev_io *bdev_io)
 	CU_ASSERT(0);
 }
 
-DEFINE_STUB(spdk_bdev_open, int,
-	    (struct spdk_bdev *bdev, bool write, spdk_bdev_remove_cb_t remove_cb,
-	     void *remove_ctx, struct spdk_bdev_desc **desc),
-	    0);
-
 DEFINE_STUB(spdk_bdev_open_ext, int,
 	    (const char *bdev_name, bool write, spdk_bdev_event_cb_t event_cb,
 	     void *event_ctx, struct spdk_bdev_desc **desc),
diff --git a/test/unit/lib/scsi/scsi_bdev.c/scsi_bdev_ut.c b/test/unit/lib/scsi/scsi_bdev.c/scsi_bdev_ut.c
index 87485b901..1632c6b81 100644
--- a/test/unit/lib/scsi/scsi_bdev.c/scsi_bdev_ut.c
+++ b/test/unit/lib/scsi/scsi_bdev.c/scsi_bdev_ut.c
@@ -40,11 +40,10 @@
 #include "spdk_cunit.h"
 
 #include "spdk_internal/mock.h"
+#include "spdk/bdev_module.h"
 
 SPDK_LOG_REGISTER_COMPONENT(scsi)
 
-struct spdk_scsi_globals g_spdk_scsi;
-
 static uint64_t g_test_bdev_num_blocks;
 
 TAILQ_HEAD(, spdk_bdev_io) g_bdev_io_queue;
@@ -77,6 +76,9 @@ DEFINE_STUB(spdk_bdev_is_md_interleaved, bool,
 DEFINE_STUB(spdk_bdev_get_data_block_size, uint32_t,
 	    (const struct spdk_bdev *bdev), 512);
 
+DEFINE_STUB(spdk_bdev_get_physical_block_size, uint32_t,
+	    (const struct spdk_bdev *bdev), 4096);
+
 uint64_t
 spdk_bdev_get_num_blocks(const struct spdk_bdev *bdev)
 {
diff --git a/test/unit/lib/thread/thread.c/thread_ut.c b/test/unit/lib/thread/thread.c/thread_ut.c
index 11e3bf3c9..fe29d4e9a 100644
--- a/test/unit/lib/thread/thread.c/thread_ut.c
+++ b/test/unit/lib/thread/thread.c/thread_ut.c
@@ -35,7 +35,7 @@
 
 #include "spdk_cunit.h"
 
-#include "spdk_internal/thread.h"
+#include "thread/thread_internal.h"
 
 #include "thread/thread.c"
 #include "common/lib/ut_multithread.c"
@@ -244,6 +244,23 @@ poller_run_pause(void *ctx)
 	return 0;
 }
 
+/* Verify the same poller can be switched multiple times between
+ * pause and resume while it runs.
+ */
+static int
+poller_run_pause_resume_pause(void *ctx)
+{
+	struct poller_ctx *poller_ctx = ctx;
+
+	poller_ctx->run = true;
+
+	spdk_poller_pause(poller_ctx->poller);
+	spdk_poller_resume(poller_ctx->poller);
+	spdk_poller_pause(poller_ctx->poller);
+
+	return 0;
+}
+
 static void
 poller_msg_pause_cb(void *ctx)
 {
@@ -285,6 +302,21 @@ poller_pause(void)
 	spdk_poller_unregister(&poller_ctx.poller);
 	CU_ASSERT_PTR_NULL(poller_ctx.poller);
 
+	/* Register a poller that switches between pause and resume itself */
+	poller_ctx.poller = spdk_poller_register(poller_run_pause_resume_pause, &poller_ctx, 0);
+	CU_ASSERT_PTR_NOT_NULL(poller_ctx.poller);
+
+	poller_ctx.run = false;
+	poll_threads();
+	CU_ASSERT_EQUAL(poller_ctx.run, true);
+
+	poller_ctx.run = false;
+	poll_threads();
+	CU_ASSERT_EQUAL(poller_ctx.run, false);
+
+	spdk_poller_unregister(&poller_ctx.poller);
+	CU_ASSERT_PTR_NULL(poller_ctx.poller);
+
 	/* Verify that resuming an unpaused poller doesn't do anything */
 	poller_ctx.poller = spdk_poller_register(poller_run_done, &poller_ctx.run, 0);
 	CU_ASSERT_PTR_NOT_NULL(poller_ctx.poller);
@@ -386,6 +418,55 @@ poller_pause(void)
 
 		spdk_poller_unregister(&poller_ctx.poller);
 		CU_ASSERT_PTR_NULL(poller_ctx.poller);
+
+		/* Register a timed poller that pauses itself */
+		poller_ctx.poller = spdk_poller_register(poller_run_pause, &poller_ctx, delay[i]);
+		CU_ASSERT_PTR_NOT_NULL(poller_ctx.poller);
+
+		spdk_delay_us(delay[i]);
+		poller_ctx.run = false;
+		poll_threads();
+		CU_ASSERT_EQUAL(poller_ctx.run, true);
+
+		poller_ctx.run = false;
+		spdk_delay_us(delay[i]);
+		poll_threads();
+		CU_ASSERT_EQUAL(poller_ctx.run, false);
+
+		spdk_poller_resume(poller_ctx.poller);
+
+		CU_ASSERT_EQUAL(poller_ctx.run, false);
+		spdk_delay_us(delay[i]);
+		poll_threads();
+		CU_ASSERT_EQUAL(poller_ctx.run, true);
+
+		spdk_poller_unregister(&poller_ctx.poller);
+		CU_ASSERT_PTR_NULL(poller_ctx.poller);
+
+		/* Register a timed poller that switches between pause and resume itself */
+		poller_ctx.poller = spdk_poller_register(poller_run_pause_resume_pause,
+				    &poller_ctx, delay[i]);
+		CU_ASSERT_PTR_NOT_NULL(poller_ctx.poller);
+
+		spdk_delay_us(delay[i]);
+		poller_ctx.run = false;
+		poll_threads();
+		CU_ASSERT_EQUAL(poller_ctx.run, true);
+
+		poller_ctx.run = false;
+		spdk_delay_us(delay[i]);
+		poll_threads();
+		CU_ASSERT_EQUAL(poller_ctx.run, false);
+
+		spdk_poller_resume(poller_ctx.poller);
+
+		CU_ASSERT_EQUAL(poller_ctx.run, false);
+		spdk_delay_us(delay[i]);
+		poll_threads();
+		CU_ASSERT_EQUAL(poller_ctx.run, true);
+
+		spdk_poller_unregister(&poller_ctx.poller);
+		CU_ASSERT_PTR_NULL(poller_ctx.poller);
 	}
 
 	free_threads();
@@ -1332,6 +1413,269 @@ device_unregister_and_thread_exit_race(void)
 	free_threads();
 }
 
+static int
+dummy_poller(void *arg)
+{
+	return SPDK_POLLER_IDLE;
+}
+
+static void
+cache_closest_timed_poller(void)
+{
+	struct spdk_thread *thread;
+	struct spdk_poller *poller1, *poller2, *poller3, *tmp;
+
+	allocate_threads(1);
+	set_thread(0);
+
+	thread = spdk_get_thread();
+	SPDK_CU_ASSERT_FATAL(thread != NULL);
+
+	poller1 = spdk_poller_register(dummy_poller, NULL, 1000);
+	SPDK_CU_ASSERT_FATAL(poller1 != NULL);
+
+	poller2 = spdk_poller_register(dummy_poller, NULL, 1500);
+	SPDK_CU_ASSERT_FATAL(poller2 != NULL);
+
+	poller3 = spdk_poller_register(dummy_poller, NULL, 1800);
+	SPDK_CU_ASSERT_FATAL(poller3 != NULL);
+
+	poll_threads();
+
+	/* When multiple timed pollers are inserted, the cache should
+	 * have the closest timed poller.
+	 */
+	CU_ASSERT(thread->first_timed_poller == poller1);
+	CU_ASSERT(RB_MIN(timed_pollers_tree, &thread->timed_pollers) == poller1);
+
+	spdk_delay_us(1000);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == poller2);
+	CU_ASSERT(RB_MIN(timed_pollers_tree, &thread->timed_pollers) == poller2);
+
+	/* If we unregister a timed poller by spdk_poller_unregister()
+	 * when it is waiting, it is marked as being unregistereed and
+	 * is actually unregistered when it is expired.
+	 *
+	 * Hence if we unregister the closest timed poller when it is waiting,
+	 * the cache is not updated to the next timed poller until it is expired.
+	 */
+	tmp = poller2;
+
+	spdk_poller_unregister(&poller2);
+	CU_ASSERT(poller2 == NULL);
+
+	spdk_delay_us(499);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == tmp);
+	CU_ASSERT(RB_MIN(timed_pollers_tree, &thread->timed_pollers) == tmp);
+
+	spdk_delay_us(1);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == poller3);
+	CU_ASSERT(RB_MIN(timed_pollers_tree, &thread->timed_pollers) == poller3);
+
+	/* If we pause a timed poller by spdk_poller_pause() when it is waiting,
+	 * it is marked as being paused and is actually paused when it is expired.
+	 *
+	 * Hence if we pause the closest timed poller when it is waiting, the cache
+	 * is not updated to the next timed poller until it is expired.
+	 */
+	spdk_poller_pause(poller3);
+
+	spdk_delay_us(299);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == poller3);
+	CU_ASSERT(RB_MIN(timed_pollers_tree, &thread->timed_pollers) == poller3);
+
+	spdk_delay_us(1);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == poller1);
+	CU_ASSERT(RB_MIN(timed_pollers_tree, &thread->timed_pollers) == poller1);
+
+	/* After unregistering all timed pollers, the cache should
+	 * be NULL.
+	 */
+	spdk_poller_unregister(&poller1);
+	spdk_poller_unregister(&poller3);
+
+	spdk_delay_us(200);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == NULL);
+	CU_ASSERT(RB_EMPTY(&thread->timed_pollers));
+
+	free_threads();
+}
+
+static void
+multi_timed_pollers_have_same_expiration(void)
+{
+	struct spdk_thread *thread;
+	struct spdk_poller *poller1, *poller2, *poller3, *poller4, *tmp;
+	uint64_t start_ticks;
+
+	allocate_threads(1);
+	set_thread(0);
+
+	thread = spdk_get_thread();
+	SPDK_CU_ASSERT_FATAL(thread != NULL);
+
+	/*
+	 * case 1: multiple timed pollers have the same next_run_tick.
+	 */
+	start_ticks = spdk_get_ticks();
+
+	poller1 = spdk_poller_register(dummy_poller, NULL, 500);
+	SPDK_CU_ASSERT_FATAL(poller1 != NULL);
+
+	poller2 = spdk_poller_register(dummy_poller, NULL, 500);
+	SPDK_CU_ASSERT_FATAL(poller2 != NULL);
+
+	poller3 = spdk_poller_register(dummy_poller, NULL, 1000);
+	SPDK_CU_ASSERT_FATAL(poller3 != NULL);
+
+	poller4 = spdk_poller_register(dummy_poller, NULL, 1500);
+	SPDK_CU_ASSERT_FATAL(poller4 != NULL);
+
+	/* poller1 and poller2 have the same next_run_tick but cache has poller1
+	 * because poller1 is registered earlier than poller2.
+	 */
+	CU_ASSERT(thread->first_timed_poller == poller1);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 500);
+	CU_ASSERT(poller2->next_run_tick == start_ticks + 500);
+	CU_ASSERT(poller3->next_run_tick == start_ticks + 1000);
+	CU_ASSERT(poller4->next_run_tick == start_ticks + 1500);
+
+	/* after 500 usec, poller1 and poller2 are expired. */
+	spdk_delay_us(500);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 500);
+	poll_threads();
+
+	/* poller1, poller2, and poller3 have the same next_run_tick but cache
+	 * has poller3 because poller3 is not expired yet.
+	 */
+	CU_ASSERT(thread->first_timed_poller == poller3);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 1000);
+	CU_ASSERT(poller2->next_run_tick == start_ticks + 1000);
+	CU_ASSERT(poller3->next_run_tick == start_ticks + 1000);
+	CU_ASSERT(poller4->next_run_tick == start_ticks + 1500);
+
+	/* after 500 usec, poller1, poller2, and poller3 are expired. */
+	spdk_delay_us(500);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 1000);
+	poll_threads();
+
+	/* poller1, poller2, and poller4 have the same next_run_tick but cache
+	 * has poller4 because poller4 is not expired yet.
+	 */
+	CU_ASSERT(thread->first_timed_poller == poller4);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 1500);
+	CU_ASSERT(poller2->next_run_tick == start_ticks + 1500);
+	CU_ASSERT(poller3->next_run_tick == start_ticks + 2000);
+	CU_ASSERT(poller4->next_run_tick == start_ticks + 1500);
+
+	/* after 500 usec, poller1, poller2, and poller4 are expired. */
+	spdk_delay_us(500);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 1500);
+	poll_threads();
+
+	/* poller1, poller2, and poller3 have the same next_run_tick but cache
+	 * has poller3 because poller3 is updated earlier than poller1 and poller2.
+	 */
+	CU_ASSERT(thread->first_timed_poller == poller3);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 2000);
+	CU_ASSERT(poller2->next_run_tick == start_ticks + 2000);
+	CU_ASSERT(poller3->next_run_tick == start_ticks + 2000);
+	CU_ASSERT(poller4->next_run_tick == start_ticks + 3000);
+
+	spdk_poller_unregister(&poller1);
+	spdk_poller_unregister(&poller2);
+	spdk_poller_unregister(&poller3);
+	spdk_poller_unregister(&poller4);
+
+	spdk_delay_us(1500);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 3000);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == NULL);
+	CU_ASSERT(RB_EMPTY(&thread->timed_pollers));
+
+	/*
+	 * case 2: unregister timed pollers while multiple timed pollers are registered.
+	 */
+	start_ticks = spdk_get_ticks();
+
+	poller1 = spdk_poller_register(dummy_poller, NULL, 500);
+	SPDK_CU_ASSERT_FATAL(poller1 != NULL);
+
+	CU_ASSERT(thread->first_timed_poller == poller1);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 500);
+
+	/* after 250 usec, register poller2 and poller3. */
+	spdk_delay_us(250);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 250);
+
+	poller2 = spdk_poller_register(dummy_poller, NULL, 500);
+	SPDK_CU_ASSERT_FATAL(poller2 != NULL);
+
+	poller3 = spdk_poller_register(dummy_poller, NULL, 750);
+	SPDK_CU_ASSERT_FATAL(poller3 != NULL);
+
+	CU_ASSERT(thread->first_timed_poller == poller1);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 500);
+	CU_ASSERT(poller2->next_run_tick == start_ticks + 750);
+	CU_ASSERT(poller3->next_run_tick == start_ticks + 1000);
+
+	/* unregister poller2 which is not the closest. */
+	tmp = poller2;
+	spdk_poller_unregister(&poller2);
+
+	/* after 250 usec, poller1 is expired. */
+	spdk_delay_us(250);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 500);
+	poll_threads();
+
+	/* poller2 is not unregistered yet because it is not expired. */
+	CU_ASSERT(thread->first_timed_poller == tmp);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 1000);
+	CU_ASSERT(tmp->next_run_tick == start_ticks + 750);
+	CU_ASSERT(poller3->next_run_tick == start_ticks + 1000);
+
+	spdk_delay_us(250);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 750);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == poller3);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 1000);
+	CU_ASSERT(poller3->next_run_tick == start_ticks + 1000);
+
+	spdk_poller_unregister(&poller3);
+
+	spdk_delay_us(250);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 1000);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == poller1);
+	CU_ASSERT(poller1->next_run_tick == start_ticks + 1500);
+
+	spdk_poller_unregister(&poller1);
+
+	spdk_delay_us(500);
+	CU_ASSERT(spdk_get_ticks() == start_ticks + 1500);
+	poll_threads();
+
+	CU_ASSERT(thread->first_timed_poller == NULL);
+	CU_ASSERT(RB_EMPTY(&thread->timed_pollers));
+
+	free_threads();
+}
+
 int
 main(int argc, char **argv)
 {
@@ -1357,6 +1701,8 @@ main(int argc, char **argv)
 	CU_ADD_TEST(suite, thread_update_stats_test);
 	CU_ADD_TEST(suite, nested_channel);
 	CU_ADD_TEST(suite, device_unregister_and_thread_exit_race);
+	CU_ADD_TEST(suite, cache_closest_timed_poller);
+	CU_ADD_TEST(suite, multi_timed_pollers_have_same_expiration);
 
 	CU_basic_set_mode(CU_BRM_VERBOSE);
 	CU_basic_run_tests();
diff --git a/test/unit/lib/util/crc32c.c/crc32c_ut.c b/test/unit/lib/util/crc32c.c/crc32c_ut.c
index 6313d7bf6..3b857bab2 100644
--- a/test/unit/lib/util/crc32c.c/crc32c_ut.c
+++ b/test/unit/lib/util/crc32c.c/crc32c_ut.c
@@ -42,7 +42,8 @@ static void
 test_crc32c(void)
 {
 	uint32_t crc;
-	char buf[1024];
+	char buf[1024], buf1[1024];
+	struct iovec iov[2] = {};
 
 	/* Verify a string's CRC32-C value against the known correct result. */
 	snprintf(buf, sizeof(buf), "%s", "Hello world!");
@@ -51,6 +52,26 @@ test_crc32c(void)
 	crc ^= 0xFFFFFFFFu;
 	CU_ASSERT(crc == 0x7b98e751);
 
+	crc = 0xFFFFFFFFu;
+	iov[0].iov_base = buf;
+	iov[0].iov_len = strlen(buf);
+	crc = spdk_crc32c_iov_update(iov, 1, crc);
+	crc ^= 0xFFFFFFFFu;
+	CU_ASSERT(crc == 0x7b98e751);
+
+	crc = 0xFFFFFFFFu;
+	snprintf(buf, sizeof(buf), "%s", "Hello");
+	iov[0].iov_base = buf;
+	iov[0].iov_len = strlen(buf);
+
+	snprintf(buf1, sizeof(buf1), "%s", " world!");
+	iov[1].iov_base = buf1;
+	iov[1].iov_len = strlen(buf1);
+
+	crc = spdk_crc32c_iov_update(iov, 2, crc);
+	crc ^= 0xFFFFFFFFu;
+	CU_ASSERT(crc == 0x7b98e751);
+
 	/*
 	 * The main loop of the optimized CRC32-C implementation processes data in 8-byte blocks,
 	 * followed by a loop to handle the 0-7 trailing bytes.
diff --git a/test/unit/unittest.sh b/test/unit/unittest.sh
index 24d78b491..9d58cfc83 100755
--- a/test/unit/unittest.sh
+++ b/test/unit/unittest.sh
@@ -40,7 +40,6 @@ function unittest_blob() {
 }
 
 function unittest_event() {
-	$valgrind $testdir/lib/event/subsystem.c/subsystem_ut
 	$valgrind $testdir/lib/event/app.c/app_ut
 	$valgrind $testdir/lib/event/reactor.c/reactor_ut
 }
@@ -87,6 +86,7 @@ function unittest_nvme() {
 	$valgrind $testdir/lib/nvme/nvme_io_msg.c/nvme_io_msg_ut
 	$valgrind $testdir/lib/nvme/nvme_pcie_common.c/nvme_pcie_common_ut
 	$valgrind $testdir/lib/nvme/nvme_fabric.c/nvme_fabric_ut
+	$valgrind $testdir/lib/nvme/nvme_opal.c/nvme_opal_ut
 }
 
 function unittest_nvmf() {
@@ -128,6 +128,10 @@ function unittest_util() {
 	$valgrind $testdir/lib/util/pipe.c/pipe_ut
 }
 
+function unittest_init() {
+	$valgrind $testdir/lib/init/subsystem.c/subsystem_ut
+}
+
 # if ASAN is enabled, use it.  If not use valgrind if installed but allow
 # the env variable to override the default shown below.
 if [ -z ${valgrind+x} ]; then
@@ -201,6 +205,7 @@ run_test "unittest_accel" $valgrind $testdir/lib/accel/accel.c/accel_engine_ut
 run_test "unittest_ioat" $valgrind $testdir/lib/ioat/ioat.c/ioat_ut
 if grep -q '#define SPDK_CONFIG_IDXD 1' $rootdir/include/spdk/config.h; then
 	run_test "unittest_idxd" $valgrind $testdir/lib/idxd/idxd.c/idxd_ut
+	run_test "unittest_idxd_user" $valgrind $testdir/lib/idxd/idxd_user.c/idxd_user_ut
 fi
 run_test "unittest_iscsi" unittest_iscsi
 run_test "unittest_json" unittest_json
@@ -234,6 +239,8 @@ if grep -q '#define SPDK_CONFIG_VHOST 1' $rootdir/include/spdk/config.h; then
 	run_test "unittest_vhost" $valgrind $testdir/lib/vhost/vhost.c/vhost_ut
 fi
 
+run_test "unittest_init" unittest_init
+
 if [ "$cov_avail" = "yes" ] && ! [[ "$CC_TYPE" == *"clang"* ]]; then
 	$LCOV -q -d . -c -t "$(hostname)" -o $UT_COVERAGE/ut_cov_test.info
 	$LCOV -q -a $UT_COVERAGE/ut_cov_base.info -a $UT_COVERAGE/ut_cov_test.info -o $UT_COVERAGE/ut_cov_total.info
diff --git a/test/vhost/common.sh b/test/vhost/common.sh
index 68303726d..3c5c600f5 100644
--- a/test/vhost/common.sh
+++ b/test/vhost/common.sh
@@ -9,6 +9,10 @@ TARGET_DIR=$VHOST_DIR/vhost
 VM_PASSWORD="root"
 
 VM_IMAGE=${VM_IMAGE:-"$DEPENDENCY_DIR/spdk_test_image.qcow2"}
+DEFAULT_FIO_BIN=${DEFAULT_FIO_BIN:-"$DEPENDENCY_DIR/fio"}
+FIO_BIN=${FIO_BIN:-"$DEFAULT_FIO_BIN"}
+
+WORKDIR=$(readlink -f "$(dirname "$0")")
 
 if ! hash $QEMU_IMG_BIN $QEMU_BIN; then
 	error 'QEMU is not installed on this system. Unable to run vhost tests.'
diff --git a/test/vhost/lvol/lvol_test.sh b/test/vhost/lvol/lvol_test.sh
index 6920eff87..fda77acc7 100755
--- a/test/vhost/lvol/lvol_test.sh
+++ b/test/vhost/lvol/lvol_test.sh
@@ -76,7 +76,6 @@ vhosttestinit
 
 spdk_mask=$vhost_0_reactor_mask
 if $distribute_cores; then
-	# FIXME: this need to be handled entirely in common.sh
 	source $testdir/autotest.config
 	# Adjust the mask so vhost runs on separate cpus than qemu instances.
 	# We know that .config sets qemus to run on single cpu so simply take
diff --git a/test/vhost/manual.sh b/test/vhost/manual.sh
index 5cdb8a453..1d6da4732 100755
--- a/test/vhost/manual.sh
+++ b/test/vhost/manual.sh
@@ -5,8 +5,6 @@ rootdir=$(readlink -f $testdir/../..)
 source $rootdir/test/common/autotest_common.sh
 source $rootdir/test/vhost/common.sh
 
-DEFAULT_FIO_BIN="$DEPENDENCY_DIR/fio"
-
 case $1 in
 	-h | --help)
 		echo "usage: $(basename $0) TEST_TYPE"
@@ -35,11 +33,8 @@ if [[ $(uname -s) != Linux ]]; then
 	exit 0
 fi
 
-: ${FIO_BIN="$DEFAULT_FIO_BIN"}
 vhosttestinit
 
-WORKDIR=$(readlink -f $(dirname $0))
-
 case $1 in
 	-hp | --hotplug)
 		echo 'Running hotplug tests suite...'
diff --git a/test/vhost/shared/bdev.json b/test/vhost/shared/bdev.json
index ad28314a5..25945fdcf 100644
--- a/test/vhost/shared/bdev.json
+++ b/test/vhost/shared/bdev.json
@@ -13,6 +13,9 @@
             "name": "VirtioBlk0",
             "trtype": "user"
           }
+        },
+        {
+          "method": "bdev_wait_for_examine"
         }
       ]
     }
diff --git a/test/vhost/vhost.sh b/test/vhost/vhost.sh
index 616d3224a..4d6deabbc 100755
--- a/test/vhost/vhost.sh
+++ b/test/vhost/vhost.sh
@@ -12,13 +12,8 @@ if [[ $(uname -s) != Linux ]]; then
 	exit 0
 fi
 
-DEFAULT_FIO_BIN="$DEPENDENCY_DIR/fio"
-
-: ${FIO_BIN="$DEFAULT_FIO_BIN"}
 vhosttestinit
 
-WORKDIR=$(readlink -f $(dirname $0))
-
 run_test "vhost_negative" $WORKDIR/other/negative.sh
 
 run_test "vhost_boot" $WORKDIR/vhost_boot/vhost_boot.sh --vm_image=$VM_IMAGE
